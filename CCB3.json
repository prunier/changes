{
    "LE2": {
        "PF-EXT": {
            "PF-EXT/ext_validation_tools": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext_st1_ps_tools_sdc-us": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_WCS_TOOLS": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_GaiaCalibrationPipeline": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_GEN_PostValidation_Prep": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_GEN_PostVal_Conf": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_GEN_GlobalValidation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext-pf4": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext_pf2": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/sef_validation_dashboards": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/pf-ext__documentation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_RUBIN": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext_ns_sim_afterburner_old": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_COADD_PPO_BUILDER": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext_ns_sim_afterburner": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext_pf1_gen_preparation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext_testing": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_GEN_PostValidation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_STACK_PSF": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_Utils": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext_pf1_gen_p2": {
                "start date": "2023-03-13T16:11:43",
                "end date": "2023-07-04T12:48:58",
                "start tag": "0.78.0",
                "end tag": "0.9.0",
                "count_files_modified": "87",
                "modifications_by_file": {
                    ".jenkinsFile": [
                        [
                            "@@ -1,4 +1,4 @@\n #!groovy\n @Library('integration-library@release-10') _\n-pipelineElements(name:\"EXT_PF1_GEN_P2\", component:'eden.3.1')\n+pipelineElements(name:\"EXT_PF1_GEN_P2\", component:'eden.3.1', skipQuality:true)\n \n",
                            "git checkout -b release-1.0",
                            "Michael",
                            "2023-07-04T12:48:58.000+02:00",
                            "54920cde02ff106bf71c488bbb654236896be915"
                        ],
                        [
                            "@@ -1,4 +1,4 @@\n #!groovy\n-@Library('integration-library@release-9') _\n-pipelineElements(artifactId:\"EXT_PF1_GEN_P2\", component:'eden.3.0', repository:'eden.3.0')\n+@Library('integration-library@release-10') _\n+pipelineElements(name:\"EXT_PF1_GEN_P2\", component:'eden.3.1')\n \n",
                            "to DM 9.2, Elements 6.2.1",
                            "Michael",
                            "2023-06-28T16:26:50.000+02:00",
                            "e8206b3871feb81926035984b090be9ab9453af8"
                        ],
                        [
                            "@@ -1,4 +1,4 @@\n #!groovy\n-@Library('integration-library@release-8') _\n-pipelineElements(artifactId:\"EXT_PF1_GEN_P2\", component:'eden.2.1', repository:'eden.2.1')\n+@Library('integration-library@release-9') _\n+pipelineElements(artifactId:\"EXT_PF1_GEN_P2\", component:'eden.3.0', repository:'eden.3.0')\n \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "CMakeLists.txt": [
                        [
                            "@@ -6,14 +6,14 @@ find_package(ElementsProject)\n #---------------------------------------------------------------\n \n # Declare project name and version\n-elements_project(EXT_PF1_GEN_P2 0.9 USE Elements 6.2.1 \n+elements_project(EXT_PF1_GEN_P2 0.9.0 USE Elements 6.2.1 \n                                         ST_DataModel 9.2.0 \n                                         ST_DataModelTools 9.2.1 \n-                                        EL_PSFExModel 10.2 \n-                                        EXT_PF1_GEN_P2_LIBS 0.9 \n-                                        CT_Swarp_cpp 10.1 \n-                                        EL_Background 10.1 \n-                                        EXT_STACK_PSF 1.10 \n-                                        EXT_WCS_TOOLS 0.3 \n-                                        SourceXtractorPlusPlus 0.2)\n+                                        EL_PSFExModel 10.2.0 \n+                                        EXT_PF1_GEN_P2_LIBS 0.9.0 \n+                                        CT_Swarp_cpp 10.1.0 \n+                                        EL_Background 10.1.0 \n+                                        EXT_STACK_PSF 1.10.0 \n+                                        EXT_WCS_TOOLS 0.3.0 \n+                                        SourceXtractorPlusPlus 0.19.4)\n \n",
                            "git checkout -b release-1.0",
                            "Michael",
                            "2023-07-04T12:48:58.000+02:00",
                            "54920cde02ff106bf71c488bbb654236896be915"
                        ],
                        [
                            "@@ -6,5 +6,14 @@ find_package(ElementsProject)\n #---------------------------------------------------------------\n \n # Declare project name and version\n-elements_project(EXT_PF1_GEN_P2 0.81 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 EXT_WCS_TOOLS 0.2) #SourceXtractorPlusPlus 0.20\n+elements_project(EXT_PF1_GEN_P2 0.9 USE Elements 6.2.1 \n+                                        ST_DataModel 9.2.0 \n+                                        ST_DataModelTools 9.2.1 \n+                                        EL_PSFExModel 10.2 \n+                                        EXT_PF1_GEN_P2_LIBS 0.9 \n+                                        CT_Swarp_cpp 10.1 \n+                                        EL_Background 10.1 \n+                                        EXT_STACK_PSF 1.10 \n+                                        EXT_WCS_TOOLS 0.3 \n+                                        SourceXtractorPlusPlus 0.2)\n \n",
                            "to DM 9.2, Elements 6.2.1",
                            "Michael",
                            "2023-06-28T16:26:50.000+02:00",
                            "e8206b3871feb81926035984b090be9ab9453af8"
                        ],
                        [
                            "@@ -6,5 +6,5 @@ find_package(ElementsProject)\n #---------------------------------------------------------------\n \n # Declare project name and version\n-elements_project(EXT_PF1_GEN_P2 0.65 USE Elements 5.12.0 ST_DataModelBindings 8.0.5 ST_DataModelTools 8.0.5 EL_PSFExModel 8.2 EXT_PF1_GEN_P2_LIBS 0.64 CT_Swarp_cpp 8.3 EXT_STACK_PSF 1.1)\n+elements_project(EXT_PF1_GEN_P2 0.81 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 EXT_WCS_TOOLS 0.2) #SourceXtractorPlusPlus 0.20\n \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -6,5 +6,5 @@ find_package(ElementsProject)\n #---------------------------------------------------------------\n \n # Declare project name and version\n-elements_project(EXT_PF1_GEN_P2 0.81 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 EXT_WCS_TOOLS 0.2)\n+elements_project(EXT_PF1_GEN_P2 0.81 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 EXT_WCS_TOOLS 0.2) #SourceXtractorPlusPlus 0.20\n \n",
                            "added ram size measurement of internal objects",
                            "Michael",
                            "2023-06-18T08:33:10.000+02:00",
                            "ab98163cb8463b56f9670451604ab4a1154d3ad7"
                        ],
                        [
                            "@@ -6,5 +6,5 @@ find_package(ElementsProject)\n #---------------------------------------------------------------\n \n # Declare project name and version\n-elements_project(EXT_PF1_GEN_P2 0.81 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 SourceXtractorPlusPlus 0.20 EXT_WCS_TOOLS 0.2)\n+elements_project(EXT_PF1_GEN_P2 0.81 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 EXT_WCS_TOOLS 0.2)\n \n",
                            "throw out SExtractor_plus_plus",
                            "Michael",
                            "2023-06-16T11:40:51.000+02:00",
                            "0306dc63083205d245b714e0792f6f4d750a603a"
                        ],
                        [
                            "@@ -6,5 +6,5 @@ find_package(ElementsProject)\n #---------------------------------------------------------------\n \n # Declare project name and version\n-elements_project(EXT_PF1_GEN_P2 0.80 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 SourceXtractorPlusPlus 0.20 EXT_WCS_TOOLS 0.2)\n+elements_project(EXT_PF1_GEN_P2 0.81 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 SourceXtractorPlusPlus 0.20 EXT_WCS_TOOLS 0.2)\n \n",
                            "first attempt with memory improvement for stackpsf_v3",
                            "Michael",
                            "2023-06-09T09:51:34.000+02:00",
                            "7b8ba31d2dafe5229f54217bca17ba2a7b32bd71"
                        ],
                        [
                            "@@ -6,5 +6,5 @@ find_package(ElementsProject)\n #---------------------------------------------------------------\n \n # Declare project name and version\n-elements_project(EXT_PF1_GEN_P2 0.79 USE Elements 6.0.1 ST_DataModel 9.0.3 ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1 EXT_PF1_GEN_P2_LIBS 0.79 CT_Swarp_cpp 9.0 EL_Background 9.0 EXT_STACK_PSF 1.8 SourceXtractorPlusPlus 0.18 EXT_WCS_TOOLS 0.1)\n+elements_project(EXT_PF1_GEN_P2 0.80 USE Elements 6.1.1 ST_DataModel 9.1.5 ST_DataModelTools 9.1.2 EL_PSFExModel 10.1 EXT_PF1_GEN_P2_LIBS 0.80 CT_Swarp_cpp 10.0 EL_Background 10.0 EXT_STACK_PSF 1.9 SourceXtractorPlusPlus 0.20 EXT_WCS_TOOLS 0.2)\n \n",
                            "update to DM 9.1.5",
                            "Michael",
                            "2023-03-20T17:50:13.000+01:00",
                            "212424335002e366458e456f1de475c2507f8922"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/README.md": [
                        [
                            "@@ -1,4 +1,4 @@\n-This file provides a human an explanation of what is inside the *.properties files. \n+This file provides a human an explanation of what is inside the *.properties files in this directory first, followed below it what is inside the *.xml files. \n \n _badpixels = ${badpixels} \n \n@@ -157,3 +157,54 @@ vis_position_cat =\n wall_mod = 3000 EXPLAINED IN \n \n xml_props = ${archive_root}/../${block}_${module}_${job_id}.properties\n+\n+EXPLANATION WHAT IS INSIDE THE *XML files\n+\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n+ xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n+    <Header>\n+        <ProductId>DpdExtConfigurationSet_GPC_v1_2021-03-19T09:00:00</ProductId>\n+        <ProductType>DpdExtConfigurationSet</ProductType>\n+        <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n+        <ProdSDC>SDC-DE</ProdSDC>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n+        <Purpose>DATA_RELEASE</Purpose>\n+        <PlanId>PlanId0</PlanId>\n+        <PPOId>PPOId0</PPOId>\n+        <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n+        <PpoStatus>COMPLETED</PpoStatus>\n+        <ManualValidationStatus>VALID</ManualValidationStatus>\n+        <Curator>Curator0</Curator>\n+        <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n+    </Header>\n+    <Data>\n+        <ConfigurationFile>\n+            <FileContainer filestatus=\"PROPOSED\">\n+                <FileName>coadd_template_GPC.properties</FileName>\n+            </FileContainer>\n+        </ConfigurationFile>\n+        <Survey>Panstarrs</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n+    </Data>\n+</co:DpdExtConfigurationSet>\n+\n",
                            "Add documentation for a human. ",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T14:34:40.000+00:00",
                            "7db9c6b0e3ebf1d19708d66c31962fc6caf1bd57"
                        ],
                        [
                            "@@ -3,80 +3,157 @@ This file provides a human an explanation of what is inside the *.properties fil\n _badpixels = ${badpixels} \n \n _combinetype = ${combinetype}\n+\n _extraweightmap = ${extraweightmap}\n+\n _keepnorm = ${keepnorm}\n+\n _project = ${project}\n+\n _zeropoint = \n-allocated_memory = ${max_memory}\n+\n+allocated_memory = ${max_memory} EXPLAINED IN\n+\n archive_node = ${target_node}\n+\n archive_root = /home/user/host/euclid-ial/workspace/coadd/data\n+\n args = --ramdisk ${ramdisk} --xml ${xml_props} --scriptname ${scriptname} --pipeline_class ${pipeline_class} --binpath ${euclid_home}/bin ${fscaleastrovar} --verbose ${verbose}\n+\n badpixels = -usebadpixels\n+\n band = all\n+\n Bandpass_PANSTARRS_i = 7535.640665609784 \n+\n binpath = ${euclid_home}/bin\n-block = rdisk_coadd_decam\n+\n+block = rdisk_coadd_decam EXPLAINED IN \n+\n ccd_end = 63\n+\n ccd_start = 1\n+\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n+\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+\n coadd_zeropoint = 30.0\n+\n combinetype = WEIGHTED\n+\n debug = 1\n+\n detect_bands = PANSTARRS_i\n+\n dummy = ${detect_bands}\n+\n euclid_conf = ${euclid_home}/conf\n+\n euclid_home = /home/user/euclid_local_home\n+\n euclid_prereq = /usr/local/\n+\n exclude_list = \n+\n exec = ${euclid_home}/python/pybin/PipelineExec.py\n+\n extraweightmap = -extraweightmap\n+\n fscaleastrovar = -fscaleastrovariable\n+\n ingestion = \n+\n installpaths = -etcpath ${euclid_home}/etc -binpath ${euclid_home}/bin -terapixpath ${euclid_prereq}/bin -outxmlpath ${outxmlpath}\n+\n installpaths_nonhomog = -etcpath ${euclid_home}/etc -binpath ${euclid_home}/bin -terapixpath ${euclid_prereq}/bin\n+\n job_id = 0001\n+\n keepnorm = 1\n+\n log = ${block}_${module}_${job_id}.log\n+\n log_dir = ${archive_root}/${run_dir}/log\n+\n logger = ${log_dir}/${log}\n+\n mask_indices =  1, 2, 4, 6, 10, 11, 12, 15\n+\n move_mask_indices_to_weight = 0, 3, 5, 7, 8, 9, 13, 14\n+\n move_mask_indices_to_new_indices = \n-max_memory = 62000\n+\n+max_memory = 62000 EXPLAINED IN \n+\n mkdirs = ${outpath}\n-module = rdisk_coadd_decam\n-multiprocesses = 4 EXPLANATION is in https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Number-of-processes-used-internally\n+\n+module = rdisk_coadd_decam EXPLAINED IN \n+\n+multiprocesses = 4 EXPLAINED in https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Number-of-processes-used-internally\n+\n nite = 20131228\n+\n outauxpath = ${archive_root}/${run_dir}/aux\n+\n outpath = ${archive_root}/${run_dir}/coadd\n+\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n+\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n+\n outxmlpath = ${archive_root}/${run_dir}/xml\n+\n pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n+\n pipeline_cleanup = 1\n+\n project = PYD\n+\n pybinpath = ${euclid_home}/python/pybin\n+\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n-ramdisk = ${archive_root}/EuclidPipeline EXPLAINED https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Location-of-temporal-storage\n+\n+ramdisk = ${archive_root}/EuclidPipeline EXPLAINED IN https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Location-of-temporal-storage\n+\n Reference_Catalog_Calibration_PANSTARRS_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+\n run_dir = rd\n+\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+\n save_backgrounds_for_validation = 0\n+\n savefpacked = 0\n+\n scriptname = rdisk_coadd_stage2_pipeline.pscript\n+\n se_catalog_x = XWIN_IMAGE\n+\n se_catalog_y = YWIN_IMAGE\n+\n stopcondition = \n+\n stackpsf_version = 1\n+\n subtract_delivered_background = 0\n+\n subtract_bg_with_swarp = 1\n-swarp_default = ${euclid_home}/conf/etc/default.swarp\n-swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n-target_node = alexandria\n+\n+swarp_default = ${euclid_home}/conf/etc/default.swarp EXPLAINED IN\n+\n+swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp EXPLAINED IN\n+\n+target_node = alexandria EXPLAINED IN\n+\n tile = ${tilename}\n+\n tilename = PYD345.000200+0.000140\n+\n verbose = 3\n+\n vis_position_cat = \n-wall_mod = 3000\n+\n+wall_mod = 3000 EXPLAINED IN \n+\n xml_props = ${archive_root}/../${block}_${module}_${job_id}.properties\n",
                            "Add documentation for a human. ",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T14:28:16.000+00:00",
                            "f4033dd0d5f2ebc415cee9dc712ec5bbf12d295e"
                        ],
                        [
                            "@@ -1,5 +1,7 @@\n This file provides a human an explanation of what is inside the *.properties files. \n+\n _badpixels = ${badpixels} \n+\n _combinetype = ${combinetype}\n _extraweightmap = ${extraweightmap}\n _keepnorm = ${keepnorm}\n",
                            "Update README.md",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T14:18:58.000+00:00",
                            "595ef759ffcc7e488cd15708f14d78de54575e8b"
                        ],
                        [
                            "@@ -0,0 +1,80 @@\n+This file provides a human an explanation of what is inside the *.properties files. \n+_badpixels = ${badpixels} \n+_combinetype = ${combinetype}\n+_extraweightmap = ${extraweightmap}\n+_keepnorm = ${keepnorm}\n+_project = ${project}\n+_zeropoint = \n+allocated_memory = ${max_memory}\n+archive_node = ${target_node}\n+archive_root = /home/user/host/euclid-ial/workspace/coadd/data\n+args = --ramdisk ${ramdisk} --xml ${xml_props} --scriptname ${scriptname} --pipeline_class ${pipeline_class} --binpath ${euclid_home}/bin ${fscaleastrovar} --verbose ${verbose}\n+badpixels = -usebadpixels\n+band = all\n+Bandpass_PANSTARRS_i = 7535.640665609784 \n+binpath = ${euclid_home}/bin\n+block = rdisk_coadd_decam\n+ccd_end = 63\n+ccd_start = 1\n+coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n+coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n+combinetype = WEIGHTED\n+debug = 1\n+detect_bands = PANSTARRS_i\n+dummy = ${detect_bands}\n+euclid_conf = ${euclid_home}/conf\n+euclid_home = /home/user/euclid_local_home\n+euclid_prereq = /usr/local/\n+exclude_list = \n+exec = ${euclid_home}/python/pybin/PipelineExec.py\n+extraweightmap = -extraweightmap\n+fscaleastrovar = -fscaleastrovariable\n+ingestion = \n+installpaths = -etcpath ${euclid_home}/etc -binpath ${euclid_home}/bin -terapixpath ${euclid_prereq}/bin -outxmlpath ${outxmlpath}\n+installpaths_nonhomog = -etcpath ${euclid_home}/etc -binpath ${euclid_home}/bin -terapixpath ${euclid_prereq}/bin\n+job_id = 0001\n+keepnorm = 1\n+log = ${block}_${module}_${job_id}.log\n+log_dir = ${archive_root}/${run_dir}/log\n+logger = ${log_dir}/${log}\n+mask_indices =  1, 2, 4, 6, 10, 11, 12, 15\n+move_mask_indices_to_weight = 0, 3, 5, 7, 8, 9, 13, 14\n+move_mask_indices_to_new_indices = \n+max_memory = 62000\n+mkdirs = ${outpath}\n+module = rdisk_coadd_decam\n+multiprocesses = 4 EXPLANATION is in https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Number-of-processes-used-internally\n+nite = 20131228\n+outauxpath = ${archive_root}/${run_dir}/aux\n+outpath = ${archive_root}/${run_dir}/coadd\n+outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n+outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n+outxmlpath = ${archive_root}/${run_dir}/xml\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n+pipeline_cleanup = 1\n+project = PYD\n+pybinpath = ${euclid_home}/python/pybin\n+qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n+ramdisk = ${archive_root}/EuclidPipeline EXPLAINED https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Location-of-temporal-storage\n+Reference_Catalog_Calibration_PANSTARRS_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+run_dir = rd\n+runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n+savefpacked = 0\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n+se_catalog_x = XWIN_IMAGE\n+se_catalog_y = YWIN_IMAGE\n+stopcondition = \n+stackpsf_version = 1\n+subtract_delivered_background = 0\n+subtract_bg_with_swarp = 1\n+swarp_default = ${euclid_home}/conf/etc/default.swarp\n+swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n+target_node = alexandria\n+tile = ${tilename}\n+tilename = PYD345.000200+0.000140\n+verbose = 3\n+vis_position_cat = \n+wall_mod = 3000\n+xml_props = ${archive_root}/../${block}_${module}_${job_id}.properties\n",
                            "Add documentation for a human. ",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T14:18:36.000+00:00",
                            "ce94b012c31b6e2e31ee5137c8cddff578142a8b"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/CMakeLists.txt": [
                        [
                            "@@ -10,7 +10,7 @@ find_package(Cfitsio)\n find_package(PythonLibs)\n \n #in dependency EXT_DES_PIPELINE\n-elements_add_library(EXT_PF1_psfex_pywrapper src/lib/psfex_pywrapper.cpp\n+elements_add_library(_EXT_PF1_psfex_pywrapper src/lib/psfex_pywrapper.cpp\n                     LINK_LIBRARIES PSFExModelModule CCfits PythonLibs\n                     INCLUDE_DIRS PythonLibs\n                     NO_PUBLIC_HEADERS)\n",
                            "to DM 9.2, Elements 6.2.1",
                            "Michael",
                            "2023-06-28T16:26:50.000+02:00",
                            "e8206b3871feb81926035984b090be9ab9453af8"
                        ],
                        [
                            "@@ -18,11 +18,26 @@ elements_add_library(EXT_PF1_psfex_pywrapper src/lib/psfex_pywrapper.cpp\n # Instruction for Python module installation\n elements_install_python_modules()\n \n+install(DIRECTORY python/EXT_PF1_GEN_P2/Pipeline/\n+        DESTINATION python/EXT_PF1_GEN_P2/Pipeline\n+        FILE_PERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ\n+               GROUP_EXECUTE GROUP_READ WORLD_EXECUTE WORLD_READ\n+        FILES_MATCHING\n+        PATTERN \"*.pscript\")\n+        \n # Install the configuration files\n elements_install_conf_files()\n \n #add python executables\n-elements_add_python_program(EXT_PF1_Coadd EXT_PF1_Coadd.pybin.IAL_Pipeline_Assembler_coadd_stage2)\n-elements_add_python_program(EXT_PF1_StackPsfExec EXT_PF1_Coadd.pybin.StackPsfExec)\n-elements_add_python_program(EXT_PF1_Fwhm_Val EXT_PF1_Coadd.Fwhm.FwhmValidation)\n+elements_add_python_program(EXT_PF1_Coadd EXT_PF1_GEN_P2.pybin.IAL_Pipeline_Assembler_coadd_stage2)\n+elements_add_python_program(EXT_PF1_StackPsfExec EXT_PF1_GEN_P2.pybin.StackPsfExec)\n+\n+#deactivated psf stackset related codes for a test\n+#are integrated as method calls\n+#elements_add_python_program(EXT_PF1_Fwhm_Val EXT_PF1_GEN_P2.Fwhm.FwhmValidation)\n+#elements_add_python_program(EXT_PF1_Catalog_PSF EXT_PF1_GEN_P2.Fwhm.CatalogPsfExtractor)\n+#elements_add_python_program(EXT_PF1_Catalog_Matcher EXT_PF1_GEN_P2.Fwhm.CatalogMatchFinder)\n+#elements_add_python_program(EXT_PF1_Debugger EXT_PF1_GEN_P2.Fwhm.StackPsfAndCatalogDebugger)\n+#elements_add_python_program(EXT_PF1_CatalogPsfTreatment EXT_PF1_GEN_P2.Fwhm.CatalogPsfTreatment)\n+\n \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/file/PSFUtils.py": [
                        [
                            "@@ -1,7 +1,7 @@\n import numpy as np\n import os\n import fitsio\n-import libEXT_PF1_psfex_pywrapper as psf\n+import lib_EXT_PF1_psfex_pywrapper as psf\n from EXT_PF1_GEN_P2_LIBS.Pipeline.FileOps import FileOps as fo\n \n __author__='Thomas Vassallo'\n",
                            "to DM 9.2, Elements 6.2.1",
                            "Michael",
                            "2023-06-28T16:26:50.000+02:00",
                            "e8206b3871feb81926035984b090be9ab9453af8"
                        ],
                        [
                            "@@ -17,6 +17,7 @@ class PSFUtils():\n         self.name=filename\n         self.closed=True\n         self.header = None\n+        self.open()\n \n     def get_header(self, extension = -1):\n         \"\"\"\n@@ -33,7 +34,7 @@ class PSFUtils():\n         try:\n             setattr(self,key,self.get_header()[key])            \n         except:\n-            self.reportEvent(\"Key %s not found.\"%key,1)\n+            print(\"Key \", key, \" not found.\")\n             setattr(self,key,None)\n \n     def get_convolution_params(self):\n@@ -51,10 +52,11 @@ class PSFUtils():\n         Get a small number indicating the machine precision limit for\n         the FITS tform.\n         \"\"\"\n-        try:\n-            return np.finfo(self.tformToNumpyType(self.getTform(field))).eps\n-        except:\n-            return 0.\n+        #deprecated and unused\n+        #try:\n+        #    return np.finfo(self.tformToNumpyType(self.getTform(field))).eps\n+        #except:\n+        return 0.\n \n     def file_exists(self, inputfile):\n         \"\"\"\n@@ -66,6 +68,7 @@ class PSFUtils():\n             self.name=inputfile\n             return 0            \n         else:\n+            #message to log\n             return '%s not found;  test failed\\n\\n'%inputfile\n \n     def fileHasHDUs(self):        \n@@ -74,11 +77,10 @@ class PSFUtils():\n         \"\"\"        \n         self.open()\n         self.hdu_num = len(self.obj)\n+        self.close()\n         if self.hdu_num>0:\n-            self.close()\n             return 0\n         else:\n-            self.close()\n             return 'file %s has no hdus'%self.name\n     \n     def open(self):\n@@ -112,4 +114,8 @@ class PSFUtils():\n         value = psf.py_psfEvaluateAll(self.name, xlist = xp, ylist = yp)\n         return np.array( value )\n     #\n+    \n+    def cleanup(self):\n+        psf.py_cleanup()\n+    #\n \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/src/lib/psfex_pywrapper.cpp": [
                        [
                            "@@ -22,7 +22,7 @@ static void py_cleanup(PyObject* self);\n #define NULL nullptr\n \n //static PyMethodDef ExtensionMethods[];\n-PyMODINIT_FUNC init_libEXT_PF1_psfex_pywrapper(void);\n+PyMODINIT_FUNC init_lib_EXT_PF1_psfex_pywrapper(void);\n \n static PsfExMasking *get_PsfExMasking(string psf_filename)\n {\n@@ -236,7 +236,7 @@ static int EXT_PF1_psfex_pywrapper_clear(PyObject *m) {\n \n static struct PyModuleDef moduledef = {\n         PyModuleDef_HEAD_INIT,\n-        \"libEXT_PF1_psfex_pywrapper\",\n+        \"lib_EXT_PF1_psfex_pywrapper\",\n         NULL,\n         sizeof(struct module_state),\n         ExtensionMethods,\n@@ -247,14 +247,14 @@ static struct PyModuleDef moduledef = {\n };\n \n \n-PyMODINIT_FUNC PyInit_libEXT_PF1_psfex_pywrapper(void) {\n+PyMODINIT_FUNC PyInit_lib_EXT_PF1_psfex_pywrapper(void) {\n     PyObject *module = PyModule_Create(&moduledef);\n     \n     if (module == NULL)\n         INITERROR;\n     struct module_state *st = GETSTATE(module);\n \n-    st->error = PyErr_NewException(\"libEXT_PF1_psfex_pywrapper.Error\", NULL, NULL);\n+    st->error = PyErr_NewException(\"lib_EXT_PF1_psfex_pywrapper.Error\", NULL, NULL);\n     if (st->error == NULL) {\n         Py_DECREF(module);\n         INITERROR;\n",
                            "to DM 9.2, Elements 6.2.1",
                            "Michael",
                            "2023-06-28T16:26:50.000+02:00",
                            "e8206b3871feb81926035984b090be9ab9453af8"
                        ],
                        [
                            "@@ -7,21 +7,56 @@\n #include <Python.h> //this needs to be the first include!\n #include \"PSFExModelModule/PsfExMasking.h\"\n #include <ctime>\n-//interface\n+#include <map>\n \n \n+//interface\n+static PsfExMasking *get_PsfExMasking(string psf_filename);\n static PyObject *py_psfEvaluate(PyObject* self, PyObject* args);\n static PyObject *py_psfEvaluateAll(PyObject* self, PyObject* args, PyObject *kwdict);\n static PyObject *py_getConvolutionParams(PyObject* self, PyObject* args);\n+static std::map<string, PsfExMasking*> psfex_objects;\n+static void py_cleanup(PyObject* self);\n+\n+//help sonar to understand the obvious\n+#define NULL nullptr\n \n //static PyMethodDef ExtensionMethods[];\n PyMODINIT_FUNC init_libEXT_PF1_psfex_pywrapper(void);\n \n+static PsfExMasking *get_PsfExMasking(string psf_filename)\n+{\n+    if ( psfex_objects.count(psf_filename) == 0){\n+        PsfExMasking *myPSF = new PsfExMasking(psf_filename);\n+        psfex_objects.insert(std::pair<std::string, PsfExMasking*>(psf_filename, myPSF));\n+        //std::cout << \"add new PsfExMasking \" << psf_filename << \"\\n\";\n+    }\n+    /*else{\n+        std::cout << \"reuse existing PsfExMasking \" << psf_filename << \"\\n\";\n+    }*/\n+    std::map<string, PsfExMasking*>::iterator it;\n+    it = psfex_objects.find(psf_filename);\n+    return it->second;\n+}\n+\n+\n+static void py_cleanup(PyObject* self)\n+{\n+    /*\n+    delete the PsfExMasking objects stored in \n+    the static map. Delete also the map object.\n+    */\n+    for (std::map<string, PsfExMasking*>::iterator it=psfex_objects.begin(); it!=psfex_objects.end(); ++it){\n+        delete it->second;\n+    }\n+    delete &psfex_objects;\n+}\n+\n \n static PyObject *py_psfEvaluate(PyObject* self, PyObject* args)\n {\n \n-  char *psfName;\n+  const char *psfName;\n   psfName = PyUnicode_AsUTF8(PyTuple_GetItem(args,0));\n   //printf(\"psfName = %s \\n\", psfName);\n   \n@@ -48,14 +83,14 @@ static PyObject *py_psfEvaluate(PyObject* self, PyObject* args)\n       PyList_SET_ITEM(pyValues,j, PyFloat_FromDouble((double)values[j]) );\n   }\n   \n-  delete myPSF;  \n-  myPSF=NULL;\n-  \n   /*\n   printf(\"pylist check %d \\n\", PyList_CheckExact(pyValues));\n   printf(\"pylist size %d \\n\", PyList_Size(pyValues));\n   */          \n   \n+  delete myPSF;\n+  myPSF=NULL;\n+  \n   return pyValues;\n }\n \n@@ -81,7 +116,10 @@ static PyObject *py_psfEvaluateAll(PyObject* self, PyObject* args, PyObject *kwd\n   \n   int nval[2];\n   int size;\n-  PsfExMasking *myPSF = new PsfExMasking(psfName);\n+  //MW make this a factory call and reuse\n+  //PsfExMasking objects\n+  //PsfExMasking *myPSF = new PsfExMasking(psfName);\n+  PsfExMasking *myPSF = get_PsfExMasking(psfName);\n   nval[0]      = myPSF->getPsfNx();\n   nval[1]      = myPSF->getPsfNy();\n   size= nval[0]* nval[1];\n@@ -97,8 +135,9 @@ static PyObject *py_psfEvaluateAll(PyObject* self, PyObject* args, PyObject *kwd\n   PyObject *allStamps;\n   allStamps=PyList_New(n);\n   \n-  //clock_t start, end, sum;\n-  //sum = 0;\n+  clock_t start, end, sum, sum_py;\n+  sum = 0;\n+  sum_py = 0;\n   \n   for (i=0; i<n; i++) {\n     xItem = PyList_GetItem(xList, i);\n@@ -109,10 +148,11 @@ static PyObject *py_psfEvaluateAll(PyObject* self, PyObject* args, PyObject *kwd\n     \n     float values[size];\n     \n-    //start = clock();\n+    start = clock();\n     myPSF->getPsfDataAt(float(xpos), float(ypos), values);\n-    //sum += clock() - start;\n+    sum += clock() - start;\n     \n+    start = clock();\n     PyObject * pyValues;\n     pyValues=PyList_New(size);\n     int j;\n@@ -120,11 +160,17 @@ static PyObject *py_psfEvaluateAll(PyObject* self, PyObject* args, PyObject *kwd\n         PyList_SET_ITEM(pyValues,j, PyFloat_FromDouble((double)values[j]) );\n     }\n     PyList_SET_ITEM(allStamps,i, pyValues );\n+    sum_py += clock() - start;\n   }\n-  //printf (\"Time elapsed: %d clicks (%f seconds).\\n\",sum,((float)sum)/CLOCKS_PER_SEC);\n-  delete myPSF;  \n-  myPSF=NULL;\n+  //printf(\"Time elapsed for psf simulation: %d clicks (%f seconds).\\n\",sum,((float)sum)/CLOCKS_PER_SEC);\n+  //printf(\"Time elapsed for py-interface handling: %d clicks (%f seconds).\\n\",sum_py,((float)sum_py)/CLOCKS_PER_SEC);\n   \n+  /*\n+  Don't delete them anymore as the objects are reused\n+  We need a destructor for the stored objects as this is all static\n+  //delete myPSF;  \n+  //myPSF=NULL;\n+  */\n   \n   return allStamps;\n }\n@@ -132,7 +178,7 @@ static PyObject *py_psfEvaluateAll(PyObject* self, PyObject* args, PyObject *kwd\n \n static PyObject *py_getConvolutionParams(PyObject* self, PyObject* args)\n {\n-  char *psfName;\n+  const char *psfName;\n   int pytuple_size;\n   \n   pytuple_size = PyTuple_Size(args);\n@@ -164,6 +210,9 @@ static PyMethodDef ExtensionMethods[] = {\n  \n   {\"py_getConvolutionParams\",  (PyCFunction)py_getConvolutionParams, METH_VARARGS,\n    \"Returns shape of the convolution kernel.\"},\n+   \n+  {\"py_cleanup\",  (PyCFunction)py_cleanup, METH_VARARGS,\n+   \"Deletes stored static variables.\"},\n   \n   {NULL, NULL, 0, NULL}     /* Sentinel */\n };\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestOutputStackedPSF_test.py": [
                        [
                            "@@ -11,13 +11,6 @@ from astropy.wcs import WCS\n import numpy as np\n import shutil\n \n-class obj:\n-    def __init__(self, header):\n-        self.header = header\n-    def returnHeader(self):\n-        return self.header\n-    #\n-#\n \n def best_match(value, vlist):\n     return vlist[ np.abs((np.array(vlist) - value)).argmin() ]\n@@ -27,7 +20,9 @@ class TestOutputStackedPSF(unittest.TestCase):\n     def setUp(self):\n         self.stampsize = 25\n         self.wd = os.getcwd()\n-        self.osp = OutputStackedPSF.OutputStackedPSF(3, self.stampsize, self.wd, \"tilename\", \"g_band\")\n+        self.logfile = \"./log.txt\"\n+        self._logger = open(self.logfile, 'w')\n+        self.osp = OutputStackedPSF.OutputStackedPSF(3, self.stampsize, self.wd, \"tilename\", \"g_band\", self._logger)\n         \n         #wcs from header\n         header = {}\n@@ -60,16 +55,17 @@ class TestOutputStackedPSF(unittest.TestCase):\n         self.dec = world[1]\n     def tearDown(self):\n         os.remove(self.coadd)\n+        self._logger.close()\n+        os.remove(self.logfile)\n     def test_add_layer_objects_stamps_to_grid_image(self):\n         self.assertTrue(1)\n     def test_add_layer_objects_to_info_table(self):\n         layer_objects = {'0': [0,1], '1': [2]}\n         \n         mag = np.array([18,19,20])\n-        coadd = obj(self.header)\n         bandpass_value = 4000\n-        self.osp.add_layer_objects_to_info_table('0', layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n-        self.osp.add_layer_objects_to_info_table('1', layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n+        self.osp.add_layer_objects_to_info_table('0', layer_objects, self.ra, self.dec, mag, self.coadd, bandpass_value)\n+        self.osp.add_layer_objects_to_info_table('1', layer_objects, self.ra, self.dec, mag, self.coadd, bandpass_value)\n         \n         #sufficient, if it does not crash \n         #the real test is in populate\n@@ -92,10 +88,8 @@ class TestOutputStackedPSF(unittest.TestCase):\n         os.makedirs(layer1, exist_ok=True)\n         shutil.copy(self.coadd, layer0)\n         shutil.copy(self.coadd, layer1)\n-        \n-        coadd = obj(self.header)\n-        \n-        self.osp.populate(layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n+                \n+        self.osp.populate(layer_objects, self.ra, self.dec, mag, self.coadd, bandpass_value)\n         \n         print(\"self.osp.info_table \", self.osp.info_table)\n         \n@@ -132,10 +126,8 @@ class TestOutputStackedPSF(unittest.TestCase):\n         os.makedirs(layer1, exist_ok=True)\n         shutil.copy(self.coadd, layer0)\n         shutil.copy(self.coadd, layer1)\n-        \n-        coadd = obj(self.header)\n-        \n-        self.osp.populate(layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n+                \n+        self.osp.populate(layer_objects, self.ra, self.dec, mag, self.coadd, bandpass_value)\n         \n         psfname = self.wd +os.sep +\"testpsfstack.fits\"\n         self.osp.write_to_fits(psfname)\n@@ -153,12 +145,11 @@ class TestOutputStackedPSF(unittest.TestCase):\n         shutil.rmtree(layer1)\n         os.remove(psfname)\n     def test_getPixels(self):\n-        o = obj(self.header)\n         wcs = WCS(Header(self.header))\n         x= 40.0\n         y = 266.7\n         skycoords = wcs.all_pix2world(x, y, 1)\n-        X,Y = self.osp.getPixels(o, skycoords)\n+        X,Y = self.osp.getPixels(self.coadd, skycoords)\n         \n         #we need X,Y in array coordinates starting with 0\n         #fits wcs works 1 based\n",
                            "to DM 9.2, Elements 6.2.1",
                            "Michael",
                            "2023-06-28T16:26:50.000+02:00",
                            "e8206b3871feb81926035984b090be9ab9453af8"
                        ],
                        [
                            "@@ -0,0 +1,171 @@\n+import unittest\n+import os\n+\n+from EXT_PF1_GEN_P2.psf import OutputStackedPSF\n+from EXT_PF1_GEN_P2.psf import WcsUtils\n+\n+from astropy.io.fits import Header, PrimaryHDU, ImageHDU, HDUList\n+from astropy.table import Table, Column\n+from astropy.wcs import WCS\n+\n+import numpy as np\n+import shutil\n+\n+class obj:\n+    def __init__(self, header):\n+        self.header = header\n+    def returnHeader(self):\n+        return self.header\n+    #\n+#\n+\n+def best_match(value, vlist):\n+    return vlist[ np.abs((np.array(vlist) - value)).argmin() ]\n+#\n+        \n+class TestOutputStackedPSF(unittest.TestCase):\n+    def setUp(self):\n+        self.stampsize = 25\n+        self.wd = os.getcwd()\n+        self.osp = OutputStackedPSF.OutputStackedPSF(3, self.stampsize, self.wd, \"tilename\", \"g_band\")\n+        \n+        #wcs from header\n+        header = {}\n+        header[\"NAXiS\"] = 2\n+        header[\"NAXIS1\"] = 1000\n+        header[\"NAXIS2\"] = 1000\n+        header[\"CRPIX1\"] = 0.0\n+        header[\"CRPIX2\"] = 0.0\n+        header[\"CRVAL1\"] = 0.0\n+        header[\"CRVAL2\"] = 0.0\n+        #1 arcsec per pixel\n+        header[\"CD1_1\"] = 2.6603E-06\n+        header[\"CD1_2\"] = 7.10338E-05\n+        header[\"CD2_1\"] = 7.10338E-05\n+        header[\"CD2_2\"] = -2.6603E-06\n+        self.header = header\n+        \n+        #coadd file\n+        data = np.zeros((1000, 1000))\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = data, header = Header(header), name = \"IMAGE\")])\n+        self.coadd = self.wd +os.sep +\"tilename_g_band.image.fits\"\n+        hdul.writeto(self.coadd, overwrite=True)\n+        \n+        #coords\n+        self.x = [10., 55., 870.]\n+        self.y = [230., 45., 87.]\n+        wcsu = WcsUtils.WcsUtils(header)\n+        world = wcsu.py_pix2world(self.x, self.y)\n+        self.ra = world[0]\n+        self.dec = world[1]\n+    def tearDown(self):\n+        os.remove(self.coadd)\n+    def test_add_layer_objects_stamps_to_grid_image(self):\n+        self.assertTrue(1)\n+    def test_add_layer_objects_to_info_table(self):\n+        layer_objects = {'0': [0,1], '1': [2]}\n+        \n+        mag = np.array([18,19,20])\n+        coadd = obj(self.header)\n+        bandpass_value = 4000\n+        self.osp.add_layer_objects_to_info_table('0', layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n+        self.osp.add_layer_objects_to_info_table('1', layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n+        \n+        #sufficient, if it does not crash \n+        #the real test is in populate\n+        self.assertTrue(1)\n+    def test_add_stamps_to_output_image(self):\n+        self.assertTrue(1)\n+    def test_get_stamp_size(self):\n+        self.assertTrue(self.osp.get_stamp_size() == self.stampsize)\n+    def test_populate(self):\n+        layer_objects = {'0': [0,1], '1': [2]}\n+        \n+        mag = np.array([18,19,20])\n+        bandpass_value = 4000\n+        \n+        #2 layers require folders layer0 and layer1\n+        layer0 = self.wd + os.sep +\"layer0\"\n+        layer1 = self.wd + os.sep +\"layer1\"\n+        \n+        os.makedirs(layer0, exist_ok=True)\n+        os.makedirs(layer1, exist_ok=True)\n+        shutil.copy(self.coadd, layer0)\n+        shutil.copy(self.coadd, layer1)\n+        \n+        coadd = obj(self.header)\n+        \n+        self.osp.populate(layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n+        \n+        print(\"self.osp.info_table \", self.osp.info_table)\n+        \n+        #info table contains a list of data\n+        self.assertTrue(1)\n+        for tupl in self.osp.info_table:\n+            self.assertTrue(abs( tupl[0] - best_match(tupl[0], self.ra) ) < 1e-5) \n+            self.assertTrue(abs( tupl[1] - best_match(tupl[1], self.dec) ) < 1e-5) \n+            \n+            #tuple 2 and 3 are 0 based x,y values\n+            #to compare to wcs 1-based x,y add 1\n+            self.assertTrue(abs( tupl[2] +1  - best_match(tupl[2] +1, self.x) ) < 1e-5) \n+            self.assertTrue(abs( tupl[3] +1 - best_match(tupl[3] +1, self.y) ) < 1e-5) \n+            self.assertTrue(tupl[6] == bandpass_value)\n+            \n+        #\n+        shutil.rmtree(layer0)\n+        shutil.rmtree(layer1)\n+        \n+    def test_set_stamp_size(self):\n+        self.osp.set_stamp_size(37)\n+        self.assertTrue(self.osp.get_stamp_size() == 37)\n+    def test_write_to_fits(self):\n+        layer_objects = {'0': [0,1], '1': [2]}\n+        \n+        mag = np.array([18,19,20])\n+        bandpass_value = 4000\n+        \n+        #2 layers require folders layer0 and layer1\n+        layer0 = self.wd + os.sep +\"layer0\"\n+        layer1 = self.wd + os.sep +\"layer1\"\n+        \n+        os.makedirs(layer0, exist_ok=True)\n+        os.makedirs(layer1, exist_ok=True)\n+        shutil.copy(self.coadd, layer0)\n+        shutil.copy(self.coadd, layer1)\n+        \n+        coadd = obj(self.header)\n+        \n+        self.osp.populate(layer_objects, self.ra, self.dec, mag, coadd, bandpass_value)\n+        \n+        psfname = self.wd +os.sep +\"testpsfstack.fits\"\n+        self.osp.write_to_fits(psfname)\n+        \n+        info = Table.read(psfname)\n+        \n+        for row in range( len(info[\"RA\"])):\n+            self.assertTrue(abs( info[\"RA\"][row] - best_match(info[\"RA\"][row], self.ra) ) < 1e-5) \n+            self.assertTrue(abs( info[\"DEC\"][row] - best_match(info[\"DEC\"][row], self.dec) ) < 1e-5) \n+            self.assertTrue(abs( info[\"X\"][row] +1 - best_match(info[\"X\"][row] +1, self.x) ) < 1e-5) \n+            self.assertTrue(abs( info[\"Y\"][row] +1 - best_match(info[\"Y\"][row] +1, self.y) ) < 1e-5) \n+            self.assertTrue(info[\"BANDPASS\"][row] == bandpass_value)\n+        \n+        shutil.rmtree(layer0)\n+        shutil.rmtree(layer1)\n+        os.remove(psfname)\n+    def test_getPixels(self):\n+        o = obj(self.header)\n+        wcs = WCS(Header(self.header))\n+        x= 40.0\n+        y = 266.7\n+        skycoords = wcs.all_pix2world(x, y, 1)\n+        X,Y = self.osp.getPixels(o, skycoords)\n+        \n+        #we need X,Y in array coordinates starting with 0\n+        #fits wcs works 1 based\n+        #so the values need to be corrected by 1\n+        \n+        self.assertTrue(abs(X + 1 - x) < 1e-5 and abs(Y +1 -y) < 1e-5)\n+    def test_prepare_sky_coordinates(self):\n+        ra = [1,3,5]\n+        dec = [2,4,6]\n+        self.assertTrue(self.osp.prepare_sky_coordinates(ra, dec) == [1,2,3,4,5,6])\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestStackPSF_test.py": [
                        [
                            "@@ -89,6 +89,7 @@ class TestStackPSF(unittest.TestCase):\n             keep_edge_obj = 0\n             min_mag = 12.0\n             ref_mag = 31.0\n+            multiprocesses = 2\n         #\n         \n         self.spf = stackPSF.StackPSF(args)\n",
                            "to DM 9.2, Elements 6.2.1",
                            "Michael",
                            "2023-06-28T16:26:50.000+02:00",
                            "e8206b3871feb81926035984b090be9ab9453af8"
                        ],
                        [
                            "@@ -14,22 +14,52 @@ You should have received a copy of the GNU Lesser General Public License along w\n the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n \n \"\"\"\n-import Pipeline\n-print(\"imported Pipeline module = \", Pipeline)\n-from EXT_PF1_Coadd.Pipeline import stackPSF\n+from EXT_PF1_GEN_P2 import Pipeline\n+from EXT_PF1_GEN_P2.psf import stackPSF, WcsUtils, ObjectList\n from EXT_PF1_GEN_P2_LIBS.astronomy import ArrayUtils\n \n+import fitsio\n from astropy.io import fits\n+from astropy.io.fits import HDUList, ImageHDU, BinTableHDU, PrimaryHDU, Header\n+from astropy.table import Table, Column\n from astropy.wcs import WCS\n import numpy as np\n import math\n import unittest\n import os\n+import shutil\n from matplotlib import pyplot as plt\n \n+def best_match(value, vlist):\n+    return vlist[ np.abs((np.array(vlist) - value)).argmin() ]\n+#\n \n class TestStackPSF(unittest.TestCase):\n     def setUp(self):\n+        #wcs from header\n+        header = {}\n+        header[\"NAXIS\"] = 2\n+        header[\"NAXIS1\"] = 1000\n+        header[\"NAXIS2\"] = 1000\n+        header[\"CRPIX1\"] = 0.0\n+        header[\"CRPIX2\"] = 0.0\n+        header[\"CRVAL1\"] = 0.0\n+        header[\"CRVAL2\"] = 0.0\n+        header[\"CTYPE1\"] = \"RA---TAN\"\n+        header[\"CTYPE2\"] = \"DEC--TAN\"\n+        #1.6 arcsec per pixel\n+        header[\"CD1_1\"] = 2.6603E-06\n+        header[\"CD1_2\"] = 7.10338E-05\n+        header[\"CD2_1\"] = 7.10338E-05\n+        header[\"CD2_2\"] = -2.6603E-06\n+\n+        self.header = header\n+        \n+        #simulate a coadd for the test\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.zeros((1000,1000)), name = \"IMAGE\", header = Header(self.header))])\n+        self.coaddname = os.getcwd() +os.sep +\"coadd_init.fits\"\n+        hdul.writeto(self.coaddname, overwrite = True)\n+        \n         #fake a class with the necessary values\n         class args:\n             def __init__(self):\n@@ -39,6 +69,7 @@ class TestStackPSF(unittest.TestCase):\n             band = \"g\"\n             outdir = os.getcwd()\n             ramdisk = \"/dev/shm/Euclid_Pipeline/\"\n+            resampledir = \"./\"\n             ra = \"X_WORLD\"\n             dec = \"Y_WORLD\"\n             mag_entry = \"MAG_AUTO\"\n@@ -49,47 +80,57 @@ class TestStackPSF(unittest.TestCase):\n             coadd_cat = None\n             max_layer_num = 4\n             sort_catalog_by_mag = 1\n-            coadd = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/rd/nhcoadd/TILE_45716_g.fits\"\n-            SElist = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/coaddswarp.list\"\n-            position_cat = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/data/merged_mer_catalog.fits\"\n+            max_separation_for_match = 0.52\n+            coadd = self.coaddname\n+            #\"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/rd/nhcoadd/TILE_45716_g.fits\"\n+            #SElist = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/coaddswarp.list\"\n+            #position_cat = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/data/merged_mer_catalog.fits\"\n             normalize_psfs = 0\n+            keep_edge_obj = 0\n+            min_mag = 12.0\n+            ref_mag = 31.0\n         #\n         \n         self.spf = stackPSF.StackPSF(args)\n+        \n     def tearDown(self):\n-        pass\n-    def test_add_PSF_stamp_to_image(self):\n+        if os.path.exists(self.spf.outdir):\n+            shutil.rmtree(self.spf.outdir)\n+        if os.path.exists(self.coaddname):\n+            os.remove(self.coaddname)\n+    def removed_add_PSF_stamp_to_image(self):\n         \"\"\"\n         add_PSF_stamp_to_image(self, data, gains, objects, object_index, stamp, psfnaxis):\n-        \n+\n         idea:\n         prepare a psf stamp filled with a centered gaussian. Do that for odd (45,45) and even (44,44) stamp sizes\n         Use stackpsf to place the stamps in a image at irregular positons, so that interpolation is necessary.\n-        \n+\n         Fit a guassian to the result and measure its location and fwhm. Compare with the original value.\n         Allow not more than 0.001 pixel deviation for success.\n-        \n+\n         Measure the sum of the values of the stamp and the image, compare.\n         Allow deviations of 0.01 percent (1exp-4)\n-        \n+\n         \"\"\"\n-        objects = stackPSF.ObjectList([25.0, 25.0], [10.5, 11.5], [-75.0, -74.0], [200.2, 1000.7], [600.05, 453.8]) #mag, ra,dec,X, Y\n+        objects = stackPSF.ObjectList([25.0, 25.0], [25.8, 24.9], [10.5, 11.5], [-75.0, -74.0], [200.2, 1000.7], [600.05, 453.8]) #mag, zp, ra,dec,X, Y\n         #object_index = 1\n         #psfnaxis = [45,45]\n         fwhm = 2.25\n-        \n-        for psfnaxis in [[44,44],[45,45]]:\n+\n+        for psfnaxis in [[45,45], [23,23]]:\n+            print(\"psfnaxis \", psfnaxis)\n             for object_index in [0,1]:\n                 data = np.zeros(shape=(4096,2048), dtype=float, order='F')\n                 gains = np.ones((4096,2048), dtype = np.float64)\n \n-                au = ArrayUtils.ArrayUtils() \n+                au = ArrayUtils.ArrayUtils()\n                 pixels = au.get_array_indices(np.empty(psfnaxis))\n-        \n+\n                 parameters = {\n                     \"amplitude\": 1,\n-                    \"center_row\": psfnaxis[0]/2.0 -0.5,\n-                    \"center_col\": psfnaxis[1]/2.0 -0.5, #0.5 offset to get the simulation really centered\n+                    \"center_row\": (psfnaxis[0] - 1)/2.0,\n+                    \"center_col\": (psfnaxis[1] - 1)/2.0, #0.5 offset to get the simulation really centered\n                     \"sigma_row\": fwhm,                   # as the simulation is 0 based (unlike fits)\n                     \"sigma_col\": fwhm,\n                     \"angle\": 0,\n@@ -97,45 +138,52 @@ class TestStackPSF(unittest.TestCase):\n                 }\n                 stamp =  au.gauss_2d(pixels, **parameters).reshape(psfnaxis)\n                 results = au.fit_gauss_2d(stamp)\n-                                \n+\n                 #is the stamp fit correct?\n-                self.assertTrue( abs(results[1][1] - (psfnaxis[0]/2.0 -0.5)) < 0.001) #center\n-                self.assertTrue( abs(results[1][0] - (psfnaxis[1]/2.0 -0.5)) < 0.001)\n-        \n+                self.assertTrue( abs(results[1][1] - (psfnaxis[0] - 1)/2.0) < 0.001) #center\n+                self.assertTrue( abs(results[1][0] - (psfnaxis[1] - 1)/2.0) < 0.001)\n+\n                 self.assertTrue( abs(results[2][1] - fwhm) < 0.001) #width 10exp-3 of a pixel\n                 self.assertTrue( abs(results[2][0] - fwhm) < 0.001)\n-        \n-        \n+\n                 d,g, shifted_stamp = self.spf.add_PSF_stamp_to_image(data, gains, objects, object_index, stamp, psfnaxis)\n-                \n-                  \n+\n+\n                 #put the data into a fits file\n+                #for manual inspection\n+                \"\"\"\n                 hdus = fits.PrimaryHDU(stamp)\n                 hdui = fits.ImageHDU(d)\n                 hduss = fits.ImageHDU(shifted_stamp)\n-                \n+\n                 hdul = fits.HDUList([hdus, hdui, hduss])\n                 hdul.writeto('./psf_' +str(psfnaxis[0]) +'.fits', overwrite = 1)\n-        \n+                \"\"\"\n+                \n+                \n                 #now fit data for gaussian and measure its location\n                 results = au.fit_gauss_2d(d)\n \n                 #is the stamp in the image fit correct?\n-                print(results)\n-                self.assertTrue( abs(results[1][1] - objects.X[object_index]) < 0.001) #center\n-                self.assertTrue( abs(results[1][0] - objects.Y[object_index]) < 0.001)\n-        \n-                self.assertTrue( abs(results[2][1] - fwhm) < 0.001) #width 10exp-3 of a pixel\n-                self.assertTrue( abs(results[2][0] - fwhm) < 0.001)\n+                #print(\"fitresults \", results, \", obj coords \", objects.Y[object_index], objects.X[object_index])\n                 \n+                #a centered psf is placed as if the coordinate is rounded\n+                #shifting is done by PsfExModel\n+                self.assertTrue( abs(results[1][1] - round(objects.X[object_index]) ) < 0.1) #center\n+                self.assertTrue( abs(results[1][0] - round(objects.Y[object_index]) ) < 0.1)\n+\n+                self.assertTrue( abs(results[2][1] - fwhm) < 0.1) #width 10exp-3 of a pixel\n+                self.assertTrue( abs(results[2][0] - fwhm) < 0.1)\n+\n                 #values\n                 #print(\"flux error \", abs(stamp.sum() - d.sum()/results[0])/stamp.sum())\n-                self.assertTrue( abs(stamp.sum() - d.sum()/results[0])/stamp.sum() < 0.0001 )   \n+                self.assertTrue( abs(stamp.sum() - d.sum()/results[0])/stamp.sum() < 0.0001 )\n+                \n             #\n-        #    \n-                 \n+        #\n+\n     #\n-    def sleeping_test_take_out_already_assigned_indices(self):\n+    def test_take_out_already_assigned_indices(self):\n         cleaned, taken_out = self.spf.take_out_already_assigned_indices([[1,2,3], [4,5,6,7]], [1,3,5,7])\n         to = set(taken_out)\n         for _l in cleaned:\n@@ -143,88 +191,53 @@ class TestStackPSF(unittest.TestCase):\n             self.assertTrue(len( res)  == 0)\n         #\n     #\n-    def sleeping_test_divide_input_positions_into_layers_bf(self):\n-        inputdata = self.spf.read_input_list()\n-        ra, dec, se_matches = self.spf.read_input_positions()\n-        min_separation, max_sep = self.spf.compute_minimum_separation_for_objects_in_layer()\n-        coadd = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC456/coadd_40012_20190314_des//coadd_40012_20190314_des/nhcoadd/EUC_TILE_00h43m27s_-18d48m00s_g.fits\"\n-        \n-        layer_objects = self.spf.divide_input_positions_into_layers_bf(max_sep, ra, dec, coadd, 4)\n-        #now, make sure, no index is duplicated anywhere\n-        #first, reformat the stupid OrderedDict\n-        lobj = list()\n-        for k in layer_objects.keys():\n-            lobj.append(sorted( layer_objects[k]))\n-        #\n-        from  bisect import bisect_left\n-        def contains(a, x, lo = 0):\n-            'Locate the leftmost value exactly equal to x'\n-            i = bisect_left(a, x, lo = lo)\n-            if i != len(a) and a[i] == x:\n-                return True\n-            return False\n-\n-        for i in range(len(lobj)):\n-            for j in range(len(lobj[i])):\n-                #for each object, check, if it appears elsewhere\n-                #the rest of this layer\n-                self.assertFalse(  contains(lobj[i], lobj[i][j], lo = j+1) )\n-                #the other layers\n-                for k in range(i+1, len(lolobj) ):       \n-                    self.assertFalse(  contains(lobj[k], lobj[i][j], lo = 0) )\n-                #\n-            #\n-        #\n+    def test_divide_input_positions_into_layers_bf(self):\n+        #prepare 2 points for 10 pixel distance\n+        wcs = WCS(self.header)\n+        radec = wcs.all_pix2world([0., 5., 11.], [0., 0., 10.], 1)\n+\n+        #return radec[0], radec[1] #ra, dec\n+        #save wcs\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.zeros((1000,1000)),  header = Header(self.header), name = \"IMAGE\")])\n+        coadd = os.getcwd() +os.sep +\"coadd.fits\"\n+        hdul.writeto(coadd, overwrite=True)\n+\n+        layer_objects = self.spf.divide_input_positions_into_layers_bf(10.0, radec[0], radec[1], coadd, 4)\n+\n+        self.assertTrue(len(layer_objects) == 2)\n+\n+        #object 0 and 2 are on the same layer\n+        self.assertTrue(layer_objects['0'] == [0,2])\n+\n+        #object 1 needs a separate layer as it is too close to object 0\n+        self.assertTrue(layer_objects['1'] == [1])\n+\n+        os.remove(coadd)\n+\n     def test_create_PSF_stamp_single_image(self):\n         self.assertTrue(1)\n     def test_evaluate_PSF_for_pixel(self):\n         self.assertTrue(1)\n-    def test_ObjectList(self):\n-        objl = stackPSF.ObjectList([20.0, 21.0], [10.5, 11.5], [-75.0, -74.0]) #mag, ra,dec\n-        \n-        self.assertTrue(objl.__dict__[\"ra\"] == [10.5, 11.5])\n-        self.assertTrue(objl.__dict__[\"dec\"] == [-75.0, -74.0])\n-        self.assertTrue(objl.__dict__[\"mag\"] == [20.0, 21.0])\n-        self.assertTrue(objl.__dict__[\"_length\"] == 2)\n-        \n-        objl._add_x_list(np.array([50, 52]))\n-        objl._add_y_list(np.array([89, 90]))\n-        self.assertTrue( all( np.array(objl.X) == np.array([50,52]) ) )\n-        self.assertTrue( all( np.array(objl.Y) == np.array([89,90]) ) )\n-        \n-        print(objl.__dict__)\n-        \n-        objl._filter([1]) #indices of the desired objects\n-        self.assertTrue(objl.ra== [11.5])\n-        self.assertTrue(objl.dec == [-74.0])\n-        self.assertTrue(objl.mag == [21.0])\n-        self.assertTrue(objl._length == 1)\n-        \n-        \n     def test_match_layer_positions_with_single_epoch_catalogue_objects(self):\n         #wrapper method\n         self.assertTrue(1)\n     #\n-    def test_project_input_coordinates_into_the_single_image_pixel_space(self):\n-        self.assertTrue(1)\n     def test_filter_out_objects_not_contained_in_the_single_image(self):\n-        class img(object):\n-            def __init__(self):\n-                setattr(self, \"naxis1\", 1024)\n-                setattr(self, \"naxis2\", 2048)\n-            #\n-        #\n         x = [-11, -10, -1, -9, 1,2,10,11,12, 1014, 1015, 1023, 1024, 1025, 1033, 1034, 1035]\n         y = [-11, -8, -1, -9, 1,2,10,11,12, 2038, 2039, 2047, 2048, 2049, 2057, 2058, 2059]\n-        objl = stackPSF.ObjectList(x, X = np.array(x),Y = np.array(y)) \n-        \n-        img = img()\n+        objl = stackPSF.ObjectList(x, X = np.array(x),Y = np.array(y))\n+\n+        img = {\"NAXIS1\":1024, \"NAXIS2\":2048}\n         self.spf.set_stamp_size(10)\n         fobj = self.spf.filter_out_objects_not_contained_in_the_single_image(img, objl)\n+        \n         self.assertTrue( all( np.array(fobj.X) > -10) )\n         self.assertTrue( all( np.array(fobj.X) < 1034) )\n         self.assertTrue( all( np.array(fobj.Y) < 2058) )\n     def test_matchCoordinates(self):\n+        \"\"\"\n+        The code matches according to max distances of 2 pixels\n+        \"\"\"\n         self.spf.pixscale = 1.0 *3600 #in arcseconds\n         ra1 = np.array([20, 10, 8, 2, 5])\n         dec1 = np.array([0, -60, -50, -40, -30])\n@@ -233,61 +246,144 @@ class TestStackPSF(unittest.TestCase):\n         AIdx, BIdx = self.spf.matchCoordinates(ra1, dec1, ra2, dec2)\n         #print(\"aidx \", AIdx)\n         #print(\"bidx \", BIdx)\n-        \n+\n         for i in range(len(AIdx)):\n             d_sq = math.pow(ra1[AIdx[i]] - ra2[BIdx[i]], 2) + math.pow(dec1[AIdx[i]] - dec2[BIdx[i]], 2)\n-            pix_sq = math.pow(2*self.spf.pixscale/3600.0, 2)\n+            pix_sq = math.pow(2*self.spf.pixscale, 2) #/3600.0\n             #print(\"d_sq \", d_sq, \" <= \", \" pix_sq \", pix_sq)\n             self.assertTrue( d_sq <= pix_sq)\n         #\n-    def sleeping_test_stamp_overlaps_image(self):\n+    def test_match_single_epoch_catalogue_objects_with_input_positions(self):\n+        #prepare an object list\n+        #a catalog with x,y, ra,dec, mag_auto, spread_model (acceptance < 0.002 )\n+        #se image for the header\n+\n+        #coords - last outside the 1000 pixel image\n+        x = [10., 55., 870.]\n+        y = [230., 45., 87.]\n+        wcsu = WcsUtils.WcsUtils(self.header)\n+        world = wcsu.py_pix2world(x, y)\n+\n+        #mag, zp, ra,dec\n+        overlapping_positions = ObjectList.ObjectList([20.0, 21.0, 22.0], [25.4, 24.9, 26.3], world[0], world[1])\n+\n+        #make the catalog with 1, 2.5 and 1.5 pixels deviation. Set spreadmodel for 1.5 to 0.5\n+        xt = [11., 55., 871.5]\n+        yt = [229., 47.5, 87.]\n+        world = wcsu.py_pix2world(xt, yt)\n+        spread = [0.001, 0.0, 0.5]\n+        mag_auto = [20.0, 21.0, 22.0]\n+        mag_psf = [20.5, 21.5, 22.5]\n+        flags = [0,0,0]\n+\n+        t = Table()\n+        t.add_column(Column(xt, name = \"XWIN_IMAGE\"))\n+        t.add_column(Column(yt, name = \"YWIN_IMAGE\"))\n+        t.add_column(Column(world[0], name = \"ALPHAWIN_J2000\"))\n+        t.add_column(Column(world[1], name = \"DELTAWIN_J2000\"))\n+        t.add_column(Column(spread, name = \"SPREAD_MODEL\"))\n+        t.add_column(Column(mag_auto, name = \"MAG_AUTO\"))\n+        t.add_column(Column(mag_psf, name = \"MAG_PSF\"))\n+        t.add_column(Column(flags, name = \"FLAGS\"))\n+\n+        catname = os.getcwd() +os.sep +\"cat.fits\"\n+        t.write(catname, overwrite = True)\n+        cat_data = fitsio.FITS(catname)[-1]\n+\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.zeros((1000,1000)), name = \"IMAGE\", header = Header(self.header))])\n+        sename = os.getcwd() +os.sep +\"se.fits\"\n+        hdul.writeto(sename, overwrite = True)\n+\n+        self.spf.pixscale = 3600.0*math.sqrt(math.pow(self.header['CD1_1'], 2) +math.pow(self.header['CD2_1'], 2) )\n+        se_matched_objects, indexes = self.spf.match_single_epoch_catalogue_objects_with_input_positions(overlapping_positions, cat_data, sename, self.header)\n+        #print(\"se_matched_objects, indexes \", se_matched_objects.ra, indexes)\n+        #print(\"self.spf.pixscale \", self.spf.pixscale)\n+\n+        self.assertTrue( se_matched_objects.ra[0] == world[0][0])\n+        self.assertTrue( se_matched_objects.ra[1] == world[0][2]) #one is filtered out\n+        self.assertTrue( se_matched_objects.mag[1] == mag_auto[2]) #galaxy-mag is left mag_auto\n+        self.assertTrue( se_matched_objects.mag[0] == mag_psf[0]) #star-mag is set to mag_psf\n+\n+\n+        os.remove(catname)\n+        os.remove(sename)\n+\n+    def test_stamp_overlaps_image(self):\n         self.spf.stamp_size = 50\n         truth = self.spf.stamp_overlaps_image(10, 10, 1024, 2048)\n         self.assertTrue( truth)\n-        \n+\n         truth = self.spf.stamp_overlaps_image(np.array([10, 1, 5]), np.array([10, -24, -26]), 1024, 2048)\n-        print(\"overlap multiple truth \", truth)\n+        #print(\"overlap multiple truth \", truth)\n         self.assertTrue( truth[0] )\n         self.assertTrue( truth[1] )\n         self.assertFalse( truth[2] )\n-    def sleeping_test_stamp_inside_image(self):\n+    def test_stamp_inside_image(self):\n         self.spf.stamp_size = 50\n         truth = self.spf.stamp_inside_image(10, 10, 1024, 2048)\n         self.assertFalse( truth)\n-        \n+\n         truth = self.spf.stamp_inside_image(np.array([25, 26, 27]), np.array([24, 25, 26]), 1024, 2048)\n-        print(\"inside multiple truth \", truth)\n+\n         self.assertFalse( truth[0] )\n         self.assertFalse( truth[1] )\n         self.assertTrue( truth[2] )\n     def sleeping_test_create_PSF_stamp_single_image(self):\n         #init image: creates an ExtractImage object from the image\n         #gets the zp from the input data\n-        #get_PSF_info: select the mag, ra, dec for all objects of the layer that \n-        #              overlap a single image \n+        #get_PSF_info: select the mag, ra, dec for all objects of the layer that\n+        #              overlap a single image\n         #              return ObjectList\n         #get_PSF_image: get the XY positions of the objects in the single image\n         #               define_maps: prepare ndarrays for image, weight, noise\n         #               read the size of the psf from the psf file header\n         #               then add the psf stamp to the image\n         self.assertTrue(1)\n-    def sleeping_test_get_indexes_of_stamp_on_image(self):\n-        pixel = (50.2,50.7)\n-        stamp_dim = np.array( (37,37) )\n-        print(\"start, end, shiftx, shifty\", self.spf.get_indexes_of_stamp_on_image(pixel, stamp_dim))\n+    def test_get_indexes_of_stamp_on_image_scipy_shift(self):\n+        \"\"\"\n+        Problem: x,y in image space is a float\n+        but the stamp can only be put into the simulated SE image in int coordinates\n+        and the simulated psf is always centered in the stamp\n+        Thus, the stamp coordinates must be shifted by the bit of difference between the float and the int\n+        This is what the method calculates\n+        \"\"\"\n+        pixel = (101.3,101.0)\n+        stamp_dim = np.array( (3,3) )\n+        start, end, shift = self.spf.get_indexes_of_stamp_on_image_scipy_shift(pixel, stamp_dim)\n+        #print(\"start, end, shift\", self.spf.get_indexes_of_stamp_on_image(pixel, stamp_dim))\n+\n+        #this could do [100,100] [104,104] [-0.2, -0.5] or\n+        #              [99,99] [103,103] [0.8, 05] <- this is done\n+        # what is this? self.assertTrue(start[0] == 99 and start[1] == 99)\n+        self.assertTrue(end[0] == 103 and end[1] == 103)\n+        self.assertTrue(abs(shift[0] - 0.3) < 1e-5 and abs(shift[1]) < 1e-5)\n         \n-        pixel = (50.2,50.7)\n+        pixel = (1.3,1.0)\n         #data_dim = (4096, 2048)\n-        stamp_dim = np.array( (2,2) )\n-        print(\"start, end, shiftx, shifty\", self.spf.get_indexes_of_stamp_on_image(pixel, stamp_dim))\n-        \n-        self.assertTrue(1)\n+        stamp_dim = np.array( (3,3) )\n+        start, end, shift = self.spf.get_indexes_of_stamp_on_image_scipy_shift(pixel, stamp_dim)\n+        #print(\"start, end, shift\", start, end, shift)\n+\n+        self.assertTrue(start[0] == 0 and start[1] == 0)\n+        self.assertTrue(end[0] == 3 and end[1] == 3)\n+        self.assertTrue(abs(shift[0] - 0.3) < 1e-5 and abs(shift[1]) < 1e-5)\n+\n+        pixel = (1.0,0.7)\n+        #data_dim = (4096, 2048)\n+        stamp_dim = np.array( (3,3) )\n+        start, end, shift = self.spf.get_indexes_of_stamp_on_image_scipy_shift(pixel, stamp_dim)\n+        print(\"start, end, shiftx, shifty\", start, end, shift)\n+\n+        self.assertTrue(start[0] == 0 and start[1] == -1)\n+        self.assertTrue(end[0] == 3 and end[1] == 2)\n+        self.assertTrue(abs(shift[0]) < 1e-5 and abs(shift[1] - 0.7) < 1e-5)\n+        self.assertTrue(shift[1] > 0.0)\n     def sleeping_test_get_swarp_lists(self):\n         layerdir = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC456/coadd_40012_20190314_des/EuclidPipeline/coadd_40012_20190314_des/EUC_TILE_00h43m27s_-18d48m00s/g/layer0\"\n         #self.single_epoch_image_list\n         inputdata = self.spf.read_input_list()\n         swarplists, selist, psflist = self.spf.get_swarp_lists(layerdir)\n-        \n+\n         #the test is, if selist and psflist are in perfect sync\n         self.assertTrue(len(selist) == len(psflist))\n         for i in range(len(psflist)):\n@@ -298,16 +394,242 @@ class TestStackPSF(unittest.TestCase):\n             self.assertTrue(sename == psfname)\n         #\n     def sleeping_test_parallel_psf_evaluation(self):\n-        psf_filename = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC456/SWF2/EXT_PF1_GEN_Coadd_2020_01_07_dev-VALIDATION-mwetzste-PLAN-0003-2020-01-08-16-40-18-0-SDC-DE-ATTEMPT-0/data/EUC_EXT_DPDEXTPSF_DECAM-90320051-38-PSFCAT_20191123T135725.8Z_00.00.psf\"    \n+        psf_filename = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC456/SWF2/EXT_PF1_GEN_Coadd_2020_01_07_dev-VALIDATION-mwetzste-PLAN-0003-2020-01-08-16-40-18-0-SDC-DE-ATTEMPT-0/data/EUC_EXT_DPDEXTPSF_DECAM-90320051-38-PSFCAT_20191123T135725.8Z_00.00.psf\"\n         x = [1,2,3,4,5,6,7,8]\n         y = [1,2,3,4,5,6,7,8]\n         x = []\n         y = []\n         stamps = self.spf.evaluate_all_psfs(x,y,psf_filename,2)\n         print(\"len stamps \", len(stamps), \", stamps \", stamps)\n-    #\n-    \n+    def test_project_input_coordinates_into_the_single_image_pixel_space(self):\n+        class obj:\n+            def __init__(self, header):\n+                self.header = header\n+                self.naxis1 = header[\"NAXIS1\"]\n+                self.naxis2 = header[\"NAXIS2\"]\n+            def returnHeader(self):\n+                return self.header\n+        #\n+\n+        #coords - last outside the 1000 pixel image\n+        x = [10., 55., 870., 1028]\n+        y = [230., 45., 87., 20]\n+        \n+        wcsu = WcsUtils.WcsUtils(self.header)\n+        world = wcsu.py_pix2world(x, y)\n+\n+        seimage  = obj(self.header)\n+\n+        #mag, zp, ra,dec\n+        layer_positions = ObjectList.ObjectList([20.0, 21.0, 22.0, 29.1], [25.4, 24.9, 26.3, 25.0], world[0], world[1])\n+        self.spf.set_stamp_size(27)\n+        overlaps = self.spf.project_input_coordinates_into_the_single_image_pixel_space(self.header, layer_positions)\n+\n+        #last is filtered out\n+        self.assertTrue(len(x) == len(overlaps.X) + 1)\n+        for i in range(len(overlaps.X)):\n+            self.assertTrue(abs(x[i] -overlaps.X[i]) < 1e-5)\n+            self.assertTrue(abs(y[i] -overlaps.Y[i]) < 1e-5)\n+        #\n+    def test_get_object_positions_from_single_epoch_catalogue(self):\n+        #make a catalog\n+        x = [10., 55., 870., 1028]\n+        y = [230., 45., 87., 20]\n+        wcsu = WcsUtils.WcsUtils(self.header)\n+        world = wcsu.py_pix2world(x, y)\n+\n+        t = Table()\n+        t.add_column(Column(x, name = \"XWIN_IMAGE\"))\n+        t.add_column(Column(y, name = \"YWIN_IMAGE\"))\n+        t.add_column(Column(world[0], name = \"ALPHAWIN_J2000\"))\n+        t.add_column(Column(world[1], name = \"DELTAWIN_J2000\"))\n+\n+        catname = os.getcwd() +os.sep +\"cat.fits\"\n+        hdul = HDUList([PrimaryHDU(), BinTableHDU(t, name = \"LDAC_OBJECTS\")])\n+        hdul.writeto(catname, overwrite = True)\n+\n+        inputdata = dict()\n+        inputdata[\"sename\"] = dict()\n+        inputdata[\"sename\"][\"psf\"] = \"psf.fits\"\n+        inputdata[\"sename\"][\"cat\"] = catname\n+        self.spf.inputdata = inputdata\n+        cat_table = self.spf.get_object_positions_from_single_epoch_catalogue(\"sename\")\n+        self.assertTrue(\"ALPHAWIN_J2000\" in cat_table.get_colnames())\n+\n+        os.remove(catname)\n+    def test_get_auxiliary_file_names(self):\n+        inputdata = dict()\n+        inputdata[\"sename\"] = dict()\n+        inputdata[\"sename\"][\"psf\"] = \"psf.fits\"\n+        inputdata[\"sename\"][\"cat\"] = \"cat.fits\"\n+\n+        self.spf.inputdata = inputdata\n+        psf, cat = self.spf.get_auxiliary_file_names(\"sename\")\n+        self.assertTrue(cat == \"cat.fits\" and psf == \"psf.fits\")\n+    def test_match_layer_positions_with_single_epoch_catalogue_objects(self):\n+        x = [25., 55., 870., 1028]\n+        y = [230., 45., 87., 20]\n+        spread = [0.001, 0.0, 0.19, 0.2]\n+        mag_auto = [20.0, 21.0, 22.0, 17.3]\n+        mag_psf = [20.5, 21.5, 22.5,17.3]\n+        flags = [0,0,0,0]\n+        wcsu = WcsUtils.WcsUtils(self.header)\n+        world = wcsu.py_pix2world(x, y)\n+        \n+        se_matches = {}\n+        for radec in zip(world[0], world[1]):\n+            se_matches['%.10f:%.10f' % (radec[0], radec[1])] = {}\n+        \n+        t = Table()\n+        t.add_column(Column(x, name = \"XWIN_IMAGE\"))\n+        t.add_column(Column(y, name = \"YWIN_IMAGE\"))\n+        t.add_column(Column(world[0], name = \"ALPHAWIN_J2000\"))\n+        t.add_column(Column(world[1], name = \"DELTAWIN_J2000\"))\n+        t.add_column(Column(spread, name = \"SPREAD_MODEL\"))\n+        t.add_column(Column(mag_auto, name = \"MAG_AUTO\"))\n+        t.add_column(Column(mag_psf, name = \"MAG_PSF\"))\n+        t.add_column(Column(flags, name = \"FLAGS\"))\n+\n+        catname = os.getcwd() +os.sep +\"cat.fits\"\n+        hdul = HDUList([PrimaryHDU(), BinTableHDU(t, name = \"LDAC_OBJECTS\")])\n+        hdul.writeto(catname, overwrite = True)\n+\n+        tref = Table()\n+        tref.add_column(Column(world[0], name = \"X_WORLD\"))\n+        tref.add_column(Column(world[1], name = \"Y_WORLD\"))\n+        tref.add_column(Column(spread, name = \"SPREAD_MODEL\"))\n+        tref.add_column(Column(mag_auto, name = \"MAG_AUTO\"))\n+        tref.add_column(Column(mag_psf, name = \"MAG_PSF\"))\n+        tref.add_column(Column(flags, name = \"FLAGS\"))\n+\n+        refcatname = os.getcwd() +os.sep +\"refcat.fits\"\n+        hdul = HDUList([PrimaryHDU(), BinTableHDU(tref, name = \"LDAC_OBJECTS\")])\n+        hdul.writeto(refcatname, overwrite = True)\n+\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.zeros((1000,1000)), name = \"IMAGE\", header = Header(self.header))])\n+        sename = os.getcwd() +os.sep +\"se.fits\"\n+        hdul.writeto(sename, overwrite = True)\n+\n+        inputdata = dict()\n+        inputdata[sename] = dict()\n+        inputdata[sename][\"psf\"] = \"psf.fits\"\n+        inputdata[sename][\"cat\"] = catname\n+\n+        layer_positions = ObjectList.ObjectList(mag_auto, [25.4, 24.9, 26.3, 25.0], world[0], world[1])\n+\n+        self.spf.inputdata = inputdata\n+        self.spf.stamp_size = 17 #this way we don't need a psf.fits\n+        self.spf.pixscale = 3600.0*math.sqrt(math.pow(self.header['CD1_1'], 2) +math.pow(self.header['CD2_1'], 2) )\n+        self.spf._args.position_cat = refcatname\n+        self.spf._args.coadd = sename\n+        self.spf.read_input_positions() #this must be done in order to initialize se_matches\n+                                        #globals must go in this code!\n+        \n+        se_matches = self.spf.match_layer_positions_with_single_epoch_catalogue_objects(layer_positions, se_matches)\n+        print(\"Se_Matches \", se_matches)\n+        \n+        #we expect 4 matches\n+        self.assertTrue(len(se_matches.keys()) == 4)\n+        for k in se_matches.keys():\n+            try:\n+                values = se_matches[k][sename]\n+\n+                self.assertTrue(values[0] in mag_auto[:-1] or values[0] in mag_psf[:-1] ) #depending on spread model value\n+                self.assertTrue(abs(values[1] - best_match(values[1], world[0][:-1])) < 1e-5 )\n+                self.assertTrue(abs(values[2] - best_match(values[2], world[1][:-1])) < 1e-5 )\n+            except:\n+                #one object is outside of the image\n+                pass\n+        #\n+        os.remove(catname)\n+        os.remove(sename)\n+        os.remove(refcatname)\n+    def test_read_input_positions(self):\n+        x = [25., 55., 870., 1028]\n+        y = [230., 45., 87., 20]\n+        spread = [0.001, 0.0, 0.19, 0.2]\n+        mag_auto = [20.0, 21.0, 22.0, 17.3]\n+        mag_psf = [20.5, 21.5, 22.5,17.3]\n+        flags = [0,0,0,0]\n+        wcsu = WcsUtils.WcsUtils(self.header)\n+        world = wcsu.py_pix2world(x, y)\n+\n+        tref = Table()\n+        tref.add_column(Column(world[0], name = \"X_WORLD\"))\n+        tref.add_column(Column(world[1], name = \"Y_WORLD\"))\n+        tref.add_column(Column(spread, name = \"SPREAD_MODEL\"))\n+        tref.add_column(Column(mag_auto, name = \"MAG_AUTO\"))\n+        tref.add_column(Column(mag_psf, name = \"MAG_PSF\"))\n+        tref.add_column(Column(flags, name = \"FLAGS\"))\n+\n+        refcatname = os.getcwd() +os.sep +\"refcat.fits\"\n+        hdul = HDUList([PrimaryHDU(), BinTableHDU(tref, name = \"LDAC_OBJECTS\")])\n+        hdul.writeto(refcatname, overwrite = True)\n+\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.zeros((1000,1000)), name = \"IMAGE\", header = Header(self.header))])\n+        sename = os.getcwd() +os.sep +\"se.fits\"\n+        hdul.writeto(sename, overwrite = True)\n+\n+        self.spf.stamp_size = 17 #this way we don't need a psf.fits\n+        self.spf.pixscale = 3600.0*math.sqrt(math.pow(self.header['CD1_1'], 2) +math.pow(self.header['CD2_1'], 2) )\n+        self.spf._args.position_cat = refcatname\n+        self.spf._args.coadd = sename\n+        ra, dec, se_matches = self.spf.read_input_positions()\n+\n+        for rectas in ra:\n+            self.assertTrue(abs(rectas - best_match(rectas, world[0]) < 1e-5 ))\n+        os.remove(refcatname)\n+        os.remove(sename)\n+    def test_flux_2_mag(self):\n+        farray = [0.0, 1.0, 10.0, 100.0]\n+        zp = 30.0\n+        mag = self.spf.flux_2_mag(farray, zp)\n+\n+        self.assertTrue(mag[0] > 50)\n+        self.assertTrue(mag[1] == zp - 2.5*math.log10(farray[1]))\n+        self.assertTrue(mag[2] == zp - 2.5*math.log10(farray[2]))\n+        self.assertTrue(mag[3] == zp - 2.5*math.log10(farray[3]))\n+    def test_getCoaddMetadata(self):\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.zeros((1000,1000)), name = \"IMAGE\", header = Header(self.header))])\n+        coaddname = os.getcwd() +os.sep +\"coadd.fits\"\n+        hdul.writeto(coaddname, overwrite = True)\n+\n+        coadd = self.spf.getCoaddMetadata(coaddname)\n+        h = coadd.header\n+        for k in self.header.keys():\n+            self.assertTrue(h[k] == self.header[k])\n+        #\n+        os.remove(coaddname)\n+    def test_init_run(self):\n+        self.spf.init_run()\n+\n+        files = [self.spf.xmldir, self.spf.auxdir, self.spf.resdir, self.spf.logfile, self.spf.outdir ]\n+        for f in files:\n+            self.assertTrue(os.path.exists(f))\n+            if os.path.isdir(f):\n+                shutil.rmtree(f)\n+            else:\n+                os.remove(f)\n+            #\n+        #\n+    def test_add_PSF_stamp_to_simulated_image(self):\n+        data = np.zeros((500, 500))\n+        weight = np.ones((500, 500))\n+        stamp = np.zeros( (49,49) )\n+        stamp[15,15] = 5. \n+        \n+        print(\"SUM data, stamp \", data.sum(), stamp.sum())\n+        \n+        \n+        new_data, new_weight, norm_stamp = self.spf.add_PSF_stamp_to_simulated_image(data, weight, stamp, 20.3, 35.8, 25, 30.0)\n+        \n+        print(\"SUM new_data, data \", new_data.sum() , data.sum())\n+        #self.assertTrue( (new_data - data).sum() > 0.0)\n     \n+    #\n+#\n+\n+\n #\n \n if __name__ == '__main__':\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -69,6 +69,7 @@ class TestStackPSF(unittest.TestCase):\n             band = \"g\"\n             outdir = os.getcwd()\n             ramdisk = \"/dev/shm/Euclid_Pipeline/\"\n+            resampledir = \"./\"\n             ra = \"X_WORLD\"\n             dec = \"Y_WORLD\"\n             mag_entry = \"MAG_AUTO\"\n@@ -611,6 +612,20 @@ class TestStackPSF(unittest.TestCase):\n                 os.remove(f)\n             #\n         #\n+    def test_add_PSF_stamp_to_simulated_image(self):\n+        data = np.zeros((500, 500))\n+        weight = np.ones((500, 500))\n+        stamp = np.zeros( (49,49) )\n+        stamp[15,15] = 5. \n+        \n+        print(\"SUM data, stamp \", data.sum(), stamp.sum())\n+        \n+        \n+        new_data, new_weight, norm_stamp = self.spf.add_PSF_stamp_to_simulated_image(data, weight, stamp, 20.3, 35.8, 25, 30.0)\n+        \n+        print(\"SUM new_data, data \", new_data.sum() , data.sum())\n+        #self.assertTrue( (new_data - data).sum() > 0.0)\n+    \n     #\n #\n \n",
                            "first attempt with memory improvement for stackpsf_v3",
                            "Michael",
                            "2023-06-09T09:51:34.000+02:00",
                            "7b8ba31d2dafe5229f54217bca17ba2a7b32bd71"
                        ],
                        [
                            "@@ -97,7 +97,7 @@ class TestStackPSF(unittest.TestCase):\n             shutil.rmtree(self.spf.outdir)\n         if os.path.exists(self.coaddname):\n             os.remove(self.coaddname)\n-    def test_add_PSF_stamp_to_image(self):\n+    def removed_add_PSF_stamp_to_image(self):\n         \"\"\"\n         add_PSF_stamp_to_image(self, data, gains, objects, object_index, stamp, psfnaxis):\n \n",
                            "fixed a test",
                            "Michael",
                            "2023-05-11T17:37:59.000+02:00",
                            "ab39f36c9eb76a2b55a40b8534dc507d69e37fe2"
                        ],
                        [
                            "@@ -36,6 +36,30 @@ def best_match(value, vlist):\n \n class TestStackPSF(unittest.TestCase):\n     def setUp(self):\n+        #wcs from header\n+        header = {}\n+        header[\"NAXIS\"] = 2\n+        header[\"NAXIS1\"] = 1000\n+        header[\"NAXIS2\"] = 1000\n+        header[\"CRPIX1\"] = 0.0\n+        header[\"CRPIX2\"] = 0.0\n+        header[\"CRVAL1\"] = 0.0\n+        header[\"CRVAL2\"] = 0.0\n+        header[\"CTYPE1\"] = \"RA---TAN\"\n+        header[\"CTYPE2\"] = \"DEC--TAN\"\n+        #1.6 arcsec per pixel\n+        header[\"CD1_1\"] = 2.6603E-06\n+        header[\"CD1_2\"] = 7.10338E-05\n+        header[\"CD2_1\"] = 7.10338E-05\n+        header[\"CD2_2\"] = -2.6603E-06\n+\n+        self.header = header\n+        \n+        #simulate a coadd for the test\n+        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.zeros((1000,1000)), name = \"IMAGE\", header = Header(self.header))])\n+        self.coaddname = os.getcwd() +os.sep +\"coadd_init.fits\"\n+        hdul.writeto(self.coaddname, overwrite = True)\n+        \n         #fake a class with the necessary values\n         class args:\n             def __init__(self):\n@@ -56,38 +80,23 @@ class TestStackPSF(unittest.TestCase):\n             max_layer_num = 4\n             sort_catalog_by_mag = 1\n             max_separation_for_match = 0.52\n-            coadd = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/rd/nhcoadd/TILE_45716_g.fits\"\n-            SElist = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/coaddswarp.list\"\n-            position_cat = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/data/merged_mer_catalog.fits\"\n+            coadd = self.coaddname\n+            #\"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/rd/nhcoadd/TILE_45716_g.fits\"\n+            #SElist = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/coaddswarp.list\"\n+            #position_cat = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/PPO_EXT_PF1_GEN_P2_SC8_GSIR_DES_SWF1_R2_TEST_DECAM_TILE_45716_4/data/merged_mer_catalog.fits\"\n             normalize_psfs = 0\n             keep_edge_obj = 0\n             min_mag = 12.0\n             ref_mag = 31.0\n         #\n-\n+        \n         self.spf = stackPSF.StackPSF(args)\n-\n-        #wcs from header\n-        header = {}\n-        header[\"NAXiS\"] = 2\n-        header[\"NAXIS1\"] = 1000\n-        header[\"NAXIS2\"] = 1000\n-        header[\"CRPIX1\"] = 0.0\n-        header[\"CRPIX2\"] = 0.0\n-        header[\"CRVAL1\"] = 0.0\n-        header[\"CRVAL2\"] = 0.0\n-        header[\"CTYPE1\"] = \"RA---TAN\"\n-        header[\"CTYPE2\"] = \"DEC--TAN\"\n-        #1.6 arcsec per pixel\n-        header[\"CD1_1\"] = 2.6603E-06\n-        header[\"CD1_2\"] = 7.10338E-05\n-        header[\"CD2_1\"] = 7.10338E-05\n-        header[\"CD2_2\"] = -2.6603E-06\n-\n-        self.header = header\n+        \n     def tearDown(self):\n         if os.path.exists(self.spf.outdir):\n             shutil.rmtree(self.spf.outdir)\n+        if os.path.exists(self.coaddname):\n+            os.remove(self.coaddname)\n     def test_add_PSF_stamp_to_image(self):\n         \"\"\"\n         add_PSF_stamp_to_image(self, data, gains, objects, object_index, stamp, psfnaxis):\n@@ -212,22 +221,15 @@ class TestStackPSF(unittest.TestCase):\n         #wrapper method\n         self.assertTrue(1)\n     #\n-    def test_project_input_coordinates_into_the_single_image_pixel_space(self):\n-        self.assertTrue(1)\n     def test_filter_out_objects_not_contained_in_the_single_image(self):\n-        class img(object):\n-            def __init__(self):\n-                setattr(self, \"naxis1\", 1024)\n-                setattr(self, \"naxis2\", 2048)\n-            #\n-        #\n         x = [-11, -10, -1, -9, 1,2,10,11,12, 1014, 1015, 1023, 1024, 1025, 1033, 1034, 1035]\n         y = [-11, -8, -1, -9, 1,2,10,11,12, 2038, 2039, 2047, 2048, 2049, 2057, 2058, 2059]\n         objl = stackPSF.ObjectList(x, X = np.array(x),Y = np.array(y))\n \n-        img = img()\n+        img = {\"NAXIS1\":1024, \"NAXIS2\":2048}\n         self.spf.set_stamp_size(10)\n         fobj = self.spf.filter_out_objects_not_contained_in_the_single_image(img, objl)\n+        \n         self.assertTrue( all( np.array(fobj.X) > -10) )\n         self.assertTrue( all( np.array(fobj.X) < 1034) )\n         self.assertTrue( all( np.array(fobj.Y) < 2058) )\n@@ -292,7 +294,7 @@ class TestStackPSF(unittest.TestCase):\n         hdul.writeto(sename, overwrite = True)\n \n         self.spf.pixscale = 3600.0*math.sqrt(math.pow(self.header['CD1_1'], 2) +math.pow(self.header['CD2_1'], 2) )\n-        se_matched_objects, indexes = self.spf.match_single_epoch_catalogue_objects_with_input_positions(overlapping_positions, cat_data, sename)\n+        se_matched_objects, indexes = self.spf.match_single_epoch_catalogue_objects_with_input_positions(overlapping_positions, cat_data, sename, self.header)\n         #print(\"se_matched_objects, indexes \", se_matched_objects.ra, indexes)\n         #print(\"self.spf.pixscale \", self.spf.pixscale)\n \n@@ -411,6 +413,7 @@ class TestStackPSF(unittest.TestCase):\n         #coords - last outside the 1000 pixel image\n         x = [10., 55., 870., 1028]\n         y = [230., 45., 87., 20]\n+        \n         wcsu = WcsUtils.WcsUtils(self.header)\n         world = wcsu.py_pix2world(x, y)\n \n@@ -419,7 +422,7 @@ class TestStackPSF(unittest.TestCase):\n         #mag, zp, ra,dec\n         layer_positions = ObjectList.ObjectList([20.0, 21.0, 22.0, 29.1], [25.4, 24.9, 26.3, 25.0], world[0], world[1])\n         self.spf.set_stamp_size(27)\n-        overlaps = self.spf.project_input_coordinates_into_the_single_image_pixel_space(seimage, layer_positions)\n+        overlaps = self.spf.project_input_coordinates_into_the_single_image_pixel_space(self.header, layer_positions)\n \n         #last is filtered out\n         self.assertTrue(len(x) == len(overlaps.X) + 1)\n@@ -471,7 +474,11 @@ class TestStackPSF(unittest.TestCase):\n         flags = [0,0,0,0]\n         wcsu = WcsUtils.WcsUtils(self.header)\n         world = wcsu.py_pix2world(x, y)\n-\n+        \n+        se_matches = {}\n+        for radec in zip(world[0], world[1]):\n+            se_matches['%.10f:%.10f' % (radec[0], radec[1])] = {}\n+        \n         t = Table()\n         t.add_column(Column(x, name = \"XWIN_IMAGE\"))\n         t.add_column(Column(y, name = \"YWIN_IMAGE\"))\n@@ -516,17 +523,22 @@ class TestStackPSF(unittest.TestCase):\n         self.spf._args.coadd = sename\n         self.spf.read_input_positions() #this must be done in order to initialize se_matches\n                                         #globals must go in this code!\n-\n-        self.spf.match_layer_positions_with_single_epoch_catalogue_objects(layer_positions)\n-\n-        #we expect 3 matches\n-        self.assertTrue(len(self.spf.se_matches.keys()) == 3)\n-        for k in self.spf.se_matches.keys():\n-            values = self.spf.se_matches[k][sename]\n-\n-            self.assertTrue(values[0] in mag_auto[:-1] or values[0] in mag_psf[:-1] ) #depending on spread model value\n-            self.assertTrue(abs(values[1] - best_match(values[1], world[0][:-1])) < 1e-5 )\n-            self.assertTrue(abs(values[2] - best_match(values[2], world[1][:-1])) < 1e-5 )\n+        \n+        se_matches = self.spf.match_layer_positions_with_single_epoch_catalogue_objects(layer_positions, se_matches)\n+        print(\"Se_Matches \", se_matches)\n+        \n+        #we expect 4 matches\n+        self.assertTrue(len(se_matches.keys()) == 4)\n+        for k in se_matches.keys():\n+            try:\n+                values = se_matches[k][sename]\n+\n+                self.assertTrue(values[0] in mag_auto[:-1] or values[0] in mag_psf[:-1] ) #depending on spread model value\n+                self.assertTrue(abs(values[1] - best_match(values[1], world[0][:-1])) < 1e-5 )\n+                self.assertTrue(abs(values[2] - best_match(values[2], world[1][:-1])) < 1e-5 )\n+            except:\n+                #one object is outside of the image\n+                pass\n         #\n         os.remove(catname)\n         os.remove(sename)\n@@ -567,24 +579,6 @@ class TestStackPSF(unittest.TestCase):\n             self.assertTrue(abs(rectas - best_match(rectas, world[0]) < 1e-5 ))\n         os.remove(refcatname)\n         os.remove(sename)\n-    def test_measure_gains_of_single_images(self):\n-        #make a test single epoch image with image and weight\n-        seimage = os.getcwd() +os.sep +\"se.fits\"\n-        d = np.zeros((1000,1000))\n-        d[0:1000, 0:500] = 1.0\n-        img = ImageHDU(data = d, header = Header(self.header), name = \"IMAGE\")\n-        weight = ImageHDU(data = np.ones((1000,1000)), header = Header(self.header), name = \"WEIGHT\")\n-        hdul = HDUList([PrimaryHDU(), img, weight])\n-        hdul.writeto(seimage, overwrite=True)\n-\n-        self.spf.inputdata = [seimage]\n-        self.spf.measure_gains_of_single_images()\n-\n-        os.remove(seimage)\n-\n-        #print(\"Gains \", self.spf.G)\n-        self.assertTrue(self.spf.G[seimage][\"A\"] == 1.0)\n-        self.assertTrue(self.spf.G[seimage][\"B\"] == 0.0)\n     def test_flux_2_mag(self):\n         farray = [0.0, 1.0, 10.0, 100.0]\n         zp = 30.0\n@@ -605,37 +599,6 @@ class TestStackPSF(unittest.TestCase):\n             self.assertTrue(h[k] == self.header[k])\n         #\n         os.remove(coaddname)\n-    def test_estimate_noise(self):\n-        sename = os.getcwd() +os.sep +\"se_ohne_bg.fits\"\n-        hdul = HDUList([PrimaryHDU(), ImageHDU(data = np.ones((1000,1000)), name = \"IMAGE\", header = Header(self.header))])\n-        hdul.writeto(sename, overwrite = True)\n-\n-        noise, err = self.spf.estimate_noise(sename)\n-        self.assertTrue(noise == 1.0 and err == 0.0)\n-\n-\n-        #with bg hdu\n-        hdul = HDUList([PrimaryHDU(),\n-                        ImageHDU(data = np.ones((1000,1000)), name = \"IMAGE\", header = Header(self.header)),\n-                        ImageHDU(data = np.zeros((1000,1000)), name = \"BACKGROUND\", header = Header(self.header))])\n-        hdul.writeto(sename, overwrite = True)\n-\n-        noise, err = self.spf.estimate_noise(sename)\n-        self.assertTrue(noise == 0.0 and err == 0.0)\n-\n-        #with header keywords\n-        header = self.header\n-        header[\"SKYBRITE\"] = 67\n-        header[\"SKYSIGMA\"] = 0.5\n-        hdul = HDUList([PrimaryHDU(),\n-                        ImageHDU(data = np.ones((1000,1000)), name = \"IMAGE\", header = Header(header)),\n-                        ImageHDU(data = np.zeros((1000,1000)), name = \"BACKGROUND\", header = Header(header))])\n-        hdul.writeto(sename, overwrite = True)\n-\n-        noise, err = self.spf.estimate_noise(sename)\n-        self.assertTrue(noise == 67 and err == 0.5)\n-\n-        os.remove(\"./se_ohne_bg.fits\")\n     def test_init_run(self):\n         self.spf.init_run()\n \n",
                            "tests",
                            "Michael",
                            "2023-03-13T17:35:42.000+01:00",
                            "56fb5cc715f0eb72aa596d3fb4ad442e63ff6ae7"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/README.md": [
                        [
                            "@@ -0,0 +1 @@\n+Note: the configuration files are all in https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2. The files in this directory have 0 bytes. \n",
                            "Merge branch 'gvk/StackedFrame' into 'gvk/StackedFrameProductionTour'",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T13:58:39.000+00:00",
                            "9cafeb230197a1a8c5b466fa6b0c5f71a45e2556"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2.conf": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/DpdExtConfigurationSet_DECam.xml": [
                        [
                            "@@ -1,43 +1,36 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<co:DpdExtConfigurationSet xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:cat=\"http://euclid.esa.org/schema/bas/cat\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:co=\"http://euclid.esa.org/schema/dpd/ext/configurationset\"\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/git/ST_DataModel_dev/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtConfigurationSet.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n     <Header>\n         <ProductId>DpdExtConfigurationSet_DECAM_v1_2021-01-16T09:00:00</ProductId>\n         <ProductType>DpdExtConfigurationSet</ProductType>\n         <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n-        <SoftwareRelease>0.5</SoftwareRelease>\n-        <EuclidPipelineSoftwareRelease>Euclid</EuclidPipelineSoftwareRelease>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n         <ProdSDC>SDC-DE</ProdSDC>\n-        <DataSetRelease>0.5</DataSetRelease>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n         <PlanId>PlanId0</PlanId>\n         <PPOId>PPOId0</PPOId>\n         <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PipelineRun>PipelineRun0</PipelineRun>\n-        <ExitStatusCode>ExitStatusCode0</ExitStatusCode>\n+        <PpoStatus>COMPLETED</PpoStatus>\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <ExpirationDate>2040-11-24T11:07:19.072000Z</ExpirationDate>\n-        <ToBePublished>false</ToBePublished>\n-        <Published>false</Published>\n         <Curator>Curator0</Curator>\n         <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n     </Header>\n@@ -47,12 +40,8 @@\n                 <FileName>coadd_template_DECAM.properties</FileName>\n             </FileContainer>\n         </ConfigurationFile>\n+        <Survey>DES</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n     </Data>\n-    <Parameters>\n-        <Parameter>\n-            <Key>Instrument</Key>\n-            <Description>The Instrument for the property file</Description>\n-            <StringValue>DECAM</StringValue>\n-        </Parameter>\n-    </Parameters>\n </co:DpdExtConfigurationSet>\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -1,42 +1,36 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<co:DpdExtConfigurationSet xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:cat=\"http://euclid.esa.org/schema/bas/cat\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:co=\"http://euclid.esa.org/schema/dpd/ext/configurationset\"\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/git/ST_DataModel_dev/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtConfigurationSet.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n     <Header>\n         <ProductId>DpdExtConfigurationSet_DECAM_v1_2021-01-16T09:00:00</ProductId>\n         <ProductType>DpdExtConfigurationSet</ProductType>\n         <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n-        <SoftwareRelease>0.5</SoftwareRelease>\n-        <EuclidPipelineSoftwareRelease>Euclid</EuclidPipelineSoftwareRelease>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n         <ProdSDC>SDC-DE</ProdSDC>\n-        <DataSetRelease>0.5</DataSetRelease>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n         <PlanId>PlanId0</PlanId>\n         <PPOId>PPOId0</PPOId>\n         <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n         <PpoStatus>COMPLETED</PpoStatus>\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <ExpirationDate>2040-11-24T11:07:19.072000Z</ExpirationDate>\n-        <ToBePublished>false</ToBePublished>\n-        <Published>false</Published>\n         <Curator>Curator0</Curator>\n         <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n     </Header>\n@@ -46,12 +40,8 @@\n                 <FileName>coadd_template_DECAM.properties</FileName>\n             </FileContainer>\n         </ConfigurationFile>\n+        <Survey>DES</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n     </Data>\n-    <Parameters>\n-        <Parameter>\n-            <Key>Instrument</Key>\n-            <Description>The Instrument for the property file</Description>\n-            <StringValue>DECAM</StringValue>\n-        </Parameter>\n-    </Parameters>\n </co:DpdExtConfigurationSet>\n",
                            "update to DM 9.1.5",
                            "Michael",
                            "2023-03-20T17:50:13.000+01:00",
                            "212424335002e366458e456f1de475c2507f8922"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/DpdExtConfigurationSet_GPC.xml": [
                        [
                            "@@ -1,43 +1,36 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<co:DpdExtConfigurationSet xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:cat=\"http://euclid.esa.org/schema/bas/cat\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:co=\"http://euclid.esa.org/schema/dpd/ext/configurationset\"\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/git/ST_DataModel_dev/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtConfigurationSet.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n     <Header>\n         <ProductId>DpdExtConfigurationSet_GPC_v1_2021-03-19T09:00:00</ProductId>\n         <ProductType>DpdExtConfigurationSet</ProductType>\n         <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n-        <SoftwareRelease>0.5</SoftwareRelease>\n-        <EuclidPipelineSoftwareRelease>Euclid</EuclidPipelineSoftwareRelease>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n         <ProdSDC>SDC-DE</ProdSDC>\n-        <DataSetRelease>0.5</DataSetRelease>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n         <PlanId>PlanId0</PlanId>\n         <PPOId>PPOId0</PPOId>\n         <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PipelineRun>PipelineRun0</PipelineRun>\n-        <ExitStatusCode>ExitStatusCode0</ExitStatusCode>\n+        <PpoStatus>COMPLETED</PpoStatus>\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <ExpirationDate>2040-11-24T11:07:19.072000Z</ExpirationDate>\n-        <ToBePublished>false</ToBePublished>\n-        <Published>false</Published>\n         <Curator>Curator0</Curator>\n         <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n     </Header>\n@@ -47,12 +40,8 @@\n                 <FileName>coadd_template_GPC.properties</FileName>\n             </FileContainer>\n         </ConfigurationFile>\n+        <Survey>Panstarrs</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n     </Data>\n-    <Parameters>\n-        <Parameter>\n-            <Key>Instrument</Key>\n-            <Description>The Instrument for the property file</Description>\n-            <StringValue>GPC</StringValue>\n-        </Parameter>\n-    </Parameters>\n </co:DpdExtConfigurationSet>\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/DpdExtConfigurationSet_HSC.xml": [
                        [
                            "@@ -1,45 +1,38 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<co:DpdExtConfigurationSet xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:cat=\"http://euclid.esa.org/schema/bas/cat\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:co=\"http://euclid.esa.org/schema/dpd/ext/configurationset\"\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/git/ST_DataModel_dev/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtConfigurationSet.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n     <Header>\n         <ProductId>DpdExtConfigurationSet_HSC_v1_2021-03-18T09:00:00</ProductId>\n         <ProductType>DpdExtConfigurationSet</ProductType>\n         <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n-        <SoftwareRelease>0.60</SoftwareRelease>\n-        <EuclidPipelineSoftwareRelease>Euclid</EuclidPipelineSoftwareRelease>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n         <ProdSDC>SDC-DE</ProdSDC>\n-        <DataSetRelease>0.60</DataSetRelease>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n         <PlanId>PlanId0</PlanId>\n         <PPOId>PPOId0</PPOId>\n         <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PipelineRun>PipelineRun0</PipelineRun>\n-        <ExitStatusCode>ExitStatusCode0</ExitStatusCode>\n+        <PpoStatus>COMPLETED</PpoStatus>\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <ExpirationDate>2040-11-24T11:07:19.072000Z</ExpirationDate>\n-        <ToBePublished>false</ToBePublished>\n-        <Published>false</Published>\n         <Curator>Curator0</Curator>\n-        <CreationDate>2021-03-18T09:00:00</CreationDate>\n+        <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n     </Header>\n     <Data>\n         <ConfigurationFile>\n@@ -47,12 +40,8 @@\n                 <FileName>coadd_template_HSC.properties</FileName>\n             </FileContainer>\n         </ConfigurationFile>\n+        <Survey>HSC WHIGS WISHES</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n     </Data>\n-    <Parameters>\n-        <Parameter>\n-            <Key>Instrument</Key>\n-            <Description>The Instrument for the property file</Description>\n-            <StringValue>HSC</StringValue>\n-        </Parameter>\n-    </Parameters>\n </co:DpdExtConfigurationSet>\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/DpdExtConfigurationSet_JPCAM.xml": [
                        [
                            "@@ -1,43 +1,36 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<co:DpdExtConfigurationSet xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:cat=\"http://euclid.esa.org/schema/bas/cat\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:co=\"http://euclid.esa.org/schema/dpd/ext/configurationset\"\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/git/ST_DataModel_dev/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtConfigurationSet.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n     <Header>\n         <ProductId>DpdExtConfigurationSet_JPCAM_v1_2021-03-19T09:00:00</ProductId>\n         <ProductType>DpdExtConfigurationSet</ProductType>\n         <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n-        <SoftwareRelease>0.5</SoftwareRelease>\n-        <EuclidPipelineSoftwareRelease>Euclid</EuclidPipelineSoftwareRelease>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n         <ProdSDC>SDC-DE</ProdSDC>\n-        <DataSetRelease>0.5</DataSetRelease>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n         <PlanId>PlanId0</PlanId>\n         <PPOId>PPOId0</PPOId>\n         <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PipelineRun>PipelineRun0</PipelineRun>\n-        <ExitStatusCode>ExitStatusCode0</ExitStatusCode>\n+        <PpoStatus>COMPLETED</PpoStatus>\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <ExpirationDate>2040-11-24T11:07:19.072000Z</ExpirationDate>\n-        <ToBePublished>false</ToBePublished>\n-        <Published>false</Published>\n         <Curator>Curator0</Curator>\n         <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n     </Header>\n@@ -47,12 +40,8 @@\n                 <FileName>coadd_template_JPCAM.properties</FileName>\n             </FileContainer>\n         </ConfigurationFile>\n+        <Survey>JEDIS</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n     </Data>\n-    <Parameters>\n-        <Parameter>\n-            <Key>Instrument</Key>\n-            <Description>The Instrument for the property file</Description>\n-            <StringValue>JPCAM</StringValue>\n-        </Parameter>\n-    </Parameters>\n </co:DpdExtConfigurationSet>\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/DpdExtConfigurationSet_LSSTCAM.xml": [
                        [
                            "@@ -1,43 +1,36 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<co:DpdExtConfigurationSet xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:cat=\"http://euclid.esa.org/schema/bas/cat\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:co=\"http://euclid.esa.org/schema/dpd/ext/configurationset\"\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/git/ST_DataModel_dev/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtConfigurationSet.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n     <Header>\n         <ProductId>DpdExtConfigurationSet_LSSTCAM_v1_2021-01-16T09:00:00</ProductId>\n         <ProductType>DpdExtConfigurationSet</ProductType>\n         <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n-        <SoftwareRelease>0.5</SoftwareRelease>\n-        <EuclidPipelineSoftwareRelease>Euclid</EuclidPipelineSoftwareRelease>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n         <ProdSDC>SDC-DE</ProdSDC>\n-        <DataSetRelease>0.5</DataSetRelease>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n         <PlanId>PlanId0</PlanId>\n         <PPOId>PPOId0</PPOId>\n         <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PipelineRun>PipelineRun0</PipelineRun>\n-        <ExitStatusCode>ExitStatusCode0</ExitStatusCode>\n+        <PpoStatus>COMPLETED</PpoStatus>\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <ExpirationDate>2040-11-24T11:07:19.072000Z</ExpirationDate>\n-        <ToBePublished>false</ToBePublished>\n-        <Published>false</Published>\n         <Curator>Curator0</Curator>\n         <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n     </Header>\n@@ -47,12 +40,8 @@\n                 <FileName>coadd_template_LSSTCAM.properties</FileName>\n             </FileContainer>\n         </ConfigurationFile>\n+        <Survey>RUBIN</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n     </Data>\n-    <Parameters>\n-        <Parameter>\n-            <Key>Instrument</Key>\n-            <Description>The Instrument for the property file</Description>\n-            <StringValue>LSSTCAM</StringValue>\n-        </Parameter>\n-    </Parameters>\n </co:DpdExtConfigurationSet>\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/DpdExtConfigurationSet_MEGACAM.xml": [
                        [
                            "@@ -1,45 +1,38 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<co:DpdExtConfigurationSet xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:cat=\"http://euclid.esa.org/schema/bas/cat\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:co=\"http://euclid.esa.org/schema/dpd/ext/configurationset\"\n+<co:DpdExtConfigurationSet xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:co=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/git/ST_DataModel_dev/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtConfigurationSet.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/configurationset file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtConfigurationSet.xsd\">\n     <Header>\n         <ProductId>DpdExtConfigurationSet_MEGACAM_v1_2021-03-18T09:00:00</ProductId>\n         <ProductType>DpdExtConfigurationSet</ProductType>\n         <SoftwareName>EXT_PF1_GEN_P2</SoftwareName>\n-        <SoftwareRelease>0.60</SoftwareRelease>\n-        <EuclidPipelineSoftwareRelease>Euclid</EuclidPipelineSoftwareRelease>\n+        <SoftwareRelease>0.80</SoftwareRelease>\n         <ProdSDC>SDC-DE</ProdSDC>\n-        <DataSetRelease>0.60</DataSetRelease>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n         <PlanId>PlanId0</PlanId>\n         <PPOId>PPOId0</PPOId>\n         <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PipelineRun>PipelineRun0</PipelineRun>\n-        <ExitStatusCode>ExitStatusCode0</ExitStatusCode>\n+        <PpoStatus>COMPLETED</PpoStatus>\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <ExpirationDate>2040-11-24T11:07:19.072000Z</ExpirationDate>\n-        <ToBePublished>false</ToBePublished>\n-        <Published>false</Published>\n         <Curator>Curator0</Curator>\n-        <CreationDate>2021-03-18T09:00:00</CreationDate>\n+        <CreationDate>2021-01-16T09:00:00.0</CreationDate>\n     </Header>\n     <Data>\n         <ConfigurationFile>\n@@ -47,12 +40,8 @@\n                 <FileName>coadd_template_MEGACAM.properties</FileName>\n             </FileContainer>\n         </ConfigurationFile>\n+        <Survey>CFIS</Survey>\n+        <Pipeline>EXT_PF1_GEN_P2</Pipeline>\n+        <DataType>Real</DataType>\n     </Data>\n-    <Parameters>\n-        <Parameter>\n-            <Key>Instrument</Key>\n-            <Description>The Instrument for the property file</Description>\n-            <StringValue>MEGACAM</StringValue>\n-        </Parameter>\n-    </Parameters>\n </co:DpdExtConfigurationSet>\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/NorthernSurveys_wavelengths.txt": [
                        [
                            "@@ -0,0 +1,6 @@\n+filter:         wavelength [\u00c5]:\n+CFIS_r 6425.697691437196\n+CFIS_u 3682.7699558340664\n+JEDIS_g 4776.664982091497\n+ps_i 7535.640665609784\n+wishes_z 8908.526460077348\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_DECAM.properties": [
                        [
                            "@@ -20,6 +20,7 @@ ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n detect_bands = i,z\n@@ -39,8 +40,8 @@ keepnorm = 1\n log = ${block}_${module}_${job_id}.log\n log_dir = ${archive_root}/${run_dir}/log\n logger = ${log_dir}/${log}\n-mask_indices = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n-move_mask_indices_to_weight = \n+mask_indices = \n+move_mask_indices_to_weight = 0,1,2,3,4,5,6,7\n move_mask_indices_to_new_indices = \n max_memory = 62000\n mkdirs = ${outpath}\n@@ -52,26 +53,35 @@ outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n+#columns in the reference catalog with calibration data\n+Reference_Catalog_Calibration_DECAM_g = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_DECAM_r = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG \n+Reference_Catalog_Calibration_DECAM_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_DECAM_z = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n stopcondition =  \n-stackpsf_version = 1\n-swarp_default = ${euclid_home}/conf/etc/default.swarp\n+stackpsf_version = 2\n+subtract_delivered_background = 0\n+subtract_bg_with_swarp = 1\n+swarp_default = ${euclid_home}/conf/etc/des.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n target_node = alexandria\n tile = ${tilename}\n tilename = PYD345.000200+0.000140\n+use_stackregions = 0\n verbose = 3\n-vis_position_cat = \n+vis_position_cat =\n wall_mod = 3000\n xml_props = ${archive_root}/../${block}_${module}_${job_id}.properties\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_GPC.properties": [
                        [
                            "@@ -17,6 +17,7 @@ ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n detect_bands = PANSTARRS_i\n@@ -36,33 +37,37 @@ keepnorm = 1\n log = ${block}_${module}_${job_id}.log\n log_dir = ${archive_root}/${run_dir}/log\n logger = ${log_dir}/${log}\n-mask_indices = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n-move_mask_indices_to_weight = \n+mask_indices =  1, 2, 4, 6, 10, 11, 12, 15\n+move_mask_indices_to_weight = 0, 3, 5, 7, 8, 9, 13, 14\n move_mask_indices_to_new_indices = \n max_memory = 62000\n mkdirs = ${outpath}\n module = rdisk_coadd_decam\n-multiprocesses = 1\n+multiprocesses = 4\n nite = 20131228\n outauxpath = ${archive_root}/${run_dir}/aux\n outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n+Reference_Catalog_Calibration_PANSTARRS_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n stopcondition = \n stackpsf_version = 1\n+subtract_delivered_background = 0\n+subtract_bg_with_swarp = 1\n swarp_default = ${euclid_home}/conf/etc/default.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n target_node = alexandria\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -65,7 +65,7 @@ scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n stopcondition = \n-stackpsf_version = 2\n+stackpsf_version = 1\n subtract_delivered_background = 0\n subtract_bg_with_swarp = 1\n swarp_default = ${euclid_home}/conf/etc/default.swarp\n",
                            "changed stackpsf default for Panstarrs to v1",
                            "Michael",
                            "2023-05-04T12:10:51.000+02:00",
                            "250c937c812206d730f76bd3ccce4a020c5d436d"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_HSC.properties": [
                        [
                            "@@ -10,16 +10,21 @@ archive_root = /home/user/host/euclid-ial/workspace/coadd/data\n args = --ramdisk ${ramdisk} --xml ${xml_props} --scriptname ${scriptname} --pipeline_class ${pipeline_class} --binpath ${euclid_home}/bin ${fscaleastrovar} --verbose ${verbose}\n badpixels = -usebadpixels\n band = all\n+Bandpass_HSC_g = 4730.214843845347\n+Bandpass_HSC_r = 6226.0\n+Bandpass_HSC_i = 7767.0\n Bandpass_HSC_z = 8908.526460077348\n+Bandpass_HSC_y = 10051.0 \n binpath = ${euclid_home}/bin\n block = rdisk_coadd_decam\n ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n-detect_bands = HSC_z\n+detect_bands = HSC_g, HSC_z\n dummy = ${detect_bands}\n euclid_conf = ${euclid_home}/conf\n euclid_home = /home/user/euclid_local_home\n@@ -37,7 +42,7 @@ log = ${block}_${module}_${job_id}.log\n log_dir = ${archive_root}/${run_dir}/log\n logger = ${log_dir}/${log}\n mask_indices = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n-move_mask_indices_to_weight = \n+move_mask_indices_to_weight = 0, 1, 3, 4, 6, 8, 9, 12\n move_mask_indices_to_new_indices = \n max_memory = 62000\n mkdirs = ${outpath}\n@@ -49,19 +54,27 @@ outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n+Reference_Catalog_Calibration_HSC_g = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_HSC_r = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_HSC_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_HSC_z = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_HSC_y = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n-stackpsf_version = 1\n+stackpsf_version = 2\n+subtract_delivered_background = 0\n+subtract_bg_with_swarp = 1\n stopcondition = \n swarp_default = ${euclid_home}/conf/etc/default.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -11,8 +11,10 @@ args = --ramdisk ${ramdisk} --xml ${xml_props} --scriptname ${scriptname} --pipe\n badpixels = -usebadpixels\n band = all\n Bandpass_HSC_g = 4730.214843845347\n-Bandpass_HSC_r = 6210.5\n+Bandpass_HSC_r = 6226.0\n+Bandpass_HSC_i = 7767.0\n Bandpass_HSC_z = 8908.526460077348\n+Bandpass_HSC_y = 10051.0 \n binpath = ${euclid_home}/bin\n block = rdisk_coadd_decam\n ccd_end = 63\n@@ -59,7 +61,10 @@ pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n Reference_Catalog_Calibration_HSC_g = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_HSC_r = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_HSC_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n Reference_Catalog_Calibration_HSC_z = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_HSC_y = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n save_backgrounds_for_validation = 0\n",
                            "added missing HSC bands",
                            "Michael",
                            "2023-04-26T12:26:20.000+02:00",
                            "48ac5da0077836040e518d3d0a63555934b87a20"
                        ],
                        [
                            "@@ -11,6 +11,7 @@ args = --ramdisk ${ramdisk} --xml ${xml_props} --scriptname ${scriptname} --pipe\n badpixels = -usebadpixels\n band = all\n Bandpass_HSC_g = 4730.214843845347\n+Bandpass_HSC_r = 6210.5\n Bandpass_HSC_z = 8908.526460077348\n binpath = ${euclid_home}/bin\n block = rdisk_coadd_decam\n",
                            "updated bandpass values for HSC",
                            "Michael",
                            "2023-04-25T19:55:55.000+02:00",
                            "ce6fc3ba8d3fc6759e5ca7d3447f33de155b8b15"
                        ],
                        [
                            "@@ -39,7 +39,7 @@ log = ${block}_${module}_${job_id}.log\n log_dir = ${archive_root}/${run_dir}/log\n logger = ${log_dir}/${log}\n mask_indices = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n-move_mask_indices_to_weight = \n+move_mask_indices_to_weight = 0, 1, 3, 4, 6, 8, 9, 12\n move_mask_indices_to_new_indices = \n max_memory = 62000\n mkdirs = ${outpath}\n",
                            "updated mask values for HSC",
                            "Michael",
                            "2023-04-25T19:52:25.000+02:00",
                            "e4bab2aff6a7c368b28b7f029129633c699bfc65"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_JPCAM.properties": [
                        [
                            "@@ -17,6 +17,7 @@ ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n detect_bands = JPCAM_g\n@@ -49,20 +50,24 @@ outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n+Reference_Catalog_Calibration_JPCAM_g = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n stopcondition = \n-stackpsf_version = 1\n+stackpsf_version = 2\n+subtract_delivered_background = 1\n+subtract_bg_with_swarp = 0\n swarp_default = ${euclid_home}/conf/etc/default.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n target_node = alexandria\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_LSST.properties": [
                        [
                            "@@ -22,6 +22,7 @@ ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n detect_bands = LSST_i,LSST_r\n@@ -54,20 +55,29 @@ outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n+Reference_Catalog_Calibration_LSST_u = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_g = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_r = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_z = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_y = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n stopcondition = \n-stackpsf_version = 1\n+stackpsf_version = 2\n+subtract_delivered_background = 0\n+subtract_bg_with_swarp = 1\n swarp_default = ${euclid_home}/conf/etc/default.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n target_node = alexandria\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_LSSTCAM.properties": [
                        [
                            "@@ -22,6 +22,7 @@ ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n detect_bands = LSST_i,LSST_r\n@@ -54,20 +55,29 @@ outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n+Reference_Catalog_Calibration_LSST_u = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_g = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_r = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_i = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_z = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_LSST_y = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n stopcondition = \n-stackpsf_version = 1\n+stackpsf_version = 2\n+subtract_delivered_background = 0\n+subtract_bg_with_swarp = 1\n swarp_default = ${euclid_home}/conf/etc/default.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n target_node = alexandria\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_MEGACAM.properties": [
                        [
                            "@@ -18,6 +18,7 @@ ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n detect_bands = MEGACAM_u,MEGACAM_r\n@@ -37,8 +38,8 @@ keepnorm = 1\n log = ${block}_${module}_${job_id}.log\n log_dir = ${archive_root}/${run_dir}/log\n logger = ${log_dir}/${log}\n-mask_indices = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n-move_mask_indices_to_weight = \n+mask_indices =\n+move_mask_indices_to_weight = 1,2,3,4,5,6,7,8\n move_mask_indices_to_new_indices = \n max_memory = 62000\n mkdirs = ${outpath}\n@@ -50,21 +51,26 @@ outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution \n ramdisk = ${archive_root}/EuclidPipeline\n+Reference_Catalog_Calibration_MEGACAM_u = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n+Reference_Catalog_Calibration_MEGACAM_r = TU_FNU_G_GAIA, TU_FNU_G_GAIA_MAG\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_stage2_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n-stackpsf_version = 1\n+stackpsf_version = 2\n+subtract_delivered_background = 0\n+subtract_bg_with_swarp = 1\n stopcondition = \n-swarp_default = ${euclid_home}/conf/etc/default.swarp\n+swarp_default = ${euclid_home}/conf/etc/cfhtls.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n target_node = alexandria\n tile = ${tilename}\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2/coadd_template_OMEGACAM.properties": [
                        [
                            "@@ -16,6 +16,7 @@ ccd_end = 63\n ccd_start = 1\n coaddheadfile = ${euclid_home}/conf/etc/desremap.head\n coaddswarp = ${archive_root}/20160908112655_PYD345.000200+0.000140/aux/coaddswarp_0001.list\n+coadd_zeropoint = 30.0\n combinetype = WEIGHTED\n debug = 1\n detect_bands = r,g\n@@ -48,7 +49,7 @@ outpath = ${archive_root}/${run_dir}/coadd\n outpath_nonhomog = ${archive_root}/${run_dir}/nhcoadd\n outputlist = ${archive_root}/${run_dir}/aux/coaddswarp_${job_id}.list\n outxmlpath = ${archive_root}/${run_dir}/xml\n-pipeline_class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.py\n+pipeline_class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.py\n pipeline_cleanup = 1\n project = PYD\n pybinpath = ${euclid_home}/python/pybin\n@@ -56,12 +57,15 @@ qa-args = -project ${project} -campaign  -nite ${nite} -qa_execution\n ramdisk = ${archive_root}/EuclidPipeline\n run_dir = rd\n runtime_dir = ${archive_root}/${run_dir}/runtime/${block}_${job_id}\n+save_backgrounds_for_validation = 0\n savefpacked = 0\n-scriptname = rdisk_coadd_pipeline.py\n+scriptname = rdisk_coadd_pipeline.pscript\n se_catalog_x = XWIN_IMAGE\n se_catalog_y = YWIN_IMAGE\n stopcondition = \n-stackpsf_version = 1\n+stackpsf_version = 2\n+subtract_delivered_background = 1\n+subtract_bg_with_swarp = 0\n swarp_default = ${euclid_home}/conf/etc/default.swarp\n swarp_flag_default = ${euclid_home}/conf/etc/default_flag.swarp\n target_node = alexandria\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/AstrometricValidation/AstrometricValidation.py": [
                        [
                            "@@ -14,6 +14,7 @@ You should have received a copy of the GNU Lesser General Public License along w\n the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n \n \"\"\"\n+from EXT_PF1_GEN_P2_LIBS.file import VariableEconomy as ve\n from astropy.table import Table\n from matplotlib import pyplot as plt\n import os\n@@ -43,167 +44,144 @@ class AstrometricValidation(object):\n     dpi = 100\n     images_per_plot = 25\n     catalog_prefix = \"MatchCatalog_\"\n-        \n-    def __init__(self, se_catalog, reference_catalog):\n+\n+    def __init__(self, se_catalog, reference_catalog, ra_ref = \"X_WORLD\", dec_ref = \"Y_WORLD\"):\n         \"\"\"\n         se_catalog a single epoch catalog. The catalog to evalute\n         reference_catalog the reference catalog that is used to evaluate the single epoch catalog\n+        ra_ref the name of the ra column of the reference catalog. Default = \"X_WORLD\"\n+        dec_ref the name of the dec column of the reference catalog. Default = \"Y_WORLD\"\n         \"\"\"\n         self.se_catalog = se_catalog\n         self.reference_catalog = reference_catalog\n+        self.ra_ref = ra_ref\n+        self.dec_ref = dec_ref\n+\n         self.range = (0,2)\n         self.ref_cat_mag_limit = 19.0\n     #\n-    \n+\n     def write_catalog(self, catalog_table, val_dir):\n         \"\"\"\n-        Writes the table to a file with the path of the se catalog and \n+        Writes the table to a file with the path of the se catalog and\n         the same filename with prefix \"MatchCatalog\"\n         \"\"\"\n         #get the path\n         catalog_filename = val_dir +os.sep +self.catalog_prefix +self.se_catalog.rsplit(os.sep, 1)[-1]\n-        \n+\n         catalog_table.write(catalog_filename, format='fits', overwrite=True)\n         fits.setval(catalog_filename, 'extname', value='LDAC_OBJECTS', ext=-1)\n     #\n-    \n+\n     def collect_catalogs(self, val_dir, reference_files = []):\n         \"\"\"\n         Collects all catalogs with the standard catalog signature from val_dir\n-        \n+\n         reference_files a list of image filenames. Returns only names matching those reference_files\n         for example to select catalogs from a band only\n         return\n         a list with full paths to all found catalog files\n         \"\"\"\n-        all_cats = [val_dir + os.sep +_f for _f in os.listdir(val_dir) if (_f.endswith(\".fits\") and _f.startswith(self.catalog_prefix))]\n-        return self.match_names(all_cats, reference_files)\n-        #    \n-    #\n-    \n-    \n-    def match_names(self, files, reference_files):\n-        \"\"\"\n-        Select from files only names, that match names of the reference_files\n-        Ignore extensions and prefixes.\n-        \"\"\"    \n-        if len(reference_files) == 0:\n-            return files\n+        all_cats = [val_dir + os.sep +_f for _f in os.listdir(val_dir) \\\n+                if (_f.endswith(\".fits\") and _f.startswith(self.catalog_prefix))]\n+        return AstrometricValidation.match_names(all_cats, reference_files)\n         #\n-        \n-        returnvalue = []\n-        for reference_file in reference_files:\n-            for _f in files:\n-                if reference_file in _f:\n-                    returnvalue.append(_f)\n-                    break\n-                #\n-            #\n-        return returnvalue\n     #\n-    \n-    def add_ra_dec_to_table(self, table, x, y, image_file):\n-        ra_se = 'ALPHA'\n-        dec_se = 'BETA'\n-        \n-        af = fits.open(image_file, \"readonly\")\n-        astropy_header = af[\"IMAGE\"].header\n-        af.close()\n-        \n-        wcs = WCS(astropy_header)\n-        radec = wcs.all_pix2world(table[x], table[y], 1)\n-        \n-             \n-        table[ra_se] = radec[0]\n-        table[dec_se] = radec[1]\n-                                \n-        return table, ra_se, dec_se \n-    #\n-    \n-    def astropy_validation(self, se_catalog_x, se_catalog_y, image_file, ra_ref = \"X_WORLD\", dec_ref = \"Y_WORLD\"):\n+\n+\n+    def astropy_validation(self, se_catalog_x, se_catalog_y, image_file, band):\n+        \"\"\"\n+        evaluates the se catalog against a reference catalog by finding the matching entries\n+        and their distances.\n+        Converts X,Y of the se catalog to RA/DEC using the WCS of the supplied image_file\n+        Creates also the catalog of the matching objects.\n+        \"\"\"\n         #http://www.astropy.org/astropy-tutorials/Coordinates.html\n         #jupyter/max_catalog_compare/astrom_check\n-        \n         #read the the se_catalog into a Table object\n-        se_cat = Table.read(self.se_catalog, \"LDAC_OBJECTS\", format=\"fits\")\n-        \n-        \n+        v = ve.Variables()\n+        v.se_cat = Table.read(self.se_catalog, \"LDAC_OBJECTS\", format=\"fits\")\n+\n+\n         #calculate ra/dec from se_catalog_x, se_catalog_y and wcs_header\n-        se_cat,ra_se, dec_se =  self.add_ra_dec_to_table(se_cat, se_catalog_x, se_catalog_y, image_file)\n-         \n-        se_cat[ra_se].unit = u.deg\n-        se_cat[dec_se].unit = u.deg     \n-        \n+        v.se_cat,v.ra_se, v.dec_se =  AstrometricValidation.add_ra_dec_to_table(v.se_cat, se_catalog_x, se_catalog_y, image_file)\n+\n+        v.se_cat[v.ra_se].unit = u.deg\n+        v.se_cat[v.dec_se].unit = u.deg\n+\n         #and the reference catalog\n-        ref_cat = Table.read(self.reference_catalog, \"LDAC_OBJECTS\", format=\"fits\")\n-        ref_cat = ref_cat[ref_cat[\"MAG_AUTO\"] < self.ref_cat_mag_limit]\n-        \n-        #at this point, we may have to calculate the ra/dec values for the catalog\n-        ref_cat[ra_ref].unit = u.deg       \n-        ref_cat[dec_ref].unit = u.deg\n-        \n-        \n+        v.ref_cat = Table.read(self.reference_catalog, \"LDAC_OBJECTS\", format=\"fits\")\n+        v.ref_cat = v.ref_cat[v.ref_cat[band] < self.ref_cat_mag_limit]\n+\n+        v.ref_cat[self.ra_ref].unit = u.deg\n+        v.ref_cat[self.dec_ref].unit = u.deg\n+\n+\n         #cut away everything from the ref_cat that does not overlap the se_cat\n         #this way, no crazy distant matches are possible\n-        se_ra = [min(se_cat[ra_se]), max(se_cat[ra_se])]\n-        se_dec = [min(se_cat[dec_se]), max(se_cat[dec_se])]\n-\n-        ii = ref_cat[ra_ref] <= se_ra[1] \n-        ii *= ref_cat[ra_ref] >= se_ra[0] #ra_min\n-        ii *= ref_cat[dec_ref] >= se_dec[0] # dec_min\n-        ii *= ref_cat[dec_ref] <= se_dec[1] #dec_max\n-        \n-        \n+\n+        ii = v.ref_cat[self.ra_ref] <= max(v.se_cat[v.ra_se])\n+        ii *= v.ref_cat[self.ra_ref] >= min(v.se_cat[v.ra_se]) #ra_min\n+        ii *= v.ref_cat[self.dec_ref] >= min(v.se_cat[v.dec_se]) # dec_min\n+        ii *= v.ref_cat[self.dec_ref] <= max(v.se_cat[v.dec_se]) #dec_max\n+\n         #also cut away everything from the se_cat that does not overlap the ref_cat\n         #otherwise very large distances will be created for the next matches\n-        ref_ra = [min(ref_cat[ra_ref]), max(ref_cat[ra_ref])]\n-        ref_dec = [min(ref_cat[dec_ref]), max(ref_cat[dec_ref])]\n-\n-        jj = se_cat[ra_se] <= ref_ra[1] \n-        jj *= se_cat[ra_se] >= ref_ra[0] #ra_min\n-        jj *= se_cat[dec_se] >= ref_dec[0] # dec_min\n-        jj *= se_cat[dec_se] <= ref_dec[1] #dec_max\n-        \n-        skycoo_se = SkyCoord(se_cat[ra_se][jj], se_cat[dec_se][jj])\n-        skycoo_ref = SkyCoord(ref_cat[ra_ref][ii], ref_cat[dec_ref][ii])\n-                \n+        jj = v.se_cat[v.ra_se] <= max(v.ref_cat[self.ra_ref])\n+        jj *= v.se_cat[v.ra_se] >= min(v.ref_cat[self.ra_ref]) #ra_min\n+        jj *= v.se_cat[v.dec_se] >= min(v.ref_cat[self.dec_ref]) # dec_min\n+        jj *= v.se_cat[v.dec_se] <= max(v.ref_cat[self.dec_ref]) #dec_max\n+\n+        v.skycoo_se = SkyCoord(v.se_cat[v.ra_se][jj], v.se_cat[v.dec_se][jj])\n+        v.skycoo_ref = SkyCoord(v.ref_cat[self.ra_ref][ii], v.ref_cat[self.dec_ref][ii])\n+\n+\n         #compare; ref has more entries, so se_cat is compared against skycoo\n         try:\n-            if skycoo_ref.size > 0:\n-                idx, d2d, _ = skycoo_se.match_to_catalog_sky(skycoo_ref)\n+            if v.skycoo_ref.size > 0:\n+                v.idx, v.d2d, _ = v.skycoo_se.match_to_catalog_sky(v.skycoo_ref)\n+\n                 #cut too large matches\n-                ij = d2d.arcsec < 1.0 * self.range[1] #arcsec -> this is a 10 pixel cut\n-                distances = d2d.arcsec[ij]\n-                                        \n-                mc = self.make_match_catalog(idx, d2d.arcsec, (se_cat[ra_se][jj], se_cat[dec_se][jj]), (ref_cat[ra_ref][ii], ref_cat[dec_ref][ii]))\n+                #use 2 arcsecs ~ 8-10 pixel for the cut\n+                ij = v.d2d.arcsec < 1.0 * self.range[1] \n+\n+                distances = v.d2d.arcsec[ij]\n+\n+                mc = self.make_match_catalog(v.idx, v.d2d.arcsec,\n+                        (v.se_cat[v.ra_se][jj], v.se_cat[v.dec_se][jj]),  \n+                        (v.ref_cat[self.ra_ref][ii], \n+                         v.ref_cat[self.dec_ref][ii]))\n+                #self.__extract_x_y(mc, v.se_cat, v.ra_se, v.dec_se, se_catalog_x, se_catalog_y)\n+\n             else:\n                 return None, None\n         except Exception as ee:\n-            \n+\n             print('exception caught by AstrometricValidation:\\n', ee)\n             import traceback\n             import sys\n-            \n+\n             exec_info =  sys.exc_info()\n             traceback.print_tb(exec_info[2])\n             #one of those catalogs can be empty\n-             \n+\n             return None, None\n         #\n         return distances, mc\n-    \n+\n+\n     def make_match_catalog(self, idx, distances, se_ra_dec, ref_ra_dec):\n         \"\"\"\n         put the matches into a catalog of the format\n         se_ra, se_dec, ref_ra, ref_dec\n-        \n+\n         put into an astropy table and return\n         \"\"\"\n         se_ra_match = []\n         se_dec_match = []\n         ref_ra_match = []\n         ref_dec_match = []\n-        \n+\n         for i in range(len(se_ra_dec[0])):\n             if distances[i] < 1.0 * self.range[1]:\n                 se_ra_match.append(se_ra_dec[0][i])\n@@ -212,59 +190,47 @@ class AstrometricValidation(object):\n                 ref_dec_match.append(ref_ra_dec[1][idx[i]])\n             #\n         #\n-        \n+\n         #put it into a table\n-        mc = Table([se_ra_match, se_dec_match, ref_ra_match, ref_dec_match], names=('se_ra','se_dec','ref_ra','ref_dec'))\n+        mc = Table([se_ra_match, se_dec_match, ref_ra_match, ref_dec_match], \n+                    names=('se_ra','se_dec','ref_ra','ref_dec'))\n         return mc\n     #\n     \n-    def get_statistics(self, distances):\n-        \"\"\"\n-        from the distances list calculate statistics\n-        \n-        return\n-        mean, median, stddev\n-        \"\"\"\n-        #select the small distances only, as there are also very\n-        #large ones from wrong matches\n-        mean = distances.mean()\n-        median = np.median(distances)\n-        stddev = np.std(distances)\n-        return mean, median, stddev\n-        \n+\n     def plot_distances(self, distances, catname, directory = \"./\"):\n         \"\"\"\n         make a histogram plot of the distances of the se_catalog to\n         the matched ref_catalog objects.\n-        \n+\n         store the plot as jpeg in directory\n         \"\"\"\n         catname = catname.rsplit(os.sep, 1)[-1].rsplit(\".\",1)[0]\n-        mean, median, stddev = self.get_statistics(distances)\n-        \n+        mean, median, stddev = AstrometricValidation.get_statistics(distances)\n+\n         #make a subplot, so we can edit the right and top axis\n         ax = plt.subplot(111, xmargin = 0, ymargin = 0, anchor = 'SW', adjustable = 'box')\n-        \n+\n         #do the plot\n         plt.hist(distances, histtype='step', range=self.range, bins=20)\n         plt.xlabel('separation [arcsec]')\n-        \n-        # Hide the right and top axes (because the title and \n+\n+        # Hide the right and top axes (because the title and\n         #annotation may cross\n         ax.spines['right'].set_visible(False)\n         ax.spines['top'].set_visible(False)\n-        \n-        annotation = 'mean: ' +str(mean) +' +/- ' +str(stddev) \n+\n+        annotation = 'mean: ' +str(mean) +' +/- ' +str(stddev)\n         annotation = annotation +'\\nmedian: ' +str(median)\n-        \n+\n         ranges = plt.axis()\n         plt.text((ranges[1] - ranges[0])/2., (ranges[3] - ranges[2])/1.333, annotation)\n-        \n+\n         #replace _ with blank, so the title (long filename)\n         #can wrap\n         plt.title(catname.replace(\"_\", \" \"), wrap = True, loc = 'left', snap = True, va = 'top')\n         plt.tight_layout()\n-        \n+\n         plotfolder = directory +os.sep +self.single_plots_dir\n         if (not os.path.exists(plotfolder) ):\n             os.makedirs(plotfolder)\n@@ -273,82 +239,80 @@ class AstrometricValidation(object):\n         plt.close()\n \n         return path\n-    #    \n-    \n+    #\n+\n     def compose_images(self, imagelist, image_name_prefix, cleanup = True):\n         \"\"\"\n-        compose the images to larger entities of \n+        compose the images to larger entities of\n         5*5 on an image for easier inspection\n-        \n+\n         imagelist: full paths to the validation plots made by this class\n-        image_name_prefix: unique name prefix for the composed images \n+        image_name_prefix: unique name prefix for the composed images\n                            (like tile_band_)\n                            the code will add a counter\n         \"\"\"\n         if len(imagelist) == 0:\n             return []\n-        \n+\n+        v = ve.Variables()\n         #sort the imagelist by filenames, so it follows some logic\n         imagelist = sorted(imagelist)\n-        images_per_plot = self.images_per_plot\n-        chunks = len(imagelist)//images_per_plot\n-        \n-        #size of the plot\n-        image = plt.imread(imagelist[0])\n-        shape = image.shape \n+        v.chunks = len(imagelist)//self.images_per_plot\n \n+        #size of the plot\n+        v.image = plt.imread(imagelist[0])\n         returnpaths = []\n-        \n-        for i in range(chunks + 1):\n-            if i == chunks:\n-                sublist = imagelist[i*images_per_plot:]\n+\n+        for i in range(v.chunks + 1):\n+            if i == v.chunks:\n+                v.sublist = imagelist[i*self.images_per_plot:]\n             else:\n-                sublist = imagelist[i*images_per_plot:(i+1)*images_per_plot]\n+                v.sublist = imagelist[i*self.images_per_plot:(i+1)*self.images_per_plot]\n             counter = 0\n-            \n-            if len(sublist) == 0:\n+\n+            if len(v.sublist) == 0:\n                 continue\n             #size of the output\n-            out_width = len(sublist) if len(sublist) <= 5 else 5\n-            out_height = len(sublist)//5 if len(sublist)%5 ==0 else 1 + len(sublist)//5\n-            #print(\"w,h = \", out_width, out_height)\n-            \n-            out_pixel_width = out_width * shape[1]\n-            out_pixel_height = out_height * shape[0]\n-            \n-            new_img = np.zeros(shape=(out_pixel_height, out_pixel_width, 3), dtype=np.uint8)\n-            #print(\"new image shape \", new_img.shape)    \n-            \n-            for row in range(out_height): \n-                for _col in range(out_width):\n-                    if counter > len(sublist) - 1:\n+            v.out_width = len(v.sublist) if len(v.sublist) <= 5 else 5\n+            v.out_height = len(v.sublist)//5 if len(v.sublist)%5 ==0 else 1 + len(v.sublist)//5\n+            #print(\"w,h = \", v.out_width, v.out_height)\n+\n+            v.out_pixel_width = v.out_width * v.image.shape[1]\n+            v.out_pixel_height = v.out_height * v.image.shape[0]\n+\n+            v.new_img = np.zeros(shape=(v.out_pixel_height, v.out_pixel_width, 3), dtype=np.uint8)\n+            #print(\"new v.image shape \", v.new_img.shape)\n+\n+            for row in range(v.out_height):\n+                for _col in range(v.out_width):\n+                    if counter > len(v.sublist) - 1:\n                         break\n                     try:\n-                        img = plt.imread(sublist[counter])\n-                        new_img[row*shape[0]:(row+1)*shape[0], \n-                                _col*shape[1]:(_col+1)*shape[1],\n-                                :]=img\n+                        v.new_img[row*v.image.shape[0]:(row+1)*v.image.shape[0],\n+                                _col*v.image.shape[1]:(_col+1)*v.image.shape[1],\n+                                :]=plt.imread(v.sublist[counter])\n                     except:\n-                        print(\"AstrometricValidation: image \", sublist[counter], \" failed to plot.\")            \n-                    counter = counter +1                    \n+                        print(\"AstrometricValidation: image \", v.sublist[counter], \" failed to plot.\")\n+                    counter = counter +1\n                 #\n             #\n             path = imagelist[0].rsplit(os.sep +self.single_plots_dir +os.sep, 1)[0] + os.sep +image_name_prefix +\"_\" +str(i+1) +\".jpg\"\n-            plt.imsave(path, new_img)\n+            plt.imsave(path, v.new_img)\n             returnpaths.append(path)\n-        #clean_up \n+            #clean_up: remove single plots as they have\n+            #been rearranged on a new summary plot\n         if cleanup:\n-            single_plots_dir = imagelist[0].rsplit(os.sep, 1)[0]\n-            if os.path.exists(single_plots_dir):\n-                shutil.rmtree(single_plots_dir, ignore_errors=True)\n+            v.single_plots_dir = imagelist[0].rsplit(os.sep, 1)[0]\n+            if os.path.exists(v.single_plots_dir):\n+                shutil.rmtree(v.single_plots_dir, ignore_errors=True)\n             #\n         return returnpaths\n     #\n-    \n+\n     def find_single_plots(self, directory, reference_files = []):\n         \"\"\"\n         reference_files list of filenames without extension whose names are like\n-        the plot names in the directory. \n+        the plot names in the directory.\n         For example to select files from a band only\n         Can be an empty list (default). In that case, the whole directories contents is returned.\n         \"\"\"\n@@ -357,7 +321,64 @@ class AstrometricValidation(object):\n             _flist = [plotfolder +os.sep +f for f in os.listdir(plotfolder) if f.endswith(\".jpg\")]\n             return self.match_names(_flist, reference_files)\n         #\n-        return []   \n+        return []\n+    #\n+\n+    @staticmethod\n+    def match_names(files, reference_files):\n+        \"\"\"\n+        Select from files only names, that match names of the reference_files\n+        Ignore extensions and prefixes.\n+        \"\"\"\n+        if len(reference_files) == 0:\n+            return files\n+        #\n+\n+        returnvalue = []\n+        for reference_file in reference_files:\n+            for _f in files:\n+                if reference_file in _f:\n+                    returnvalue.append(_f)\n+                    break\n+                #\n+            #\n+        return returnvalue\n+    #\n+    \n+    @staticmethod\n+    def add_ra_dec_to_table(table, x, y, image_file):\n+        ra_se = 'ALPHA'\n+        dec_se = 'BETA'\n+\n+        af = fits.open(image_file, \"readonly\")\n+        astropy_header = af[\"IMAGE\"].header\n+        af.close()\n+\n+\n+        wcs = WCS(astropy_header)\n+        radec = wcs.all_pix2world(table[x], table[y], 1)\n+\n+        table[ra_se] = radec[0]\n+        table[dec_se] = radec[1]\n+\n+        return table, ra_se, dec_se\n+    #\n+\n+    @staticmethod\n+    def get_statistics(distances):\n+        \"\"\"\n+        from the distances list calculate statistics\n+\n+        return\n+        mean, median, stddev\n+        \"\"\"\n+        #select the small distances only, as there are also very\n+        #large ones from wrong matches\n+        mean = distances.mean()\n+        median = np.median(distances)\n+        stddev = np.std(distances)\n+        return mean, median, stddev\n     #\n #\n-    \n\\ No newline at end of file\n+\n+\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/AstrometricValidation/__init__.py": [
                        [
                            "@@ -1,7 +1,7 @@\n \"\"\"\n Package hierarchy support file.\n \"\"\"\n-#append to sys.path in order to treat the stage2 packages \n+#append to sys.path in order to treat the stage2 packages\n #the same as the stage 1 packages\n import sys\n import os\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/CatalogMatchFinder.py": [
                        [
                            "@@ -0,0 +1,287 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+import sys\n+import os\n+import argparse\n+import traceback\n+\n+from EXT_PF1_GEN_P2.Fwhm import SEReader\n+from EXT_PF1_GEN_P2.Fwhm import PsfComposition\n+from EXT_PF1_GEN_P2.Fwhm import CatalogPsfExtractor\n+from EXT_PF1_GEN_P2.Fwhm.LayerIds import LayerIds\n+\n+import numpy\n+\n+from astropy.table import Table\n+from astropy.table import Column\n+from astropy.io.fits import BinTableHDU\n+from astropy.wcs import WCS\n+from astropy.coordinates import SkyCoord\n+\n+\n+\n+class CatalogMatchFinder(LayerIds):\n+    \"\"\"\n+    Finds matches in reference catalog and stacked psf info table\n+    Part of a code suite to identify the best catalog matches with the same layer composition\n+    (same stackset)\n+    \"\"\"\n+\n+    def __init__(self, args = None):\n+        LayerIds.__init__(self)\n+\n+        if args is not None:\n+            #read layertable\n+            #this is the stackpsf stackset encoding\n+            cpf = CatalogPsfExtractor.CatalogPsfExtractor()\n+            self.wcs_dict, self.layertable = cpf.read(args.layercodes)\n+\n+            #following code is identical to PsfExtractor\n+            #-> merge it somewhere?\n+\n+            #now get the layer signatures for the reference (MER) catalog\n+            table = Table.read(args.catalog, format='fits', hdu = -1)\n+            table = self.add_objid_to_catalog(table)\n+\n+            ra = table[args.ra].data\n+            dec = table[args.dec].data\n+            object_id = table[args.objectid].data\n+\n+            pc =  PsfComposition.PsfComposition(ra, dec)\n+            wcs_dict = pc.measure_overlaps(self.wcs_dict)\n+\n+            #make an object: catalog ra, dec, idlist\n+            table_ids = list()\n+            for i in range(len(ra)):\n+                #catalog ra, catalog dec, indices (volatile), layer signature, \"matched ra\", \"matched dec\",\n+                #\"distance arcsecs\", \"stackpsf_idx\", \"catalog object id\"\n+                table_ids.append([ra[i], dec[i], [], \"\", \"matched ra\", \"matched dec\",\n+                                  \"distance arcsecs\", -1, object_id[i]])\n+            #\n+            table_ids = self.extract_layer_ids(wcs_dict, table_ids, layerid_key = 2)\n+            table_ids = self.encode_layer_ids(table_ids, layerid_key = 2)\n+\n+            table_ids = self.find_matches(self.layertable, table_ids)\n+            print(\"current work dir (disk quota failure) \", os.getcwd())\n+            _write = args.output.rsplit(\".fits\", 1)[0] +\"_nearest_neighbour_stackset_matches.fits\"\n+            self.find_nearest_matches_only(self.layertable, table_ids, _write = _write)\n+\n+            self.write(table_ids, args.output)\n+        #\n+    #\n+\n+\n+    def add_objid_to_catalog(self, refcat_table, objid=\"OBJECT_ID\"):\n+        \"\"\"\n+        checks, if objid is a column in reference_catalog.\n+        Adds a range, if not.\n+        \"\"\"\n+        if objid not in refcat_table.colnames:\n+            d = refcat_table[refcat_table.colnames[0]].data\n+            refcat_table.add_column(Column( list(range(d.shape[0])) ), name = objid)\n+        #\n+        return refcat_table\n+    #\n+\n+\n+    def find_matches(self, layertable, table_ids):\n+        \"\"\"\n+        layertable: the ra/dec of the psfstack arranged in columns with a layersignature\n+        table_ids: the ra/dec of the reference catalog, also with its layersignatures\n+\n+        finds the colum with the layersignature for each ra/dec catalog entry and extracts the closest match\n+        to a psfstack entry from it.\n+\n+        Puts the result into the table_ids list\n+        \"\"\"\n+        import time\n+        start = time.time()\n+        print(\"find matches \")\n+        #print(layertable.colnames)\n+        for i in range(len(table_ids)):\n+            tableid = table_ids[i]\n+            #print(\"tableid[4] \", tableid[4])\n+\n+            if tableid[3] in layertable.colnames:\n+                try:\n+                    catalog_coords = SkyCoord(tableid[0], tableid[1], unit=\"deg\")\n+                    layertable_data = layertable[tableid[3]].data\n+\n+                    _ra = layertable_data[0]\n+                    _dec = layertable_data[1]\n+                    _stackpsf_tableidx = layertable_data[2]\n+\n+                    if type(_ra) == type(numpy.float64(0.0)):\n+                        #here we have a single number, not an array\n+                        _ra = [_ra]\n+                        _dec = [_dec]\n+                        _stackpsf_tableidx = [_stackpsf_tableidx]\n+\n+                    coords_to_search_the_match_in = SkyCoord(_ra, _dec, unit=\"deg\")\n+\n+                    #make the match\n+                    idx, d2d, _ = catalog_coords.match_to_catalog_sky(coords_to_search_the_match_in)\n+\n+                    tableid[4] = _ra[idx]\n+                    tableid[5] = _dec[idx]\n+                    tableid[6] = d2d.arcsec[0]\n+                    tableid[7] = _stackpsf_tableidx[idx]\n+                    table_ids[i] = tableid\n+\n+                    #print(\"found match \", tableid)\n+                except Exception as e:\n+                    exec_info =  sys.exc_info()\n+                    traceback.print_tb(exec_info[2])\n+            #\n+        #\n+        print(\"time \", time.time() - start)\n+        return table_ids\n+    #\n+\n+    def find_nearest_matches_only(self, layertable, table_ids, _write = 0):\n+        \"\"\"\n+        resolve the layertable again into one big list\n+        \"\"\"\n+        stackpsf_encodings = [] #[(ra, dec, layercodes), ...]\n+        for col in layertable.colnames:\n+            data = layertable[col].data\n+            if len(data.shape) > 1:\n+                for i in range(data.shape[1]):\n+                    stackpsf_encodings.append((data[0][i], data[1][i], col))\n+                #\n+            else:\n+                #no array, single values\n+                stackpsf_encodings.append((data[0], data[1], col))\n+            #\n+        #\n+        \n+        t = Table()\n+        t.add_column(Column([]), name = \"Catalog ra\")\n+        t.add_column(Column([]), name = \"Catalog dec\")\n+        t.add_column(Column([]), name = \"nearest stackpsf ra\")\n+        t.add_column(Column([]), name = \"nearest stackpsf dec\")\n+        t.add_column(Column([]), name = \"distance (arcsec)\")\n+        t.add_column(Column([]), name = \"same stackset\")\n+        \n+        #convert to ra/dec coordinates\n+        ra_dec_codes = list(zip(*stackpsf_encodings))\n+        \n+        \n+        #return the empty table if there is no data\n+        _ra = list(ra_dec_codes[0])\n+        _dec = list(ra_dec_codes[1])\n+\n+        coords_to_search_the_match_in = SkyCoord(_ra, _dec, unit=\"deg\")\n+\n+        for i in range(len(table_ids)):\n+            tableid = table_ids[i]\n+            #print(\"tableid[4] \", tableid[4])\n+\n+            catalog_coords = SkyCoord(tableid[0], tableid[1], unit=\"deg\")\n+\n+            #make the match\n+            idx, d2d, _ = catalog_coords.match_to_catalog_sky(coords_to_search_the_match_in)\n+            #print(\"catalog: \", tableid[0], tableid[1], tableid[3], \", stackpsf \", stackpsf_encodings[idx][0],\n+            #stackpsf_encodings[idx][1], stackpsf_encodings[idx][2], \", equal? \",\n+            #stackpsf_encodings[idx][2] == tableid[3], d2d.arcsec)\n+            t.add_row( (tableid[0], tableid[1], stackpsf_encodings[idx][0], stackpsf_encodings[idx][1],\n+                       d2d.arcsec, stackpsf_encodings[idx][2] == tableid[3] ))\n+        #\n+    \n+        if _write:\n+            t.write(_write, format='fits', overwrite = 1)\n+        else:\n+            return t\n+        #\n+    #\n+\n+\n+    def write(self, table_ids, outfile):\n+        #convert from sets to columns\n+        table = list( zip( *table_ids) )\n+\n+        columns = dict()\n+        columns[\"catalog ra\"] = Column(table[0])\n+        columns[\"catalog dec\"] = Column(table[1])\n+        columns[\"catalog object id\"] = Column(table[8])\n+        columns[\"best match ra\"] = Column(table[4])\n+        columns[\"best match dec\"] = Column(table[5])\n+        columns[\"distance arcsec\"] = Column(table[6])\n+        columns[\"stackpsf index\"] = Column(table[7])\n+\n+        t = Table(columns)\n+        t.write(outfile, format='fits', overwrite = 1)\n+    #\n+\n+#\n+\n+\n+if __name__ == \"__main__\":\n+    argparser = argparse.ArgumentParser()\n+\n+    if not len(sys.argv) > 1:\n+        argparser.print_help()\n+        sys.exit()\n+    args = argparser.parse_args()\n+    result = mainMethod(args)\n+#\n+\n+def mainMethod(args):\n+    \"\"\"\n+    Sonar request as docstring.\n+    what about: MainMethod?\n+    \"\"\"\n+    print(\"mainMethod. Args \", args)\n+    CatalogMatchFinder(args)\n+#\n+\n+\n+def defineSpecificProgramOptions():\n+    \"\"\"\n+    defines specific program options\n+    \"\"\"\n+    #prepare the command line\n+    argparser = argparse.ArgumentParser()\n+    argparser.add_argument(\"--catalog\",  type = str, default = None, required=False,\n+                           help = 'path to the catalogue containing the \\\n+                           positions on the sky on  which we want to extract \\\n+                           the psfs')\n+    argparser.add_argument(\"--ra\",            type = str, default = 'RA', required=False,\n+                           help = 'name of the catalogue field containing ra \\\n+                           positions')\n+    argparser.add_argument(\"--dec\",           type = str, default = 'DEC', required=False,\n+                           help = 'name of the catalogue field containing \\\n+                           dec positions')\n+    argparser.add_argument(\"--objectid\",   type = str, default = 'OBJECT_ID', required=False,\n+                           help = 'name of the catalogue field containing \\\n+                           the object id')\n+    argparser.add_argument(\"--layercodes\", type = str, default =\n+                           None, required=False, help = 'path to the file \\\n+                           containing the list of SE image overlapping the \\\n+                           coadd, plus the matching psfs and catalogs')\n+    argparser.add_argument(\"--output\",        type = str, default =\n+                           \"./catalog_matches.fits\", required=False, help = 'path to the file \\\n+                           containing the list of layers.')\n+    argparser.add_argument(\"--clean_up\",        type = int, default = True,\n+                           help = 'remove all temporary garbage.')\n+    argparser.add_argument(\"--multiprocesses\", type = int, default = 1,\n+                           help = 'Number of parallel processes. Default = 1.')\n+    argparser.add_argument(\"--debug\", type = bool, default = False,\n+                           help = 'debug mode. Default = False.')\n+    return argparser\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/CatalogPsfExtractor.py": [
                        [
                            "@@ -0,0 +1,246 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+import sys\n+import os\n+import argparse\n+from EXT_PF1_GEN_P2.Fwhm import SEReader\n+from EXT_PF1_GEN_P2.Fwhm import PsfComposition\n+from EXT_PF1_GEN_P2.Fwhm.LayerIds import LayerIds\n+\n+import numpy\n+from astropy.io import fits\n+from astropy.table import Table\n+from astropy.table import Column, MaskedColumn\n+from astropy.io.fits import TableHDU, BinTableHDU\n+\n+\n+class CatalogPsfExtractor(LayerIds):\n+    \"\"\"\n+    Class to find the nearest psf of an object but considering the\n+    same layer composition (= stackset)\n+    \"\"\"\n+    def __init__(self, args = None):\n+        LayerIds.__init__(self)\n+\n+        if args is not None:\n+            table = Table.read(args.stackpsf, format='fits', hdu = args.tablename)\n+            ra = table[args.ra].data\n+            dec = table[args.dec].data\n+            \n+            if len(ra) < 1:\n+                #no objects. Don't start any processing\n+                raise Exception(args.stackpsf +\" has no objects. Terminate stack psf update.\")\n+            #\n+            sereader = SEReader.SEReader(args.SElist, args.band)\n+            #print(\"wcs \", sereader.wcs)\n+            pc =  PsfComposition.PsfComposition(ra, dec)\n+            wcs_dict = pc.measure_overlaps(sereader.wcs)\n+\n+            #make an object: stackpsf_tableindex, catalog ra, dec, idlist\n+            table_ids = list()\n+            for i in range(len(ra)):\n+                table_ids.append([i, ra[i], dec[i], [], \"\"])\n+            #\n+            table_ids = self.extract_layer_ids(wcs_dict, table_ids, layerid_key = 3)\n+            table_ids = self.encode_layer_ids(table_ids, layerid_key = 3)\n+\n+            tables = self.make_tables(table_ids)\n+\n+\n+            self.write(sereader.to_hdus(), tables, args.output)\n+        #\n+    #\n+\n+\n+    def write(self, wcshdus, bintables, outfile):\n+        for key in bintables:\n+            wcshdus.append(BinTableHDU(data = bintables[key], name = key))\n+        wcshdus.writeto(outfile, overwrite = 1)\n+    #\n+\n+\n+    def read(self, layercodes):\n+        \"\"\"\n+        read the encoded layer information back into a table\n+        \"\"\"\n+        layertable = Table()\n+        wcs_dict = dict()\n+        hdul = fits.open(layercodes)\n+        for hdu in hdul:\n+            if hdu.name.lower().startswith(\"layertable\"):\n+                #make this a proper table\n+                if layertable is None:\n+                    layertable = Table(hdu.data)\n+                else:\n+                    #append\n+                    layertable.add_columns( list( Table(hdu.data).itercols() ) )\n+                #\n+            else:\n+                header = hdu.header\n+                #print (\"header \", header)\n+                if \"index\" in header:\n+                    index = int(header[\"index\"])\n+                    wcs_dict[index] = dict()\n+\n+                    #redo secured values\n+                    header[\"NAXIS\"] = header[\"AXIS\"]\n+                    header[\"NAXIS1\"] = header[\"AXIS1\"]\n+                    header[\"NAXIS2\"] = header[\"AXIS2\"]\n+                    del header[\"AXIS\"]\n+                    del header[\"AXIS1\"]\n+                    del header[\"AXIS2\"]\n+\n+                    wcs_dict[index][\"WCS\"] = header\n+                    wcs_dict[index][\"file\"] = hdu.name\n+                #\n+            #\n+        #\n+        print(\"layertable no columns \", len(layertable.colnames))\n+        return wcs_dict, layertable\n+    #\n+\n+\n+    def maximum_number_of_objects_on_layer(self, sorted_table_ids):\n+        start_idx = 0\n+        current_id = sorted_table_ids[start_idx][4]\n+        max_number = -1\n+        counter = 0\n+        for i in range(len(sorted_table_ids)):\n+            if sorted_table_ids[i][4] !=  current_id:\n+                max_number = max(max_number, counter)\n+                current_id = sorted_table_ids[i][4]\n+                counter = 1\n+            else:\n+                counter = counter +1\n+            #\n+        #\n+        return max(max_number, counter)\n+    #\n+\n+\n+    def make_tables(self, table_ids, max_columns = 500):\n+        \"\"\"\n+        find the unique ids and put the corresponding\n+        ra/dec values into a table column\n+        \"\"\"\n+        column_ids = []\n+        def add_column(t, st, start_idx, i, current_id):\n+            \"\"\"\n+            adds a column to t, containing ra, dec, idx with the\n+            name current_id\n+            \"\"\"\n+            idx, ra, dec, _, _ = zip( *st[start_idx:i])\n+            c = Column([ra, dec, idx], name = str(current_id))\n+            t.add_column(c)\n+            column_ids.append(current_id)\n+        #\n+\n+        t= Table()\n+        column_counter = 0\n+        dict_of_tables = dict() #{tablename:table_data}\n+\n+        #sort the table ids by id\n+        st = sorted(table_ids, key=lambda idstring:idstring[4])\n+        \n+        if len(st) < 1:\n+            return dict_of_tables\n+        #how many unique ids do we have?\n+        start_idx = 0\n+        current_id = st[start_idx][4]\n+        next_col_name = 0\n+        for i in range(len(st)):\n+            if column_counter == max_columns:\n+                dict_of_tables[\"layertable_\" +str(len(dict_of_tables))] = t\n+                t = Table()\n+                column_counter = 0\n+            #\n+            if st[i][4] !=  current_id:\n+                #add a column to the table\n+                #print(\"st[start_idx:i] \", st[start_idx:i])\n+                add_column(t, st, start_idx, i, current_id)\n+\n+                start_idx = i\n+                current_id = st[start_idx][4]\n+                column_counter = column_counter + 1\n+            #\n+        #\n+        #add the last column\n+        add_column(t, st, start_idx, i+1, current_id)\n+\n+        print(\"column_ids \", len(column_ids), \", unique? \", len(set(column_ids)))\n+\n+        #add the last table\n+        dict_of_tables[\"layertable_\" +str(len(dict_of_tables))] = t\n+\n+        #dict_of_tables[dict_of_tables.keys()[0]].write(\"./layers.fits\", format = 'fits', overwrite = 1)\n+        return dict_of_tables\n+    #\n+\n+\n+if __name__ == \"__main__\":\n+    argparser = argparse.ArgumentParser()\n+\n+    if not len(sys.argv) > 1:\n+        argparser.print_help()\n+        sys.exit()\n+    args = argparser.parse_args()\n+    result = mainMethod(args)\n+#\n+\n+def mainMethod(args):\n+    CatalogPsfExtractor(args)\n+#\n+\n+\n+def defineSpecificProgramOptions():\n+    \"\"\"\n+    ERun EXT_PF1_GEN_P2  EXT_PF1_Catalog_PSF --SElist /data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/Main/analysis/PPO_DES_SC8_723_R1_v1_SDC-DE_TEST_DECAM_TILE_66269_0/bgsub_g.list --catalog /data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/Main/analysis/PPO_DES_SC8_723_R1_v1_SDC-DE_TEST_DECAM_TILE_66269_0/data/merged_mer_catalog.fits --ra X_WORLD --dec Y_WORLD\n+    \"\"\"\n+    #prepare the command line\n+    argparser = argparse.ArgumentParser()\n+    argparser.add_argument(\"--stackpsf\",  type = str, default = None, required=False,\n+                           help = 'path to the catalogue containing the \\\n+                           positions on the sky on  which we want to extract \\\n+                           the psfs. ')\n+    argparser.add_argument(\"--tablename\",  type = str, default = \"INFO\", required=False,\n+                           help = 'hdu name within stackpsf with the ra/dec values. ')\n+    argparser.add_argument(\"--ra\",            type = str, default = 'RA', required=False,\n+                           help = 'name of the catalogue field containing ra \\\n+                           positions')\n+    argparser.add_argument(\"--dec\",           type = str, default = 'DEC', required=False,\n+                           help = 'name of the catalogue field containing \\\n+                           dec positions')\n+    argparser.add_argument(\"--SElist\",        type = str, default =\n+                           None, required=False, help = 'path to the file \\\n+                           containing the list of SE image overlapping the \\\n+                           coadd, plus the matching psfs and catalogs')\n+    argparser.add_argument(\"--band\",        type = str, required=True,\n+                            help = 'band of the data to select from the SEList. \\\n+                           The band must be contained in the list.')\n+    argparser.add_argument(\"--output\",        type = str, default =\n+                           \"./layers.fits\", required=False, help = 'path to the file \\\n+                           containing the list of layers.')\n+    argparser.add_argument(\"--clean_up\",        type = int, default = True,\n+                           help = 'remove all temporary garbage.')\n+    argparser.add_argument(\"--multiprocesses\", type = int, default = 1,\n+                           help = 'Number of parallel processes. Default = 1.')\n+    argparser.add_argument(\"--debug\", type = bool, default = False,\n+                           help = 'debug mode. Default = False.')\n+    return argparser\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/CatalogPsfTreatment.py": [
                        [
                            "@@ -0,0 +1,130 @@\n+\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+import os\n+import sys\n+import argparse\n+\n+from astropy.table import Column, Table\n+from EXT_PF1_GEN_P2_LIBS.file import VariableEconomy as ve\n+from EXT_PF1_GEN_P2.Fwhm.CatalogPsfExtractor import CatalogPsfExtractor\n+from EXT_PF1_GEN_P2.Fwhm.CatalogMatchFinder import CatalogMatchFinder\n+from EXT_PF1_GEN_P2.Fwhm.StackPsfAndCatalogDebugger import StackPsfAndCatalogDebugger\n+\n+class CatalogPsfTreatment:\n+    \"\"\"\n+    Wrapper class to identify the best matching stacked psfs for objects in a reference catalog\n+    in a way to the matching psfs are close to the reference catalog objects but also have\n+    the same layer composition (= stackset)\n+    \"\"\"\n+    def __init__(self, args):\n+        \"\"\"\n+        Call the 3 processing steps\n+        \"\"\"\n+\n+        args_CatalogPsfExtractor = ve.Variables()\n+        args_CatalogPsfExtractor.SElist = args.SElist\n+        args_CatalogPsfExtractor.band = args.band\n+        args_CatalogPsfExtractor.stackpsf = args.stackpsf\n+        args_CatalogPsfExtractor.ra = args.ra\n+        args_CatalogPsfExtractor.dec = args.dec\n+        args_CatalogPsfExtractor.tablename = \"INFO\"\n+        args_CatalogPsfExtractor.output = args.outfolder + os.sep +\"layercodes_\" +args.band +\".fits\"\n+        CatalogPsfExtractor(args_CatalogPsfExtractor)\n+\n+\n+        args_CatalogMatchFinder = ve.Variables()\n+        args_CatalogMatchFinder.catalog = args.reference_catalog\n+        args_CatalogMatchFinder.ra = args.ra_ref\n+        args_CatalogMatchFinder.dec = args.dec_ref\n+        args_CatalogMatchFinder.objectid = \"OBJECT_ID\"\n+        args_CatalogMatchFinder.layercodes = args_CatalogPsfExtractor.output\n+        args_CatalogMatchFinder.output = args.outfolder + os.sep +\"catalog_matches_\" +args.band +\".fits\"\n+        CatalogMatchFinder(args_CatalogMatchFinder)\n+\n+\n+        args_StackPsfAndCatalogDebugger = ve.Variables()\n+        args_StackPsfAndCatalogDebugger.reference_catalog = args.reference_catalog\n+        args_StackPsfAndCatalogDebugger.ra = args.ra_ref\n+        args_StackPsfAndCatalogDebugger.dec = args.dec_ref\n+        args_StackPsfAndCatalogDebugger.match_catalog = args_CatalogMatchFinder.output\n+        args_StackPsfAndCatalogDebugger.stackpsf = args.stackpsf\n+        args_StackPsfAndCatalogDebugger.outfolder = args.outfolder\n+        args_StackPsfAndCatalogDebugger.modprefix = \"mod_\" +args.band +\"_\"\n+        StackPsfAndCatalogDebugger(args_StackPsfAndCatalogDebugger)\n+    #\n+#\n+\n+\n+\n+if __name__ == \"__main__\":\n+    argparser = argparse.ArgumentParser()\n+\n+    if not len(sys.argv) > 1:\n+        argparser.print_help()\n+        sys.exit()\n+    args = argparser.parse_args()\n+    result = mainMethod(args)\n+#\n+\n+def mainMethod(args):\n+    print(\"mainMethod. Args \", args)\n+    CatalogPsfTreatment(args)\n+\n+\n+#\n+\n+\n+def defineSpecificProgramOptions():\n+    #prepare the command line\n+    argparser = argparse.ArgumentParser()\n+    argparser.add_argument(\"--SElist\", type = str, default =\n+                           None, required=False, help = 'path to the file \\\n+                           containing the list of SE image overlapping the \\\n+                           coadd, plus the matching psfs and catalogs')\n+    argparser.add_argument(\"--band\", type = str, required=True,\n+                            help = 'band of the data to select from the SEList. \\\n+                           The band must be contained in the list.')\n+\n+    argparser.add_argument(\"--stackpsf\", type = str, default = None, required=False,\n+                           help = 'stackpsf that was the base of the match_catalog.')\n+    argparser.add_argument(\"--ra\", type = str, default = 'RA', required=False,\n+                           help = 'name of the stackpsf info table field containing ra \\\n+                           positions')\n+    argparser.add_argument(\"--dec\", type = str, default = 'DEC', required=False,\n+                           help = 'name of the stackpsf info table field containing \\\n+                           dec positions')\n+\n+    argparser.add_argument(\"--reference_catalog\", type = str, default = None, required=False,\n+                           help = 'path to the catalogue containing the \\\n+                           positions on the sky on  which we want to extract \\\n+                           the psfs')\n+    argparser.add_argument(\"--ra_ref\", type = str, default = 'RIGHT_ASCENSION', required=False,\n+                           help = 'name of the reference catalog field containing ra \\\n+                           positions')\n+    argparser.add_argument(\"--dec_ref\", type = str, default = 'DECLINATION', required=False,\n+                           help = 'name of the reference catalog field containing \\\n+                           dec positions')\n+\n+    argparser.add_argument(\"--outfolder\", type = str, default =\n+                           \"./\", required=False, help = 'folder with the results.')\n+    argparser.add_argument(\"--clean_up\", type = int, default = True,\n+                           help = 'remove all temporary garbage.')\n+\n+    argparser.add_argument(\"--debug\", type = bool, default = False,\n+                           help = 'debug mode. Default = False.')\n+    return argparser\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/FwhmValidation.py": [
                        [
                            "@@ -1,3 +1,19 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n from scipy import ndimage\n import numpy as np\n from astropy.io import fits\n@@ -18,10 +34,16 @@ from matplotlib import pyplot as plt\n import EXT_PF1_GEN_P2_LIBS.astronomy\n from EXT_PF1_GEN_P2_LIBS.astronomy import ArrayUtils\n import argparse\n+import logging\n \n class FwhmValidation():\n+    \"\"\"\n+    \n+    \"\"\"\n     \n-    def __init__(self, coadd, stackpsf, mer_star_cat, catalog_column, pixelsize):\n+    def __init__(self, coadd, stackpsf, mer_star_cat, **kwargs):\n+        catalog_column = kwargs.get(\"catalog_column\", \"TU_FNU_G_DECAM\")\n+        pixelsize = kwargs.get(\"pixelsize\", 0.26)\n         self.coadd = coadd\n         self.stackpsf = stackpsf\n         self.mer_star_cat = mer_star_cat\n@@ -29,14 +51,14 @@ class FwhmValidation():\n         self.pixelsize = pixelsize\n         self.plot_title = \"coadd \" +coadd.rsplit(os.sep, 1)[-1] \\\n           +\"\\nfwhm vs. stackpsf fwhm\"\n-    \n+\n     def read_tu_catalog(self, fn):\n         return Table.read(fn)\n \n     def flux_2_mag(self, flux):\n-        mag = (2.5 * (23. - np.log10(flux)) - 48.6) \n+        mag = (2.5 * (23. - np.log10(flux)) - 48.6)\n         return mag\n-        \n+\n     def match(self, cRA_REF, cDec_REF, cRA, cDec):\n         \"\"\"\n         Catalog matching\n@@ -63,10 +85,10 @@ class FwhmValidation():\n \n         idx_REF, d2d_REF, __ = coo.match_to_catalog_sky(coo_REF)\n         return idx_REF, d2d_REF\n-        \n-    \n+\n+\n     def fit_fwhm(self, low_mag_limit = 19, high_mag_limit = 24, distance_limit = 0.1):\n-    \n+\n         #read the table\n         hdul = fits.open(self.stackpsf)\n         stackpsf_table = hdul[\"INFO\"].data\n@@ -75,22 +97,22 @@ class FwhmValidation():\n         stackpsf_image = hdul[\"IMAGE\"].data\n         stamp = hdul[\"IMAGE\"].header[\"STMPSIZE\"]\n         s_2 = int(stamp//2)\n-        \n+\n         #read the coadd image\n         hdul = fits.open(self.coadd)\n         coadd_image = hdul[\"IMAGE\"].data\n \n         #loop the stackpsf and try a fit\n         gridxy = zip(stackpsf_table[\"GRIDX\"], stackpsf_table[\"GRIDY\"], stackpsf_table[\"X\"], stackpsf_table[\"Y\"], stackpsf_table[\"RA\"], stackpsf_table[\"DEC\"])\n-        \n+\n \n         #and the refcat\n         refcat = self.read_tu_catalog(self.mer_star_cat)\n \n         refcat[\"RA\"].unit  = u.deg\n         refcat[\"DEC\"].unit  = u.deg\n-        \n-        \n+\n+\n         val = {\"stack\":[], \"coadd\":[]}\n         elongated = 0\n         _all = 0\n@@ -100,28 +122,28 @@ class FwhmValidation():\n                 #only between mag 19 and mag 24 (flux 1e-4 t0 1e-6)\n                 #only close matches: d2d.dms = [deg, min, sec]\n                 #only non elongated fits (from the coadd)\n-                #    \n+                #\n                 idx, d2d = self.match(refcat[\"RA\"], refcat[\"DEC\"], Column(ra, unit = u.deg), Column(dec, unit = u.deg))\n                 #mag = flux_2_mag(refcat[\"TU_FNU_VIS\"][idx])\n                 decam_g = self.flux_2_mag(refcat[self.catalog_column][idx])\n                 #two_mass = refcat[\"TU_MAG_H_2MASS\"][idx]\n-        \n+\n                 _all = _all +1\n                 if decam_g > low_mag_limit and decam_g < high_mag_limit and (d2d.dms[0] == 0.0 and d2d.dms[1] == 0.0 and d2d.dms[2] < distance_limit):\n                     #cut out the stackpsf stamp: y:x\n                     cut = stackpsf_image[ int(gridy - s_2): int(gridy + s_2), int(gridx - s_2): int(gridx + s_2) ]\n-                \n-                    au = ArrayUtils.ArrayUtils() \n+\n+                    au = ArrayUtils.ArrayUtils()\n                     fit = au.fit_gauss_2d(cut)\n                     val1 = fit[2].mean()\n-            \n+\n                     #coadd cut is also y:x\n                     imgcut = coadd_image[int(y - s_2) : int(y + s_2), int(x - s_2) : int(x + s_2)]\n \n-                    au = ArrayUtils.ArrayUtils() \n+                    au = ArrayUtils.ArrayUtils()\n                     fit = au.fit_gauss_2d(imgcut)\n                     val2 = fit[2].mean()\n-            \n+\n                     if abs(fit[2][0] - fit[2][1])/fit[2][0] > 0.1:\n                         #use only round stars nothing elongated\n                         elongated = elongated + 1\n@@ -133,59 +155,68 @@ class FwhmValidation():\n             #\n             except Exception as e:\n                 #print (\"Exception. Skipped \", e)\n-                pass\n+                logging.debug(\"Exception. Skipped %s\", str(e))\n+            #\n+        #\n         print(\"elongated \", elongated, \" of \", _all)\n         return val[\"stack\"], val[\"coadd\"]\n     #\n-    \n+\n     def get_statistics(self, stack_fwhm, coadd_fwhm, in_arcsec = True):\n         #select both within 10 sigma\n-        sel_coadd = (np.abs(coadd_fwhm - np.median(coadd_fwhm)) < 10*mad(coadd_fwhm)) \n+        sel_coadd = (np.abs(coadd_fwhm - np.median(coadd_fwhm)) < 10*mad(coadd_fwhm))\n         sel_stack = (np.abs(stack_fwhm - np.median(stack_fwhm)) < 10*mad(stack_fwhm))\n         sel = np.all( [sel_coadd, sel_stack], axis = 0)\n \n-        \n+\n         conv_factor = 1.0\n         if in_arcsec:\n             #gauss fit sigma to fwhm\n             #fwhm from pixel to arsecs\n-            conv_factor = 2.35*self.pixelsize \n+            conv_factor = 2.35*self.pixelsize\n         #\n-        \n+\n         coadd = conv_factor * np.array(coadd_fwhm)[sel]\n         stack = conv_factor * np.array(stack_fwhm)[sel]\n-        \n-        \n+\n+\n         median_coadd = (np.median(coadd), mad(coadd))\n         median_stack = (np.median(stack), mad(stack))\n-        \n+\n         mean_coadd = (coadd.mean(), coadd.std())\n         mean_stack = (stack.mean(), stack.std())\n-        \n+\n         print(\"median coadd \", np.median(coadd) , \" +/- \",  mad(coadd))\n         print(\"median psf \", np.median(stack) , \" +/- \",  mad(stack))\n \n         print(\"mean coadd \", coadd.mean() , \" +/- \",  coadd.std())\n         print(\"mean psf \", stack.mean() , \" +/- \",  stack.std())\n \n-        print(\"offset (percent based on median) \",  (np.median(stack) - np.median(coadd))/np.median(stack) ) \n+        print(\"offset (percent based on median) \",  (np.median(stack) - np.median(coadd))/np.median(stack) )\n         print(\"samplesize \", len(coadd) )\n-        \n+\n         return coadd, stack, median_coadd, median_stack, mean_coadd, mean_stack\n     #\n-    \n-    def plot_results(self, coadd, stack, median_coadd, median_stack, mean_coadd, mean_stack, outfile):\n+\n+    def plot_results(self, coadd, stack, outfile, **statistics):\n         \"\"\"\n         coadd fwhm from the coadd as list\n         stack fwhm from the psfstack as list\n+\n+        **statistics:\n         median_coadd tuple (median, mad)\n-        (...)\n+        median_stack tuple (median, mad)\n         mean_coadd tuple (mean, stddev)\n-        (...)\n+        mean_stack tuple (mean, stddev)\n         \"\"\"\n-        \n+\n+        median_coadd = statistics.get(\"median_coadd\")\n+        median_stack = statistics.get(\"median_stack\")\n+        mean_coadd = statistics.get(\"mean_coadd\")\n+        mean_stack = statistics.get(\"mean_stack\")\n+\n         #convert pixel values to arcsecs\n-        \n+\n         fig, ax = plt.subplots()\n \n         ax.plot(coadd, stack, 'r.')\n@@ -195,7 +226,7 @@ class FwhmValidation():\n         ax.set_ylim(0.5,1.5)\n         ax.set_xlabel(\"coadd psf (arcsec)\")\n         ax.set_ylabel(\"stack psf (arcsec)\")\n-        \n+\n         median_coadd_str = \"{:.4}\".format(median_coadd[0])\n         mad_coadd_str = \"{:.4}\".format(median_coadd[1])\n \n@@ -216,20 +247,20 @@ class FwhmValidation():\n         annotation = annotation +\"mean psf \" + mean_psf_str + \" +/- \" +  std_psf_str +\"\\n\"\n \n \n-        annotation = annotation +\"offset (percent based on median) \" +  \"{:.2%}\".format( (np.median(stack) - np.median(coadd))/np.median(stack) ) \n+        annotation = annotation +\"offset (percent based on median) \" +  \"{:.2%}\".format( (np.median(stack) - np.median(coadd))/np.median(stack) )\n         annotation = annotation + \"\\n\" +\"samplesize \" +str(len(coadd))\n \n         plt.text(1.98,0.52, annotation, ha = 'right')\n         plt.title(self.plot_title)\n-        \n+\n         plt.savefig(outfile)\n         plt.close()\n-  \n-  \n-  \n+\n+\n+\n if __name__ == \"__main__\":\n     argparser = argparse.ArgumentParser()\n-    \n+\n     if not len(sys.argv) > 1:\n         argparser.print_help()\n         sys.exit()\n@@ -239,12 +270,13 @@ if __name__ == \"__main__\":\n \n def mainMethod(args):\n     print(\"mainMethod. Args \", args)\n-    fwhmval  = FwhmValidation(args.coadd, args.psfstack, args.star_cat, args.star_cat_column, args.pixelsize)\n+    fwhmval  = FwhmValidation(args.coadd, args.psfstack, args.star_cat, catalog_column= args.star_cat_column, pixelsize = args.pixelsize)\n+\n     import time\n     a = time.time()\n     stack_fwhm, coadd_fwhm = fwhmval.fit_fwhm()\n     coadd, stack, median_coadd, median_stack, mean_coadd, mean_stack = fwhmval.get_statistics(stack_fwhm, coadd_fwhm)\n-    fwhmval.plot_results(coadd, stack, median_coadd, median_stack, mean_coadd, mean_stack, args.outfile)\n+    fwhmval.plot_results(coadd, stack, args.outfile, median_coadd = median_coadd, median_stack = median_stack, mean_coadd = mean_coadd, mean_stack = mean_stack)\n     print(\"time for execution of FwhmValidation \", time.time() - a)\n #\n \n@@ -252,20 +284,19 @@ def mainMethod(args):\n def defineSpecificProgramOptions():\n     #prepare the command line\n     argparser = argparse.ArgumentParser()\n-    argparser.add_argument(\"--coadd\",      type = str, default = '', \n+    argparser.add_argument(\"--coadd\",      type = str, default = '',\n                            help = 'tile name')\n-    argparser.add_argument(\"--psfstack\",        type = str, default = \n+    argparser.add_argument(\"--psfstack\",        type = str, default =\n                            '',     help = 'stackpsf fits file')\n-    argparser.add_argument(\"--star_cat\", type = str, default = \n-                           '',     help = '(MER) star catalogs with fluxes.')                       \n-    argparser.add_argument(\"--star_cat_column\", type = str, default = '', \n+    argparser.add_argument(\"--star_cat\", type = str, default =\n+                           '',     help = '(MER) star catalogs with fluxes.')\n+    argparser.add_argument(\"--star_cat_column\", type = str, default = '',\n                            help = 'column of the star_cat to use for fluxes, like TU_FNU_G_DECAM')\n-    argparser.add_argument(\"--pixelsize\", type = float, default = 0.26, \n+    argparser.add_argument(\"--pixelsize\", type = float, default = 0.26,\n                            help = 'Pixelsize in arcseconds.')\n-    argparser.add_argument(\"--outfile\", type = str, default = \"./fwhm_val.jpg\", \n-                           help = 'Full filename incl. path for the result.')                       \n-    argparser.add_argument(\"--debug\", type = bool, default = False, \n+    argparser.add_argument(\"--outfile\", type = str, default = \"./fwhm_val.jpg\",\n+                           help = 'Full filename incl. path for the result.')\n+    argparser.add_argument(\"--debug\", type = bool, default = False,\n                            help = 'debug mode. Default = False.')\n     return argparser\n #\n-  \n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/LayerIds.py": [
                        [
                            "@@ -0,0 +1,62 @@\n+import numpy\n+\n+class LayerIds:\n+    \n+    def __init__(self):\n+        pass\n+    #\n+    \n+    def extract_layer_ids(self, wcs_dict, table_ids, layerid_key = 3):\n+        \"\"\"\n+            wcs_dict: {key:{file:, wcs:, contains}}\n+            for every image:\n+                add the image id to the catalog object\n+            finally: sort the ids and turn them into a string\n+            \n+            table_ids = list of tuples  (stackpsf_tableindex, ra, dec, [id1, id2 ... ], string of ids id1_id2_...) \n+        \"\"\"\n+        print(\"extract layer ids\")\n+        for key in wcs_dict.keys():\n+            #this is a loop over image_ids\n+            #print(\"wcs_dict \", wcs_dict[key][\"contains\"])\n+            \n+            #true_ids is an array of indices where the object is on the image\n+            true_ids = numpy.where(numpy.array(wcs_dict[key][\"contains\"]) == True)[0]\n+            \n+            for _id in true_ids:\n+                table_ids[_id][layerid_key].append(key)\n+            #\n+        #\n+        return table_ids\n+    #\n+    \n+    def encode_layer_ids(self, table_ids, layerid_key = 3):\n+        \"\"\"\n+        grab the list of ids, sort them an generate\n+        a string. Store the sting id in colum 3\n+        \"\"\"\n+        print(\"encode layer ids\")\n+        for table_id in range( len(table_ids)):\n+            if len(table_ids[table_id][layerid_key]) > 0:\n+                table_ids[table_id][layerid_key +1] = self.generate_layer_id( sorted(table_ids[table_id][layerid_key]) )\n+        return table_ids\n+    #\n+    \n+    def generate_layer_id(self, sorted_idlist):\n+        \"\"\"\n+        get an array of layer ids and generate \n+        a \"_\" separated string.\n+        Return the hash of the string as layer_id, as \n+        otherwise in clear text it may become too long\n+        for a column name. \n+        Replace also \"-\" with \"_\"\n+        \"\"\"\n+        layer_id = \"\"\n+        for item in sorted_idlist:\n+            layer_id = layer_id + str(item) +\"_\"\n+        hashed_layer_id = str( hash( layer_id[:-1] ) )\n+\n+        #remove a \"-\" as that is not allowed in fits column names            \n+        return hashed_layer_id.replace(\"-\", \"_\")\n+    #\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/PsfComposition.py": [
                        [
                            "@@ -0,0 +1,24 @@\n+from astropy.coordinates import SkyCoord\n+from astropy.wcs import WCS\n+\n+class PsfComposition:\n+    \n+    def __init__(self, catalog_ra, catalog_dec):\n+        #prepare a datastructure that contains the values\n+        self.ra = catalog_ra\n+        self.dec = catalog_dec\n+        \n+    #\n+    \n+    def measure_overlaps(self, wcs_dict): \n+        c = SkyCoord(self.ra, self.dec, unit=\"deg\")\n+        \n+        #find all ra/dec overlapping the single images\n+        for key in wcs_dict.keys():\n+            wcs = WCS(wcs_dict[key][\"WCS\"])\n+            \n+            wcs_dict[key][\"contains\"] = list(wcs.footprint_contains(c))\n+        #\n+        return wcs_dict\n+    #\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/SEReader.py": [
                        [
                            "@@ -0,0 +1,51 @@\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+from astropy.wcs import WCS\n+from astropy.io import fits\n+from astropy.io.fits import HDUList\n+from astropy.io.fits import ImageHDU\n+import numpy\n+\n+class SEReader:\n+    wcs = dict()\n+    \n+    def __init__(self, coaddswarplist, band):\n+        #generate the pylist of the SE files (of a band)\n+        selist = coaddswarplist\n+        f = open(coaddswarplist, 'r')\n+        files = f.readlines()\n+        f.close()\n+        counter = 0\n+        for line in files:\n+            if band not in line:\n+                continue\n+            self.wcs[counter] = {\"file\":line.split(\" \", 1)[0]}\n+            self.wcs[counter][\"WCS\"] = self.generate_wcs(self.wcs[counter][\"file\"])\n+            counter = counter + 1\n+        #\n+    #\n+    \n+    def generate_wcs(self, f):\n+        imgh = FitsServices.get_image_hdu(f)\n+        hdul = fits.open(f)\n+        header = hdul[imgh].header\n+        #remove the trouble\n+        if \"MJD-OBS\" in header:\n+            del header[\"MJD-OBS\"]\n+        return header\n+    #\n+    \n+    def to_hdus(self):\n+        hdul = HDUList()\n+        for counter in self.wcs:\n+            header = self.wcs[counter][\"WCS\"]\n+            #header[\"SEfile\"] = self.wcs[counter][\"file\"]\n+            header[\"index\"] = counter\n+            #secure vital values that astropy is going to delete \n+            #without a matching data array\n+            header[\"AXIS\"] = header[\"NAXIS\"]\n+            header[\"AXIS1\"] = header[\"NAXIS1\"]\n+            header[\"AXIS2\"] = header[\"NAXIS2\"]\n+            \n+            hdul.append(ImageHDU(header = header, name = self.wcs[counter][\"file\"]))\n+        #\n+        return hdul\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/StackPsfAndCatalogDebugger.py": [
                        [
                            "@@ -0,0 +1,234 @@\n+#!/usr/bin/env python3       \n+\n+import sys\n+import os\n+import argparse\n+import traceback\n+import math\n+\n+from EXT_PF1_GEN_P2.Fwhm import SEReader\n+from EXT_PF1_GEN_P2.Fwhm import PsfComposition\n+from EXT_PF1_GEN_P2.Fwhm import CatalogMatchFinder\n+from EXT_PF1_GEN_P2.Fwhm.LayerIds import LayerIds\n+\n+import numpy\n+\n+from astropy.table import Table\n+from astropy.table import Column\n+from astropy.io import fits\n+from astropy.io.fits import PrimaryHDU, BinTableHDU, ImageHDU, HDUList\n+from astropy.wcs import WCS\n+from astropy.coordinates import SkyCoord\n+\n+\n+\n+class StackPsfAndCatalogDebugger:\n+    \"\"\"\n+    Based on the match_catalog between a reference catalog (MER) and \n+    the psfs in stackpsf, \n+    - removes all objects without match from the reference catalog\n+    - sets the precise coordinates of the matched objects to the stackpsf info table\n+    \n+    Both, reference catalog and stackpfs are copied and saved with the modifications.\n+    \n+    \"\"\"\n+    def __init__(self, args = None):\n+        if args is not None:\n+            refcat = self.read_catalog(args.reference_catalog, hdu = -1)\n+            cmf = CatalogMatchFinder.CatalogMatchFinder()\n+            refcat = cmf.add_objid_to_catalog(refcat)\n+            \n+            matchcat = self.read_catalog(args.match_catalog, hdu = -1)\n+            \n+            stackpsf_infotable = self.read_catalog(args.stackpsf, hdu = \"INFO\")\n+            hdul = fits.open(args.stackpsf)\n+            \n+            stackpsf_image = hdul[\"IMAGE\"].data\n+            stackpsf_header = hdul[\"IMAGE\"].header\n+            hdul.close()\n+            \n+            clean_refcat, clean_matchcat = self.clean_reference_catalog(refcat, matchcat)\n+            \n+            self.save_catalog(clean_refcat, args.reference_catalog, args.outfolder, args.modprefix)\n+            self.save_catalog(clean_matchcat, args.match_catalog, args.outfolder, args.modprefix)\n+            \n+            new_image, new_infotable = self.rebuild_stackpsf(clean_matchcat, stackpsf_infotable, stackpsf_image, stackpsf_header)\n+            \n+            self.save_stackpsf(new_image, new_infotable, stackpsf_header, args.outfolder +os.sep +\"mod_\" +args.stackpsf.rsplit(os.sep, 1)[-1] )\n+        #\n+    #\n+    \n+    \n+    def save_stackpsf(self, new_image, new_infotable, header, outname):\n+        print(\"write modified stackpsf to \", outname)\n+        hdu0 = fits.PrimaryHDU()\n+        \n+        hdu1 = fits.ImageHDU(data = new_image, header = header)\n+        hdu2 = fits.BinTableHDU(data = new_infotable)\n+        fits.HDUList([hdu0, hdu1, hdu2]).writeto(outname, overwrite=True)\n+        \n+        fits.setval(outname, 'extname', value='IMAGE', ext=1)\n+        fits.setval(outname, 'extname', value='INFO', ext=2)\n+    #\n+    \n+    \n+    def rebuild_stackpsf(self, clean_matchcat, stackpsf_infotable, stackpsf_image, stackpsf_header):\n+        \"\"\"\n+        array orientation:\n+        stackpsf_image.shape = (y,x)\n+        \"\"\"\n+        stampsize = int(stackpsf_header[\"STMPSIZE\"])\n+        step = stampsize//2\n+        \n+        wcs = WCS(stackpsf_header)\n+        \n+        #we may have double references, so recalculate the image sizeS\n+        no_objects = clean_matchcat[\"stackpsf index\"].data.shape[0]\n+        img_shape = math.ceil(math.sqrt(no_objects))*stampsize\n+        \n+        print(\"new image shape \", img_shape, \", before \", stackpsf_image.shape)\n+        new_image = numpy.zeros((img_shape, img_shape))\n+        new_infotable = stackpsf_infotable[0: 0].copy()\n+        if \"BANDPASS\" in new_infotable.colnames:\n+            #stackpsf v1\n+            new_infotable.remove_column(\"BANDPASS\")\n+        new_infotable.add_column(Column(), name = \"distance psf-best match (arcsec)\")\n+        \n+        #print(\"info \", clean_matchcat.info)\n+        \n+        clean_matchcat.sort(\"stackpsf index\")\n+        new_image_x = step +1\n+        new_image_y = step +1\n+        (sizey, sizex) = new_image.shape\n+        \n+        def new_image_coordinates(old_x, old_y):\n+            new_x = old_x + stampsize\n+            new_y = old_y\n+            if new_x >= sizex:\n+                new_x = step +1\n+                new_y = old_y +stampsize\n+            return new_x, new_y\n+        #\n+        row_counter_new_info = 0\n+        for row in clean_matchcat:\n+            ra = row[\"catalog ra\"]\n+            dec = row[\"catalog dec\"]\n+            pix = wcs.all_world2pix(ra, dec, 1)\n+            x = pix[0]\n+            y = pix[1]\n+            \n+            stackpsf_infotable_row = stackpsf_infotable[int(row[\"stackpsf index\"])]\n+            stackpsf_image_x = int(stackpsf_infotable_row[\"GRIDX\"])\n+            stackpsf_image_y = int(stackpsf_infotable_row[\"GRIDY\"])\n+            \n+            \n+            stamp = stackpsf_image[stackpsf_image_y - step -1: stackpsf_image_y + step, stackpsf_image_x - step -1:stackpsf_image_x + step]\n+            \n+            #put the stamp into the new new_image\n+            new_image[new_image_y - step -1: new_image_y + step, new_image_x - step -1: new_image_x + step] = stamp\n+            #print(\"take from \", stackpsf_image_x, stackpsf_image_y, \" put to \", new_image_x, new_image_y,  \"index \", row[\"stackpsf index\"],\" value \", stamp[25][25])\n+            \n+            #fill the row in the new_infotable\n+            #new_infotable.add_row((ra, dec, x, y, new_image_x, new_image_y, bool(float(row[\"distance arcsec\"]) < 0.5)))\n+            new_infotable.add_row((ra, dec, x, y, new_image_x, new_image_y, row[\"distance arcsec\"]))\n+            \n+            \n+            #and get the coordinates for the next step\n+            new_image_x, new_image_y = new_image_coordinates(new_image_x, new_image_y)\n+        #\n+        \n+        return new_image, new_infotable\n+    #\n+    \n+    \n+    def read_catalog(self, catalog, hdu = -1):\n+        \"\"\"\n+        we assume the hdu with data is the last one\n+        \"\"\"\n+        return Table.read(catalog, format='fits', hdu = hdu)\n+    #\n+    \n+    \n+    def clean_reference_catalog(self, refcat, match_catalog):\n+        \"\"\"\n+        Remove all missed matches from the reference catalog and the match catalog.\n+        Save the catalog under the name mod_refcat\n+        \"\"\"\n+        #find the match_catalog entries with no stackpsf_idx \n+        #extract the object_id of the refcat\n+        #delete those rows in both catalogs\n+        #record the numbers\n+        refact_objects_to_delete = match_catalog[match_catalog[\"stackpsf index\"] == -1][\"catalog object id\"]\n+        if refact_objects_to_delete.data.shape[0] == 0:\n+            return refcat, match_catalog\n+        #\n+        \n+        m = refcat[\"OBJECT_ID\"] != refact_objects_to_delete[0]\n+        for od in refact_objects_to_delete[1:]:\n+            #is there a smarter integral way?\n+            m *= refcat[\"OBJECT_ID\"] != od\n+        clean_refcat = refcat[ m]            \n+        clean_match_catalog = match_catalog[(match_catalog[\"stackpsf index\"] != -1)]\n+        \n+        return clean_refcat, clean_match_catalog\n+    #\n+    \n+    def save_catalog(self, catalog_table, filename, outfolder, modprefix = \"mod_\"):\n+        splits = [\"\", filename]\n+        if os.sep in filename:\n+            splits = filename.rsplit(os.sep, 1)\n+        #\n+        newfile = outfolder +os.sep +modprefix +splits[-1]\n+        print(\"save catalog to \", newfile)\n+        catalog_table.write(newfile, format='fits', overwrite = 1)\n+#\n+\n+\n+if __name__ == \"__main__\":\n+    argparser = argparse.ArgumentParser()\n+    \n+    if not len(sys.argv) > 1:\n+        argparser.print_help()\n+        sys.exit()\n+    args = argparser.parse_args()\n+    result = mainMethod(args)\n+#\n+\n+def mainMethod(args):\n+    print(\"mainMethod. Args \", args)\n+    cpe  = StackPsfAndCatalogDebugger(args)\n+    \n+    \n+#\n+\n+\n+def defineSpecificProgramOptions():\n+    #prepare the command line\n+    argparser = argparse.ArgumentParser()\n+    argparser.add_argument(\"--reference_catalog\",  type = str, default = None, required=False,\n+                           help = 'path to the catalogue containing the \\\n+                           positions on the sky on  which we want to extract \\\n+                           the psfs')\n+    argparser.add_argument(\"--ra\",            type = str, default = 'RA', required=False,\n+                           help = 'name of the catalogue field containing ra \\\n+                           positions')\n+    argparser.add_argument(\"--dec\",           type = str, default = 'DEC', required=False,\n+                           help = 'name of the catalogue field containing \\\n+                           dec positions')\n+    argparser.add_argument(\"--stackpsf\",  type = str, default = None, required=False,\n+                           help = 'stackpsf that was the base of the match_catalog.')\n+    argparser.add_argument(\"--match_catalog\",        type = str, default = \n+                           \"./catalog_matches.fits\", required=False, help = 'path to the file \\\n+                           containing the match between catalog and stackpsf.')\n+    argparser.add_argument(\"--modprefix\", type = str, default = \"mod_\",\n+                           required=False, help = 'prefix for the modified data.')\n+    argparser.add_argument(\"--outfolder\", type = str, default = \n+                           \"./\", required=False, help = 'folder with the results.')\n+    argparser.add_argument(\"--clean_up\",        type = int, default = True,\n+                           help = 'remove all temporary garbage.')\n+    argparser.add_argument(\"--multiprocesses\", type = int, default = 1, \n+                           help = 'Number of parallel processes. Default = 1.')\n+    argparser.add_argument(\"--debug\", type = bool, default = False, \n+                           help = 'debug mode. Default = False.')\n+    return argparser\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm/__init__.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Masks/Masks.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Masks/__init__.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Pipeline/ProcessManager.py": [
                        [
                            "@@ -0,0 +1,53 @@\n+import multiprocessing as mp\n+import time\n+\n+class ProcessManager:\n+    \n+    def __init__(self, number_of_Processes):\n+        self.number_of_Processes = number_of_Processes\n+        \n+        #initialize process dict\n+        self.processes = dict()\n+        for i in range(self.number_of_Processes):\n+            self.processes[str(i)] = None\n+        #\n+    #\n+    \n+    def add_process_and_start(self, method, args):\n+        pnumbers = self.free()\n+        if len(pnumbers) > 0:\n+            p = mp.Process(target=method, args=args)            \n+            p.start()\n+            p.join()\n+            self.processes[ pnumbers[0] ] = p\n+            \n+            self.block_until_free_process()\n+            return True \n+        #\n+        return False\n+    #\n+        \n+    def free(self):\n+        \"\"\"\n+        count the number of free processes\n+        \"\"\"\n+        pnumbers = list()\n+        for k in self.processes.keys():\n+            if self.processes[k] is None:\n+                pnumbers.append(k)\n+            elif not self.processes[k].is_alive():\n+                self.processes[k] = None\n+                pnumbers.append(k)\n+            #\n+        return pnumbers\n+    #\n+    \n+    def block_until_free_process(self):\n+        while len(self.free()) == 0:\n+            time.sleep(5)\n+        #\n+        return\n+                \n+        \n+                        \n+                                        \n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Pipeline/RDisk_coadd_stage2.py": [
                        [
                            "@@ -17,18 +17,31 @@ from __future__ import unicode_literals\n \n \n import os\n+import sys\n+import traceback\n+\n from EXT_PF1_GEN_P2_LIBS.Pipeline.RDisk_coadd import RDisk_coadd\n-from AstrometricValidation import AstrometricValidation\n+from EXT_PF1_GEN_P2.AstrometricValidation import AstrometricValidation\n+from EXT_PF1_GEN_P2.Fwhm import CatalogPsfTreatment\n+\n \n from astropy.io import fits as astrofits\n+from astropy.table import Table, Column\n+\n import numpy as np\n from fitsio import FITS,FITSHDR\n from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n-from EXT_PF1_Coadd.Masks import Masks\n- \n+from EXT_PF1_GEN_P2_LIBS.file import VariableEconomy as ve\n+from EXT_PF1_GEN_P2.psf import Psfid\n \n-import tarfile\n+from EXT_PF1_GEN_P2.Masks import Masks\n \n+import logging\n+ \n+import shutil\n+import tarfile\n+import random\n+        \n class RDisk_coadd_stage2(RDisk_coadd):\n     \"\"\"\n     This class contains methods that are special to the rdisk_coadd_pipeline.\n@@ -38,6 +51,11 @@ class RDisk_coadd_stage2(RDisk_coadd):\n     def __init__(self):\n         RDisk_coadd.__init__(self)\n         self.validator = self\n+        self.skip_astrometric_validation = 0\n+        self.background_extension = \"_background.fits\"\n+        self.SExtractor_bg = \"SExtractor_bg\"\n+        self.Swarp_bg = \"Swarp_bg\"\n+        \n     #\n     \n     \n@@ -52,10 +70,92 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         return self\n     #\n     \n-    def astrometric_validation(self, se_cat, ref_cat, val_dir, wcs_header, se_catalog_x, se_catalog_y):\n+    def update_stackpsf_with_reference_catalog_positions(self, selist, band, stackpsf, reference_catalog, outfolder, **kwargs):\n+        \"\"\"\n+        takes the stacked psfs of stackpsf and finds for every reference catalog position the closest\n+        stecksdpsf with the same stackset (= same layer of coadded single epoch images). \n+        For those closest stacked psfs the image and the info table of stackpsf are remade so that every \n+        reference catalog object gets a psf at the reference catalog position.\n+        \n+        parameters:\n+        @param selist path to the file containing the list of SE image overlapping the\n+                      coadd, plus the matching psfs and catalogs\n+        @param band band of the stackpsf data to select from the SEList.\n+                    The band must be contained in the list.\n+        @param stackpsf stackpsf file of the correct band\n+        @param reference_catalog path to the catalogue containing the\n+                                 positions on the sky from  which we want to extract\n+                                 the psfs\n+        @param outfolder folder where the results are stored\n+        \n+        \n+        kwargs:\n+        @param tablename the extname of the infotable in the stackpsf fitsfile\n+                         Default is INFO\n+        @param ra name of the stackpsf info table field containing ra\n+                  positions. Default is RA\n+        @param dec name of the stackpsf info table field containing\n+                   dec positions. Default is DEC\n+        @param ra_ref name of the reference catalog field containing ra \n+                      positions. Default is RIGHT_ASCENSION\n+        @param dec_ref name of the reference catalog field containing dec \n+                       positions. Default is DECLINATION\n+        @param clean_up remove all temporary garbage. Default is True\n+        @param debug debug mode. Default is False\n+        \"\"\"\n+        try:\n+            args_CatalogPsfTreatment = ve.Variables()\n+            args_CatalogPsfTreatment.SElist = selist\n+            args_CatalogPsfTreatment.band = band\n+            args_CatalogPsfTreatment.stackpsf = stackpsf\n+            \n+            args_CatalogPsfTreatment.reference_catalog = reference_catalog\n+            args_CatalogPsfTreatment.outfolder = outfolder\n+            \n+            \n+            args_CatalogPsfTreatment.ra = kwargs.get(\"ra\", \"RA\")\n+            args_CatalogPsfTreatment.dec = kwargs.get(\"dec\", \"DEC\")\n+            args_CatalogPsfTreatment.ra_ref = kwargs.get(\"ra_ref\", \"RIGHT_ASCENSION\")\n+            args_CatalogPsfTreatment.dec_ref = kwargs.get(\"dec_ref\", \"DECLINATION\")\n+            \n+            args_CatalogPsfTreatment.clean_up = kwargs.get(\"clean_up\", True)\n+            args_CatalogPsfTreatment.debug = kwargs.get(\"debug\", False)\n+            \n+            args_CatalogPsfTreatment.tablename = kwargs.get(\"tablename\", \"INFO\")\n+            args_CatalogPsfTreatment.output = outfolder + os.sep +\"layercodes_\" +band +\".fits\"\n+            \n+            CatalogPsfTreatment.CatalogPsfTreatment(args_CatalogPsfTreatment)\n+        except Exception as e:\n+            #don't fail the pipeline here, just print the stack trace\n+            #for later repair\n+            print('exception caught by update_stackpsf_with_reference_catalog_positions:\\n', e)\n+            exec_info =  sys.exc_info()\n+            traceback.print_tb(exec_info[2])\n+        #\n+    #\n+        \n+        \n+    \n+    def select_reference_catalog(self, coadd_cat, ref_cat, band):\n+        if not os.path.exists(ref_cat):\n+            return coadd_cat, \"ALPHAMODEL_J2000\", \"DELTAMODEL_J2000\", \"MAG_PSF\"\n+        return ref_cat, \"X_WORLD\", \"Y_WORLD\", band\n+    #\n+    \n+    def astrometric_validation(self, se_cat, ref_cat, val_dir, wcs_header, **kwargs):\n+        se_catalog_x = kwargs.get(\"se_catalog_x\", \"X_WORLD\")\n+        se_catalog_y = kwargs.get(\"se_catalog_y\", \"Y_WORLD\")\n+        band = kwargs.get(\"band\")\n+        \n         #always create a new object\n+        if not os.path.exists(ref_cat):\n+            if not self.skip_astrometric_validation:\n+                print(\"Reference catalog \" +ref_cat +\" does not exist: skip astrometric validation.\")\n+                self.skip_astrometric_validation = 1\n+            return self.skip_astrometric_validation\n+        #\n         av = AstrometricValidation.AstrometricValidation(se_cat, ref_cat)\n-        d2d, match_catalog = av.astropy_validation(se_catalog_x, se_catalog_y, wcs_header)\n+        d2d, match_catalog = av.astropy_validation(se_catalog_x, se_catalog_y, wcs_header, band)\n         if d2d is not None and d2d.size > 0:\n             path = av.plot_distances(d2d, se_cat, directory = val_dir)\n             av.write_catalog(match_catalog, val_dir)\n@@ -65,12 +165,19 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         #\n     #\n     \n-    def compose_validation_data(self, tilename, band, val_dir, reference_files = []):\n+    def compose_validation_data(self, tilename, band, val_dir, **kwargs):\n         \"\"\"\n         collect all the validation images and catalog fits files\n         compress them into one tar archive\n         return the path to the compressed archive\n         \"\"\"\n+        reference_files = kwargs.get(\"reference_files\", [])\n+        skip_astrometric_validation = kwargs.get(\"skip_astrometric_validation\", 0)\n+        \n+        if self.skip_astrometric_validation:\n+            return\n+        #\n+        \n         for i in range(len(reference_files)):\n             #remove path and extension\n             reference_files[i] = reference_files[i].rsplit(os.sep,1)[-1].split(\".\", 1)[0]\n@@ -93,17 +200,20 @@ class RDisk_coadd_stage2(RDisk_coadd):\n             #whole path is stored in the tar archive\n             try:\n                 tar.add(name.rsplit(os.sep, 1)[-1])\n-            except Exception as e:\n+            except:\n                 #drop what cannot be found\n-                pass\n+                logging.debug(\"Validation file missing %s\", name)\n+\n+            #\n         for name in cpaths:\n             try:\n                 tar.add(name.rsplit(os.sep, 1)[-1])\n                 #clean up to prevent this file to be\n                 #added several times to an archive\n                 os.remove(name)\n-            except Exception as e:\n-                pass #dito\n+            except:\n+               logging.debug(\"Validation file not available for cleanup %s\", name)\n+            #\n         tar.close()\n         os.chdir(cwd)\n         \n@@ -118,6 +228,75 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         return [val_dir + os.sep +_f for _f in os.listdir(val_dir) if _f.startswith(\"Validation_\") and _f.endswith(\".tar.gz\")]\n     #\n     \n+    def add_config_file_to_validation(self, validation_data, run_dir):\n+        config_file = run_dir +os.sep +\"pipeline.properties\"\n+\n+        if self.exists(config_file) is None:\n+            #do nothing\n+            return validation_data\n+        #\n+        copied_config_file = run_dir +os.sep +\"data\" +os.sep +\"pipeline_property_file.properties\"\n+        shutil.copy(config_file, copied_config_file)\n+        validation_data.append(copied_config_file)\n+        \n+        return validation_data\n+    \n+    def put_background_data_to_outputs(self, bgsub_dir, outputs, tar = True, gzip = True, save_backgrounds_for_validation = 0):\n+        \"\"\"\n+        collects all background files from bgsub_dir as a list of the full paths.\n+        \n+        Parameter:\n+        @param bgsub_dir the directory with the background files\n+        @param tar defines, if the background files should be tarred\n+        @param gzip defines, if the background files should be gzipped\n+        @outputs a dict with all the outputs of the pipeline\n+        @param save_backgrounds_for_validation a boolean that determines, if this method adds the background data to\n+        the output dict (1) or not (0). Default is 0.\n+        \n+        Returns background list, a tar file or a tag.gz file (both also as list with one element)\n+        \"\"\"\n+        if not save_backgrounds_for_validation:\n+            return outputs\n+        #\n+        \n+        bg_files = [bgf for bgf in os.listdir(bgsub_dir) if bgf.endswith(self.background_extension)]\n+        if len(bg_files) == 0:\n+            return outputs\n+        #\n+        \n+        tfile = bgsub_dir +os.sep +\"Validation_backgrounds.tar.gz\"\n+        if self.exists(tfile) is not None:\n+            #has been made before\n+            outputs[\"background_images\"] = [tfile]\n+        elif not tar:\n+            outputs[\"background_images\"] = bg_files\n+        else:\n+            #make a tar\n+            cwd = os.getcwd()\n+            os.chdir(bgsub_dir)\n+            \n+            compression_mode = \"w:gz\" if gzip else \"W\"\n+            \n+            tar = tarfile.open(tfile, compression_mode)\n+            for name in bg_files:\n+                #add only names without path, otherwise the \n+                #whole path is stored in the tar archive\n+                try:\n+                    tar.add(name.rsplit(os.sep, 1)[-1])\n+                except:\n+                    #drop what cannot be found\n+                    logging.debug(\"Background file could not included in tar archive %s\", name)\n+\n+                #\n+            #\n+            tar.close()\n+            outputs[\"background_images\"] = [tfile]\n+            os.chdir(cwd)\n+        #\n+        \n+        return outputs\n+    #\n+    \n     def match_src_and_cat(self, src_list):\n         \"\"\"\n         analyze the band entry in the src_list entries \n@@ -152,6 +331,38 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         return bandlists\n     #\n     \n+    def randomly_cut_catalog(self, catalog:str, to_percent:int):\n+        \"\"\"\n+        choose a random subset of the catalog\n+        store under new name\n+        \n+        return this new name\n+        \"\"\"\n+        if to_percent == 100:\n+            return catalog\n+        #\n+        t = Table.read(catalog, format='fits')\n+        \n+        length = len(t)\n+        print(\"randomly_cut_catalog: originally \", length, \" indices\")\n+        \n+        remove = int( length *(100 - to_percent)/100.0 )\n+        rows = []\n+        for i in range(remove):\n+            idx = random.randint(0, length - 1)\n+            while idx in rows:\n+                idx = random.randint(0, length - 1)\n+            rows.append(idx)\n+        #\n+        t.remove_rows(rows)\n+        new_catalog = catalog.rsplit(\".fits\", 1)[0] +\"_shortened.fits\"\n+        t.write(new_catalog, format='fits', overwrite=True)\n+        \n+        return new_catalog\n+    #\n+        \n+    \n+    \n     def select_catalogs(self, images, catlist):\n         \"\"\"\n         images a list of images\n@@ -198,6 +409,22 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         return bgsublist\n     #\n     \n+    def clean_up_header_updated_data(self, coaddswarp):\n+        \"\"\"\n+        delete the obsolete directory with the header_updated_data\n+        \"\"\"\n+        f = open(coaddswarp, 'r')\n+        lines = f.readlines()\n+        f.close()\n+        img = lines[0].split()[0]\n+        folder = img.rsplit(os.sep, 1)[0]\n+        \n+        print(\"remove header updated data folder \", folder)\n+        if self.exists(folder) is not None:\n+            shutil.rmtree(folder)\n+        #\n+    #\n+        \n     \n     def resampled_input_list(self, mask_list, resampledir_nhcoadd, postfix = \".resamp_mask.fits\"):\n         \"\"\"\n@@ -241,6 +468,101 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         return outlist \n     #\n     \n+    def psfid_2_catalog(self, coadd_fits_file, catalog_outputfile, gridspacing = 50, ra_name = \"RA\", dec_name = \"DEC\", mag_name = \"MAG\"):\n+        psfid = Psfid.Psfid(coadd_fits_file)\n+        master_catalog = psfid.set_psf_points_within_a_psfid_section(gridspacing)\n+        psfid.make_catalog(master_catalog, catalog_outputfile, mag = 18.0, ra_name = ra_name, dec_name = dec_name, mag_name = mag_name)\n+        \n+        return catalog_outputfile, ra_name, dec_name, mag_name\n+    #\n+    \n+    \n+    def make_background(self, image_fitsfile, swarp_parameters, background_dir, subtract_bg, _which = None,  **kwargs):\n+        \"\"\"\n+        use sourceXtractor++ and produce a from the image and weight\n+        extension of the image_fitsfile\n+        \n+        @param image_fitsfile an image file with at least an IMAGE and WEIGHT extension\n+        @param swarp_parameters a swarp configuration file with settings for BACK_DEFAULT, BACK_FILTERSIZE, BACK_FILTTHRESH, BACK_SIZE\n+        @param background_dir the directory, where the background file should be stored\n+        @param subtract_bg_with_swarp a boolean to tell, if this method should be executed or skipped\n+        \n+        @return the path to the swarp background fitsfile\n+        \n+        \"\"\"\n+        if not subtract_bg:\n+            return None\n+        #\n+        \n+        #read the swarp parameters\n+        f = open(swarp_parameters, 'r')\n+        swarp_parameter_list = f.readlines()\n+        f.close()\n+        \n+        def extract_swarp_value(parameter_name):\n+            for line in swarp_parameter_list:\n+                if parameter_name in line:\n+                    v = line.split(\"#\", 1)[0] #remove comment\n+                    vv = v.split(parameter_name, 1)[-1] #remove parameter name\n+                    return vv.split()[0] #remove whitespace\n+                 #\n+            #\n+            return \"\"\n+        #\n+        \n+        def execute_SExtractor():\n+            image_hdu = str(FitsServices.get_image_hdu(image_fitsfile))\n+            weight_hdu = str(FitsServices.get_weight_hdu(image_fitsfile))\n+        \n+            args1 = [\"--detection-image\", image_fitsfile +\"[\" +image_hdu +\"]\", \"--weight-image\", image_fitsfile +\"[\" +weight_hdu +\"]\"]\n+            args2 = [\"--weight-type\", \"weight\", \"--background-cell-size\", BACK_SIZE, \"--smoothing-box-size\", BACK_FILTERSIZE]\n+            args3 = [\"--check-image-background\", bg_file]\n+        \n+            retval = self.execute('sourcextractor++', args1+args2+args3, wait = True)\n+        #\n+        \n+        def execute_Swarp():\n+            image_file = FitsServices.write_extension(image_fitsfile, \"IMAGE\", background_dir)\n+            weight_file = FitsServices.write_extension(image_fitsfile, \"WEIGHT\", background_dir)\n+        \n+            args1 = [\"--in_image_file_name\", image_file, \"--in_weight_file_name\", weight_file]\n+            args2 = [\"--weight_type\", \"WEIGHT\", \"--background_cell_size\", BACK_SIZE, \"--filter_box_size\", BACK_FILTERSIZE,]\n+            args3 = [\"--out_image_file_name\", bg_file]\n+        \n+            retval = self.execute('WsBackgroundMain', args1+args2+args3, wait = True)\n+        \n+            os.remove(image_file)\n+            os.remove(weight_file)\n+        #\n+            \n+            \n+        BACK_DEFAULT = extract_swarp_value(\"BACK_DEFAULT\")\n+        BACK_FILTERSIZE = extract_swarp_value(\"BACK_FILTERSIZE\")\n+        BACK_FILTTHRESH = extract_swarp_value(\"BACK_FILTTHRESH\")\n+        BACK_SIZE = extract_swarp_value(\"BACK_SIZE\")\n+        \n+        BACK_DEFAULT = kwargs.get(\"BACK_DEFAULT\", BACK_DEFAULT)\n+        BACK_FILTERSIZE = kwargs.get(\"BACK_FILTERSIZE\", BACK_FILTERSIZE)\n+        BACK_FILTTHRESH = kwargs.get(\"BACK_FILTTHRESH\", BACK_FILTTHRESH)\n+        BACK_SIZE = kwargs.get(\"BACK_SIZE\", BACK_SIZE)\n+        \n+        #make background\n+        filename = image_fitsfile.rsplit(os.sep, 1)[-1]\n+        basename = filename.rsplit(\".fits\", 1)[0]\n+        extension = filename[len(basename):]\n+        extension = filename[len(basename):] #if it is fits.fz? bg_file will be uncompressed, so .fits can be hardcoded\n+        bg_file = background_dir +os.sep +basename +self.background_extension\n+        \n+        if _which == self.SExtractor_bg:\n+            execute_SExtractor()\n+        elif _which == self.Swarp_bg:\n+            execute_Swarp()\n+        #\n+        \n+        return bg_file\n+    #\n+    \n+    \n     def add_mask_2_weight(self, fitsimage, mask_indices):\n         \"\"\"\n         Set the weight to 0 at the pixels, where the mask has\n@@ -287,11 +609,11 @@ class RDisk_coadd_stage2(RDisk_coadd):\n             bp_value = float(bandpass_as_string)\n             a = np.full((naxis2, naxis1), bp_value, dtype=np.float64)\n         except:\n-            pass\n+            logging.debug(\"bandpass value could not be used %s\", bandpass_as_string)\n         #\n         \n-        fits = FITS(path, 'rw', overwrite = True)\n-        fits.write(a)\n+        fits = FITS(path, 'rw')\n+        fits.write(a, clobber=True)\n         fits.close()\n         \n         return path\n@@ -313,6 +635,7 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         output is a dict containing dicts{band, product}\n         \"\"\"\n         outputs = super().test_output_integrity(outputs)\n+        print(\"outputs stage 1\" , outputs)\n         from astropy.io import fits\n         remove = list()\n         for band in outputs[\"psfs\"]:\n@@ -350,6 +673,7 @@ class RDisk_coadd_stage2(RDisk_coadd):\n             i = int(round(i))\n             hist[i] = hist.get(i, 0) + 1\n         return hist\n+    #\n     \n     \n     def extract_properties(self, xmlcontents, process):\n@@ -380,7 +704,6 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         for s in ['mask_indices','move_mask_indices_to_weight','move_mask_indices_to_new_indices']:\n             try:\n                 val = xmlcontents[s].split(',')\n-                print(\"found mask value \", val)\n                 if len(val) == 1 and len(val[0]) == 0:\n                     #treat an empty value string \n                     #this should lead to an empty list\n@@ -411,7 +734,37 @@ class RDisk_coadd_stage2(RDisk_coadd):\n         except:\n             d['_stackpsf_version'] = 1\n         #\n+        try:\n+            d['_coadd_zp'] = float(xmlcontents['coadd_zeropoint'])\n+        except:\n+            d['_coadd_zp'] = 30.0\n+        #   \n+        \n+        #background\n+        try:\n+            d['_subtract_delivered_background'] = int(xmlcontents['subtract_delivered_background'])\n+            d['_subtract_bg_with_swarp'] = int(xmlcontents['subtract_bg_with_swarp'])\n+            d['_save_backgrounds_for_validation'] = int(xmlcontents['save_backgrounds_for_validation'])\n+        except:\n+            #as default use pre-keyword settings. Fails, if no bg is delivered\n+            d['_subtract_delivered_background'] = 1\n+            d['_subtract_bg_with_swarp'] = 0\n+            d['_save_backgrounds_for_validation'] = 0\n+        #\n+        \n+        #stackregions\n+        try:\n+            d['_use_stackregions'] = int(xmlcontents['use_stackregions'])\n+        except:\n+            d['_use_stackregions'] = 0\n+        #\n         \n+        #random catalog cut\n+        try:\n+            d['_cut_catalog_to_percent'] = int(xmlcontents['cut_catalog_to_percent'])\n+        except:\n+            d['_cut_catalog_to_percent'] = 100\n+        #\n         return d\n     #    \n #\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -612,8 +612,8 @@ class RDisk_coadd_stage2(RDisk_coadd):\n             logging.debug(\"bandpass value could not be used %s\", bandpass_as_string)\n         #\n         \n-        fits = FITS(path, 'rw', overwrite = True)\n-        fits.write(a)\n+        fits = FITS(path, 'rw')\n+        fits.write(a, clobber=True)\n         fits.close()\n         \n         return path\n",
                            "add a separate folder for swarp temp (took it out of the general temp)",
                            "Michael",
                            "2023-05-16T20:42:42.000+02:00",
                            "c148ca2fc218a328e8f62903fa9cec79bd5a5489"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Pipeline/SigmaClip.py": [
                        [
                            "@@ -2,21 +2,26 @@ import numpy as np\n import copy\n \n class SigmaClip():\n-    def __init__(self, inarr, inweight):\n+    def __init__(self, inarr, inweight, max_iter = 1000):\n         \"\"\"\n         Performs a sigma-clipping of inarr.\n         max_iter  sets the maximum number of iterations\n         thresh    sets the number of sigmas for a clip iteration\n         tolerance sets the threshold for convergence\n         \"\"\"\n-        self.max_iter   = 1000\n+        self.max_iter   = max_iter\n         self.thresh     = 2.5\n         self.tolerance  = 1.e-6\n \n         _iter            = 1\n         mean, rms       = self.get_statistics(inarr)\n-        inarr, inweight, prev_mean, mean, rms = self.clip_iteration(\n-            inarr, inweight, mean, rms)\n+        \n+        if max_iter > 0:\n+            inarr, inweight, prev_mean, mean, rms = self.clip_iteration(\n+                inarr, inweight, mean, rms)\n+        else:\n+            prev_mean = mean\n+        #\n         while(np.abs(prev_mean - mean) >= self.tolerance and \n               _iter < self.max_iter):\n             _iter += 1 \n@@ -26,7 +31,7 @@ class SigmaClip():\n         self.bkWeight = inweight\n         self.mean     = mean\n         self.rms      = rms\n-        #print(\"sigmaclip needed iterations \", _iter)\n+        #print(\"sigmaclip needed iterations \", _iter, \", final mean \", mean, \", final rms \", rms)\n \n     def get_statistics(self, inarr):\n         \"\"\"\n@@ -38,6 +43,19 @@ class SigmaClip():\n         return mean, rms\n         \n     def clip_iteration(self, inarr, inweight, mean, rms):\n+        \"\"\"\n+        Performs one sigma-cliping iteration\n+        MW: vectorized some operations. Speed up factor 50\n+        \"\"\"\n+        idx       = np.asarray(np.abs(inarr - mean) <= self.thresh * rms)\n+        inarr     = inarr[idx]\n+        if inweight is not None:\n+            inweight   = inweight[idx]\n+        #\n+        new_mean, new_rms = self.get_statistics(inarr)\n+        return inarr, inweight, mean , new_mean, new_rms  \n+    \n+    def old_iteration(self, inarr, inweight, mean, rms):\n         \"\"\"\n         Performs one sigma-cliping iteration\n         \"\"\"\n@@ -47,6 +65,6 @@ class SigmaClip():\n         inarr     = [inarr[i]  for i in idx]\n         if inweight is not None:\n             inweight  = [inweight[i] for i in idx]\n-\n+        #\n         mean, rms = self.get_statistics(inarr)\n-        return inarr, inweight, prev_mean , mean, rms           \n+        return inarr, inweight, prev_mean , mean, rms  \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Pipeline/__init__.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Pipeline/rdisk_coadd_stage2_pipeline.pscript": [
                        [
                            "@@ -27,7 +27,8 @@ import time\n \n #parameters, globally set by the calling python class\n Pipeline = _Pipeline\n-ramdisk = _ramdisk #the location of the project directory on ramdisk (ramdisk including a top-level project folder) \n+ramdisk = _ramdisk #the location of the project directory on ramdisk (ramdisk including a top-level project folder)\n+swarp_temp = _swarp_temp #this can be specially set by ial\n project = _project\n coaddswarp = _coaddswarp #the src list for the swarp input\n ccds = _ccds #ccd is a loop parameter. _ccds is a list of values\n@@ -71,6 +72,13 @@ mask_indices = _mask_indices\n move_mask_indices_to_weight = _move_mask_indices_to_weight\n move_mask_indices_to_new_indices = _move_mask_indices_to_new_indices\n timestamp = time.time()\n+coadd_zp = _coadd_zp\n+subtract_delivered_background = _subtract_delivered_background\n+subtract_bg_with_swarp = _subtract_bg_with_swarp\n+save_backgrounds_for_validation = _save_backgrounds_for_validation\n+use_stackregions = _use_stackregions #base the reference catalog on locations of unique stacksets\n+cut_catalog_to_percent = _cut_catalog_to_percent\n+\n Pipeline.log(\"Pipeline script. Making directories\")\n \n \n@@ -81,7 +89,7 @@ norm_to_disk = False\n temp_value_norm_to_disk = norm_to_disk\n \n \n-swarp_temp = ramdisk + os.sep + 'swarptemp'\n+swarp_temp = swarp_temp + os.sep + 'swarptemp'\n Pipeline.mkdir(swarp_temp)\n \n xml_dir = archive_root +os.sep +run_dir +os.sep +'xml'\n@@ -125,12 +133,12 @@ if coadd_to_disk:\n prereq_bin = euclid_prereq +os.sep +'bin'\n euclid_bin = euclid_home +os.sep +'bin'\n \n-        \n+\n #split the complete input data (incl. fwhm) by band\n cats_by_band = Pipeline.file_by_band(coaddswarp, -1, 5)\n \n #map image and catalog files\n-catlist = Pipeline.match_src_and_cat(coaddswarp) \n+catlist = Pipeline.match_src_and_cat(coaddswarp)\n \n \n #get the tile parameters from the tile configuration file\n@@ -138,62 +146,64 @@ catlist = Pipeline.match_src_and_cat(coaddswarp)\n tileinfo = Pipeline.list_to_pylist(auxdir +os.sep +tilename +'.list')[0].split(' ')\n \n #put tileinfo into a file in the xml dir\n-tileheadfile, fitsheadfile = Pipeline.mk_desremap(coaddheadfile, tileinfo, xml_dir) \n+tileheadfile, fitsheadfile = Pipeline.mk_desremap(coaddheadfile, tileinfo, xml_dir)\n \n-#experimental: loop over all images in one go, drop the band based loop\n images_by_band = Pipeline.separate_band_from_src(coaddswarp, bandindex = 1, maxsplit=3)\n-images = []\n-for a in images_by_band.values(): images.extend(a)\n-\n-print(\"process \", len(images), \"images.\")\n \n-\n-#the first section is i/o intensen\n+#the first section is i/o intense\n #use more processes\n multiprocesses = multiprocesses *4\n+skip_astrometric_validation = 0\n \n-for image in images:        \n-    #<start name = \"parallel1\">        \n-    pipelinehash = str(hash(Pipeline))\n-    \n-    #swarpcall image\n-    (infile, fileid, unused, zp, zperr) = Pipeline.resolve_image_params(image)\n-    print('process image ', (infile, fileid, unused, zp, zperr))\n-    \n-    #get the hdu sequence and extnames to a standard\n-    temp_infile = Pipeline.rename_hdus(infile, swarp_temp + os.sep +infile.rsplit(os.sep, 1)[-1], True)\n-    \n-    #subtract background from infile\n-    #bg is supposed to be a hdu with the name \"background\"\n-    #the file is put into the bgsub_dir and gets the same name as the original file\n-    infile  = Pipeline.subtract_background(temp_infile, swarp_temp + os.sep +temp_infile.rsplit(os.sep, 1)[-1], write_compressed = savefpack, pass_bgsub = False)\n-    Pipeline.delete_file(temp_infile)\n-    Qa_log_reporter.output_file_exists('Background subtracted image', infile)\n-    \n-    #move wcs header values into all hdus\n-    wcs_header = Pipeline.homogenize_headers(infile)\n-    Qa_log_reporter.output_file_exists('Transferred header values within image', infile)\n-    \n-    Pipeline.astrometric_validation(catlist[image.split()[0]], vis_position_cat, valdir, infile, se_catalog_x, se_catalog_y)\n-    \n-    #move selected mask bits to the weight hdu\n-    Pipeline.add_mask_2_weight(infile, move_mask_indices_to_weight)\n-    \n-    #leave only the selected mask bits in the mask hdu\n-    #move them to standard values, if those are defined\n-    Pipeline.set_maskbits_in_mask(infile, mask_indices, move_mask_indices_to_new_indices)\n-    \n-    #edit the image for the log\n-    image = (infile.split()[0]).rsplit(os.sep, 1)[-1]\n-    Pipeline.saveLog(ramdisk, 'bgsub log, band, image ' +image)  \n-    \n-    #clean up ramdisk\n-    Pipeline.move(infile, _to = bgsub_dir)\n-    Pipeline.delete_dir(remap, cleanup = clean_up)\n-    #<end name = \"parallel1\">\n+for band in images_by_band.keys():\n+    for imagecounter in range(len(images_by_band[band])):\n+        image = images_by_band[band][imagecounter]\n+        #<start name = \"parallel1\">\n+        pipelinehash = str(hash(Pipeline))\n+        \n+        #swarpcall image\n+        (infile, fileid, unused, zp, zperr) = Pipeline.resolve_image_params(image)\n+        print('process image ', (infile, fileid, unused, zp, zperr))\n+        \n+        #get the hdu sequence and extnames to a standard\n+        temp_infile = Pipeline.rename_hdus(infile, swarp_temp + os.sep +infile.rsplit(os.sep, 1)[-1], False)\n+        \n+        #move selected mask bits to the weight hdu\n+        Pipeline.add_mask_2_weight(temp_infile, move_mask_indices_to_weight)\n+        \n+        #leave only the selected mask bits in the mask hdu\n+        #move them to standard values, if those are defined\n+        Pipeline.set_maskbits_in_mask(temp_infile, mask_indices, move_mask_indices_to_new_indices)\n+        \n+        \n+        #make a background file if requested by subtract_bg_with_swarp\n+        #if not requested, bg_fitsfile will be None and ignored in the next step\n+        bg_fitsfile = Pipeline.make_background(temp_infile, swarp_default, bgsub_dir, subtract_bg_with_swarp, _which = Pipeline.Swarp_bg)\n+        \n+        #subtract background from infile\n+        #bg is supposed to be a hdu with the name \"background\"\n+        #the file is put into the bgsub_dir and gets the same name as the original file\n+        infile  = Pipeline.subtract_background(temp_infile, bgsub_dir + os.sep +temp_infile.rsplit(os.sep, 1)[-1], bg_fitsfile = bg_fitsfile, write_compressed = savefpack, subtract_delivered_background = _subtract_delivered_background, imagenumber = imagecounter)\n+        Pipeline.delete_file(temp_infile)\n+        Qa_log_reporter.output_file_exists('Background subtracted image', infile)\n+        \n+        #move wcs header values into all hdus\n+        wcs_header = Pipeline.homogenize_headers(infile)\n+        Qa_log_reporter.output_file_exists('Transferred header values within image', infile)\n+        \n+        skip_astrometric_validation = Pipeline.astrometric_validation(catlist[image.split()[0]], vis_position_cat, valdir, infile, band = band, se_catalog_x = se_catalog_x, se_catalog_y = se_catalog_y)\n+        \n+        #edit the image for the log\n+        image = (infile.split()[0]).rsplit(os.sep, 1)[-1]\n+        Pipeline.saveLog(ramdisk, 'bgsub log, band, image ' +image)\n+        \n+        #clean up ramdisk\n+        Pipeline.move(infile, _to = bgsub_dir)\n+        Pipeline.delete_dir(remap, cleanup = clean_up)\n+        #<end name = \"parallel1\">\n+    #\n #\n Pipeline.log(\"End first parallel section\")\n-#Pipeline.merge_logs(ramdisk, ramdisk + os.sep +'logs' +os.sep +' make bgsub images.log')\n Pipeline.merge_logs(ramdisk, archive_root +os.sep +run_dir + os.sep +'log' +os.sep +'Bgsub_headers_astrom_validation.log')\n \n \n@@ -202,6 +212,7 @@ Pipeline.merge_logs(ramdisk, archive_root +os.sep +run_dir + os.sep +'log' +os.s\n bgsublist = Pipeline.coadd_list_2_bgsublist(coaddswarp, bgsub_dir)\n bandlists = Pipeline.separate_band_from_src(bgsublist, bandindex = 1, maxsplit=3)\n bands = list(bandlists.keys())\n+#Pipeline.clean_up_header_updated_data(coaddswarp)\n \n \n #return to defined processes\n@@ -211,10 +222,10 @@ print(\"bands \", bands)\n Pipeline.log(\"Start loop over bands\")\n \n for band in bands:\n-    #<start name = \"parallel2\">  \n+    #<start name = \"parallel2\">\n     \n-    bgsub_bandlist = bandlists[band] \n-    Pipeline.compose_validation_data(tilename, band, valdir, cats_by_band[band])\n+    bgsub_bandlist = bandlists[band]\n+    Pipeline.compose_validation_data(tilename, band, valdir, reference_files = cats_by_band[band], skip_astrometric_validation = skip_astrometric_validation)\n     \n     #coadds from bg subtracted images\n     Pipeline.log(\"First swarp call (image, weights)\")\n@@ -227,23 +238,22 @@ for band in bands:\n     Pipeline.mkdir(nhcoadd_out)\n     \n     \n-    nh_list = Pipeline.assemble_swarp_input_lists_from_pylist(bgsub_bandlist, outauxpath_nhcoadd, tilename, band)\n-    #returns lists of: image, weight, weight2, fluxscale, imageid, mag_base, mask, coverage\n+    nh_list = Pipeline.assemble_swarp_input_lists_from_pylist(bgsub_bandlist, outauxpath_nhcoadd, tilename, band, mag_base = coadd_zp)\n+    #returns lists of: image, weight, weight2, fluxscale, imageid, mag_base, mask, coverage, background, psfid\n     #print(\"nh_list \", nh_list)\n     \n     xmlfile = out_xml_dir + str(Pipeline.get_random_int()) +os.sep +tilename +'_' +band +'_swarp.xml'\n     imageout_name = tilename +'_' +band +'.image.fits'\n     weightout_name = tilename +'_' +band +'.weight.fits'\n-    args0 = ['-c',  swarp_default, '-PIXELSCALE_TYPE', 'MANUAL', '-PIXEL_SCALE', tileinfo[3] +\",\" +tileinfo[3]] \n+    args0 = ['-c',  swarp_default, '-PIXELSCALE_TYPE', 'MANUAL', '-PIXEL_SCALE', tileinfo[3] +\",\" +tileinfo[3]]\n     args1 = ['-CENTER_TYPE', 'MANUAL', '-CENTER', tileinfo[1] +',' +tileinfo[2], '-IMAGE_SIZE', tileinfo[4] +',' +tileinfo[5]]\n     args2 = ['-SUBTRACT_BACK', 'N', '-BLANK_BADPIXELS', 'Y', '-COPY_KEYWORDS', 'PSF_FWHM,PSF_BETA,FWHMHOMO', '-DELETE_TMPFILES','Y']\n     args3 = ['-FSCALASTRO_TYPE', 'VARIABLE', '-FSCALE_DEFAULT', '@'+ nh_list[3], '-FSCALE_KEYWORD', 'nokey', '-RESAMPLE', 'Y', '-COMBINE', 'Y', '-WEIGHT_TYPE', 'MAP_WEIGHT']\n     args4 = ['-WEIGHT_IMAGE', '@'+ nh_list[1], '-HEADER_ONLY', 'N', '-NTHREADS',  str(  int( multiprocesses/len(bands)) )]\n     args5 = ['@'+ nh_list[0], '-COMBINE_TYPE', combinetype, '-WRITE_XML', 'Y', '-XML_NAME', xmlfile, '-TILE_COMPRESS', Pipeline.evaluate_arg(savefpack, 'Y','N')]\n     args6 = ['-IMAGEOUT_NAME', imageout_name, '-WEIGHTOUT_NAME', weightout_name, '-RESAMPLE_DIR', resampledir_nhcoadd]\n-    #args7 = ['-RESAMPLING_TYPE', 'BILINEAR']\n     args7 = ['-RESAMPLING_TYPE', 'LANCZOS3'] #default\n-    coadd_nonhomog_args = args0+args1+args2+args3+args4+args5+args6 +args7 \n+    coadd_nonhomog_args = args0+args1+args2+args3+args4+args5+args6 +args7\n     \n     #set the workdir\n     cwd = os.getcwd()\n@@ -262,25 +272,25 @@ for band in bands:\n     coverageout_name = tilename +'_' +band +'.coverage.fits'\n     coverage_weightout_name = tilename +'_' +band +'.coverage_weight.fits'\n     \n-    args0 = ['-c',  swarp_default, '-PIXELSCALE_TYPE', 'MANUAL', '-PIXEL_SCALE', tileinfo[3] +\",\" +tileinfo[3]] \n+    args0 = ['-c',  swarp_default, '-PIXELSCALE_TYPE', 'MANUAL', '-PIXEL_SCALE', tileinfo[3] +\",\" +tileinfo[3]]\n     args1 = ['-CENTER_TYPE', 'MANUAL', '-CENTER', tileinfo[1] +',' +tileinfo[2], '-IMAGE_SIZE', tileinfo[4] +',' +tileinfo[5]]\n-    args2 = ['-SUBTRACT_BACK', 'N', '-BLANK_BADPIXELS', 'Y', '-COPY_KEYWORDS', 'PSF_FWHM,PSF_BETA,FWHMHOMO', '-DELETE_TMPFILES','Y']\n-    args3 = ['-FSCALASTRO_TYPE', 'VARIABLE', '-FSCALE_DEFAULT', '1.0', '-FSCALE_KEYWORD', 'nokey', '-RESAMPLE', 'Y', '-COMBINE', 'Y', '-WEIGHT_TYPE', 'MAP_WEIGHT']\n-    args4 = ['-WEIGHT_IMAGE', '@'+ nh_list[1], '-HEADER_ONLY', 'N', '-NTHREADS',  str(  int( multiprocesses/len(bands)) )]\n+    args2 = ['-SUBTRACT_BACK', 'N', '-BLANK_BADPIXELS', 'Y', '-DELETE_TMPFILES','Y']\n+    args3 = ['-FSCALASTRO_TYPE', 'NONE', '-FSCALE_DEFAULT', '1.0', '-FSCALE_KEYWORD', 'nokey', '-RESAMPLE', 'Y', '-COMBINE', 'Y', '-WEIGHT_TYPE', 'NONE']\n+    args4 = ['-HEADER_ONLY', 'N', '-NTHREADS',  str(  int( multiprocesses/len(bands)) )]\n     args5 = ['@'+ nh_list[7], '-COMBINE_TYPE', 'SUM', '-WRITE_XML', 'N', '-TILE_COMPRESS', 'N']\n     args6 = ['-IMAGEOUT_NAME', coverageout_name, '-WEIGHTOUT_NAME', coverage_weightout_name, '-RESAMPLE_DIR', resampledir_nhcoadd]\n     args7 = ['-RESAMPLING_TYPE', 'LANCZOS3'] #default\n-    coadd_nonhomog_args = args0+args1+args2+args3+args4+args5+args6 +args7 \n+    coadd_nonhomog_args = args0+args1+args2+args3+args4+args5+args6 +args7\n     \n     retval = Pipeline.execute('CT_SWarp', coadd_nonhomog_args, wait = True)\n     Qa_log_reporter.output_file_exists('Swarp nonhomog coverage output', coverageout_name)\n     \n     #2 further swarp calls to combine the masks\n     Pipeline.log(\"Third swarp call (resample masks)\")\n-    #first: resample \n+    #first: resample\n     xml_flag_file = out_xml_dir + str(Pipeline.get_random_int()) +os.sep +tilename +'_' +band +'_swarp_flag.xml'\n     maskout_name = tilename +'_' +band +'.mask.fits'\n-    args0 = ['-c',  swarp_flag_default, '-PIXELSCALE_TYPE', 'MANUAL', '-PIXEL_SCALE', tileinfo[3] +\",\" +tileinfo[3]] \n+    args0 = ['-c',  swarp_flag_default, '-PIXELSCALE_TYPE', 'MANUAL', '-PIXEL_SCALE', tileinfo[3] +\",\" +tileinfo[3]]\n     args1 = ['-CENTER_TYPE', 'MANUAL', '-CENTER', tileinfo[1] +',' +tileinfo[2], '-IMAGE_SIZE', tileinfo[4] +',' +tileinfo[5]]\n     args2 = ['@'+ nh_list[6], '-COMBINE_TYPE', 'OR', '-WRITE_XML', 'Y', '-XML_NAME', xml_flag_file, '-RESAMPLE', 'Y', '-COMBINE', 'N']\n     args3 = ['-IMAGEOUT_NAME', maskout_name, '-RESAMPLE_DIR', resampledir_nhcoadd]\n@@ -305,6 +315,27 @@ for band in bands:\n     Qa_log_reporter.output_file_exists('Swarp nonhomog mask output', maskout_name)\n     \n     \n+    ####\n+    Pipeline.log(\"Fifth swarp call (resample psfids)\")\n+    #first: resample\n+    xml_flag_file = out_xml_dir + str(Pipeline.get_random_int()) +os.sep +tilename +'_' +band +'_swarp_psfid.xml'\n+    psfid_out_name = tilename +'_' +band +'.psfid.fits'\n+    psfid_weightout_name = tilename +'_' +band +'.psfid_weight.fits'\n+    args0 = ['-c',  swarp_flag_default, '-PIXELSCALE_TYPE', 'MANUAL', '-PIXEL_SCALE', tileinfo[3] +\",\" +tileinfo[3]]\n+    args1 = ['-CENTER_TYPE', 'MANUAL', '-CENTER', tileinfo[1] +',' +tileinfo[2], '-IMAGE_SIZE', tileinfo[4] +',' +tileinfo[5]]\n+    args2 = ['-SUBTRACT_BACK', 'N', '-BLANK_BADPIXELS', 'Y', '-DELETE_TMPFILES','Y']\n+    args3 = ['-FSCALASTRO_TYPE', 'NONE', '-FSCALE_DEFAULT', '1.0', '-FSCALE_KEYWORD', 'nokey', '-RESAMPLE', 'Y', '-COMBINE', 'Y', '-WEIGHT_TYPE', 'NONE']\n+    args4 = ['-HEADER_ONLY', 'N', '-NTHREADS',  str(  int( multiprocesses/len(bands)) )]\n+    args5 = ['@'+ nh_list[9], '-COMBINE_TYPE', 'SUM', '-WRITE_XML', 'N', '-TILE_COMPRESS', 'N']\n+    args6 = ['-IMAGEOUT_NAME', psfid_out_name, '-WEIGHTOUT_NAME', psfid_weightout_name, '-RESAMPLE_DIR', resampledir_nhcoadd]\n+    args7 = ['-RESAMPLING_TYPE', 'LANCZOS3', '-RESAMPLE_SUFFIX', '.resamp_psfid.fits']\n+    coadd_nonhomog_args = args0+args1+args2+args3+args4 +args5+args6 +args7\n+    \n+    retval = Pipeline.execute('CT_SWarp', coadd_nonhomog_args, wait = True)\n+    Qa_log_reporter.output_file_exists('Swarp nonhomog psfid output', psfid_out_name)\n+    ####\n+    \n+    \n     #bandpasses\n     Pipeline.log(\"Fill bandpasses\")\n     bandpassout_name = Pipeline.fill_bandpasses(bandpasses[\"Bandpass_\" +band], resampledir_nhcoadd, int(tileinfo[4]), int(tileinfo[5]))\n@@ -314,28 +345,30 @@ for band in bands:\n     Pipeline.log(\"Update fits headers\")\n     \n     #extname must be written in small letters, never capitals\n-    Pipeline.update_fits_header(imageout_name, {1:[{'name':'SEXMGZPT', 'value': nh_list[5], 'comment':'Mag ZP'}, \\\n+    Pipeline.update_fits_header(imageout_name, {1:[{'name':'SEXMGZPT', 'value': coadd_zp, 'comment':'Mag ZP'}, \\\n                                                    {'name':'Filter', 'value':band, 'comment':'Filter'}, \\\n                                                    {'name':'extname', 'value':'IMAGE', 'comment':'image extension'}]})\n     Pipeline.update_fits_header(weightout_name, {1:[{'name':'extname', 'value':'WEIGHT', 'comment':'weight extension'}]})\n     Pipeline.update_fits_header(maskout_name, {1:[{'name':'extname', 'value':'MASK', 'comment':'mask extension'}]})\n     Pipeline.update_fits_header(bandpassout_name, {1:[{'name':'extname', 'value':'BANDPASS', 'comment':'bandpass extension'}]})\n     Pipeline.update_fits_header(coverageout_name, {1:[{'name':'extname', 'value':'COVERAGE', 'comment':'coverage extension'}]})\n+    Pipeline.update_fits_header(psfid_out_name, {1:[{'name':'extname', 'value':'PSFID', 'comment':'psfid extension'}]})\n     \n     \n     #fitscombine to output image\n-    Pipeline.log(\"Fitscombine image, weight, mask, bandpass, coverage to output coadd\")\n+    Pipeline.log(\"Fitscombine image, weight, mask, bandpass, coverage psfid to output coadd\")\n     outimage = nhcoadd_out +os.sep +tilename +'_' +str(band) +'.fits' +Pipeline.evaluate_arg(savefpack, '.fz','')\n     args = ['-cleanup', imageout_name +Pipeline.evaluate_arg(savefpack, '.fz',''), weightout_name +Pipeline.evaluate_arg(savefpack, '.fz','')]\n     args = args +[maskout_name +Pipeline.evaluate_arg(savefpack, '.fz',''), bandpassout_name] #bandpassout_name is not compressed\n     args = args +[coverageout_name] #coverageout_name is not compressed\n+    args = args +[psfid_out_name]\n     args = args +[outimage, '-verbose', verbose] +[Pipeline.evaluate_arg(savefpack, '-no0exthead', '')]\n-    retval = Pipeline.execute('EXT_PF1_DES_fitscombine', args, wait = True) #qa_file = qafile, \n+    retval = Pipeline.execute('EXT_PF1_DES_fitscombine', args, wait = True) #qa_file = qafile,\n     Qa_log_reporter.output_file_exists('Fitscombinend coadd nonhomog', outimage)\n     Pipeline.log_coverage_histogram(outimage)\n     \n     \n-    Pipeline.saveLog(ramdisk, 'coadd nonhomogeneous, band ' +band)  \n+    Pipeline.saveLog(ramdisk, 'coadd nonhomogeneous, band ' +band)\n     \n     Metadata_Collector.collect_metadata(outauxpath_nhcoadd, globals(), filetype = 'aux', newer = timestamp, file_endings = ('.list', '.dat'), BAND = band)\n     Metadata_Collector.collect_metadata(nhcoadd_out, globals(), filetype = 'nhcoadd', BAND = band, newer = timestamp)\n@@ -374,7 +407,7 @@ coadd_detector_list, _ = Pipeline.determine_detbands(ramdisk,  nhcoadd_out, tile\n Pipeline.mk_imageid(idlist, auxdir, tilename)\n \n ##construct swarp call and run swarp to make the det image\n-swarpargs = Pipeline.construct_swarp_call(fitslist, nhcoadd_out, etcpath, 0, savefpack, verbose)\n+swarpargs = Pipeline.construct_swarp_call(fitslist, nhcoadd_out, etcpath, extraweightmap=0, savefpack=savefpack, verbose=verbose)\n Pipeline.execute(\"CT_SWarp\", swarpargs, wait=True )\n \n #<start stopcondition = \"fitscombine2\">\n@@ -398,7 +431,7 @@ Qa_log_reporter.output_file_exists('Fitscombined detimage', outimage)\n #--- start to inline mk_detpsf here\n (outimage, detimagename, detweightname, detpsffile, wd) = Pipeline.get_detnames(nhcoadd_out, tilename, createpsf = 1, savefpack = savefpack, use_relative_paths = True)\n \n-#switch to coadd workdir in order to use \n+#switch to coadd workdir in order to use\n #relative (=short) paths for SExtractor\n #the rest of this processing until psfstacks is done in the coadd outdir\n cwd = os.getcwd()\n@@ -408,12 +441,17 @@ os.chdir(wd)\n \n Pipeline.log(\"SExtractor catalog for detection image.\")\n \n-args = [detimagename, \"-c\", etcpath +\"default.sex\", \"-CATALOG_NAME\", detpsffile] \n-args += [\"-CATALOG_TYPE\", \"FITS_LDAC\", \"-WEIGHT_TYPE\", \"MAP_WEIGHT\"]\n-args += [\"-WEIGHT_IMAGE\", detweightname, \"-PARAMETERS_NAME\"] \n-args += [etcpath +\"sex.param_psfex\", \"-FILTER_NAME\", etcpath +\"sex.conv\"]\n+args = [detimagename, \"-c\", etcpath +\"default.sex\", \"-CATALOG_NAME\", detpsffile]\n+args += [\"-CATALOG_TYPE\", \"FITS_LDAC\"]\n+\n+#leave out the weight image as this seems to require proper image units\n+#drop the weight\n+#args += [\"-WEIGHT_IMAGE\", detweightname, \"-WEIGHT_TYPE\", \"MAP_WEIGHT\"]\n+args += [\"-PARAMETERS_NAME\", etcpath +\"sex.param_psfex\", \"-FILTER_NAME\", etcpath +\"sex.conv\"]\n args += [\"-STARNNW_NAME\", etcpath +\"sex.nnw\", \"-SATUR_LEVEL\", \"65000\", \"-DETECT_MINAREA\", \"3\"]\n+args += [\"-DETECT_THRESH\", \"1.6\",\"-THRESH TYPE\", \"RELATIVE\", \"-MAG_ZEROPOINT\", str(coadd_zp)]\n args += [\"-NTHREADS\", str(int(multiprocesses))]\n+\n Pipeline.execute(\"sex\", args, wait=True )\n Qa_log_reporter.output_file_exists('Detimage SExtractor catalog', detpsffile)\n \n@@ -473,11 +511,14 @@ for band in bands:\n         psffile = cc_image_ext.split(\".fits\")[0] +\"_psfcat.fits\"\n         args = [cc_image_ext, \"-c\", etcpath +\"default.sex\"]\n         args += [\"-CATALOG_NAME\", psffile, \"-CATALOG_TYPE\", \"FITS_LDAC\"]\n-        args += [\"-WEIGHT_TYPE\", \"MAP_WEIGHT\", \"-WEIGHT_IMAGE\", cc_weightname]\n+        #drop the weight\n+        #args += [\"-WEIGHT_TYPE\", \"MAP_WEIGHT\", \"-WEIGHT_IMAGE\", cc_weightname]\n         args += [\"-PARAMETERS_NAME\", etcpath +\"sex.param_psfex\"]\n         args += [\"-FILTER_NAME\", etcpath +\"sex.conv\"]\n         args += [\"-STARNNW_NAME\", etcpath +\"sex.nnw\"]\n         args += [\"-SATUR_LEVEL\", \"65000\", \"-DETECT_MINAREA\", \"3\"]\n+        args += [\"-DETECT_THRESH\", \"1.6\",\"-THRESH TYPE\", \"RELATIVE\", \"-MAG_ZEROPOINT\", str(coadd_zp)]\n+        \n         Pipeline.execute(\"sex\", args, wait=True )\n         Qa_log_reporter.output_file_exists('Sextractor catalog for psf', psffile)\n         \n@@ -490,27 +531,31 @@ for band in bands:\n         #---- start inline constuct_coadd_catalog_sexcall\n         psfoutfile = cc_image_ext.split(\".fits\")[0] +\"_psfcat.psf\"\n         detpsffile = detection_image.split(\".fits\")[0] + \"_psfcat.psf\"\n+        minus_objectimage = cc_image_ext.split(\".fits\")[0] +\"_mobjects.fits\"\n         catalog_name = Pipeline.get_catalog_name(nhcoadd_out, tilename, band, use_relative_paths = True)\n         Qa_log_reporter.output_file_exists('Psf from coadd', psfoutfile)\n         Qa_log_reporter.output_file_exists('Psf from detimage', detpsffile)\n         \n         args = [detection_image+\",\"+cc_image_ext, \"-c\", etcpath +\"sex.config \", \"-FILTER_NAME\", kernelfile]\n         args += [\"-STARNNW_NAME\", etcpath +\"sex.nnw\", \"-PARAMETERS_NAME\", etcpath +'sex.param_coadd_sersic']\n-        args += [\"-CATALOG_TYPE\", \"FITS_1.0\", \"-CATALOG_NAME\", catalog_name, \"-WEIGHT_TYPE\", \"MAP_WEIGHT\"]\n-        args += [\"-WEIGHT_IMAGE\", detection_weight_image+\",\"+cc_weightname, \"-MEMORY_BUFSIZE\", \"2048 \"]\n+        args += [\"-CATALOG_TYPE\", \"FITS_1.0\", \"-CATALOG_NAME\", catalog_name, \"-MEMORY_BUFSIZE\", \"2048 \"]\n+        #drop the weight\n+        #args += [\"-WEIGHT_TYPE\", \"MAP_WEIGHT\", \"-WEIGHT_IMAGE\", detection_weight_image+\",\"+cc_weightname]\n         args += [\"-MAG_ZEROPOINT\", str(hdr_magzp), \"-PSF_NAME\", detpsffile+\",\"+psfoutfile, \"-VERBOSE_TYPE\", Pipeline.evaluate_arg(verbose, \"NORMAL\", \"QUIET\")]\n         args += ['-BACK_TYPE','MANUAL', '-BACK_VALUE', '0.0,0.0'] #don't subtract backgrounds\n+        #args += ['-BACK_TYPE','AUTO'] #subtract background\n+        args += ['-CHECKIMAGE_TYPE', '-OBJECTS', '-CHECKIMAGE_NAME', minus_objectimage]\n         #---- end inline construct_coadd_catalog_sexcall\n         \n         retval = Pipeline.execute(\"sex\", args, wait=True )\n         Qa_log_reporter.output_file_exists('Coadd double detection catalog from detimage', cc_outcat)\n         #\n-        \n+    #\n     #retval = Pipeline.execute(euclid_bin + os.sep +'coadd_catalog', args, wait = True)\n-    qafile = nhcoadd_out + os.sep + tilename +'_' +band +'_cat.fits' \n+    qafile = nhcoadd_out + os.sep + tilename +'_' +band +'_cat.fits'\n     #Qa.qa_check(coadd_catalog, qa_file = qafile)\n     print(\"*** Pythonised coadd catalog completed here! ***\")\n-    Pipeline.saveLog(ramdisk, 'coadd_catalog, band ' +band) \n+    Pipeline.saveLog(ramdisk, 'coadd_catalog, band ' +band)\n     \n     print('catalog ', band, ' done')\n     Metadata_Collector.collect_metadata(qafile, globals(), filetype = 'red_cat', BAND = band)\n@@ -528,24 +573,43 @@ if stackpsf_version == 1:\n     for band in bands:\n         #<start name = \"parallel4\">\n         #stackPSF\n-        print('excute stackpsf for band ', band)\n+        Pipeline.log(\"excute stackpsf for band \" + str(band))\n         \n         #duplicate the coadd name from the fitscombine coadd section\n         coaddname = nhcoadd_out +os.sep +tilename +'_' +str(band) +'.fits' +Pipeline.evaluate_arg(savefpack, '.fz','')\n-        \n         catalog_name = Pipeline.get_catalog_name(nhcoadd_out, tilename, band)\n+        reference_catalog, ra_column, dec_column, mag_column = Pipeline.select_reference_catalog(catalog_name, vis_position_cat, band)\n+        \n+        Pipeline.log(\"got input parameters ...\")\n+        \n+        #as default, use this catalog for stackpsf\n+        stackpsf_catalog = reference_catalog\n+        stackpsf_ra_column = ra_column\n+        stackpsf_dec_column = dec_column\n+        \n+        if use_stackregions:\n+            #in this case, overwrite the catalog for stackpsf\n+            catalog_outputfile = nhcoadd_out +os.sep +tilename +'_' +str(band) +'_psfid_catalog.fits'\n+            stackpsf_catalog, stackpsf_ra_column, stackpsf_dec_column, mag_column = Pipeline.psfid_2_catalog(coaddname, catalog_outputfile)\n+        #\n+        \n         #psfargs0 = [os.getenv(\"EUCLID_HOME\") +os.sep +\"python\" +os.sep +\"pybin\" +os.sep +\"StackPsfExec.py\"]\n-        psfargs1 = [\"--position_cat\", vis_position_cat, \"--ra\", \"X_WORLD\", \"--dec\", \"Y_WORLD\"]\n-        psfargs2 = [\"--tilename\", tilename, \"--band\", band, \"--SElist\", coaddswarp, \"--outdir\", nhcoadd_out]\n-        psfargs3 = [\"--ramdisk\", ramdisk, \"--coadd\", coaddname, \"--max_layer_num\", \"1000\", \"--clean_up\", str(clean_up)]\n+        psfargs1 = [\"--position_cat\", stackpsf_catalog, \"--ra\", stackpsf_ra_column, \"--dec\", stackpsf_dec_column, \"--mag_entry\", mag_column]\n+        psfargs2 = [\"--coadd_zp\", str(coadd_zp), \"--tilename\", tilename, \"--band\", band, \"--SElist\", bgsublist, \"--outdir\", nhcoadd_out]\n+        psfargs3 = [\"--ramdisk\", ramdisk, \"--resampledir\", resampledir_nhcoadd, \"--coadd\", coaddname, \"--max_layer_num\", \"1000\", \"--clean_up\", str(clean_up)]\n         psfargs4 = [\"--keep_edge_obj\", \"--coadd_cat\", catalog_name, \"--multiprocesses\", str(  int( multiprocesses/len(bands) )  ) ]\n         #psfargs4 = [\"--keep_edge_obj\", \"--multiproceses\", str(  int( multiprocesses/len(bands) )  ) ]\n-        psfargs5 = [\"--bandpass_value\", str( bandpasses[\"Bandpass_\" +band] ), \"--swarp_resampling\", \"LANCZOS3\"]\n+        psfargs5 = [\"--bandpass_value\", str( bandpasses[\"Bandpass_\" +band] ), \"--swarp_resampling\", \"LANCZOS3\", \"--max_separation_for_match\", str(2*float(tileinfo[3]))]\n         psfargs = psfargs1 +psfargs2 +psfargs3 +psfargs4 +psfargs5\n         \n-        print(psfargs)\n+        Pipeline.log(\"psfargs \" +str(psfargs))\n         Pipeline.execute(\"EXT_PF1_StackPsfExec\", psfargs, wait=True )\n         \n+        output_stackpsf = nhcoadd_out +os.sep +tilename +'_' +str(band) +'_stackedPSF.fits' \n+        \n+        #update stackpsf with the real reference catalog\n+        Pipeline.update_stackpsf_with_reference_catalog_positions(bgsublist, band, output_stackpsf, reference_catalog, nhcoadd_out, ra_ref = ra_column, dec_ref = dec_column)\n+        \n         Pipeline.saveLog(ramdisk, 'stackpsf, band ' +band)\n         \n         print('stackpsf ', band, ' done')\n@@ -554,7 +618,7 @@ if stackpsf_version == 1:\n     #\n else:\n     Pipeline.log(\"Parallel section: stack psf version 2\")\n-    (infile, fileid, unused, zp, zperr) = Pipeline.resolve_image_params(images[0])\n+    (infile, fileid, unused, zp, zperr) = Pipeline.resolve_image_params(list(bandlists.values())[0][0])\n     naxis1, naxis2 = Pipeline.image_dim( infile )\n     for band in bands:\n         #stackPSF\n@@ -562,14 +626,28 @@ else:\n         \n         #duplicate the coadd name from the fitscombine coadd section\n         coaddname = nhcoadd_out +os.sep +tilename +'_' +str(band) +'.fits' +Pipeline.evaluate_arg(savefpack, '.fz','')\n+        catalog_name = Pipeline.get_catalog_name(nhcoadd_out, tilename, band)\n+        reference_catalog, ra_column, dec_column, mag_column = Pipeline.select_reference_catalog(catalog_name, vis_position_cat, band)\n         \n-        psfargs0 = [\"--position_cat\", vis_position_cat, \"--ra\", \"X_WORLD\", \"--dec\", \"Y_WORLD\", \"--mag\", \"MAG_AUTO\"]\n-        psfargs1 = [\"--tilename\", tilename, \"--band\", band, \"--SElist\", coaddswarp, \"--outdir\", nhcoadd_out]\n+        #optionally: cut the reference catalog\n+        stackpsf_catalog = Pipeline.randomly_cut_catalog(reference_catalog, cut_catalog_to_percent)\n+        stackpsf_ra_column = ra_column\n+        stackpsf_dec_column = dec_column\n+        \n+        if use_stackregions:\n+            catalog_outputfile = nhcoadd_out +os.sep +tilename +'_' +str(band) +'_psfid_catalog.fits'\n+            stackpsf_catalog, stackpsf_ra_column, stackpsf_dec_column, mag_column = Pipeline.psfid_2_catalog(coaddname, catalog_outputfile)\n+        #\n+        \n+        psfargs0 = [\"--position_cat\", stackpsf_catalog, \"--ra\", stackpsf_ra_column, \"--dec\", stackpsf_dec_column, \"--mag\", mag_column]\n+        psfargs1 = [\"--tilename\", tilename, \"--band\", band, \"--SElist\", bgsublist, \"--outdir\", nhcoadd_out]\n         psfargs2 = [\"--ramdisk\", ramdisk, \"--coadd\", coaddname, \"--swarp_resampling\", \"LANCZOS3\" ]\n         psfargs3 = [\"--multiprocesses\", str(  int( multiprocesses ) ), \"--naxis1\", str(naxis1), \"--naxis2\", str(naxis2) ]\n-        #[\"--max_zp\", \"33.5\", \"--min_zp\", \"27.0\", \"--default_zp\", \"31.5\" ]\n-        psfargs4 = [\"--swarp_defaults\", etcpath +\"default.swarp\"]\n-        psfargs5 = [\"--log-level\", \"DEBUG\", \"--clean_up\", str(clean_up)]\n+        #[\"--max_zp\", \"33.5\", \"--min_zp\", \"27.0\", \"--default_zp\", str(coadd_zp) ]\n+        psfargs4 = [\"--swarp_defaults\", etcpath +\"default.swarp\", \"--default_zp\", str(coadd_zp)]\n+        #2*tileinfo[3]: 2 pixels of the input science images\n+        #the coadd is produced with the same pixelsize than the input images\n+        psfargs5 = [\"--log-level\", \"DEBUG\", \"--clean_up\", str(clean_up), \"--max_separation_for_match\", str(2*float(tileinfo[3]))]\n         #psfargs5 = [\"--bandpass_value\", str( bandpasses[\"Bandpass_\" +band] )]\n         \n         psfargs = psfargs0 +psfargs1 +psfargs2 +psfargs3 +psfargs4 +psfargs5\n@@ -577,20 +655,23 @@ else:\n         print(psfargs)\n         Pipeline.execute(\"EXT_StackPSF\", psfargs, wait=True )\n         \n+        output_stackpsf = nhcoadd_out +os.sep +tilename +os.sep +band +os.sep +tilename +'_' +str(band) +'_stackedPSF.fits' \n+        Pipeline.update_stackpsf_with_reference_catalog_positions(bgsublist, band, output_stackpsf, reference_catalog, nhcoadd_out, ra_ref = ra_column, dec_ref = dec_column)\n+        \n         Pipeline.saveLog(ramdisk, 'stackpsf, band ' +band)\n         \n         print('stackpsf ', band, ' done')\n     #\n     Pipeline.log(\"End section: stack psfs\")\n #\n-\n Pipeline.merge_logs(ramdisk, archive_root +os.sep +run_dir + os.sep +'log' +os.sep +'StackPSF_' +job_id +'.log')\n \n Pipeline.log(\"Get outputs\")\n \n-#browse the coadd - outdir for products \n-coadds, catalogs, psfs = Pipeline.get_outputs(nhcoadd_out, tilename, bands, stage = 2)\n+#browse the coadd - outdir for products\n+coadds, catalogs, psfs = Pipeline.get_outputs(nhcoadd_out, tilename, bands, stage = 2, modprefix = Pipeline.evaluate_arg(use_stackregions, \"mod_\", \"\"))\n validation_images = Pipeline.get_validation_data(valdir)\n+validation_images = Pipeline.add_config_file_to_validation(validation_images, archive_root)\n \n #check coadds, catalogs and psfs, don't crash, if not all validation images are there\n outputs = Pipeline.test_output_integrity({'coadds':coadds, 'coadd_catalogs': catalogs, 'psfs':psfs})\n@@ -598,6 +679,8 @@ outputs = Pipeline.test_output_integrity({'coadds':coadds, 'coadd_catalogs': cat\n #get an se_image: the bandlist contains more than just the filename. Extract it\n outputs['se_image'] = Pipeline.get_se_images( bandlists)\n outputs['validation_images'] = validation_images\n+outputs = Pipeline.put_background_data_to_outputs(bgsub_dir, outputs, tar = True, gzip = True, save_backgrounds_for_validation = save_backgrounds_for_validation)\n+\n \n Pipeline.log(\"Put outputs into output queue\")\n \n@@ -609,7 +692,6 @@ Metadata_Collector.collect_logs()\n \n Pipeline.log(\"cleanup\")\n #remove the list files but not the data\n-#move some files for ingestion...\n \n #outauxpath_nhcoadd is per band in ramdisk\n #-> fix that\n@@ -618,8 +700,5 @@ Pipeline.log(\"cleanup\")\n #Pipeline.delete_dir(outauxpath_nhcoadd, cleanup = copy_success)\n \n \n-#ingest\n-#Postprocessing.execute([Postprocessing.metadata_ingestion, Postprocessing.qa_merger])\n-\n Pipeline.log(\"Pipeline script done\")\n \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -28,6 +28,7 @@ import time\n #parameters, globally set by the calling python class\n Pipeline = _Pipeline\n ramdisk = _ramdisk #the location of the project directory on ramdisk (ramdisk including a top-level project folder)\n+swarp_temp = _swarp_temp #this can be specially set by ial\n project = _project\n coaddswarp = _coaddswarp #the src list for the swarp input\n ccds = _ccds #ccd is a loop parameter. _ccds is a list of values\n@@ -88,7 +89,7 @@ norm_to_disk = False\n temp_value_norm_to_disk = norm_to_disk\n \n \n-swarp_temp = ramdisk + os.sep + 'swarptemp'\n+swarp_temp = swarp_temp + os.sep + 'swarptemp'\n Pipeline.mkdir(swarp_temp)\n \n xml_dir = archive_root +os.sep +run_dir +os.sep +'xml'\n@@ -595,7 +596,7 @@ if stackpsf_version == 1:\n         #psfargs0 = [os.getenv(\"EUCLID_HOME\") +os.sep +\"python\" +os.sep +\"pybin\" +os.sep +\"StackPsfExec.py\"]\n         psfargs1 = [\"--position_cat\", stackpsf_catalog, \"--ra\", stackpsf_ra_column, \"--dec\", stackpsf_dec_column, \"--mag_entry\", mag_column]\n         psfargs2 = [\"--coadd_zp\", str(coadd_zp), \"--tilename\", tilename, \"--band\", band, \"--SElist\", bgsublist, \"--outdir\", nhcoadd_out]\n-        psfargs3 = [\"--ramdisk\", ramdisk, \"--coadd\", coaddname, \"--max_layer_num\", \"1000\", \"--clean_up\", str(clean_up)]\n+        psfargs3 = [\"--ramdisk\", ramdisk, \"--resampledir\", resampledir_nhcoadd, \"--coadd\", coaddname, \"--max_layer_num\", \"1000\", \"--clean_up\", str(clean_up)]\n         psfargs4 = [\"--keep_edge_obj\", \"--coadd_cat\", catalog_name, \"--multiprocesses\", str(  int( multiprocesses/len(bands) )  ) ]\n         #psfargs4 = [\"--keep_edge_obj\", \"--multiproceses\", str(  int( multiprocesses/len(bands) )  ) ]\n         psfargs5 = [\"--bandpass_value\", str( bandpasses[\"Bandpass_\" +band] ), \"--swarp_resampling\", \"LANCZOS3\", \"--max_separation_for_match\", str(2*float(tileinfo[3]))]\n",
                            "add a separate folder for swarp temp (took it out of the general temp)",
                            "Michael",
                            "2023-05-16T20:42:42.000+02:00",
                            "c148ca2fc218a328e8f62903fa9cec79bd5a5489"
                        ],
                        [
                            "@@ -572,14 +572,15 @@ if stackpsf_version == 1:\n     for band in bands:\n         #<start name = \"parallel4\">\n         #stackPSF\n-        print('excute stackpsf for band ', band)\n+        Pipeline.log(\"excute stackpsf for band \" + str(band))\n         \n         #duplicate the coadd name from the fitscombine coadd section\n         coaddname = nhcoadd_out +os.sep +tilename +'_' +str(band) +'.fits' +Pipeline.evaluate_arg(savefpack, '.fz','')\n-        \n         catalog_name = Pipeline.get_catalog_name(nhcoadd_out, tilename, band)\n         reference_catalog, ra_column, dec_column, mag_column = Pipeline.select_reference_catalog(catalog_name, vis_position_cat, band)\n         \n+        Pipeline.log(\"got input parameters ...\")\n+        \n         #as default, use this catalog for stackpsf\n         stackpsf_catalog = reference_catalog\n         stackpsf_ra_column = ra_column\n@@ -600,7 +601,7 @@ if stackpsf_version == 1:\n         psfargs5 = [\"--bandpass_value\", str( bandpasses[\"Bandpass_\" +band] ), \"--swarp_resampling\", \"LANCZOS3\", \"--max_separation_for_match\", str(2*float(tileinfo[3]))]\n         psfargs = psfargs1 +psfargs2 +psfargs3 +psfargs4 +psfargs5\n         \n-        print(psfargs)\n+        Pipeline.log(\"psfargs \" +str(psfargs))\n         Pipeline.execute(\"EXT_PF1_StackPsfExec\", psfargs, wait=True )\n         \n         output_stackpsf = nhcoadd_out +os.sep +tilename +'_' +str(band) +'_stackedPSF.fits' \n",
                            "resolve a dependency to EXT_STACK_PSF",
                            "Michael",
                            "2023-05-11T17:01:33.000+02:00",
                            "7c1de7cb54eed2946858240bc4d1f5b3763d5630"
                        ],
                        [
                            "@@ -441,12 +441,16 @@ os.chdir(wd)\n Pipeline.log(\"SExtractor catalog for detection image.\")\n \n args = [detimagename, \"-c\", etcpath +\"default.sex\", \"-CATALOG_NAME\", detpsffile]\n-args += [\"-CATALOG_TYPE\", \"FITS_LDAC\", \"-WEIGHT_TYPE\", \"MAP_WEIGHT\"]\n-args += [\"-WEIGHT_IMAGE\", detweightname, \"-PARAMETERS_NAME\"]\n-args += [etcpath +\"sex.param_psfex\", \"-FILTER_NAME\", etcpath +\"sex.conv\"]\n+args += [\"-CATALOG_TYPE\", \"FITS_LDAC\"]\n+\n+#leave out the weight image as this seems to require proper image units\n+#drop the weight\n+#args += [\"-WEIGHT_IMAGE\", detweightname, \"-WEIGHT_TYPE\", \"MAP_WEIGHT\"]\n+args += [\"-PARAMETERS_NAME\", etcpath +\"sex.param_psfex\", \"-FILTER_NAME\", etcpath +\"sex.conv\"]\n args += [\"-STARNNW_NAME\", etcpath +\"sex.nnw\", \"-SATUR_LEVEL\", \"65000\", \"-DETECT_MINAREA\", \"3\"]\n-#args += [\"-DETECT_THRESH\", \"28.5,\" +str(coadd_zp)]\n+args += [\"-DETECT_THRESH\", \"1.6\",\"-THRESH TYPE\", \"RELATIVE\", \"-MAG_ZEROPOINT\", str(coadd_zp)]\n args += [\"-NTHREADS\", str(int(multiprocesses))]\n+\n Pipeline.execute(\"sex\", args, wait=True )\n Qa_log_reporter.output_file_exists('Detimage SExtractor catalog', detpsffile)\n \n@@ -506,12 +510,14 @@ for band in bands:\n         psffile = cc_image_ext.split(\".fits\")[0] +\"_psfcat.fits\"\n         args = [cc_image_ext, \"-c\", etcpath +\"default.sex\"]\n         args += [\"-CATALOG_NAME\", psffile, \"-CATALOG_TYPE\", \"FITS_LDAC\"]\n-        args += [\"-WEIGHT_TYPE\", \"MAP_WEIGHT\", \"-WEIGHT_IMAGE\", cc_weightname]\n+        #drop the weight\n+        #args += [\"-WEIGHT_TYPE\", \"MAP_WEIGHT\", \"-WEIGHT_IMAGE\", cc_weightname]\n         args += [\"-PARAMETERS_NAME\", etcpath +\"sex.param_psfex\"]\n         args += [\"-FILTER_NAME\", etcpath +\"sex.conv\"]\n         args += [\"-STARNNW_NAME\", etcpath +\"sex.nnw\"]\n         args += [\"-SATUR_LEVEL\", \"65000\", \"-DETECT_MINAREA\", \"3\"]\n-        #args += [\"-DETECT_THRESH\", \"3.0\"]\n+        args += [\"-DETECT_THRESH\", \"1.6\",\"-THRESH TYPE\", \"RELATIVE\", \"-MAG_ZEROPOINT\", str(coadd_zp)]\n+        \n         Pipeline.execute(\"sex\", args, wait=True )\n         Qa_log_reporter.output_file_exists('Sextractor catalog for psf', psffile)\n         \n@@ -531,8 +537,9 @@ for band in bands:\n         \n         args = [detection_image+\",\"+cc_image_ext, \"-c\", etcpath +\"sex.config \", \"-FILTER_NAME\", kernelfile]\n         args += [\"-STARNNW_NAME\", etcpath +\"sex.nnw\", \"-PARAMETERS_NAME\", etcpath +'sex.param_coadd_sersic']\n-        args += [\"-CATALOG_TYPE\", \"FITS_1.0\", \"-CATALOG_NAME\", catalog_name, \"-WEIGHT_TYPE\", \"MAP_WEIGHT\"]\n-        args += [\"-WEIGHT_IMAGE\", detection_weight_image+\",\"+cc_weightname, \"-MEMORY_BUFSIZE\", \"2048 \"]\n+        args += [\"-CATALOG_TYPE\", \"FITS_1.0\", \"-CATALOG_NAME\", catalog_name, \"-MEMORY_BUFSIZE\", \"2048 \"]\n+        #drop the weight\n+        #args += [\"-WEIGHT_TYPE\", \"MAP_WEIGHT\", \"-WEIGHT_IMAGE\", detection_weight_image+\",\"+cc_weightname]\n         args += [\"-MAG_ZEROPOINT\", str(hdr_magzp), \"-PSF_NAME\", detpsffile+\",\"+psfoutfile, \"-VERBOSE_TYPE\", Pipeline.evaluate_arg(verbose, \"NORMAL\", \"QUIET\")]\n         args += ['-BACK_TYPE','MANUAL', '-BACK_VALUE', '0.0,0.0'] #don't subtract backgrounds\n         #args += ['-BACK_TYPE','AUTO'] #subtract background\n",
                            "remove weights during catalog production",
                            "Michael",
                            "2023-03-15T14:07:39.000+01:00",
                            "efbb41eacf05570bd2074d25405cca0dc007ae45"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Pipeline/stackPSF.original": [
                        [
                            "@@ -3,7 +3,6 @@\n import datetime\n import argparse\n import sys\n-import itertools\n import numpy as np\n from astropy.coordinates import SkyCoord \n from astropy import units as u \n@@ -23,8 +22,8 @@ import  scipy.ndimage.measurements\n import file\n from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n from EXT_PF1_GEN_P2_LIBS.file import ExtractImage\n-from EXT_PF1_Coadd.file import PSFUtils\n-from EXT_PF1_Coadd.Pipeline import SigmaClip\n+from EXT_PF1_GEN_P2.file import PSFUtils\n+from EXT_PF1_GEN_P2.Pipeline import SigmaClip\n from EXT_PF1_GEN_P2_LIBS.Pipeline.Pipeline import Pipeline\n import matplotlib.pyplot as plt\n import math\n@@ -34,12 +33,14 @@ import time\n from EXT_PF1_GEN_P2_LIBS.astronomy.ArrayUtils import ArrayUtils\n from enum import Enum\n import multiprocessing as mp\n+from EXT_PF1_GEN_P2_LIBS.Pipeline import ProcessManager\n import traceback\n-\n+import operator\n+import logging\n \n __author__='Thomas Vassallo'\n __email__ ='thomas.vassallo@physik.lmu.de'\n-\n+\"\"\"\n class Stamp():\n     def __init__(self, image, x, y, stampSize, data_dim):\n         self.stampShift = (stampSize - 1) // 2\n@@ -63,6 +64,43 @@ class Stamp():\n                        int(stamp_start[1]):int(stamp_end[1])] = image[\n                            int(data_start[0]):int(data_end[0]),\n                            int(data_start[1]):int(data_end[1])]\n+\"\"\"                           \n+\n+                           \n+class Stamp():\n+    def __init__(self, image, stampSize, data_dim):\n+        self.image = image\n+        self.stampSize = stampSize\n+        self.data_dim = data_dim               \n+        self.stamp = np.zeros((int(stampSize), int(stampSize)))\n+    #\n+                           \n+    def get_stamp_at(self, x, y):\n+        #print(\"get stamp at x,y \", x, y)\n+        stampShift = (self.stampSize - 1) // 2\n+        start = np.array((np.floor(y - stampShift), \n+                          np.floor(x - stampShift)), \n+                         dtype = 'int')\n+        end   = np.array((self.stampSize, self.stampSize)) + start\n+        stamp_dim   = np.array((self.stampSize, self.stampSize))\n+        delta_start = 0 - start\n+        delta_start[delta_start < 0] = 0\n+        delta_end   = self.data_dim - end\n+        delta_end[delta_end > 0]     = 0\n+        if np.all((end + delta_end) - (start + delta_start) > 0):\n+            data_start  = start + delta_start\n+            data_end    = end   + delta_end\n+            stamp_start = delta_start\n+            stamp_end   = stamp_dim + delta_end\n+            \n+            self.stamp[int(stamp_start[0]):int(stamp_end[0]),\n+                       int(stamp_start[1]):int(stamp_end[1])] = self.image[\n+                           int(data_start[0]):int(data_end[0]),\n+                           int(data_start[1]):int(data_end[1])]\n+        #\n+        return self.stamp\n+    #\n+#\n \n class WcsUtils():\n     def __init__(self, image_header):\n@@ -169,7 +207,7 @@ class OutputStackedPSF():\n         #self.info_table['MAG'][int(self.firstidx):int(self.lastidx)] = mag[layer_objects[str(layer_index)]]\n         \n         # compute the output positions in the coadd pixel space\n-        skycoords  = self.prepare_sky_coordinates(self.info_table['RA'][int(self.firstidx):int(self.lastidx)],\n+        skycoords  = StackPSF.prepare_sky_coordinates(self.info_table['RA'][int(self.firstidx):int(self.lastidx)],\n                                                   self.info_table['DEC'][int(self.firstidx):int(self.lastidx)]) \n         #MW: due to the filtering, also other layers \n         #than layer 0 may have no objects\n@@ -228,8 +266,7 @@ class OutputStackedPSF():\n         \"\"\"\n         for objidx in range(self.firstidx, self.lastidx):\n             data_dim = (psfstack.naxis2, psfstack.naxis1)\n-            stamp  = Stamp(psfstack.image,  self.info_table['X'][objidx], \n-                           self.info_table['Y'][objidx], self.get_stamp_size(), data_dim)\n+            stamp  = Stamp(psfstack.image, self.get_stamp_size(), data_dim)\n \n             gridstart = np.array((\n                 np.floor(self.grididx %  self.nobj)     * self.get_stamp_size(),\n@@ -246,24 +283,15 @@ class OutputStackedPSF():\n                 self.grididx // self.nobj + 0.5)        * self.get_stamp_size() + 0.5\n             \n             norm = 1.0\n+            stamp_image = stamp.get_stamp_at(self.info_table['X'][objidx],self.info_table['Y'][objidx])\n             if normalize:\n-                norm = stamp.image.max() if stamp.image.max() != 0.0 else norm\n+                norm = stamp_image.max() if stamp_image.max() != 0.0 else norm\n             #\n             self.grid_image[int(gridstart[1]):int(gridend[1]),\n-                            int(gridstart[0]):int(gridend[0])] += stamp.image/norm\n+                            int(gridstart[0]):int(gridend[0])] += stamp_image/norm\n             self.grididx += 1\n     \n-\n-    def prepare_sky_coordinates(self, ra, dec):\n-        \"\"\"\n-        Stores the input sky coordinates in a format that can be used for\n-        projection into pixel space from the class  _getWcsLibCorners.py_world2pix\n-        \"\"\"\n-        iters      = [iter(ra), iter(dec)]\n-        skycoords  = list(it.__next__() for it in itertools.cycle(iters))\n-        return skycoords\n-\n-\n+    \n     def write_to_fits(self, outname):\n         \"\"\"\n         Creates the output FITS file and writes the extensions\n@@ -311,19 +339,26 @@ class OutputStackedPSF():\n         output_fits_file.close()\n     \"\"\"\n class ObjectList():\n-    def __init__(self, mag, ra = None, dec = None, X = None, Y = None):\n+    def __init__(self, mag, zp = None, ra = None, dec = None, X = None, Y = None):\n         self._length = 0\n+        self.mag = mag\n+        self.zp = zp\n+        self.ra = ra\n+        self.dec = dec\n+        self.X = X\n+        self.Y = Y\n+        \n         self._add_attribute('mag', mag)\n-        if ra is not None:\n-            self._add_attribute('ra',  ra)\n-        if dec is not None:\n-            self._add_attribute('dec', dec)\n-        if X is not None:\n-            self._add_attribute('X', X)\n-        if Y is not None:\n-            self._add_attribute('Y', Y)\n-            \n+        self._add_attribute('ra',  ra)\n+        self._add_attribute('zp',  zp)\n+        self._add_attribute('dec', dec)\n+        self._add_attribute('X', X)\n+        self._add_attribute('Y', Y)\n+        \n+    \n     def _add_attribute(self, name, lista):\n+        if lista is None:\n+            return\n         if self._length == 0:\n             setattr(self, name, lista)\n             self._length = len(lista)\n@@ -332,20 +367,28 @@ class ObjectList():\n                 setattr(self, name, lista)\n             else:\n                 print(\"attribute %s could not be set: wrong length\" % name)\n-\n+    \n     def _add_x_list(self, lista):\n-        self._add_attribute('X', lista)\n-\n+        self.X = lista\n+    \n     def _add_y_list(self, lista):\n-        self._add_attribute('Y', lista)     \n-\n+        self.Y = lista\n+    \n     def _filter(self, idx):\n-        for a in dir(self):\n-            if not a.startswith('_'):\n-                lista = getattr(self, a)\n+        for a in [\"mag\",\"zp\",\"ra\",\"dec\",\"X\",\"Y\"]:\n+            lista = getattr(self, a)\n+            try:\n+                #overwrite\n                 setattr(self, a, [lista[i] for i in idx])\n+            except:\n+                #a list may be None, so looping \n+                #them all is not a good idea\n+                logging.debug(\"ObjectList._filter(self, idx)\")\n+            #\n+        #\n         self._length = len(self.mag)\n-\n+    #\n+    \n class StackPSF(Pipeline):\n     def __init__(self, args):\n         \"\"\"\n@@ -404,6 +447,7 @@ class StackPSF(Pipeline):\n     # One class method, that is called from StackPSF but also from OutputStackedPSF\n     #\n     ################################################################################\n+    @staticmethod\n     def getPixels(obj, skycoords):\n         \"\"\"\n         Reads the WCS of obj, and projects skycoords\n@@ -427,6 +471,24 @@ class StackPSF(Pipeline):\n         return X, Y\n     #\n     \n+    \n+    @staticmethod\n+    def prepare_sky_coordinates(ra, dec):\n+        \"\"\"\n+        Stores the input sky coordinates in a format [ra0, dec0, r1, dec1, ...] that can be used for\n+        projection into pixel space from the class  _getWcsLibCorners.py_world2pix\n+        \"\"\"\n+        #iters      = [iter(ra), iter(dec)]\n+        #skycoords  = list(it.__next__() for it in itertools.cycle(iters))\n+        skycoords = (len(ra) + len(dec))*[0]\n+        for i in range(len(ra)):\n+            skycoords[2*i] = ra[i]\n+            skycoords[2*i+1] = dec[i]\n+        #\n+        return skycoords\n+    #\n+    \n+    \n     def get_stamp_size(self):\n         if self.stamp_size < 0:\n             min_separation, max_sep = self.compute_minimum_separation_for_objects_in_layer()\n@@ -483,47 +545,63 @@ class StackPSF(Pipeline):\n         #Loop over the list of layers\n         self.writeLog(\"Loop over the list of layers\")\n         for layer in self.layer_objects:\n+            start = time.time()\n             self.writeLog(\"init_layer\")\n             self.init_layer(layer)\n+            print(\"time for init layer \",  time.time() - start)\n             \n             # Define the subset of input objects that belong to \n             # the current layer\n+            start = time.time()\n             self.writeLog(\"define_object_subset_for_layer\")\n             layer_positions = self.define_object_subset_for_layer(layer)\n+            print(\"time for define_object_subset_for_laye \",  time.time() - start)\n             \n             # For each object, append to self.se_matches the list of single \n             # images containing the objects subset for the current layer:  \n             #data values ra, dec, mag are taken from single epoch catalogs\n+            start = time.time()\n             self.writeLog(\"match_layer_positions_with_single_epoch_catalogue_objects\")\n             self.match_layer_positions_with_single_epoch_catalogue_objects(layer_positions)\n+            print(\"time for match_layer_positions \",  time.time() - start)\n             \n             # For each object, propagate the flux information\n             # to the single images on which he object is not detected\n             # (only in case the coadd catalogue is not given as an\n             # input, otherwise we use the coadd flux)\n             if self._args.coadd_cat is None:\n+                start = time.time()\n                 self.writeLog(\"extend_flux_measurements_to_undetected_objects\")\n                 self.extend_flux_measurements_to_undetected_objects(layer_positions)\n-                \n+                print(\"time for extend_flux_measurements \",  time.time() - start)\n+            \n             # Loop over the list of SE images\n+            pm = ProcessManager.ProcessManager(self._args.multiprocesses)\n             self.writeLog(\"Looping over the list of SE images\")\n+            start = time.time()\n             for seimage in self.inputdata:\n                 ##### This part can be parallelized! (TBD) ###################\n                 self.writeLog(\"create_PSF_stamp_single_image\")\n-                self.create_PSF_stamp_single_image(layer_positions, seimage, layer)\n+                #self.create_PSF_stamp_single_image(layer_positions, seimage, layer)\n+                pm.add_process_and_start(self.create_PSF_stamp_single_image, args = (layer_positions, seimage, layer))\n                 ##### End of the parallel part ##############################\n-\n+            print(\"time for create_PSF_stamp_single \",  time.time() - start)\n             # Produce the stacked PSF image for the current layer\n+            start = time.time()\n             self.writeLog(\"create_stacked_PSF_image_for_layer\")\n             self.create_stacked_PSF_image_for_layer(layer)\n-\n+            print(\"time for create_stacked_PSF_image \",  time.time() - start)\n+            \n             # remove the temporary PSF images for the layer\n+            start = time.time()\n             self.writeLog(\"clean_up_layer_directory\")\n             self.clean_up_layer_directory(layer)\n+            print(\"time for clean_up_layer_directory \",  time.time() - start)\n         #\n         # Produce the output FITS file\n         self.writeLog(\"produce_output_FITS_file\")\n         self.produce_output_FITS_file()\n+    #\n     \n     def init_run(self):\n         \"\"\"\n@@ -596,6 +674,17 @@ class StackPSF(Pipeline):\n             return False\n         #\n         \n+        def layers_sorted_for_fewest_objects(lists:OrderedDict):\n+            layers = []\n+            for i in lists:\n+                layers.append( (i,len(lists[i]) ) )\n+            #\n+            #print(\"layers \", layers)\n+            slayers = sorted( layers, key=operator.itemgetter(1) )\n+            returnvalue = list(zip(*slayers))[0]\n+            #print(\"return \", returnvalue)\n+            return returnvalue\n+        #\n         \n         #now start the brute force\n         import time\n@@ -607,7 +696,13 @@ class StackPSF(Pipeline):\n         for obj_index in range(1,len(ra)):\n             #check, if existing layers already contains too close objects\n             object_has_found_a_layer = False\n-            for layer in lists:\n+            #distribute from reverse, as the latest layer has fewest objects\n+            #and thus processes fastest\n+            #-> this should start with the layer with the fewest objects\n+            #k = list(lists.keys())\n+            #k.reverse()\n+            for layer in layers_sorted_for_fewest_objects(lists):\n+                #for layer in lists:\n                 if not layer_contains_too_close_objects(lists[layer], obj_index):\n                     lists[layer].append(obj_index)\n                     object_has_found_a_layer = True\n@@ -623,9 +718,9 @@ class StackPSF(Pipeline):\n                 lists[len(lists)-1].append(obj_index)\n                 objects_in_layers = objects_in_layers +1\n             #\n-            if objects_in_layers > 0.85*totobj:\n-                self.writeLog(\"Distributed 85 percent of objects (= %s) to layers: \" %objects_in_layers )\n-                break\n+            #if objects_in_layers > 0.85*totobj:\n+            #    self.writeLog(\"Distributed 85 percent of objects (= %s) to layers: \" %objects_in_layers )\n+            #    break\n             #next obj_index\n             \n         #Here we should have all layers nicely filled\n@@ -655,173 +750,6 @@ class StackPSF(Pipeline):\n         \n         return self.layer_objects\n     #\n-    \n-    \n-    #MW: retired for the moment and replaced by divide_input_positions_into_layers_bf\n-    def divide_input_positions_into_layers(self, min_separation, ra, dec):\n-        \"\"\"\n-        Find the indexes of the objects that are so close, that their PSF \n-        overlap. Defines the dictionary self.layer_objects, containing the \n-        list of objects belonging to each layer.\n-        \"\"\"\n-        # Get the minimum angular separation for non overlapping PSFs\n-        self.min_separation = np.deg2rad(min_separation / 3600.)\n-        \n-        self.writeLog(\n-            \"Define the layers of non-overlapping PSFs\")\n-        print(\"divide_input_positions_into_layers separation\", self.min_separation)\n-        coordinates    = np.array([(x,y) for x,y in zip(\n-            np.deg2rad(ra), np.deg2rad(dec) )])\n-        tree           = BallTree(coordinates, metric = 'haversine')\n-        self.counts    = tree.query_radius(coordinates, \n-                                           r = self.min_separation,\n-                                           count_only = True)\n-        print(\"self.counts = \" , self.counts, \", len \", len(self.counts), \", compare to objects \", len(ra))\n-        \n-        #for all points (ra,dec) return a list of the indices \n-        #of the neighbours within distance min_separation\n-        indexes        = tree.query_radius(coordinates, \n-                                           r = self.min_separation)\n-        \n-        lists          = OrderedDict()\n-        self.cluster_objects = []\n-        \n-        # Get the indexes of the objects that do not overlap\n-        #MW: int(x[0]) as these values are numpy.int64,which cannot be json stored\n-        #counts is a 1d array with integer numbers denoting how many\n-        #neighbours have been found within max_separation\n-        single_objects = [int(x[0]) for x in indexes[\n-            np.where(self.counts == 1)[0]]]\n-        \n-        #number of objects to distribute onto layers\n-        no_objects = len(self.counts) - len(single_objects) \n-        \n-        max_order   = max(self.counts)\n-        \n-        # get the groups of objects\n-        for cc in range(2, max_order + 1):\n-            idxs    = np.where(self.counts == cc)[0]\n-            #collect all entries of indexes with cc entries\n-            all_idx = indexes[idxs]\n-            #reformat to a list of lists with cc entries\n-            set_idx = [list(set(x)) for x in all_idx]\n-            #print(\"set_idx for \", cc, \" \", set_idx)\n-            set_idx.sort() #what does that sort?\n-            #cluster_objects contains all lists of indices sorted\n-            #by length: first all 2 neightbours, then all 3 neighbours etc.\n-            self.cluster_objects += list(\n-                set_idx for set_idx,_ in itertools.groupby(set_idx))\n-        already_assigned_indexes = []\n-\n-        # Loop on the layers\n-        #why not range(max_layers) ?\n-        for layer in range(1000):\n-            if (len(self.cluster_objects) == 0) or (\n-                    len(already_assigned_indexes) == no_objects):\n-                break\n-            lists[layer] = []\n-            \n-            self.cluster_objects, already_assigned_indexes = \\\n-                     self.take_out_already_assigned_indices(self.cluster_objects, already_assigned_indexes)\n-            \n-            # Loop on the group of objects\n-            for idx in range(len(self.cluster_objects) - 1, -1, -1):\n-                if len(already_assigned_indexes) == no_objects:\n-                    #all is done\n-                    break\n-\n-                # Check if the group of overlapping objects contains objects \n-                # that have been already assigned to the current layer. \n-                # Remove them, and change group of objects\n-                if len(list(set(lists[layer]).intersection(\n-                        set(self.cluster_objects[idx])))) > 0:\n-                    continue\n-\n-                # Loop inside the group of objects\n-                indx = 0\n-                loop_in_the_group = True\n-                while loop_in_the_group:\n-                    # Break if the index exceeds the dimension of the group\n-                    if indx >= len(self.cluster_objects[idx]):\n-                        break\n-\n-                    skip = False\n-                    # Check if this element of the list appears in some other \n-                    # group, paired with objects that are already in the current\n-                    # layer. In this case, skip to the next object in the  group\n-                    for idx2 in range(len(self.cluster_objects) - 1, -1, -1):\n-                        if (self.cluster_objects[idx][indx] in (\n-                                self.cluster_objects[idx2])) and (\n-                                    len(list(set(lists[layer]).intersection(\n-                                        set(self.cluster_objects[idx2])))) > 0) :\n-                            skip = True\n-                            break\n-                    if skip:\n-                        indx += 1\n-                        continue\n-                    # If the element in the list is not paired to an element \n-                    # that appears in the current layer for any group of \n-                    # objects, can be added to the current layer, and to the \n-                    #list of already assigned objects\n-                    loop_in_the_group = False\n-                    #MW: int( copy.deepcopy etc.) as these values are numpy.int64,which cannot be json stored\n-                    lists[layer].append(int(copy.deepcopy(\n-                        self.cluster_objects[idx][indx])))\n-                    already_assigned_indexes.append(copy.deepcopy(\n-                        self.cluster_objects[idx][indx]))\n-\n-                    if len(already_assigned_indexes) == (\n-                            len(self.counts) - len(single_objects)):\n-                        break\n-                \n-            self.cluster_objects = [x for x in self.cluster_objects if x != []] \n-        \n-        #add all single objects to layer 0\n-        #they have nothing around them so it should be safe\n-        #to put them anywhere\n-        print(\"layer 1 contains \", len(lists[0]) , \" objects. Add single objects \", len(single_objects))\n-        lists[0] += single_objects\n-        tot_obj   = 0\n-        totobj    = len(coordinates)\n-        for k in lists:\n-            tot_obj += len(lists[k])\n-        if tot_obj  != totobj:\n-            self.writeLog(\n-                'Error: the sum of the number of objects in layers %s' % tot_obj)\n-            self.writeLog(\n-                'differs from the total number of objects %s !' % totobj)\n-\n-        self.layer_objects = OrderedDict() \n-\n-        max_layers     = self._args.max_layer_num\n-        if max_layers <= 0 or max_layers > max_order:\n-            max_layers = max_order\n-\n-        layeridx       = 0\n-        missing_obj    = 0\n-\n-        self.writeLog(\"Total number of objects in the input catalog: %s\" % totobj)\n-        for k in lists:\n-            if layeridx < max_layers:\n-                self.layer_objects[str(layeridx)] = lists[k]\n-                numb = len(self.layer_objects[str(layeridx)])\n-                self.writeLog(\"Layer %2d contains %5d objects (%2.2f %%)\" % (\n-                    layeridx, numb, 100 * float(numb) / float(totobj)))\n-                layeridx += 1\n-            else:\n-                numb = len(lists[k])\n-                self.writeLog(\"Layer %2d contains %5d objects (%2.2f %%)\" % (\n-                    k,        numb, 100 * float(numb) / float(totobj)))\n-                missing_obj += len(lists[k])\n-        \n-        self.writeLog('The total number of layers is set to %s.' % max_layers)\n-        self.writeLog('You are missing the PSF for %s objects (%2.2f %%).' % \n-                      (missing_obj, 100 * float(missing_obj) / float(totobj)))\n-\n-        json_dump = json.dumps(self.layer_objects, indent = 4)\n-        f         = open(\"%s/layer_obj.json\" % self.tmpdir,\"w\")\n-        f.write(json_dump)\n-        f.close()\n         \n         \n     def take_out_already_assigned_indices(self, cluster_objects, already_assigned_indexes):\n@@ -929,13 +857,21 @@ class StackPSF(Pipeline):\n             \n             for img in sedict:\n                 if sedict[img][0] == self._args.default_zp:\n+                    self.writeLog(\"remaining sedict with default zp \" +str(sedict[img]))\n                     self.se_matches[\n-                        '%.10f:%.10f' % (objects.ra[i], objects.dec[i])][img][0] = mm - self.inputdata[img][\"zp\"] \n-        \n+                        '%.10f:%.10f' % (objects.ra[i], objects.dec[i])][img][0] = mm - self.inputdata[img][\"zp\"]\n+                else:\n+                    self.writeLog(\"extend_flux_measurements_to_undetected_objects \" +str( sedict[img][0]) )\n+                #\n+            #\n+        #\n+    #\n+    \n     def create_PSF_stamp_single_image(self, layer_positions, seimage, layer):\n         \"\"\"\n         creates the single image containing the PSF stamps\n         \"\"\"\n+        s = time.time()\n         self.writeLog(\"%s\" % seimage)\n         single_epoch_image = self.init_image(seimage)\n         zp                 = self.inputdata[single_epoch_image.name][\"zp\"] \n@@ -945,14 +881,19 @@ class StackPSF(Pipeline):\n         se_overlapping_objects = self.get_PSF_info(single_epoch_image.name, layer_positions)\n         if se_overlapping_objects._length == 0:\n             return\n+        print(\"time for init_image etc. \", time.time() - s)\n         \n+        s = time.time()\n         psfimage, psfweight, _, _ = self.get_PSF_image(\n             single_epoch_image, se_overlapping_objects)\n+        print(\"time for adding psfstamps to image \", time.time() - s)\n         \n+        s = time.time()\n         if psfimage is None:\n             self.writeLog('  Warning: PSF image could not be created')\n             return\n         self.write_to_fits(layer, single_epoch_image, psfimage, psfweight)\n+        print(\"time for writing psfstamps to fitsimage \", time.time() - s)\n         \n     def create_stacked_PSF_image_for_layer(self, layer):\n         \"\"\"\n@@ -1039,8 +980,10 @@ class StackPSF(Pipeline):\n             self.FilterType = FilterTypes.DES\n         elif \"omegacam\" in instrument:\n             self.FilterType = FilterTypes.KIDS\n-        else: #add condition here. #LSST = ?\n+        elif \"LSST\" in instrument:\n             self.FilterType = FilterTypes.LSST\n+        else: #add condition here. #LSST = ?\n+            self.FilterType = FilterTypes.NOS\n         #\n     #\n     \n@@ -1071,27 +1014,28 @@ class StackPSF(Pipeline):\n             for obj in self.layer_objects[str(i)]:\n                 x = X[obj]\n                 y = Y[obj]\n-                stamp  = Stamp(psfstack.image, x, y, self.get_stamp_size(), data_dim)\n+                stamp  = Stamp(psfstack.image, self.get_stamp_size(), data_dim)\n+                stamp_image = stamp.get_stamp_at(x,y)\n                 \n-                _max = abs(np.amax(stamp.image))\n-                _min = abs(np.amin(stamp.image))\n-                _sum = np.sum(stamp.image)\n+                _max = abs(np.amax(stamp_image))\n+                _min = abs(np.amin(stamp_image))\n+                _sum = np.sum(stamp_image)\n                 #print(\" min, max \", _min, _max, \", same order of magnitude? \", (_max - _min)/_max < 0.9, \", sum \", _sum )\n                 if abs(_sum) < 0.0001 or _sum < 0:\n                     print(\"sum of the values is too low. Reject.\", _sum)\n                     continue\n                 try:\n-                    amplitude, center, sigma = ArrayUtils.fit_simple_gauss_2d(stamp.image)\n+                    amplitude, center, sigma = ArrayUtils.fit_simple_gauss_2d(stamp_image)\n                     #print(\"sigma \", sigma)\n                     if amplitude < 0:\n                         print(\"Negative fit amplitude. Reject.\")\n                         continue\n-                    psf_dim = np.array(stamp.image.shape)\n+                    psf_dim = np.array(stamp_image.shape)\n                     idxs = ArrayUtils.get_array_indices(psf_dim)\n-                    shape = stamp.image.shape\n+                    shape = stamp_image.shape\n                     gauss = ArrayUtils.simple_gauss_2d(idxs, amplitude, center[1], center[0], sigma)\n-                    print(np.sqrt(np.mean((stamp.image.flatten() - gauss) ** 2.))/amplitude)\n-                    if np.sqrt(np.mean((stamp.image.flatten() - gauss) ** 2.))/amplitude < 0.05:\n+                    print(np.sqrt(np.mean((stamp_image.flatten() - gauss) ** 2.))/amplitude)\n+                    if np.sqrt(np.mean((stamp_image.flatten() - gauss) ** 2.))/amplitude < 0.05:\n                         temp.append(obj)\n                     else:\n                         print('PSF centered at %.3f, %.3f skipped: bad PSF shape' % (x, y))\n@@ -1178,7 +1122,6 @@ class StackPSF(Pipeline):\n         #optionally, sort ra,dec,mag by magnitude\n         if True: #self._args.sort_catalog_by_mag:\n             #convert the lists to one list of tuples\n-            import operator\n             def sort_table(table, col=0):\n                 #invert the table to get columns instead of rows\n                 sorted_table = sorted( list(zip(*table)), key=operator.itemgetter(col))\n@@ -1192,6 +1135,10 @@ class StackPSF(Pipeline):\n             self.mag = np.array(sorted_table[2])\n         #\n         \n+        #we have assigned self._args.default_zp as an default value\n+        #is it still in the maglist after coadd catalog matches hav been assigned?\n+        self.writeLog(\"read_input_positions: number of default mags remaining in list \" + str( sorted_table[2].count(self._args.default_zp) ))\n+        \n         # Initialize the counter of single images in which we \n         # find the given object\n         self.se_matches = {}\n@@ -1219,7 +1166,7 @@ class StackPSF(Pipeline):\n         dec    = cat_data[self._args.dec].read()\n         mag    = cat_data[self._args.mag_entry].read()\n         \n-        idx = np.where(mag <= self._args.ref_mag)\n+        idx = np.where( np.logical_and((mag >= self._args.min_mag), (mag <= self._args.ref_mag)) )\n         print(\"positions from refcat \", len(ra), \", filtered by magnitude \", str(self._args.ref_mag), \" \", len(ra[idx]))\n         return ra[idx], dec[idx], mag[idx]\n     \n@@ -1269,15 +1216,17 @@ class StackPSF(Pipeline):\n         \n         #good      = np.where(mag < 90)[0]\n         #for consistency...\n-        good = np.where(mag <= self._args.ref_mag)\n+        good = np.where( np.logical_and((mag >= self._args.min_mag), (mag <= self._args.ref_mag)) )\n         mag       = mag[good]\n         cox       = cox[good]\n         coy       = coy[good]\n         \n         # Project the detected object position to the sky to \n         # compute the ra/dec positions\n-        ite       = [iter(cox), iter(coy)]\n-        pixcoo    = list(it.__next__() for it in itertools.cycle(ite))\n+        #MW: retired\n+        #ite       = [iter(cox), iter(coy)]\n+        #pixcoo    = list(it.__next__() for it in itertools.cycle(ite))\n+        pixcoo    = StackPSF.prepare_sky_coordinates(cox, coy)\n         \n         #MW: do this consistently with astropy\n         #MW: no, as this crashes KIDS processing\n@@ -1305,7 +1254,7 @@ class StackPSF(Pipeline):\n         \"\"\"\n         #get a extract image object of the coadd\n         self.coadd = self.getCoaddMetadata(self._args.coadd)\n-        skycoords  = self.prepare_sky_coordinates(ra, dec)\n+        skycoords  = StackPSF.prepare_sky_coordinates(ra, dec)\n         X, Y       = StackPSF.getPixels(self.coadd, skycoords)\n         \n         #MW: getPixels subtracts 1, here it is added again\n@@ -1314,15 +1263,6 @@ class StackPSF(Pipeline):\n         #write a test\n         #also: check, if X and Y must be swapped\n         return X + 1, Y + 1\n-\n-    def prepare_sky_coordinates(self, ra, dec):\n-        \"\"\"\n-        Stores the input sky coordinates in a format that can be used for\n-        projection into pixel space from the class  _getWcsLibCorners.py_world2pix\n-        \"\"\"\n-        iters      = [iter(ra), iter(dec)]\n-        skycoords  = list(it.__next__() for it in itertools.cycle(iters))\n-        return skycoords\n     \n     \n     def measure_gains_of_single_images(self):\n@@ -1427,7 +1367,7 @@ class StackPSF(Pipeline):\n         #layer_positions must be copied and then thrown away\n         temp_layer_pos = copy.deepcopy(layer_positions)\n         \n-        skycoords  = self.prepare_sky_coordinates(temp_layer_pos.ra, temp_layer_pos.dec)\n+        skycoords  = StackPSF.prepare_sky_coordinates(temp_layer_pos.ra, temp_layer_pos.dec)\n         # Project the coordinates onto the se image pixel space\n         WCSU       = WcsUtils(single_epoch_image.returnHeader())\n         se_x, se_y   = WCSU.py_world2pix(skycoords)\n@@ -1628,9 +1568,9 @@ class StackPSF(Pipeline):\n                 (y < naxis2 + 0.5 + comparator)\n         except Exception as e:\n             print(e)\n-            printf(\"comparator %i, type %s \", comparator, type(comparator))\n-            printf(\"naxis1 %i, type %s \", naxis1, type(naxis1))\n-            printf(\"naxis2 %i, type %s \", naxis2, type(naxis2))\n+            print(\"comparator \", comparator, \", type \", type(comparator))\n+            print(\"naxis1 \", naxis1, \", type \", type(naxis1))\n+            print(\"naxis2 \", naxis2, \", type \", type(naxis2))\n             \n     #\n     \n@@ -1699,9 +1639,10 @@ class StackPSF(Pipeline):\n                 \"\"\"\n \n                 magnitude = overlapping_positions.mag[i]\n-                if \"DECam\" in instrument:\n-                    #correct for incorrect DECam single epoch magnitudes\n-                    overlapping_positions.mag[i] - self.inputdata[seobj.name][\"zp\"] + self._args.default_zp\n+                #MW: DECAM is now calibrated\n+                #if \"DECam\" in instrument:\n+                #    #correct for incorrect DECam single epoch magnitudes\n+                #    overlapping_positions.mag[i] - self.inputdata[seobj.name][\"zp\"] + self._args.default_zp\n                 if self.stamp_overlaps_image(overlapping_positions.X[i], overlapping_positions.Y[i], seobj.naxis1, seobj.naxis2):\n                     self.se_matches[\"%.10f:%.10f\" % (overlapping_positions.ra[i], overlapping_positions.dec[i])][seobj.name] = [\n                         magnitude, \n@@ -1738,6 +1679,7 @@ class StackPSF(Pipeline):\n                     self.se_matches[\"%.10f:%.10f\" % (overlapping_positions.ra[i], overlapping_positions.dec[i])][seobj.name] = [\n                         overlapping_positions.mag[i] - self.inputdata[seobj.name][\"zp\"] + self._args.default_zp, \n                         overlapping_positions.ra[i], overlapping_positions.dec[i]]\n+                    self.writeLog(\"fill_se_matches: overlapping_positions.mag[i] \" +str(overlapping_positions.mag[i]) +\", self.inputdata[seobj.name]['zp'] \" +str(self.inputdata[seobj.name]['zp']) +\", self._args.default_zp \" +str(self._args.default_zp) )\n                 elif self.stamp_overlaps_image(overlapping_positions.X[i], overlapping_positions.Y[i], seobj.naxis1, seobj.naxis2):\n                     self.se_matches[\"%.10f:%.10f\" % (overlapping_positions.ra[i], overlapping_positions.dec[i])][seobj.name] = [\n                         -99, overlapping_positions.ra[i], overlapping_positions.dec[i]]\n@@ -1757,7 +1699,8 @@ class StackPSF(Pipeline):\n         \"\"\"\n         if retval !=0:\n             self.writeLog(retval)\n-            raise\n+        #\n+    #\n         \n     def get_auxiliary_file_names(self, sename):\n         \"\"\"\n@@ -1856,7 +1799,7 @@ class StackPSF(Pipeline):\n         self.writeLog('The total number of layers is set to %s.' % max_layers)\n         self.writeLog('You are missing the PSF for %s objects.' % \n                       missing_obj)\n-\n+        \n     def write_to_fits(self, layer, seobj, psfimage, psfweight):\n         \"\"\"\n         Write the PSF single image to a FITS file\n@@ -1875,7 +1818,28 @@ class StackPSF(Pipeline):\n                 header = seobj.returnHeader(seobj.firsthdu + 2), \n                 extver = 3) \n         f.close()\n-\n+    #\n+    \n+    def new_write_to_fits(self, layer, seobj, psfimage, psfweight):\n+        \"\"\"\n+        Write the PSF single image to a FITS file\n+        \"\"\"\n+        layerdir = self.tmpdir + os.sep + \"layer%s\" % layer\n+        tmp_out_name   = layerdir + os.sep + self.out_name % (\n+            os.path.basename(seobj.name.split(\".fits\")[0])) \n+        \n+        hdul = fits.open(seobj.name)\n+        \n+        hdu0 = fits.PrimaryHDU(data = np.array(psfimage, dtype = 'float32'), \n+            header = hdul[seobj.firsthdu].header)\n+        \n+        hdu1 = fits.ImageHDU(data = np.zeros(psfimage.shape, dtype = 'float32'), header = hdul[seobj.firsthdu +1].header)\n+        hdu2 = fits.ImageHDU(data = np.array(psfweight,      dtype = 'float32'), header = hdul[seobj.firsthdu +2].header)\n+        fits.HDUList([hdu0, hdu1, hdu2]).writeto(tmp_out_name, overwrite=True)\n+        hdul.close()\n+    #\n+    \n+    \n     def writeLog(self, message):\n         \"\"\"\n         prints the message and writes it in the log file\n@@ -1888,7 +1852,7 @@ class StackPSF(Pipeline):\n         \"\"\"\n         Return  the PSF grid image and weights\n         \"\"\"\n-        skycoords = self.prepare_sky_coordinates(se_objects.ra, se_objects.dec)\n+        skycoords = StackPSF.prepare_sky_coordinates(se_objects.ra, se_objects.dec)\n         X, Y      = StackPSF.getPixels(seobj, skycoords)\n         if len(X) == 0:\n             self.writeLog(\"No projected positions\")\n@@ -1903,11 +1867,11 @@ class StackPSF(Pipeline):\n         #get the psf stamp size from the psf file header\n         psfnaxis               = psfobj.get_convolution_params()\n         \n-        objects                = ObjectList(se_objects.mag, X = X, Y = Y)\n+        #MW here the objects need a zp\n+        zp                 = self.inputdata[seobj.name][\"zp\"] \n+        objects                = ObjectList(se_objects.mag, zp = [zp]*len(X), X = X, Y = Y)\n         \n         # Evaluate the PSF for each pixel\n-        #get the array of stamps\n-        s = time.time()\n         \n         #this takes time\n         #can we parallelize it?\n@@ -1946,7 +1910,7 @@ class StackPSF(Pipeline):\n             #\n             \n             counter+= 1\n-        print(\"time for adding \", counter, \" psfstamps to image \", time.time() - s)\n+        print(\"added \", counter, \" psfstamps to image \")\n         weights = gains / (data + noise)\n         \n         #weights = 1.0/ (data + float(seobj.getKey('skysigma'))*float(seobj.getKey('skysigma')) )\n@@ -2045,8 +2009,11 @@ class StackPSF(Pipeline):\n         else:\n             bg_hdu = FitsServices.get_bg_hdu(sename)\n             bg = FitsServices.read_data(sename, bg_hdu)\n-            \n-            return np.mean(bg), np.std(bg)\n+            if bg is not None:\n+                return np.mean(bg), np.std(bg)\n+            #\n+        #MW: fix that header value/background hdu problem asap!    \n+        return 0.01, 0.001 \n         #\n     #\n     \n@@ -2076,7 +2043,7 @@ class StackPSF(Pipeline):\n         gains[:, (data_dim[1] // 2):] *= self.G[seobj.name]['B']\n         return data, noise, gains\n \n-    def get_normalization_for_stamp(self, mag):\n+    def get_normalization_for_stamp(self, mag, zp):\n         \"\"\"\n         Evaluate the normalization for the PSF stamp\n         \n@@ -2089,12 +2056,11 @@ class StackPSF(Pipeline):\n         if mag == -99:\n             return 0.\n         else:\n-            #convert the magnitudes to flux using the default zp\n+            #convert the magnitudes to flux using the zp\n             #-> this should use the real zp of the image\n-            #well, these are single epoch catalog values\n-            #so default ZP is correct for DECam, and for no-one else\n-            return 10 ** ((self._args.default_zp - mag) / 2.5)\n-\n+            #self.writeLog(\"get_normalization_for_stamp \" +str(mag) +\", zp \" +str(zp))\n+            return 10 ** ((zp - mag) / 2.5)\n+    \n     def evaluate_PSF_for_pixel(self, pixel, psfobj, norm):\n         \"\"\"\n         Evaluate the PSF from the PSF model for the given pixel\n@@ -2117,7 +2083,11 @@ class StackPSF(Pipeline):\n             # pixel is in the SE coordinate reference frame\n             pixel     = (objects.X[object_index], objects.Y[object_index])\n             #convert the magnitude to flux (=norm)\n-            norm      = self.get_normalization_for_stamp(mag)\n+            \n+            #MW here we need the real zp from the se_image of the objects origin\n+            zp = objects.zp[object_index]\n+            #self.writeLog(\"add_PSF_stamp_to_image: objects zp \" +str(zp) )\n+            norm      = self.get_normalization_for_stamp(mag, zp)\n             \n             \n             #which mags to we deal with?\n@@ -2186,6 +2156,7 @@ class StackPSF(Pipeline):\n                     gains[int(start[1]):int(end[1]), int(start[0]):int(end[0])] = 0\n                 #\n             #\n+            return data, gains, stamp/norm\n         #\n         except Exception as e:\n             #if there is a problem for one object, \n@@ -2194,7 +2165,7 @@ class StackPSF(Pipeline):\n             exec_info =  sys.exc_info()\n             traceback.print_tb(exec_info[2])\n         #\n-        return data, gains, stamp/norm\n+        \n         \n     def get_indexes_of_stamp_on_image(self, pixel, stamp_dim):\n         \"\"\"\n@@ -2449,7 +2420,9 @@ class StackPSF(Pipeline):\n         f = open(fscaleList, \"w+\")\n         for sename in single_epoch_list_for_layer:\n             zp = self.inputdata[sename.split(\" \")[0]][\"zp\"]\n-            fluxscale = pow(10, ((self._args.default_zp - zp) / 2.5))\n+            #fluxscale for swarp: sets the coadd to default_zp\n+            #-> adapt that to coadd_zp?\n+            fluxscale = pow(10, ((self._args.coadd_zp - zp) / 2.5))\n             f.write(\"%s\\n\" % fluxscale)\n         f.close()\n         return fscaleList\n@@ -2555,7 +2528,8 @@ class FilterType(object):\n class FilterTypes(Enum):\n     DES = FilterType(\"DES\", 0.0001, 0.3, 0.0, 0.05)\n     KIDS = FilterType(\"KIDS\", 0.0001, 0.3, 0.0, 0.2)\n-    LSST = FilterType(\"LSST\", 0.0, 0.0, 0.0, 0.0)\n+    LSST = FilterType(\"LSST\", 0.0001, 0.2, 0.0, 0.05)\n+    NOS = FilterType(\"NOS\", 0.0001, 0.3, 0.0, 0.05)\n #\n \n class PsfStampFilter(object):\n@@ -2582,7 +2556,10 @@ class PsfStampFilter(object):\n             return layer_objects\n         #\n         temp_layer_objects = OrderedDict()\n-        \n+        counter_zeroes = 0\n+        counter_badshape = 0\n+        counter_stampEmpty = 0\n+        zero_fraction = list()\n         for i in range(len(layer_objects)):\n             #get the coadd image\n             layer_coadd = layer_coadds[str(i)]\n@@ -2596,40 +2573,49 @@ class PsfStampFilter(object):\n             for obj in layer_objects[str(i)]:\n                 x = objects_x[obj]\n                 y = objects_y[obj]\n-                stamp  = Stamp(imgdata, x, y, self.get_stamp_size(), imgdata.shape)\n+                stamp  = Stamp(imgdata, self.get_stamp_size(), imgdata.shape)\n+                stamp_image = stamp.get_stamp_at(x,y)\n+                \n+                _max = abs(np.amax(stamp_image))\n+                _min = abs(np.amin(stamp_image))\n+                _sum = np.sum(stamp_image)\n                 \n-                _max = abs(np.amax(stamp.image))\n-                _min = abs(np.amin(stamp.image))\n-                _sum = np.sum(stamp.image)\n                 \n-                \"\"\"\n                 #print(\" min, max \", _min, _max, \", same order of magnitude? \", (_max - _min)/_max < 0.9, \", sum \", _sum )\n-                if 1.0*np.sum(stamp.image == 0)/(self.get_stamp_size()*self.get_stamp_size())  > self.filtertype.fraction_zeroes_upper_limit:\n+                z_f = 1.0*np.sum(stamp_image == 0)/(self.get_stamp_size()*self.get_stamp_size())\n+                zero_fraction.append(z_f)\n+                if z_f > self.filtertype.fraction_zeroes_upper_limit:\n                     #if abs(_sum) < self.filtertype.limit_abs_sum or _sum < self.filtertype.sum_upper_limit:\n-                    print(\"Contains 0.0. Reject.\", np.sum(stamp.image == 0))\n+                    #print(\"Contains 0.0. Reject.\", np.sum(Stamp_image == 0))\n+                    counter_zeroes = counter_zeroes +1\n                     continue\n+                \"\"\"\n                 try:\n-                    amplitude, center, sigma = ArrayUtils.fit_simple_gauss_2d(stamp.image)\n+                    amplitude, center, sigma = ArrayUtils.fit_simple_gauss_2d(Stamp_image)\n                     #print(\"sigma \", sigma)\n                     if amplitude < self.filtertype.amplitude_upper_limit:\n-                        print(\"Negative fit amplitude. Reject.\")\n+                        #print(\"Negative fit amplitude. Reject.\")\n+                        counter_badshape = counter_badshape + 1\n                         continue\n-                    psf_dim = np.array(stamp.image.shape)\n+                    psf_dim = np.array(Stamp_image.shape)\n                     idxs = ArrayUtils.get_array_indices(psf_dim)\n-                    shape = stamp.image.shape\n+                    shape = Stamp_image.shape\n                     gauss = ArrayUtils.simple_gauss_2d(idxs, amplitude, center[1], center[0], sigma)\n-                    print(np.sqrt(np.mean((stamp.image.flatten() - gauss) ** 2.))/amplitude)\n-                    if np.sqrt(np.mean((stamp.image.flatten() - gauss) ** 2.))/amplitude < self.filtertype.uppler_limit_fit_deviation:\n+                    print(np.sqrt(np.mean((Stamp_image.flatten() - gauss) ** 2.))/amplitude)\n+                    if np.sqrt(np.mean((Stamp_image.flatten() - gauss) ** 2.))/amplitude < self.filtertype.uppler_limit_fit_deviation:\n                         temp.append(obj)\n                     else:\n-                        print('PSF centered at %.3f, %.3f skipped: bad PSF shape' % (x, y))\n+                        #print('PSF centered at %.3f, %.3f skipped: bad PSF shape' % (x, y))\n+                        counter_badshape = counter_badshape + 1\n                         continue\n                 except:\n                     print('PSF centered at %.3f, %.3f skipped: could not fit a gaussian' % (x, y))\n+                    counter_badshape = counter_badshape + 1\n                 \"\"\"\n                 #test only for completely empty stamps\n                 if abs(_sum) < 0.0001 or _sum < 0:\n-                    print(\"reject for low sum \", _sum, \", x \", x, \"y \", y)\n+                    #print(\"reject for low sum \", _sum, \", x \", x, \"y \", y)\n+                    counter_stampEmpty = counter_stampEmpty +1\n                     continue\n                 else:\n                     temp.append(obj)\n@@ -2637,6 +2623,12 @@ class PsfStampFilter(object):\n             #\n             temp_layer_objects[str(i)] = temp\n         print(\"  \", [len(temp_layer_objects[x]) for x in temp_layer_objects])\n+        #print(zero_fraction)\n+        print(\"stamps filtered out:\")\n+        print(\"bad shape \", counter_badshape)\n+        print(\"incomplete coverage (too many zeroes) \", counter_zeroes)\n+        print(\"stamp completely empty \", counter_stampEmpty)\n+        \n         return temp_layer_objects\n     #\n #\n@@ -2661,6 +2653,8 @@ if __name__ == \"__main__\":\n     argparser.add_argument(\"--dec\",           type = str, default = 'DEC', \n                            help = 'name of the catalogue field containing \\\n                            dec positions')\n+    argparser.add_argument(\"--mag_entry\",  type = str, default = 'MAG_AUTO', \n+                           help = 'magnitude entry in the position catalog.')\n     argparser.add_argument(\"--tilename\",      type = str, default = '', \n                            help = 'tile name')\n     argparser.add_argument(\"--band\",          type = str, default = '', \n@@ -2698,14 +2692,9 @@ if __name__ == \"__main__\":\n                            in the final PSF product. If set to 0, the code \\\n                            will use the maximum value from the layer \\\n                            computation')\n-    argparser.add_argument(\"--min_zp\",        type = float, default = 24.5, \n-                           help = 'Minimum value of the zero point to be used \\\n-                           in the analysis. Single images with lower zero \\\n-                           points will be dropped')    \n-    argparser.add_argument(\"--max_zp\",        type = float, default = 25.5, \n-                           help = 'Maximum value of the zero point to be used \\\n-                           in the analysis. Single images with higher zero \\\n-                           points will be dropped')\n+    argparser.add_argument(\"--coadd_zp\", type = float, default = 30.0, \n+                           help = 'Value of the coadd zeropoint. This is where the \\\n+                           coadded simulated psfs are flux-scaled to')    \n     argparser.add_argument(\"--default_zp\",        type = float, default = 25., \n                            help = 'Value of the uncalibrated zero point')\n     argparser.add_argument(\"--clean_up\",        type = int, default = True, \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/RMS/RMSAnalysis.py": [
                        [
                            "@@ -0,0 +1,288 @@\n+from matplotlib import pyplot as plt\n+\n+import numpy as np\n+\n+from astropy.io import fits\n+from astropy.table import Table\n+\n+import argparse\n+\n+import sys\n+import math\n+\n+\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+\n+\n+class RMSAnalysis(object):\n+    image_fits_file = \"\"\n+\n+\n+    def __init__(self, image_fits_file):\n+        \"\"\"\n+        For each image, this class needs to be re-instatiated\n+        \"\"\"\n+        self.image_fits_file = image_fits_file\n+    #\n+    \n+    def get_rms_data(self):\n+        \"\"\"\n+        Reads the data from the hdu with extname = RMS from the image fits file. \n+        Returns an ndarray.\n+        \"\"\"\n+        return FitsServices.read_rms_data(self.image_fits_file)\n+    #\n+    \n+    def get_image_data(self):     \n+        \"\"\"\n+        Reads the data from the hdu with extname = IMAGE from the image fits file.\n+        In case an hdu with the extname BACKGROUND is found as well, the bg data is \n+        subtracted from the image data first.\n+        \n+        Returns an ndarray.\n+        \"\"\"\n+        img = FitsServices.read_image_data(self.image_fits_file)\n+        \n+        if FitsServices.get_bg_hdu(self.image_fits_file) is not None:\n+            bg =  FitsServices.read_bg_data(self.image_fits_file)\n+            img = img - bg\n+        return img\n+    #\n+    \n+    def get_masked_image_data(self, masked_image):     \n+        \"\"\"\n+        Reads the data from the last (only) hdu of the masked_image fits file.\n+        \n+        Returns an ndarray.\n+        \"\"\"\n+        return FitsServices.read_data(masked_image, -1)        \n+    #\n+    \n+    def measure_rms(self, img, filter_size = 10):\n+        \"\"\"\n+        Runs the rms algorithm on img with filter_size.\n+        Returns an ndarray with the rms data.\n+        \"\"\"\n+        rms_measured = np.zeros(img.shape, dtype=np.float64)\n+        rms_measured = self.rms_sections(img, rms_measured, filter_size)\n+        \n+        return rms_measured\n+    #\n+    \n+    def rms_sections(self, img, rms, section_size, ignore = 0.0):\n+        \"\"\"\n+        Measures the rms for a section of size 2*section_size\n+        and assigns the rms value to every pixel in that section.\n+        The rms is evaluated from all pixels in the img with the section window.\n+        \"\"\"\n+        def env(y,x):\n+            env = img[y-section_size:y+section_size, x-section_size: x+section_size]\n+            env = env[env != ignore]\n+            sum = np.sum( np.multiply(env, env, env) )\n+            rms = sum/env.size if env.size > 0 else 0.0\n+            return rms\n+        #\n+        y_range = range(section_size, img.shape[0], 2*section_size)\n+        x_range = range(section_size, img.shape[1], 2*section_size)\n+        for y in y_range:\n+            for x in x_range:\n+                #rms[y,x] = np.sqrt(np.mean(env(y,x)**2))\n+                rms[y-section_size : y+section_size,x-section_size : x+section_size] = env(y,x)\n+            #\n+        #\n+        return np.sqrt(rms)\n+    #\n+    \n+    def bins(self, a, nobins = 20):\n+        \"\"\"\n+        Make an estimate of a set of histogram bins.\n+        Uses as binsize 2*(median(a) - min(a))/nobins and thus places a set\n+        of bins around the median value.\n+        \"\"\"\n+        median = np.median(a)\n+        min = np.amin(a)\n+        bins = []\n+        \n+        stepsize = (median - min)/nobins\n+        \n+        for i in range(nobins):\n+            bins.append(min +i*2*stepsize)\n+        #    \n+        return bins\n+    #\n+    \n+    def mask_objects(self, img, x, y, radius, **kwargs):\n+        \"\"\"\n+        mask all objects with coords x,y in a square\n+        of n times the radius.\n+        In case of rubbish radius values, the default_radius is used as a fallback.\n+        mask means, set values to 0.0\n+        \"\"\"\n+        n = kwargs.get(\"n\", 5)\n+        default_radius = kwargs.get(\"default_radius\", 5)\n+        \n+        maxrad = -1\n+\n+        for idx in range(len(x)):\n+            rad = radius[idx] if (radius[idx] > 0.0 and radius[idx] < 50) else default_radius\n+            maxrad = max(maxrad, rad)\n+            startx = math.floor(x[idx] - n*rad)\n+            endx = math.ceil(x[idx] + n*rad)\n+            starty = math.floor(y[idx] - n*rad)\n+            endy = math.ceil(y[idx] + n*rad)\n+            \n+            img[starty:endy,startx:endx] = 0.0\n+        return img\n+    #\n+    \n+    def read_catalog(self, catalog_fits_file, **kwargs ):\n+        \"\"\"\n+        Reads the catalog fits file and extracts x, y, radius for all catalog entries with MAG_AUTO <= mag_limit (or MAG_PSF, \n+        if MAG_AUTOis not in the catalog).\n+        \n+        Returns a set of 3 1d ndarrays.\n+        \"\"\"\n+        #get the optional parameters\n+        x = kwargs.get(\"x\", \"X_IMAGE\") \n+        y = kwargs.get(\"y\", \"Y_IMAGE\")\n+        radius = kwargs.get(\"radius\", \"FLUX_RADIUS\")\n+        mag_limit = kwargs.get(\"mag_limit\", 30.0)\n+        \n+        \n+        chdul = fits.open(catalog_fits_file)\n+        try:\n+            tab = Table(chdul[\"OBJECTS\"].data)\n+        except:\n+            tab = Table(chdul[\"LDAC_OBJECTS\"].data)\n+\n+        #cut the catalog at mag 30\n+        mag = [\"MAG_AUTO\", \"MAG_PSF\"]\n+\n+        for m in mag:\n+            if m in tab.colnames:\n+                mag = m\n+                print(\"select mag from \", mag)\n+                break\n+            #\n+        #\n+        tab = tab[tab[mag] <= mag_limit]\n+        #\n+        return tab[x].data, tab[y].data, tab[radius].data\n+    #       \n+    \n+    def plot_rms(self, instrument, rms_measured, rms_hdu, plot_file):\n+        \"\"\"\n+        Plots 2 histograms side by side from rms_measured and rms_hdu.\n+        Imstrument is used as a tile.\n+        The plot is save to plot_file.\n+        \"\"\"\n+        #reshape the data\n+        #and remove pathological values\n+        s = rms_hdu.shape\n+        rms_hdu = rms_hdu.reshape((s[0]*s[1]))\n+        rms_hdu = rms_hdu[rms_hdu > 0.0]\n+        rms_hdu = rms_hdu[rms_hdu < 1e+16]\n+        \n+        s = rms_measured.shape\n+        rms_measured = rms_measured.reshape((s[0]*s[1]))\n+        rms_measured = rms_measured[rms_measured > 0.0]\n+        rms_measured = rms_measured[rms_measured < 1e+16]\n+        \n+        #prepare the plot\n+        fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n+        \n+        _bins = self.bins(rms_hdu, 40)\n+        axs[0].hist(rms_hdu, bins = _bins)\n+        axs[0].set_title(str(instrument) +' rms from fits file')\n+        axs[0].set_xlabel('RMS')\n+        \n+        _bins = self.bins(rms_measured, 40)\n+        axs[1].hist(rms_measured, bins = _bins)\n+        axs[1].set_title(str(instrument) +' rms as measured')\n+        axs[1].set_xlabel('RMS')\n+        \n+        fig.savefig(plot_file)\n+    #\n+    \n+    def save_rms_to_fits(self, rms, filename):\n+        \"\"\"\n+        Creates a fits file with filename that contains a primary hdu and \n+        a hdu with the rms data (extname =  RMS).\n+        \"\"\"\n+        hdu0 = fits.PrimaryHDU()\n+        hdu1 = fits.ImageHDU(data = rms)\n+        \n+        fits.HDUList([hdu0, hdu1]).writeto(filename, overwrite=True)\n+        fits.setval(filename, 'extname', value='RMS', ext=1)\n+    #\n+    \n+if __name__ == \"__main__\":\n+    argparser = argparse.ArgumentParser()\n+    argparser.add_argument(\"--image\",  type = str, \n+                       help = \"The fits image to calculate the rms for. \"\\\n+                              \"It is expected to have extnames IMAGE and RMS\")\n+    argparser.add_argument(\"--masked_image\",  type = str, \n+                       help = \"Alternatively, instead of image, a masked image can be used as separate fits file. \"\\\n+                              \"It is expected to have one hdu\", default = \"\")\n+    argparser.add_argument(\"--filtersize\", type = int, default = 5,  \n+                        help = \"Defines the size of the filter window, in which the rms is evaluated.\" \\\n+                               \"The window size will be 2*filtersize.\")\n+    argparser.add_argument(\"--plot_filename\",  type = str, default = \"rms.jpg\", \n+                            help = \"full path to the plot file.\")\n+    argparser.add_argument(\"--fits_file\",  dest = \"fits_file\", action = 'store_const', \n+                            const = True, default = False, \n+                            help = \"If set, a fits file with the rms is stored under the same name as \"\\\n+                            \"the plot (with the extension .fits instead).\")\n+    argparser.add_argument(\"--catalog\",  type = str, default = \"\", \n+                            help = \"full path to the image catalog in case the objects in the image should be masked.\" \\\n+                            \"The catalog should contain X_IMAGE, Y_IMAGE positions and a FLUX_RADIUS\")\n+    argparser.add_argument(\"--instrument\",  type = str, \n+                            help = \"The instrument name. Appears in the plot title.\")\n+    \n+    if not len(sys.argv) > 1:\n+        argparser.print_help()\n+        sys.exit()\n+    args = argparser.parse_args()\n+    \n+    \n+    rms = RMSAnalysis(args.image)\n+    img = None #initialize                                                          \n+    \n+    \"\"\"\n+    the logic is this:\n+    \n+    we have only an image with sources:\n+    -> use this for the rms (with sources inside, if necessary)\n+    we have a catalog on top of the image:\n+    -> use the catalog to mask the sources in the image\n+    \n+    we have a masked_image fits file (mask has been applied outside, by SEXtractor for example):\n+    -> use that as image to obtain the rms from\n+    \n+    \"\"\"\n+    if args.masked_image == \"\":\n+        img = rms.get_image_data()\n+        if args.catalog != \"\":\n+            \"\"\"\n+            if the objects in the image are not masked, we\n+            should use a catalog and mask them\n+            \"\"\"\n+            x, y, fluxrad = rms.read_catalog(args.catalog)\n+            img = rms.mask_objects(img, x, y, fluxrad)\n+        #\n+    else:\n+        #we have a masked image\n+        img = rms.get_masked_image_data(args.masked_image)\n+    #\n+    \n+    rms_measured = rms.measure_rms(img, args.filtersize)\n+    rms_hdu = rms.get_rms_data()\n+    \n+    rms.plot_rms(args.instrument, rms_measured, rms_hdu, args.plot_filename)\n+    \n+    if args.fits_file:\n+        rms_fitsfile = args.plot_filename.rsplit(\".\", 1)[0] +\".fits\"\n+        rms.save_rms_to_fits(rms_measured, rms_fitsfile)\n+    #\n+#\n+\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/RMS/__init__.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/__init__.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/file/__init__.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/Filter.py": [
                        [
                            "@@ -0,0 +1,145 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+from enum import Enum\n+from collections import OrderedDict\n+import numpy as np\n+\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+from EXT_PF1_GEN_P2.psf.Stamp import Stamp\n+\n+class FilterType(object):\n+    limit_abs_sum = None\n+    def __init__(self, name, limit_abs_sum, fraction_zeroes_upper_limit, amplitude_upper_limit, uppler_limit_fit_deviation):\n+        self.name = name\n+        self.limit_abs_sum = limit_abs_sum\n+        self.fraction_zeroes_upper_limit = fraction_zeroes_upper_limit\n+        self.amplitude_upper_limit = amplitude_upper_limit\n+        self.uppler_limit_fit_deviation = uppler_limit_fit_deviation\n+    #\n+    \n+    def getName(self):\n+        return self.name\n+    #\n+#\n+\n+\n+class FilterTypes(Enum):\n+    DES = FilterType(\"DES\", 0.0001, 0.3, 0.0, 0.05)\n+    KIDS = FilterType(\"KIDS\", 0.0001, 0.3, 0.0, 0.2)\n+    LSST = FilterType(\"LSST\", 0.0001, 0.2, 0.0, 0.05)\n+    NOS = FilterType(\"NOS\", 0.0001, 0.3, 0.0, 0.05)\n+#\n+\n+class PsfStampFilter(object):\n+    \n+    def __init__(self, stamp_size, filtertype = FilterTypes.DES, execfilter = True):\n+        #define the filtersettings\n+        self.filtertype = filtertype.value #enum value is the Filtertype\n+        self.stamp_size = stamp_size\n+        self.execfilter = execfilter\n+    #\n+    \n+    def get_stamp_size(self):\n+        #print(\"get stamp size \", self.stamp_size)\n+        return self.stamp_size\n+    def set_stamp_size(self, stamp_size):\n+        self.stamp_size = stamp_size\n+    #\n+    def filter(self, layer_objects, layer_coadds, objects_x, objects_y):\n+        \"\"\"\n+        Identifies the PSFs that cannot be fit with a gaussian shape\n+        and removes them from the output file\n+        \"\"\"\n+        if not self.execfilter:\n+            return layer_objects\n+        #\n+        temp_layer_objects = OrderedDict()\n+        counter_zeroes = 0\n+        counter_badshape = 0\n+        counter_stampEmpty = 0\n+        zero_fraction = list()\n+        for i in range(len(layer_objects)):\n+            #get the coadd image\n+            layer_coadd = layer_coadds[str(i)]\n+            if layer_coadd is None:\n+                print(\"coadd for layer \", i, \" does not exist.\")\n+                continue\n+            #this is a fits image with only one hdu. Read it as hdunum 0\n+            imgdata = FitsServices.read_data(layer_coadd, 0)\n+            \n+            temp = []\n+            for obj in layer_objects[str(i)]:\n+                x = objects_x[obj]\n+                y = objects_y[obj]\n+                stamp  = Stamp(imgdata, self.get_stamp_size(), imgdata.shape)\n+                stamp_image = stamp.get_stamp_at(x,y)\n+                \n+                _max = abs(np.amax(stamp_image))\n+                _min = abs(np.amin(stamp_image))\n+                _sum = np.sum(stamp_image)\n+                \n+                \n+                #print(\" min, max \", _min, _max, \", same order of magnitude? \", (_max - _min)/_max < 0.9, \", sum \", _sum )\n+                z_f = 1.0*np.sum(stamp_image == 0)/(self.get_stamp_size()*self.get_stamp_size())\n+                zero_fraction.append(z_f)\n+                if z_f > self.filtertype.fraction_zeroes_upper_limit:\n+                    #if abs(_sum) < self.filtertype.limit_abs_sum or _sum < self.filtertype.sum_upper_limit:\n+                    #print(\"Contains 0.0. Reject.\", np.sum(Stamp_image == 0))\n+                    counter_zeroes = counter_zeroes +1\n+                    continue\n+                \"\"\"\n+                try:\n+                    amplitude, center, sigma = ArrayUtils.fit_simple_gauss_2d(Stamp_image)\n+                    #print(\"sigma \", sigma)\n+                    if amplitude < self.filtertype.amplitude_upper_limit:\n+                        #print(\"Negative fit amplitude. Reject.\")\n+                        counter_badshape = counter_badshape + 1\n+                        continue\n+                    psf_dim = np.array(Stamp_image.shape)\n+                    idxs = ArrayUtils.get_array_indices(psf_dim)\n+                    shape = Stamp_image.shape\n+                    gauss = ArrayUtils.simple_gauss_2d(idxs, amplitude, center[1], center[0], sigma)\n+                    print(np.sqrt(np.mean((Stamp_image.flatten() - gauss) ** 2.))/amplitude)\n+                    if np.sqrt(np.mean((Stamp_image.flatten() - gauss) ** 2.))/amplitude < self.filtertype.uppler_limit_fit_deviation:\n+                        temp.append(obj)\n+                    else:\n+                        #print('PSF centered at %.3f, %.3f skipped: bad PSF shape' % (x, y))\n+                        counter_badshape = counter_badshape + 1\n+                        continue\n+                except:\n+                    print('PSF centered at %.3f, %.3f skipped: could not fit a gaussian' % (x, y))\n+                    counter_badshape = counter_badshape + 1\n+                \"\"\"\n+                #test only for completely empty stamps\n+                if abs(_sum) < 0.0001 or _sum < 0:\n+                    #print(\"reject for low sum \", _sum, \", x \", x, \"y \", y)\n+                    counter_stampEmpty = counter_stampEmpty +1\n+                    continue\n+                else:\n+                    temp.append(obj)\n+                #\n+            #\n+            temp_layer_objects[str(i)] = temp\n+        print(\"  \", [len(temp_layer_objects[x]) for x in temp_layer_objects])\n+        #print(zero_fraction)\n+        print(\"stamps filtered out:\")\n+        print(\"bad shape \", counter_badshape)\n+        print(\"incomplete coverage (too many zeroes) \", counter_zeroes)\n+        print(\"stamp completely empty \", counter_stampEmpty)\n+        \n+        return temp_layer_objects\n+    #\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/ObjectList.py": [
                        [
                            "@@ -0,0 +1,68 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+import logging\n+\n+class ObjectList():\n+    def __init__(self, mag, zp = None, ra = None, dec = None, X = None, Y = None):\n+        self._length = 0\n+        self.mag = mag\n+        self.zp = zp\n+        self.ra = ra\n+        self.dec = dec\n+        self.X = X\n+        self.Y = Y\n+        \n+        self._add_attribute('mag', mag)\n+        self._add_attribute('ra',  ra)\n+        self._add_attribute('zp',  zp)\n+        self._add_attribute('dec', dec)\n+        self._add_attribute('X', X)\n+        self._add_attribute('Y', Y)\n+        \n+    \n+    def _add_attribute(self, name, lista):\n+        if lista is None:\n+            return\n+        if self._length == 0:\n+            setattr(self, name, lista)\n+            self._length = len(lista)\n+        else:\n+            if len(lista) == self._length:\n+                setattr(self, name, lista)\n+            else:\n+                print(\"attribute %s could not be set: wrong length\" % name)\n+    \n+    def _add_x_list(self, lista):\n+        self.X = lista\n+    \n+    def _add_y_list(self, lista):\n+        self.Y = lista\n+    \n+    def _filter(self, idx):\n+        for a in [\"mag\",\"zp\",\"ra\",\"dec\",\"X\",\"Y\"]:\n+            lista = getattr(self, a)\n+            try:\n+                #overwrite\n+                setattr(self, a, [lista[i] for i in idx])\n+            except:\n+                #a list may be None, so looping \n+                #them all is not a good idea\n+                logging.debug(\"ObjectList._filter(self, idx)\")\n+            #\n+        #\n+        self._length = len(self.mag)\n+    #\n+    \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/OutputStackedPSF.py": [
                        [
                            "@@ -0,0 +1,259 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+from EXT_PF1_GEN_P2.psf.Stamp import Stamp\n+from EXT_PF1_GEN_P2.psf.WcsUtils import WcsUtils\n+\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+from EXT_PF1_GEN_P2_LIBS.astronomy.ArrayUtils import ArrayUtils\n+\n+\n+import os\n+import copy\n+import datetime\n+import fitsio\n+import numpy as np\n+from astropy.io import fits\n+\n+\n+class OutputStackedPSF():\n+    def __init__(self, totobj, stamp_size, tmpdir, tilename, band, _logger):\n+        self.logger = _logger\n+        self.tmpdir     = tmpdir\n+        self.tilename   = tilename\n+        self.band       = band\n+        self.totobj     = totobj\n+        self.nobj       = np.ceil(np.sqrt(self.totobj))\n+        self.stamp_size = stamp_size\n+        npixside        = self.nobj * self.stamp_size\n+        self.grid_image = np.zeros((int(npixside), int(npixside)))\n+        self.info_table = np.zeros(\n+            self.totobj, dtype = [('RA',      'f8'),\n+                                  ('DEC',     'f8'),   \n+                                  ('X',       'f8'), \n+                                  ('Y',       'f8'), \n+                                  ('GRIDX',   'f8'),\n+                                  ('GRIDY',   'f8'),\n+                                  ('BANDPASS','f8')])\n+                                  #('num_obj', 'i4')])\n+                                  #('MAG', 'f8')])\n+        self.grididx  = 0\n+        self.firstidx = 0\n+        self.lastidx  = 0\n+        #initialize a header, in case there are no layer coadds\n+        self.header = fitsio.FITSHDR()\n+        \n+    def get_stamp_size(self):\n+        return self.stamp_size\n+    def set_stamp_size(self, stamp_size):\n+        self.stamp_size = stamp_size\n+    #\n+    def populate(self, layer_objects, ra, dec, mag, coadd, bandpass_value, normalize = False):\n+        print(\"populate \", ra, dec, mag, coadd, bandpass_value, normalize)\n+        first = 1\n+        for layer_index in layer_objects.keys():\n+            self.logger.write(\"add_layer_objects_to_info_table: layer \" +str(layer_index))\n+            print(\"\\n%s\" % datetime.datetime.now(), \"add_layer_objects_to_info_table: layer \" +str(layer_index))\n+            self.lastidx = self.add_layer_objects_to_info_table(layer_index, layer_objects, ra, dec, mag, coadd, bandpass_value)\n+            self.logger.write(\"add_layer_objects_stamps_to_grid_image\")\n+            print(\"\\n%s\" % datetime.datetime.now(), \"add_layer_objects_stamps_to_grid_image\")\n+            self.add_layer_objects_stamps_to_grid_image(layer_index, normalize, first)\n+            self.logger.write(\"next layer\")\n+            print(\"\\n%s\" % datetime.datetime.now(), \"next layer\")\n+            first = 0\n+            self.firstidx = copy.deepcopy(self.lastidx)\n+        #\n+\n+    def add_layer_objects_to_info_table(self, layer_index, layer_objects, ra, dec, mag, coadd, bandpass_value):\n+        \"\"\"\n+        Add the objects contained in the layer to the oputput info table\n+        \"\"\"\n+        self.lastidx += len(layer_objects[layer_index])\n+        self.info_table['RA'][int(self.firstidx):int(self.lastidx)]      = ra[layer_objects[layer_index]]\n+        self.info_table['DEC'][int(self.firstidx):int(self.lastidx)]     = dec[layer_objects[layer_index]]\n+        \n+        # compute the output positions in the coadd pixel space\n+        skycoords  = OutputStackedPSF.prepare_sky_coordinates(self.info_table['RA'][int(self.firstidx):int(self.lastidx)],\n+                                                  self.info_table['DEC'][int(self.firstidx):int(self.lastidx)]) \n+        #MW: due to the filtering, also other layers \n+        #than layer 0 may have no objects\n+        if len(skycoords) == 0: # and layer_index == 0: \n+            print(\"No objects for layer \", str(layer_index), \" made it to the stacked PSF file\")\n+            return\n+        X, Y       = OutputStackedPSF.getPixels(coadd, skycoords)\n+        \n+        self.info_table['X'][int(self.firstidx):int(self.lastidx)] = X\n+        self.info_table['Y'][int(self.firstidx):int(self.lastidx)] = Y\n+        #MW: finally, one bandpass value for all objects\n+        self.info_table['BANDPASS'][int(self.firstidx):int(self.lastidx)] = [bandpass_value]*(int(self.lastidx) - int(self.firstidx) )\n+        \n+        return self.lastidx\n+    #\n+    \n+    \n+    def add_layer_objects_stamps_to_grid_image(self, layer_index, normalize, first = 0):\n+        \"\"\"\n+        For each object in the layer, add the corresponding\n+        stacked PSF stamp to the output FITS grid image extension\n+        \"\"\"\n+        #read the layer coadd image\n+        layer_coadd = self.tmpdir +os.sep +\"layer\" +str(layer_index) +os.sep +self.tilename +\"_\" +self.band +\".image.fits\"\n+        if first:\n+            #save a fitsio header object\n+            self.header = copy.deepcopy(FitsServices.read_image_header(layer_coadd))\n+        \n+        self.add_stamps_to_output_image(layer_coadd, normalize) \n+    \n+    def add_stamps_to_output_image(self, layer_coadd, normalize):\n+        \"\"\"\n+        Add the PSF stamp extracted from the coadd\n+        to the image extension\n+        \"\"\"\n+        import time\n+        plotcounter = 0\n+        get_stamp_time = 0\n+        print(\"add_stamps_to_output_image \", layer_coadd, normalize, self.firstidx, self.lastidx)\n+        \n+        start = time.time()\n+        data_dim = (self.header[\"NAXIS2\"], self.header[\"NAXIS1\"])\n+        stamp  = Stamp(FitsServices.read_image_data(layer_coadd), self.get_stamp_size(), data_dim)\n+        print(\"\\n%s\" % datetime.datetime.now(), \" make stamp object \" +str(time.time() - start))\n+        \n+        for objidx in range(self.firstidx, self.lastidx):\n+            gridstart = np.array((\n+                np.floor(self.grididx %  self.nobj)     * self.get_stamp_size(),\n+                np.floor(self.grididx // self.nobj)     * self.get_stamp_size())) \n+\n+            gridend   = np.array((\n+                np.floor(self.grididx %  self.nobj + 1) * self.get_stamp_size(),\n+                np.floor(self.grididx // self.nobj + 1) * self.get_stamp_size()))\n+\n+            self.info_table['GRIDX'][objidx] = (\n+                self.grididx % self.nobj  + 0.5)        * self.get_stamp_size() + 0.5\n+\n+            self.info_table['GRIDY'][objidx] = (\n+                self.grididx // self.nobj + 0.5)        * self.get_stamp_size() + 0.5\n+            \n+            norm = 1.0\n+            start = time.time()\n+            stamp_image = stamp.get_stamp_at(self.info_table['X'][objidx],self.info_table['Y'][objidx], interpolate = 1)\n+            if normalize:\n+                norm = stamp_image.max() if stamp_image.max() != 0.0 else norm\n+            #\n+            get_stamp_time = get_stamp_time +(time.time() -  start)\n+            self.grid_image[int(gridstart[1]):int(gridend[1]),\n+                            int(gridstart[0]):int(gridend[0])] += stamp_image/norm\n+            self.grididx += 1\n+            \n+            #printout a couple of values\n+            #if plotcounter < 20:\n+            #    au = ArrayUtils()\n+            #    print(\"stampshape \", stamp_image.shape, \", gauss: \", au.fit_gauss_2d(stamp_image) )\n+            #    plotcounter = plotcounter +1\n+        #\n+        self.logger.write(\"time for get_stamp_at \" +str(get_stamp_time))\n+        print(\"\\n%s\" % datetime.datetime.now(), \"time for get_stamp_at \" +str(get_stamp_time))\n+    #\n+    \n+    \n+    def write_to_fits(self, outname):\n+        \"\"\"\n+        Creates the output FITS file and writes the extensions\n+        \"\"\"\n+        print('Writing output file: %s' % outname)\n+        #fitsio.write(outname, None, clobber = True)\n+        #output_fits_file = fitsio.FITS(outname, 'rw')            \n+        self.header['STMPSIZE'] = int(np.floor(self.get_stamp_size()))\n+        \n+        hdu0 = fits.PrimaryHDU()\n+        \n+        #make an astropy header from the fitsio\n+        #self.header\n+        ah = fits.Header()\n+        for r in self.header.records(): \n+            try:\n+                c = \"\"\n+                if \"comment\" in r:\n+                    c = r[\"comment\"]\n+                ah.append((r[\"name\"], r[\"value\"], c)) \n+            except Exception as e:\n+                print(e)\n+        #\n+        \n+        hdu1 = fits.ImageHDU(data = self.grid_image, header  = ah, name = \"IMAGE\")\n+        hdu2 = fits.BinTableHDU(data = self.info_table, name = \"INFO\")\n+        fits.HDUList([hdu0, hdu1, hdu2]).writeto(outname, overwrite=True)\n+    #\n+    \n+    ################################################################################\n+    #\n+    # One class method, that is called from StackPSF but also from OutputStackedPSF\n+    #\n+    ################################################################################\n+    @staticmethod\n+    def getPixels(obj, skycoords):\n+        \"\"\"\n+        Reads the WCS of obj, and projects skycoords\n+        into the obj pixels reference frame.\n+        Returns two lists X, Y containing the ordered list of pixels positions\n+        \"\"\"\n+        # Project the sky positions into the obj pixels reference frame        \n+        WCSU       = WcsUtils(FitsServices.read_image_header(obj))\n+        X, Y       = WCSU.py_world2pix(skycoords)\n+        \n+        #MW: why -1?\n+        #fits start to count at 1\n+        #we want array coordinates starting at 0\n+        X = X - 1\n+        Y = Y - 1\n+        return X, Y\n+    #\n+    \n+    \n+    @staticmethod\n+    def getPixels_from_header(header, skycoords):\n+        \"\"\"\n+        Reads the WCS of obj, and projects skycoords\n+        into the obj pixels reference frame.\n+        Returns two lists X, Y containing the ordered list of pixels positions\n+        \"\"\"\n+        # Project the sky positions into the obj pixels reference frame        \n+        WCSU       = WcsUtils(header)\n+        X, Y       = WCSU.py_world2pix(skycoords)\n+        \n+        #MW: why -1?\n+        #fits start to count at 1\n+        #we want array coordinates starting at 0\n+        X = X - 1\n+        Y = Y - 1\n+        return X, Y\n+    #\n+    \n+    \n+    @staticmethod\n+    def prepare_sky_coordinates(ra, dec):\n+        \"\"\"\n+        Stores the input sky coordinates in a format [ra0, dec0, r1, dec1, ...] that can be used for\n+        projection into pixel space from the class  _getWcsLibCorners.py_world2pix\n+        \"\"\"\n+        skycoords = (len(ra) + len(dec))*[0]\n+        for i in range(len(ra)):\n+            skycoords[2*i] = ra[i]\n+            skycoords[2*i+1] = dec[i]\n+        #\n+        return skycoords\n+    #\n+    \n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -22,13 +22,15 @@ from EXT_PF1_GEN_P2_LIBS.astronomy.ArrayUtils import ArrayUtils\n \n import os\n import copy\n+import datetime\n import fitsio\n import numpy as np\n from astropy.io import fits\n \n \n class OutputStackedPSF():\n-    def __init__(self, totobj, stamp_size, tmpdir, tilename, band):\n+    def __init__(self, totobj, stamp_size, tmpdir, tilename, band, _logger):\n+        self.logger = _logger\n         self.tmpdir     = tmpdir\n         self.tilename   = tilename\n         self.band       = band\n@@ -62,8 +64,14 @@ class OutputStackedPSF():\n         print(\"populate \", ra, dec, mag, coadd, bandpass_value, normalize)\n         first = 1\n         for layer_index in layer_objects.keys():\n+            self.logger.write(\"add_layer_objects_to_info_table: layer \" +str(layer_index))\n+            print(\"\\n%s\" % datetime.datetime.now(), \"add_layer_objects_to_info_table: layer \" +str(layer_index))\n             self.lastidx = self.add_layer_objects_to_info_table(layer_index, layer_objects, ra, dec, mag, coadd, bandpass_value)\n+            self.logger.write(\"add_layer_objects_stamps_to_grid_image\")\n+            print(\"\\n%s\" % datetime.datetime.now(), \"add_layer_objects_stamps_to_grid_image\")\n             self.add_layer_objects_stamps_to_grid_image(layer_index, normalize, first)\n+            self.logger.write(\"next layer\")\n+            print(\"\\n%s\" % datetime.datetime.now(), \"next layer\")\n             first = 0\n             self.firstidx = copy.deepcopy(self.lastidx)\n         #\n@@ -113,12 +121,17 @@ class OutputStackedPSF():\n         Add the PSF stamp extracted from the coadd\n         to the image extension\n         \"\"\"\n+        import time\n         plotcounter = 0\n+        get_stamp_time = 0\n         print(\"add_stamps_to_output_image \", layer_coadd, normalize, self.firstidx, self.lastidx)\n+        \n+        start = time.time()\n+        data_dim = (self.header[\"NAXIS2\"], self.header[\"NAXIS1\"])\n+        stamp  = Stamp(FitsServices.read_image_data(layer_coadd), self.get_stamp_size(), data_dim)\n+        print(\"\\n%s\" % datetime.datetime.now(), \" make stamp object \" +str(time.time() - start))\n+        \n         for objidx in range(self.firstidx, self.lastidx):\n-            data_dim = (self.header[\"NAXIS2\"], self.header[\"NAXIS1\"])\n-            stamp  = Stamp(FitsServices.read_image_data(layer_coadd), self.get_stamp_size(), data_dim)\n-\n             gridstart = np.array((\n                 np.floor(self.grididx %  self.nobj)     * self.get_stamp_size(),\n                 np.floor(self.grididx // self.nobj)     * self.get_stamp_size())) \n@@ -134,10 +147,12 @@ class OutputStackedPSF():\n                 self.grididx // self.nobj + 0.5)        * self.get_stamp_size() + 0.5\n             \n             norm = 1.0\n+            start = time.time()\n             stamp_image = stamp.get_stamp_at(self.info_table['X'][objidx],self.info_table['Y'][objidx], interpolate = 1)\n             if normalize:\n                 norm = stamp_image.max() if stamp_image.max() != 0.0 else norm\n             #\n+            get_stamp_time = get_stamp_time +(time.time() -  start)\n             self.grid_image[int(gridstart[1]):int(gridend[1]),\n                             int(gridstart[0]):int(gridend[0])] += stamp_image/norm\n             self.grididx += 1\n@@ -148,6 +163,8 @@ class OutputStackedPSF():\n             #    print(\"stampshape \", stamp_image.shape, \", gauss: \", au.fit_gauss_2d(stamp_image) )\n             #    plotcounter = plotcounter +1\n         #\n+        self.logger.write(\"time for get_stamp_at \" +str(get_stamp_time))\n+        print(\"\\n%s\" % datetime.datetime.now(), \"time for get_stamp_at \" +str(get_stamp_time))\n     #\n     \n     \n",
                            "log ramdisk and swarptemp size, improve stackpsf code",
                            "Michael",
                            "2023-06-16T10:48:31.000+02:00",
                            "d365302409e4a3268d1217c8fbf6ed9777bfa84b"
                        ],
                        [
                            "@@ -143,11 +143,12 @@ class OutputStackedPSF():\n             self.grididx += 1\n             \n             #printout a couple of values\n-            if plotcounter < 20:\n-                au = ArrayUtils()\n-                print(\"stampshape \", stamp_image.shape, \", gauss: \", au.fit_gauss_2d(stamp_image) )\n-                plotcounter = plotcounter +1\n-            #\n+            #if plotcounter < 20:\n+            #    au = ArrayUtils()\n+            #    print(\"stampshape \", stamp_image.shape, \", gauss: \", au.fit_gauss_2d(stamp_image) )\n+            #    plotcounter = plotcounter +1\n+        #\n+    #\n     \n     \n     def write_to_fits(self, outname):\n@@ -192,7 +193,7 @@ class OutputStackedPSF():\n         Returns two lists X, Y containing the ordered list of pixels positions\n         \"\"\"\n         # Project the sky positions into the obj pixels reference frame        \n-        WCSU       = WcsUtils(obj.returnHeader())\n+        WCSU       = WcsUtils(FitsServices.read_image_header(obj))\n         X, Y       = WCSU.py_world2pix(skycoords)\n         \n         #MW: why -1?\n",
                            "first attempt with memory improvement for stackpsf_v3",
                            "Michael",
                            "2023-06-09T09:51:34.000+02:00",
                            "7b8ba31d2dafe5229f54217bca17ba2a7b32bd71"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/Psfid.py": [
                        [
                            "@@ -0,0 +1,152 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+import os\n+import time\n+\n+from astropy.io import fits\n+from astropy.io.fits import HDUList, PrimaryHDU, BinTableHDU\n+from astropy.table import Table, Column\n+\n+import numpy as np\n+from scipy import ndimage\n+\n+from EXT_PF1_GEN_P2.psf import WcsUtils\n+\n+\n+class Psfid():\n+    \n+    \"\"\"\n+    Class that uses the psfid extension of a coadd to put an evenly spaced grid of psf locations\n+    over the coadd. \n+    After this, the alogorithm checks each field with a unique psfid, if a point has been put into it.\n+    If not, adds a point at the center of gravity according to scipy.\n+    This way, a regular grid of psf locations is established plus some extra points to complete the \n+    coverage to all fields with a unique psfid.\n+    \"\"\"\n+    def __init__(self, path_to_coadd, psfid_extname = \"PSFID\", wcs_extname = \"IMAGE\"):\n+        hdul = fits.open(path_to_coadd)\n+        self.psfid = hdul[psfid_extname].data\n+        self.header = hdul[wcs_extname].header\n+        hdul.close()\n+        \n+        #value to use for gridpoint assigned\n+        self.markvalue = -1; \n+    #\n+    \n+    def get_psfid_array(self):\n+        return self.psfid\n+    #\n+\n+    def get_number_of_unique_psfids(self):\n+        uniq = set(sorted(self.psfid.copy().flatten().tolist()))\n+        return len(uniq), uniq\n+    #\n+    \n+    def regular_psf_grid(self, array, gridspacing, markvalue = -1):\n+        \"\"\"\n+        extract a regular grid of psf positions and mark them \n+        in the array.\n+        \n+        return marked_array, master_catalog [[y], [x]]\n+        \"\"\"\n+        master_cat = [[],[]]\n+        \n+        for y in range(gridspacing, array.shape[0], 2*gridspacing):\n+            for x in range(gridspacing,array.shape[1], 2*gridspacing):\n+                array[y,x] = markvalue\n+                master_cat[0].append(y) #dec\n+                master_cat[1].append(x) #ra\n+            #\n+        #\n+        return array, master_cat\n+    #\n+    \n+    def get_psf_points_cm_add(self, psfid, output, value, markvalue = -1):\n+        sel = (psfid == value)\n+        if output[sel].min() != markvalue:\n+            cp = psfid.copy()\n+            \n+            #0.0 is the neutral value for center of mass calculation\n+            cp[psfid != value] = 0.0\n+            cm = ndimage.center_of_mass(cp)\n+            \n+            return cm, 1\n+        #\n+        return None, 0    \n+    #\n+    \n+\n+    def set_psf_points_within_a_psfid_section(self, gridspacing = 50):\n+        output = self.psfid.copy()\n+        output, master_cat = self.regular_psf_grid(output, gridspacing, markvalue = self.markvalue)\n+        \n+        no_uniqu_values, uniq = self.get_number_of_unique_psfids()\n+        start = time.time()\n+        counter = 0\n+        empty = 0\n+        \n+        for uvalue in list(uniq):\n+            \n+            if counter%100 == 0:\n+                print(counter, \", time: \", time.time() - start, \", empty \", empty)\n+        \n+            cm, added_cm_value = self.get_psf_points_cm_add(self.psfid, output, uvalue, markvalue = self.markvalue)\n+    \n+            if added_cm_value:\n+                try:\n+                    #cm may be affected by div zero and scipy does not\n+                    #raise an exception to catch\n+                    output[int(cm[0]), int(cm[1])] = self.markvalue #for test purposes\n+                    master_cat[0].append(int(cm[0])) #int(): put the psfs into the center of a coadd pixel\n+                    master_cat[1].append(int(cm[1]))\n+                    empty = empty +1\n+                except:\n+                    pass\n+                #\n+            #\n+            counter = counter +1\n+        #\n+        print(\"time \", time.time() - start,  len( master_cat[0] ) , \", empty \", empty)\n+        \n+        return master_cat\n+    #\n+    \n+    def _write_cat(self, ra, dec, mag, x, y, ra_name, dec_name, mag_name, catalog_path):\n+        t = Table()\n+        t[ra_name] = ra\n+        t[dec_name] = dec\n+        t[mag_name] = mag\n+        t[\"X\"] = x\n+        t[\"Y\"] = y\n+        \n+        hdul = HDUList( [PrimaryHDU(), BinTableHDU(t)])\n+        hdul.writeto(catalog_path, overwrite=True)\n+        hdul.close()\n+        \n+        return os.path.exists(catalog_path)\n+    #\n+    \n+    def make_catalog(self, master_cat, catalog_path, mag = 18.0, ra_name = \"RA\", dec_name = \"DEC\", mag_name = \"MAG\"):\n+        wcsu = WcsUtils.WcsUtils(self.header)\n+        y = master_cat[0]\n+        x = master_cat[1]\n+        ra, dec = wcsu.py_pix2world(x, y)\n+        mag_col = [mag]*len(x)\n+        return self._write_cat(ra, dec, mag_col, x, y, ra_name, dec_name, mag_name, catalog_path)\n+    #\n+        \n+        \n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/Stamp.py": [
                        [
                            "@@ -0,0 +1,123 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+import numpy as np\n+import math\n+from scipy import ndimage\n+\n+\n+class Stamp():\n+    def __init__(self, image, stampSize, data_dim):\n+        self.image = image\n+        self.stampSize = stampSize\n+        self.data_dim = data_dim               \n+        self.stamp = np.zeros((int(stampSize), int(stampSize)))\n+    #\n+                           \n+    def original_get_stamp_at(self, x, y):\n+        #print(\"get stamp at x,y \", x, y)\n+        stampShift = (self.stampSize - 1) // 2\n+        start = np.array((np.floor(y - stampShift), \n+                          np.floor(x - stampShift)), \n+                         dtype = 'int')\n+        end   = np.array((self.stampSize, self.stampSize)) + start\n+        stamp_dim   = np.array((self.stampSize, self.stampSize))\n+        delta_start = 0 - start\n+        delta_start[delta_start < 0] = 0\n+        delta_end   = self.data_dim - end\n+        delta_end[delta_end > 0]     = 0\n+        if np.all((end + delta_end) - (start + delta_start) > 0):\n+            data_start  = start + delta_start\n+            data_end    = end   + delta_end\n+            stamp_start = delta_start\n+            stamp_end   = stamp_dim + delta_end\n+            \n+            self.stamp[int(stamp_start[0]):int(stamp_end[0]),\n+                       int(stamp_start[1]):int(stamp_end[1])] = self.image[\n+                           int(data_start[0]):int(data_end[0]),\n+                           int(data_start[1]):int(data_end[1])]\n+        #\n+        return self.stamp\n+    #\n+    \n+    def get_stamp_at(self, x, y, interpolate = 1):\n+        #in the nd-arrays:\n+        #center of a pixel is coordinate .0\n+        #and the center of the stamp must be at .0 as well \n+        #(in the center of the central pixel)\n+\n+\n+        #shift psf center at x,y to pixel center at .0,.0        \n+        interpolate_x = math.floor(x) - x\n+        interpolate_y = math.floor(y) - y\n+        \n+        #stampsize is always odd\n+        stampShift = (self.stampSize - 1) // 2\n+        \n+        \n+        #get the cuts\n+        #do it so that the shifted center is in the center of the cutout\n+        #enlarge by one, so interpolation does not generate voids\n+        startx = math.floor(x) - stampShift -1 #-1 because we want it larger for interpolation\n+        endx = math.floor(x) + stampShift +2 #+1: odd stampsize, +1: enlarge by 1 \n+        \n+        starty = math.floor(y) - stampShift -1\n+        endy = math.floor(y) + stampShift +2\n+        \n+        try:\n+            stamp = self.image[int(starty):int(endy),\n+                            int(startx):int(endx)]\n+        except:\n+            return np.zeros((int(self.stampSize), int(self.stampSize)))\n+        #\n+        if (stamp.shape[0] != self.stampSize +2) or (stamp.shape[1] != self.stampSize +2):\n+            return np.zeros((int(self.stampSize), int(self.stampSize)))\n+        #\n+        \n+        if interpolate:\n+            stamp = ndimage.shift(stamp, (interpolate_y, interpolate_x), order=5) #test1 instead of 5\n+        #\n+        \n+        \"\"\"\n+        #shall we treat all the part psf cases around the edges?\n+        if startx < 0 or starty < 0:\n+            #part of a stamp\n+            #put it onto a zero array\n+            zero_array = np.zeros((int(self.stampSize), int(self.stampSize)))\n+            stamp = stamp[1:-1, 1:-1]\n+            zero_array[self.stampSize - stamp.shape[0]: self.stampSize, self.stampSize - stamp.shape[1]: self.stampSize ] = stamp\n+            return zero_array        \n+        if endx > self.image.shape[1] or endy > self.image.shape[0]:\n+            #don't worry about half cut psfs, they are useless\n+            #so drop that case\n+            zero_array = np.zeros((int(self.stampSize), int(self.stampSize)))\n+            stamp = stamp[1:-1, 1:-1]\n+            zero_array[0: stamp.shape[0], 0:stamp.shape[1]] = stamp        \n+            return zero_array \n+        #\n+        \n+        \n+        print(\"interpolate_x \", interpolate_x)\n+        print(\"interpolate_y \", interpolate_y)\n+        print(\"startx, endx \", startx, endx)\n+        print(\"starty, endy \", starty, endy)\n+        print(\"stampShift \", stampShift)\n+        \"\"\"\n+        \n+        #correct the enlargement\n+        return stamp[1:-1, 1:-1]\n+    #\n+#\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/TotalSizeOf.py": [
                        [
                            "@@ -0,0 +1,55 @@\n+from __future__ import print_function\n+from sys import getsizeof, stderr\n+from itertools import chain\n+from collections import deque\n+try:\n+    from reprlib import repr\n+except ImportError:\n+    pass\n+\n+def total_size(o, handlers={}, verbose=False):\n+    \"\"\" Returns the approximate memory footprint an object and all of its contents.\n+\n+    Automatically finds the contents of the following builtin containers and\n+    their subclasses:  tuple, list, deque, dict, set and frozenset.\n+    To search other containers, add handlers to iterate over their contents:\n+\n+        handlers = {SomeContainerClass: iter,\n+                    OtherContainerClass: OtherContainerClass.get_elements}\n+\n+    \"\"\"\n+    dict_handler = lambda d: chain.from_iterable(d.items())\n+    all_handlers = {tuple: iter,\n+                    list: iter,\n+                    deque: iter,\n+                    dict: dict_handler,\n+                    set: iter,\n+                    frozenset: iter,\n+                   }\n+    all_handlers.update(handlers)     # user handlers take precedence\n+    seen = set()                      # track which object id's have already been seen\n+    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__\n+\n+    def sizeof(o):\n+        if id(o) in seen:       # do not double count the same object\n+            return 0\n+        seen.add(id(o))\n+        s = getsizeof(o, default_size)\n+\n+        if verbose:\n+            print(s, type(o), repr(o), file=stderr)\n+\n+        for typ, handler in all_handlers.items():\n+            if isinstance(o, typ):\n+                s += sum(map(sizeof, handler(o)))\n+                break\n+        return s\n+\n+    return sizeof(o)\n+\n+\n+##### Example call #####\n+\n+if __name__ == '__main__':\n+    d = dict(a=1, b=2, c=3, d=[4,5,6,7], e='a string of chars')\n+    print(total_size(d, verbose=True))\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/WcsUtils.py": [
                        [
                            "@@ -0,0 +1,99 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+from astropy.wcs import WCS\n+import numpy as np\n+\n+\n+class WcsUtils():\n+    def __init__(self, image_header):\n+        self.image_header = image_header\n+        if \"PV1_0\" in self.image_header:\n+            self.image_header[\"CTYPE1\"] = \"RA---TPV\"\n+            self.image_header[\"CTYPE2\"] = \"DEC--TPV\"\n+        if \"ZNAXIS1\" in self.image_header:\n+            self.image_header[\"NAXIS\"]  = self.image_header[\"ZNAXIS\"]\n+            self.image_header[\"NAXIS1\"] = self.image_header[\"ZNAXIS1\"]\n+            self.image_header[\"NAXIS2\"] = self.image_header[\"ZNAXIS2\"]\n+            \n+            \n+            hits = []\n+            m = self.image_header._record_map\n+            for k in m:\n+                if type(m[k][\"value\"]) == type( () ):\n+                    hits.append(k)\n+                #\n+            \n+            for h in hits:\n+                self.image_header.delete(h)\n+                #print(\"delete key \", h)\n+            #\n+        if \"ERRORS\" in self.image_header:\n+            self.image_header.delete(\"ERRORS\")\n+        #\n+        ##calculate the pixel scale\n+        cdelt1   = np.sqrt(image_header[\"CD1_1\"] ** 2 + image_header[\"CD1_2\"] ** 2)\n+        cdelt2   = np.sqrt(image_header[\"CD2_1\"] ** 2 + image_header[\"CD2_2\"] ** 2)\n+        pixscale = 0.5 * (cdelt1 + cdelt2) * 3600.0\n+        \n+        self.image_header[\"PIXSCALE\"] = pixscale\n+    #\n+    \n+    def get_header(self):\n+        \"\"\"\n+        return the internal header that has been extended\n+        by the pixel size\n+        \"\"\"\n+        return self.image_header\n+    #\n+    \n+    def py_pix2world(self, x, y):\n+        \"\"\"\n+        For historical reasons, skycoords is a linear array\n+        of the format [ra1, dec1, ra2, dec2, ..., ran, decn]\n+        The output is a ndarray of ra and an ndarray of dec\n+        \"\"\"\n+        wcs = WCS(self.image_header)\n+        #print(\"wcs = \", wcs)\n+        radec = wcs.all_pix2world(x, y, 1)\n+        \n+        return radec[0], radec[1] #ra, dec\n+    #\n+    \n+    \n+    def py_world2pix(self, coords):\n+        \"\"\"\n+        For historical reasons, skycoords is a linear array\n+        of the format [ra1, dec1, ra2, dec2, ..., ran, decn]\n+        The output is a ndarray of ra and an ndarray of dec\n+        \"\"\"\n+        wcs = WCS(self.image_header)\n+        \n+        pix = wcs.all_world2pix(coords[0::2], coords[1::2], 1)\n+        \n+        return pix[0], pix[1] #x, y\n+    #\n+    \n+    def py_world2pix_ra_dec(self, ra, dec):\n+        \"\"\"\n+        Same as above, but with ra, dec\n+        \"\"\"\n+        wcs = WCS(self.image_header)\n+        \n+        pix = wcs.all_world2pix(ra, dec, 1)\n+        \n+        return pix[0], pix[1] #x, y\n+    #\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -14,6 +14,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n \n \"\"\"\n from astropy.wcs import WCS\n+import numpy as np\n \n \n class WcsUtils():\n@@ -27,21 +28,37 @@ class WcsUtils():\n             self.image_header[\"NAXIS1\"] = self.image_header[\"ZNAXIS1\"]\n             self.image_header[\"NAXIS2\"] = self.image_header[\"ZNAXIS2\"]\n             \n+            \n             hits = []\n             m = self.image_header._record_map\n             for k in m:\n                 if type(m[k][\"value\"]) == type( () ):\n                     hits.append(k)\n                 #\n-            #\n+            \n             for h in hits:\n                 self.image_header.delete(h)\n                 #print(\"delete key \", h)\n             #\n         if \"ERRORS\" in self.image_header:\n             self.image_header.delete(\"ERRORS\")\n+        #\n+        ##calculate the pixel scale\n+        cdelt1   = np.sqrt(image_header[\"CD1_1\"] ** 2 + image_header[\"CD1_2\"] ** 2)\n+        cdelt2   = np.sqrt(image_header[\"CD2_1\"] ** 2 + image_header[\"CD2_2\"] ** 2)\n+        pixscale = 0.5 * (cdelt1 + cdelt2) * 3600.0\n+        \n+        self.image_header[\"PIXSCALE\"] = pixscale\n     #\n-\n+    \n+    def get_header(self):\n+        \"\"\"\n+        return the internal header that has been extended\n+        by the pixel size\n+        \"\"\"\n+        return self.image_header\n+    #\n+    \n     def py_pix2world(self, x, y):\n         \"\"\"\n         For historical reasons, skycoords is a linear array\n",
                            "first attempt with memory improvement for stackpsf_v3",
                            "Michael",
                            "2023-06-09T09:51:34.000+02:00",
                            "7b8ba31d2dafe5229f54217bca17ba2a7b32bd71"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psf/stackPSF.py": [
                        [
                            "@@ -0,0 +1,2322 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+import datetime\n+import argparse\n+import sys\n+import numpy as np\n+from astropy.coordinates import SkyCoord\n+from astropy import units as u\n+from astropy.wcs import WCS\n+from astropy.io import fits\n+from scipy import ndimage\n+import matplotlib.pyplot as plt\n+#from db import DBConnection\n+#import _getWcsLibCorners\n+#from queries import ImageQ_django as IQ\n+import fitsio\n+import os\n+import copy\n+import json\n+from collections import OrderedDict\n+import  scipy.ndimage.measurements\n+import file\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+from EXT_PF1_GEN_P2_LIBS.file import ExtractImage\n+from EXT_PF1_GEN_P2.Pipeline import SigmaClip\n+from EXT_PF1_GEN_P2_LIBS.Pipeline.Pipeline import Pipeline\n+#from EXT_PF1_GEN_P2.file import PSFUtils\n+from   EXT_PF1_GEN_P2.psfmodel import PsfModel\n+import matplotlib.pyplot as plt\n+\n+\n+import math\n+import shutil\n+import time\n+from EXT_PF1_GEN_P2_LIBS.astronomy.ArrayUtils import ArrayUtils\n+import multiprocessing as mp\n+from EXT_PF1_GEN_P2_LIBS.Pipeline import ProcessManager\n+import traceback\n+import operator\n+import logging\n+\n+#externalized classes\n+from EXT_PF1_GEN_P2.psf.Stamp import Stamp\n+from EXT_PF1_GEN_P2.psf.WcsUtils import WcsUtils\n+from EXT_PF1_GEN_P2.psf.ObjectList import ObjectList\n+from EXT_PF1_GEN_P2.psf.Filter import FilterTypes, PsfStampFilter\n+from EXT_PF1_GEN_P2.psf.OutputStackedPSF import OutputStackedPSF\n+from EXT_PF1_GEN_P2.psf import TotalSizeOf\n+\n+__author__='Thomas Vassallo'\n+__email__ ='thomas.vassallo@physik.lmu.de'\n+\n+\n+\n+\n+class StackPSF(Pipeline):\n+    def __init__(self, args):\n+        \"\"\"\n+        Computes the stacked PSF of a coadd image for a grid of points in the\n+        sky.\n+        The PSF is computed as follows: we project each point on all the single\n+        images contributing to the coadd using the WCS, and there we compute a\n+        PSF stamp using the single image PSF model. We generate mock single\n+        images of the same size of the 'oringinal' ones, containing all the PSF\n+        stamp.\n+        Those images are then swarped to obtain a 'coadd' image containing the\n+        stacked PSFs.\n+        In case the PSF overlap on the single images, we repeat the procedure\n+        multiple times, each time taking into account only one of the\n+        overlapping PSFs.\n+        \"\"\"\n+        print(\"execute \", self.__class__)\n+        Pipeline.__init__(self)\n+        self.starttime  = time.time()\n+        self._args      = args\n+        self.tilename   = self._args.tilename\n+        self.band       = self._args.band\n+        self.outdir     = os.path.abspath(\"%s/%s/%s/\"    % (\n+            self._args.outdir,  self.tilename, self.band))\n+        #\n+        self.tmpdir     = os.path.abspath(\"%s/%s/%s/\"    % (\n+            self._args.ramdisk, self.tilename, self.band))\n+        self.resampledir = os.path.abspath(\"%s/%s/%s/\"    % (\n+            self._args.resampledir, self.tilename, self.band))\n+            \n+        self.auxdir     = os.path.abspath(\"%s/aux/\"      % self.outdir)\n+        self.xmldir     = os.path.abspath(\"%s/xml/\"      % self.resampledir)\n+        self.resdir     = os.path.abspath(\"%s/resample/\" % self.resampledir)\n+        self.out_name   = \"%s_psfstamps.fits\"\n+\n+        #MW: remove existing dirs and recreate them\n+        for _dir in [self.tmpdir, self.outdir, self.xmldir, self.auxdir, self.resdir]:\n+            if os.path.isdir(_dir):\n+                print(\"removing \", _dir)\n+                shutil.rmtree(_dir)\n+            os.makedirs(_dir)\n+        #\n+        self.logfile    = self.outdir + os.sep + \"StackPSF.log\"\n+        self.logger     = open(self.logfile, 'w')\n+        self.stamp_size = -1 #define a negative default value\n+\n+        #name of the catalogue field containing X (ra) and Y (dec)\n+        #positions in the single epoch catalogues.'\n+        self.possible_xy_catalog_values = [('XMODEL_IMAGE', 'YMODEL_IMAGE'), ('XWIN_IMAGE', 'YWIN_IMAGE')]\n+        self.coadd_x    = self._args.coadd_x\n+        self.coadd_y    = self._args.coadd_y\n+        self.epsilon    = np.finfo(np.float32).eps\n+        \n+        #MW: store the coadd psf images per layer\n+        self.layer_coadds = dict()\n+        self.standard_magnitude_for_all_objects = 18.0\n+        self.gauss_stamp = None #for testing and debugging\n+        self.coadd_wcs = WcsUtils(FitsServices.read_image_header(args.coadd))\n+        print(\"PIXELSCALE from WcsUtils \", self.coadd_wcs.get_header()[\"PIXSCALE\"])\n+        \n+        \n+        mps = self._args.multiprocesses\n+        if mps > 1:\n+            #save one cpu for preparing the next step\n+            #while swarp does its job with mps - 1\n+            self._args.multiprocesses = mps - 1\n+        #\n+        \n+        #start a memory measuring daemon thread\n+        self.memthread, self.memlog = self.measure_memory(self.outdir)\n+    #\n+    \n+    \n+    def measure_memory(self, logdir, interval = 60):\n+        \"\"\"\n+        measure the use of the memory at the given intervals.\n+        Log the results in the log_folder.\n+        csv of time stamp [seconds], size [bytes]\n+        \"\"\"\n+        #import only on demand\n+        import psutil\n+        import threading\n+        \n+        #put the logs in the logdir, then it will automatically be saved later\n+        if not os.path.exists(logdir):\n+            os.makedirs(logdir)\n+        lname  = logdir +os.sep +\"stackpsf_mem.log\"\n+        logfile = open(lname, 'a')\n+        logfile.write(\"time, mem.total, mem.available, mem.percent\\n\")\n+        \n+        print(\"Mem logfile exists \", lname, os.path.exists(lname) )\n+        \n+        def log(_logfile):\n+            running = True\n+            while running:\n+                try:\n+                    mem = psutil.virtual_memory()\n+                    #svmem(total=10367352832, available=6472179712, percent=37.6, used=8186245120, free=2181107712\n+                    line = str(time.time()) +\",\" +str(mem.total) +\",\" +str(mem.available) +\",\" +str(mem.percent) +\"\\n\"\n+                    _logfile.write(line)\n+                    _logfile.flush()\n+                    \n+                    time.sleep(interval)\n+                except Exception as e:\n+                    print(e)\n+                    #when we close the logfile\n+                    running = False\n+                #\n+                #print(\"write to stackpsf_mem.log thread alive? \", running)\n+            #\n+        #\n+        t = threading.Thread(target=log, name='memory_use_thread', args=(logfile,))\n+        #set as daemon, so it does not prevent python to quit\n+        t.daemon = True\n+        t.start()\n+        \n+        print(\"created memory log file \", logdir +os.sep +\"stackpsf_mem.log\")\n+        print(\"mem thread running \", t.is_alive())\n+\n+        return t, logfile\n+    #\n+\n+\n+    def get_stamp_size(self):\n+        if self.stamp_size < 0:\n+            min_separation, max_sep = self.compute_minimum_separation_for_objects_in_layer()\n+            self.set_stamp_size(max_sep)\n+            #print(\"get stamp size \", self.stamp_size, \" == max_sep in pixel \", max_sep)\n+        return self.stamp_size\n+    #\n+    def set_stamp_size(self, stampsize):\n+        self.stamp_size = stampsize\n+    #\n+    ######################################################################################\n+    #\n+    # Instance methods of StackPSF\n+    #\n+    #######################################################################################\n+\n+    def run(self):\n+        \"\"\"\n+        RUN!\n+        \"\"\"\n+        #self.writeLog(\"init_run\")\n+        #self.init_run()\n+\n+        self.writeLog(\"read_input_list\")\n+        inputdata = self.read_input_list()\n+\n+        #get ra, dec from reference catalog, mags matches from EXT coadd\n+        #sorted by coadd magnitudes\n+        #se_matches is empty (contains only keys with ra:dec, dummy values)\n+        self.writeLog(\"read_input_positions\")\n+        ra, dec, se_matches = self.read_input_positions()\n+        \n+        \n+        #find out the instrument for later use\n+        self.writeLog(\"setInstrumentFilter\")\n+        self.setInstrumentFilter(self.inputdata.__iter__().__next__())\n+        \n+        self.writeLog(\"divide_input_positions_into_layers_bf\")\n+        layer_objects = self.divide_input_positions_into_layers_bf(math.sqrt(2.0)*self.get_stamp_size(), ra, dec, self._args.coadd, self._args.max_layer_num)\n+        \n+        \n+        layers = list(self.layer_objects.keys())\n+        mp_queue = mp.Queue()\n+        \n+        self.writeLog(\"prepare_layer_processing for layer \" +str(layers[0]))\n+        pm2 = ProcessManager.ProcessManager(1, \"Prepare Initial Layer Process\")\n+        pm2.add_process_and_start(self.prepare_layer_processing, args = (layers[0], se_matches, mp_queue))\n+        \n+        \n+        #Loop over the list of layers\n+        self.writeLog(\"Loop over the list of layers\")\n+        \n+        for _layer in range(len(layers)):\n+            layer = layers[_layer]\n+            \n+            outputs = mp_queue.get(block = True)\n+            #check, this should return instantly as the queue blocks\n+            pm2.block_until_all_processes_finished()\n+            \n+            print(\"got output from queue \", type(outputs))\n+            print(\"queue empty? \", mp_queue.empty())\n+            print(\"process finished? \", pm2.free())\n+\n+            layer_positions = outputs[\"layer_positions\"]\n+            all_stamps_for_layer = outputs[\"all_stamps_for_layer\"]\n+            se_matches = outputs[\"se_matches\"]\n+            self.writeLog(\"size of outputs \" +str(TotalSizeOf.total_size(outputs)))\n+            self.writeLog(\"size of layer_positions \" +str(TotalSizeOf.total_size(layer_positions)))\n+            self.writeLog(\"size of all_stamps_for_layer \" +str(TotalSizeOf.total_size(all_stamps_for_layer)))\n+            self.writeLog(\"size of se_matches \" +str(TotalSizeOf.total_size(se_matches)))\n+            \n+            # Loop over the list of SE images\n+            start = time.time()\n+              \n+            pm = ProcessManager.ProcessManager(self._args.multiprocesses, \"Image Simulator Process\")\n+            self.writeLog(\"Looping over the list of SE images\")\n+            for seimage in self.inputdata:\n+                if seimage not in all_stamps_for_layer:\n+                    continue\n+                ##### This part can be parallelized! (TBD) ###################\n+                #self.writeLog(\"simulate_single_image\")\n+                pm.add_process_and_start(self.simulate_single_image, args = (seimage, all_stamps_for_layer[seimage], layer))\n+                ##### End of the parallel part ##############################\n+            \n+            #while the images are simulated and swarp runs, prepare the next layer:\n+            \n+            if len(layers) > _layer +1:\n+                pm2 = ProcessManager.ProcessManager(1, \"Prepare Next Layer Process\")\n+                pm2.add_process_and_start(self.prepare_layer_processing, args = (layers[_layer +1], se_matches, mp_queue))\n+            #\n+            \n+            #wait with the swarp call until pm is ready\n+            pm.block_until_all_processes_finished()\n+            print(\"time for simulate_single_image for all images \",  time.time() - start)\n+            \n+            self.writeLog(\"sizes of objects before cleanup\")\n+            _keys = list(all_stamps_for_layer.keys())\n+            self.writeLog(\"lenght of all_stamps_for_layer list \" +str(len(_keys)) +\", \" +str(len(all_stamps_for_layer[_keys[0]])))\n+            self.writeLog(\"size of outputs \" +str(TotalSizeOf.total_size(outputs)))\n+            self.writeLog(\"size of layer_positions \" +str(TotalSizeOf.total_size(layer_positions)))\n+            self.writeLog(\"size of all_stamps_for_layer \" +str(TotalSizeOf.total_size(all_stamps_for_layer)))\n+            self.writeLog(\"size of se_matches \" +str(TotalSizeOf.total_size(se_matches)))\n+            self.writeLog(\"clean objects for next layer\")\n+            del outputs\n+            del layer_positions\n+            del all_stamps_for_layer\n+            #del se_matches\n+                \n+            # Produce the stacked PSF image for the current layer\n+            start = time.time()\n+            self.writeLog(\"create_stacked_PSF_image_for_layer\")\n+            self.create_stacked_PSF_image_for_layer(layer)\n+            print(\"time for create_stacked_PSF_image \",  time.time() - start)\n+            \n+            \n+            # remove the temporary PSF images for the layer\n+            start = time.time()\n+            self.writeLog(\"clean_up_layer_directory\")\n+            self.clean_up_layer_directory(layer)\n+            print(\"time for clean_up_layer_directory \",  time.time() - start)\n+            \n+            #wait with the next layer until pm2 is ready\n+            #no don't as the queue needs to be empty to close the process\n+            #pm2._block_until_all_processes_finished()\n+            \n+            self.writeLog(\"compute_total_object_number_for_layer\")\n+        #\n+        \n+        total_object_number = self.compute_total_object_number(se_matches)\n+\n+        # Produce the output FITS file\n+        self.writeLog(\"produce_output_FITS_file\")\n+        self.produce_output_FITS_file(total_object_number)\n+    #\n+    \n+    \n+    def prepare_layer_processing(self, layer, se_matches, mp_queue):\n+        start = time.time()\n+        #creates the workdirs for a layer, threadsafe\n+        self.init_layer(layer)\n+        \n+        # Define the subset of input objects that belong to\n+        # the current layer\n+        start = time.time()\n+        self.writeLog(\"define_object_subset_for_layer\")\n+        layer_positions = self.define_object_subset_for_layer(layer)\n+        print(\"time for define_object_subset_for_laye \",  time.time() - start)\n+        \n+        # For each object, append to self.se_matches the list of single\n+        # images containing the objects subset for the current layer:\n+        # data values ra, dec, mag are taken from single epoch catalogs\n+        # looks thread safe as well\n+        start = time.time()\n+        self.writeLog(\"match_layer_positions_with_single_epoch_catalogue_objects\")\n+        se_matches = self.match_layer_positions_with_single_epoch_catalogue_objects(layer_positions, se_matches)\n+        print(\"time for match_layer_positions \",  time.time() - start)\n+        \n+        # For each object, propagate the flux information\n+        # to the single images on which he object is not detected\n+        # (only in case the coadd catalogue is not given as an\n+        # input, otherwise we use the coadd flux)\n+        if self._args.coadd_cat is None:\n+            start = time.time()\n+            self.writeLog(\"extend_flux_measurements_to_undetected_objects\")\n+            se_matches = self.extend_flux_measurements_to_undetected_objects(layer_positions, se_matches)\n+            print(\"time for extend_flux_measurements \",  time.time() - start)\n+        #\n+        \n+        start = time.time()\n+        self.writeLog(\"all_psf_stamps_for_layer\")\n+        all_stamps_for_layer = self.all_psf_stamps_for_layer(layer_positions, se_matches)\n+        self.writeLog(\"time for all_psf_stamps_for_layer \" + str(layer) + \" in sequence \" +str(time.time() - start))\n+        \n+        self.writeLog(\"put results into the queue\")\n+        mp_queue.put({\"layer_positions\" :layer_positions, \"all_stamps_for_layer\": all_stamps_for_layer, \"se_matches\": se_matches})\n+        self.writeLog(\"done put results into the queue \")\n+        \n+        self.writeLog(\"compute_non_empty_se_matches\")\n+        #this is the memory peak!\n+        #self.compute_non_empty_se_matches(se_matches)\n+        self.writeLog(\"finished compute_non_empty_se_matches \")\n+        \n+        return\n+    #\n+    \n+    \n+    def init_run(self):\n+        \"\"\"\n+        Creates the working directories. If the output directory\n+        already exists, removes it.\n+        Creates the logger file\n+        \"\"\"\n+        if os.path.isdir(self.outdir):\n+            print(\"\\nRemoving directory %s\" % self.outdir)\n+            shutil.rmtree(self.outdir, ignore_errors=True)\n+        if not os.path.isdir(self.xmldir):\n+            os.makedirs(self.xmldir)\n+        if not os.path.isdir(self.auxdir):\n+            os.makedirs(self.auxdir)\n+        if not os.path.isdir(self.resdir):\n+            os.makedirs(self.resdir)\n+        self.logfile    = self.outdir + os.sep + \"StackPSF.log\"\n+        self.logger     = open(self.logfile, 'w')\n+\n+\n+\n+    def divide_input_positions_into_layers_bf(self, min_separation : int, ra : list, dec : list, coadd : str, max_layers : int):\n+        \"\"\"\n+        bf: brute force :o)\n+        does the direct distribution of ra/dec object indices from the VIS catalog into layers.\n+        Tries to put the low indices onto low layer numbers and thus allows to sort by mag, if not all\n+        layers are used (max_layers set).\n+        Does the calculation wether two coordinates are closer than min_separation in euclidian\n+        coadd space using the coadds WCS.\n+\n+        parameters:\n+        min_separation: the minimum distance between tow objects on a layer in pixel\n+        ra: a list of ra from the VIS reference catalog\n+        dec: dito\n+        coadd: the path to the coadd, using its WCS to transform ra/dec into pixel space\n+\n+        \"\"\"\n+        self.writeLog(\"Distribute to different layer, if objects are less than %s pixels apart\" %min_separation )\n+        hdul = fits.open(coadd)\n+        wcs = WCS(hdul[\"IMAGE\"].header)\n+\n+        #transform: pix[0] = x (ra)\n+        #pix[1] = y (dec)\n+        pix = wcs.all_world2pix(ra, dec, 1)\n+        hdul.close()\n+\n+        #lists contains the indexes of the layer\n+        #lists[layer] = []\n+        lists = OrderedDict()\n+\n+        #make the first layer and add the first object in the list\n+        lists[0] = []\n+        lists[0].append(0)\n+\n+\n+        def layer_contains_too_close_objects(layer : list, obj_index : int ) -> bool:\n+            #save the sqrt and compare the squares\n+            sep_comparator = min_separation*min_separation\n+            for obj in layer:\n+                ra_diff = pix[0][obj] - pix[0][obj_index]\n+                dec_diff = pix[1][obj] - pix[1][obj_index]\n+\n+                d_sq = ra_diff*ra_diff + dec_diff*dec_diff\n+\n+                if d_sq < sep_comparator:\n+                    #there is an object too close on this layer\n+                    return True\n+                #\n+            #no close object found\n+            return False\n+        #\n+\n+        def layers_sorted_for_fewest_objects(lists:OrderedDict):\n+            layers = []\n+            for i in lists:\n+                layers.append( (i,len(lists[i]) ) )\n+            #\n+            #print(\"layers \", layers)\n+            slayers = sorted( layers, key=operator.itemgetter(1) )\n+            returnvalue = list(zip(*slayers))[0]\n+            #print(\"return \", returnvalue)\n+            return returnvalue\n+        #\n+\n+        #now start the brute force\n+        import time\n+        start = time.time()\n+        objects_in_layers = 0\n+        totobj      = len(ra)\n+        self.writeLog(\"Total number of objects in the input catalog: %s\" %totobj )\n+\n+        for obj_index in range(1,len(ra)):\n+            #check, if existing layers already contains too close objects\n+            object_has_found_a_layer = False\n+            #distribute from reverse, as the latest layer has fewest objects\n+            #and thus processes fastest\n+            #-> this should start with the layer with the fewest objects\n+            #k = list(lists.keys())\n+            #k.reverse()\n+            for layer in layers_sorted_for_fewest_objects(lists):\n+                #for layer in lists:\n+                if not layer_contains_too_close_objects(lists[layer], obj_index):\n+                    lists[layer].append(obj_index)\n+                    object_has_found_a_layer = True\n+                    objects_in_layers = objects_in_layers +1\n+                    break\n+                #\n+            #\n+            if not object_has_found_a_layer and len(lists) < max_layers:\n+                #add a layer and put the object there\n+                #if max_layers is too small, the object will be dropped\n+                print(\"add layer number \", len(lists))\n+                lists[len(lists)] = []\n+                lists[len(lists)-1].append(obj_index)\n+                objects_in_layers = objects_in_layers +1\n+            #\n+            #if objects_in_layers > 0.85*totobj:\n+            #    self.writeLog(\"Distributed 85 percent of objects (= %s) to layers: \" %objects_in_layers )\n+            #    break\n+            #next obj_index\n+\n+        #Here we should have all layers nicely filled\n+        print(\"bf took seconds: \", time.time() - start)\n+\n+        #report\n+        #layer_objects is used in the class, so it must remain global for the moment\n+        self.layer_objects = OrderedDict()\n+\n+        layeridx    = 0\n+        for k in lists:\n+            self.layer_objects[str(k)] = lists[k]\n+            numb = len(lists[k])\n+            #objects_in_layers += numb\n+            self.writeLog(\"Layer %2d contains %5d objects (%2.2f %%)\" % (\n+                k, numb, 100 * float(numb) / float(totobj)))\n+\n+        #report missing objects\n+        self.writeLog('The total number of layers is set to %s.' %len(lists))\n+        self.writeLog('You are missing the PSF for %s objects (%2.2f %%).' %\n+                      (totobj - objects_in_layers, 100 * float(totobj - objects_in_layers) / float(totobj)))\n+\n+        json_dump = json.dumps(self.layer_objects, indent = 4)\n+        f         = open(\"%s/layer_obj.json\" % self.tmpdir,\"w\")\n+        f.write(json_dump)\n+        f.close()\n+\n+        return self.layer_objects\n+    #\n+\n+\n+    def take_out_already_assigned_indices(self, cluster_objects, already_assigned_indexes):\n+        #loop over all cluster objects, starting with the many neighbours\n+        #indexes\n+        for idx in range(len(cluster_objects) - 1, -1, -1):\n+            # Check if the group of overlapping objects contains objects\n+            # that have already been assigned to a layer, and remove them\n+            #if len(list(set(already_assigned_indexes).intersection(\n+            #        set(cluster_objects[idx])))) > 0:\n+            cluster_objects[idx] = [\n+                    x for x in cluster_objects[idx] if (\n+                        x not in already_assigned_indexes)]\n+            #\n+        #\n+        return cluster_objects, already_assigned_indexes\n+    #\n+\n+\n+    def init_layer(self, layer):\n+        \"\"\"\n+        perform operations to prepare the processing\n+        of the current layer\n+        \"\"\"\n+        self.writeLog(\"Layer %s\" % layer)\n+        self.create_working_directory_for_layer(layer)\n+\n+    def define_object_subset_for_layer(self, layer):\n+        \"\"\"\n+        Define the subset of input objects that belong to\n+        the current layer\n+        \"\"\"\n+        layer_positions = ObjectList(self.mag, ra = self.ra, dec = self.dec)\n+        layer_positions._filter(self.layer_objects[layer])\n+        return layer_positions\n+\n+\n+    def match_layer_positions_with_single_epoch_catalogue_objects(self, layer_positions:ObjectList, se_matches):\n+        \"\"\"\n+        For each object defined by the input ra,dec, checks if the object\n+        is included in any single epoch image in the input list.\n+        If so, matches the input positions with the positions of the objects\n+        in the single epoch catalogue, and filters out the flagged objects.\n+        Fills an entry in the self.se_matches dictionary for each single image\n+        included in the input list containing the object.\n+        This entry is a list containing magnitude, ra and dec of that object.\n+        The magnitudes, ra and dec positions are taken from the single epoch\n+        catalogue when available, otherwise we use the magnitude value already\n+        defined in self.getSkyPosition.\n+\n+        layer_positions: an ObjectList object which contains ra, dec, mag of the layer objects\n+        \"\"\"\n+        remove_items = []\n+        for se_image_name in self.inputdata:\n+            try:\n+                #create an ExtractImage object\n+                #MW this must go. Far too heavyweight for Panstarrs data\n+                #single_epoch_image    = self.init_image(se_image_name)\n+                \n+                se_image_header = FitsServices.read_image_header(se_image_name)\n+                overlapping_positions = self.project_input_coordinates_into_the_single_image_pixel_space(\n+                    se_image_header, layer_positions)\n+                if overlapping_positions._length == 0:\n+                    continue\n+                    \n+                #get the catalog hdu of the single epoch catalog\n+                cat = self.get_object_positions_from_single_epoch_catalogue(se_image_name)\n+\n+                #se_matched_objects contains values ra, dec, mag from the single epoch catalogs!\n+                se_matched_objects, indexes = self.match_single_epoch_catalogue_objects_with_input_positions(overlapping_positions, cat, se_image_name, se_image_header)\n+                \n+                \n+                se_matches = self.fill_se_matches(se_matches, overlapping_positions, se_matched_objects, se_image_name, se_image_header, indexes)\n+            except Exception as e:\n+                #MW: take out this image from self.inputdata (OrderedDict)\n+                self.writeLog(\"Found a problem with an se-image/catalog. It will be taken out: %s.\" % se_image_name)\n+                exec_info =  sys.exc_info()\n+                traceback.print_tb(exec_info[2])\n+                remove_items.append(se_image_name)\n+            #\n+        #MW:\n+        for img in remove_items:\n+            self.writeLog(\"remove image without overlapping objects in the catalog %s.\" % img)\n+            del self.inputdata[img]\n+        #\n+        return se_matches\n+    #\n+\n+    def extend_flux_measurements_to_undetected_objects(self, objects, se_matches):\n+        \"\"\"\n+        For each object, propagate the flux information\n+        to the single images on which he object is not detected\n+        (only in case the coadd catalogue is not given as an\n+        input, otherwise we use the coadd flux)\n+        \"\"\"\n+        for i in range(objects._length):\n+            sedict =  se_matches['%.10f:%.10f' % (objects.ra[i], objects.dec[i])]\n+            #sedict =  copy.deepcopy(self.se_matches['%.10f:%.10f' % (objects.ra[i], objects.dec[i])])\n+            #del sedict['bad']\n+            maglist = [\n+                sedict[img][0] + self.inputdata[img][\"zp\"]\n+                for img in sedict if (\n+                        sedict[img][0] != self._args.default_zp) and (\n+                            sedict[img][0] > -90)]\n+            if len(maglist) == 0:\n+                continue\n+            mm = np.mean(maglist)\n+            objects.mag[i] = mm\n+\n+            for img in sedict:\n+                if sedict[img][0] == self._args.default_zp:\n+                    self.writeLog(\"remaining sedict with default zp \" +str(sedict[img]))\n+                    se_matches[\n+                        '%.10f:%.10f' % (objects.ra[i], objects.dec[i])][img][0] = mm - self.inputdata[img][\"zp\"]\n+                else:\n+                    self.writeLog(\"extend_flux_measurements_to_undetected_objects \" +str( sedict[img][0]) )\n+                #\n+            #\n+        #\n+        return se_matches\n+    #\n+    \n+    \n+    def all_psf_stamps_for_layer(self, layer_positions, se_matches):\n+        \"\"\"\n+        interpolate all psfstamps for all image of a layer.\n+        Parallelizing this lead to concurrency problems before.\n+        Execute linearly and measure execution time.\n+        \"\"\"\n+        stamps = {}\n+        for seimage in self.inputdata:\n+            se_overlapping_objects = self.get_PSF_info(seimage, layer_positions, se_matches)\n+            \n+            skycoords = OutputStackedPSF.prepare_sky_coordinates(se_overlapping_objects.ra, se_overlapping_objects.dec)\n+            X, Y      = OutputStackedPSF.getPixels_from_header(FitsServices.read_image_header(seimage), skycoords)\n+            if len(X) == 0:\n+                #no result for this image\n+                continue\n+            #\n+            \n+            psf_file_name, catname = self.get_auxiliary_file_names(seimage)\n+            \n+            \"\"\"\n+            psfobj                 = PSFUtils.PSFUtils(psf_file_name)\n+            \n+            stamps[seimage] = list(zip( psfobj.evaluateAll( list(X +1), list(Y +1)), list(X), list(Y), list(se_overlapping_objects.mag)))\n+            \"\"\"\n+            psfobj = PsfModel.PsfModel(psf_file_name)\n+            \n+            stamps[seimage] = list(zip( psfobj.getAllPsfStamps( list(X +1), list(Y +1)), list(X), list(Y), list(se_overlapping_objects.mag)))\n+            \n+        #\n+        return stamps\n+    #\n+    \n+    def get_empty_image_for_simulation(self, seimage):\n+        \"\"\"\n+        Prepares the arrays containing the output maps\n+        for the single epoch PSF stamp\n+        \"\"\"\n+        header = FitsServices.read_image_header(seimage)\n+        data_dim  = (header[\"NAXIS2\"], header[\"NAXIS1\"])\n+        \n+        return np.zeros(data_dim, dtype = np.float64)        \n+    #\n+    \n+    def get_weight(self, seimage):\n+        return FitsServices.read_weight_data(seimage)\n+    #\n+    \n+    \n+    def simulate_single_image(self, seimage, stamplist, layer):\n+        \"\"\"\n+        stamplist: list osf tuples [(stamp, x, y, mag)]\n+        \"\"\"\n+        data = self.get_empty_image_for_simulation(seimage)\n+        weight = self.get_weight(seimage)\n+        zp = self.inputdata[seimage][\"zp\"]\n+        \n+        counter = 0\n+        #for stmp in stamplist:\n+        \n+        #MW: loop from reverse, so the objects can be deleted \n+        #right after use without affecting the loop\n+        #self.writeLog(\"size of stamplist before insert \" +str(TotalSizeOf.total_size(stamplist)))\n+        for i in range(len(stamplist) -1, -1, -1):\n+            stmp = stamplist[i]\n+            \n+            data, weight, shifted_stamp = self.add_PSF_stamp_to_simulated_image(data, weight, stmp[0], stmp[1], stmp[2], stmp[3], zp)\n+\n+            if self._args.debug:\n+                \"\"\"\n+                save the stamp and the stamp and the image into a fits file\n+                for inspection - this will be many fits files!\n+                \"\"\"\n+                def fit_gauss(image_to_fit, center = []):\n+                    au = ArrayUtils()\n+                    return au.fit_gauss_2d(image_to_fit, center)\n+                #\n+                \n+                stamp = stmp[0]\n+                \n+                #MW: eh????\n+                stamp.shape = ( int(psfnaxis[0]), int(psfnaxis[1]) )\n+                hdus = fits.PrimaryHDU(stamp)\n+                \n+                psffit = fit_gauss(stamp)\n+                \n+                hdr = fits.Header()\n+                hdr['X'] = str(x)\n+                hdr['Y'] = str(y)\n+                hdr['psf_center_x'] = psffit[1][0]\n+                hdr['psf_center_y'] = psffit[1][1]\n+                \n+                #psffit_image = fit_gauss(data, center = [objects.X[object_index], objects.Y[object_index]])\n+                psffit_image = fit_gauss(data)\n+                hdr['image_psf_center_x'] = psffit_image[1][0]\n+                hdr['image_psf_center_y'] = psffit_image[1][1]\n+                \n+                shift_psffit = fit_gauss(shifted_stamp)\n+                hdr['shifted_stamp_psf_center_x'] = shift_psffit[1][0]\n+                hdr['shifted_stamp_psf_center_y'] = shift_psffit[1][1]\n+                \n+                hdui = fits.ImageHDU(data, header = hdr)\n+                hduss = fits.ImageHDU(shifted_stamp)\n+\n+                hdul = fits.HDUList([hdus, hdui,hduss])\n+                path = seobj.name.rsplit(\".\", 1)[0] +'_psf_' +str(counter) +'.fits'\n+                print(\"write stamps to \", path)\n+                hdul.writeto(path, overwrite = 1)\n+\n+                #reset data to 0, otherwise the gauss fit has \n+                #to deal with multiple psfs which it cannot\n+                data = np.zeros(shape=data.shape, dtype=float, order='F')\n+            #\n+            #free mem as those are huge objects all together\n+            try:\n+                del shifted_stamp\n+                del stmp\n+                del stamplist[i] \n+            except:\n+                #don't fail in this case\n+                pass\n+            counter+= 1\n+        #\n+        #print(\"added \", counter, \" psfstamps to image \")\n+        #self.writeLog(\"size of stamplist after insert \" +str(TotalSizeOf.total_size(stamplist)))\n+        \n+        self.write_simulated_image_to_fits(layer, seimage, data, weight)\n+        \n+        #MW; is this necessary?\n+        del data\n+        del weight\n+    #\n+    \n+    \n+    def add_PSF_stamp_to_simulated_image(self, data, weight, stamp, x, y, mag, zp):\n+        \"\"\"\n+        Adds the PSF stamp to the image\n+        \"\"\"\n+        \n+        try:\n+            \n+            # pixel is in the SE coordinate reference frame\n+            pixel     = (x,y)\n+            norm = self.get_normalization_for_stamp(mag, zp)\n+            \n+            #which mags to we deal with?\n+            if abs(norm) > 1.e38 or mag > 90.:\n+                return data, weight, stamp\n+            #\n+            if self._args.normalize_psfs:\n+                #normalize!\n+                #this is a debug option\n+                stamp = stamp/max(stamp)\n+            else:\n+                #this is the usual way\n+                stamp = stamp*norm\n+            #\n+            #MW: eh????\n+            if len(stamp.shape) == 1:\n+                stamp.shape = ( int(math.sqrt(stamp.shape[0])), int(math.sqrt(stamp.shape[0])) )\n+            stamp_dim = np.array(stamp.shape)\n+\n+\n+            data_dim  = data.shape\n+            start, end  = self.get_indexes_of_stamp_on_image_psfmodel_shift(pixel, stamp_dim)\n+            \n+            #does the stamp completely overlap?\n+            #if not, adapt the indices\n+            stampcut = [0,0,stamp_dim[0],stamp_dim[1]]\n+            modify = False\n+            if start[0] < 0:\n+                #cut the stamp\n+                stampcut[0] = -start[0]\n+                start[0] = 0\n+                modify = True\n+            if start[1] < 0:\n+                stampcut[1] = -start[1]\n+                start[1] = 0\n+                modify = True\n+            if end[0] > data_dim[1]:\n+                #cut the stamp\n+                stampcut[2] = data_dim[1] - end[0]\n+                end[0] = data_dim[1]\n+                modify = True\n+            if end[1] > data_dim[0]:\n+                #cut the stamp\n+                stampcut[3] = data_dim[0] - end[1]\n+                end[1] = data_dim[0]\n+                modify = True\n+            if modify:\n+                stamp = stamp[int(stampcut[1]):int(stampcut[3]), int(stampcut[0]):int(stampcut[2]) ]\n+            #\n+            #print(\"data positions \", int(start[1]),int(end[1]), int(start[0]),int(end[0]), \", stamp \", stamp.shape)\n+\n+            if np.all(np.array(stamp.shape) > 0):\n+                #if not, the stamp is outside of the array\n+                #this may happen in some cases as the stampsize is the max of the psfnaxes\n+                if not modify:\n+                    #only add not modified stamps\n+                    data[int(start[1]):int(end[1]), int(start[0]):int(end[0])] += stamp\n+                else:\n+                    #if stamps are modified, set the gain to 0, so they don't contribute\n+                    weight[int(start[1]):int(end[1]), int(start[0]):int(end[0])] = 0\n+                #\n+                if norm == 0. or mag == -99:\n+                    #gains are set to 0, means also weights will be 0\n+                    weight[int(start[1]):int(end[1]), int(start[0]):int(end[0])] = 0\n+                #\n+            #\n+            return data, weight, stamp/norm\n+        #\n+        except Exception as e:\n+            #if there is a problem for one object,\n+            #don't fail. Report and go on.\n+            print(\"An object could not be added to the layer image. \")\n+            exec_info =  sys.exc_info()\n+            traceback.print_tb(exec_info[2])\n+        #\n+    \n+    \n+    def create_stacked_PSF_image_for_layer(self, layer):\n+        \"\"\"\n+        Produce the stacked PSF image for the current layer\n+        \"\"\"\n+        layerdir = self.tmpdir + os.sep + \"layer%s\" % layer\n+        swarp_lists, _, _ = self.get_swarp_lists(layerdir)\n+        IMAGEOUT_PATH, WEIGHTOUT_PATH = self.run_swarp(layerdir, swarp_lists)\n+\n+        #MW: store the output image in the layer_objects list\n+        print(\"Layer \", layer, \", IMAGEOUT_PATH \", IMAGEOUT_PATH, \", exists \", self.exists(IMAGEOUT_PATH))\n+        self.layer_coadds[layer] = self.exists(IMAGEOUT_PATH) #if self.exists(IMAGEOUT_PATH) is not None else IMAGEOUT_PATH\n+\n+        #set an extname, so ExtractImage will later work\n+        if self.layer_coadds[layer] is not None:\n+            fits.setval(self.layer_coadds[layer], 'extname', value='IMAGE', ext=0)\n+            fits.setval(self.layer_coadds[layer], 'DES_EXT', value='IMAGE', ext=0)\n+        #\n+\n+        #do the same for the weight image\n+        weight_image = self.exists(WEIGHTOUT_PATH)\n+        if weight_image is not None:\n+            fits.setval(weight_image, 'extname', value='WEIGHT', ext=0)\n+            fits.setval(weight_image, 'DES_EXT', value='WEIGHT', ext=0)\n+        #\n+        print(\"stored coadd image in layer_objects \", self.layer_coadds[layer])\n+        print(\"stored coadd weight \", weight_image)\n+\n+\n+    def clean_up_layer_directory(self, layer):\n+        \"\"\"\n+        remove the temporary PSF images for the layer\n+        if the option clean_up is given\n+        \"\"\"\n+        if self._args.clean_up:\n+            layerdir = self.tmpdir + os.sep + \"layer%s\" % layer\n+            for fname in os.listdir(layerdir):\n+                if fname.endswith(\"_psfstamps.fits\"):\n+                    os.remove(os.path.join(layerdir, fname))\n+\n+    def produce_output_FITS_file(self, total_object_number):\n+        \"\"\"\n+        Creates the output fits file containing all layers of non-overlapping\n+        stacked PSFs, plus a table per extension containing the PSFs positions.\n+        \"\"\"\n+        X, Y  = self.project_positions_onto_coadd_pixel_space(self.ra, self.dec)\n+        \n+        self.logger.write(\"produce_output_FITS_file: PsfStampFilter \")\n+        psf = PsfStampFilter(self.get_stamp_size(), filtertype = self.getInstrumentFilter(), execfilter = self._args.execfilter)\n+        self.layer_objects = psf.filter(self.layer_objects, self.layer_coadds, X, Y)\n+        #self.layer_objects = filtered_layer_objects\n+        \n+        \n+        # Initialize the FITS image and the FITS table in the output\n+        # file extensions\n+        self.logger.write(\"produce_output_FITS_file: OutputStackedPSF \")\n+        \n+        output_obj = OutputStackedPSF(total_object_number, self.get_stamp_size(), self.tmpdir, self.tilename, self.band, self.logger)\n+        self.logger.write(\"produce_output_FITS_file: populate \")\n+        \n+        output_obj.populate(self.layer_objects, self.ra, self.dec, self.mag, self._args.coadd, self._args.bandpass_value)\n+        self.logger.write(\"write this file \" + self._args.outdir +os.sep +self.tilename +\"_\" +self.band +\"_\" +\"_stackedPSF.fits\" )\n+        output_obj.write_to_fits(\"%s/%s_%s_stackedPSF.fits\" % (self._args.outdir, self.tilename, self.band))\n+\n+        #write a second normalized image for quality control\n+        #output_obj = OutputStackedPSF(total_object_number, self.get_stamp_size(), self.tmpdir, self.tilename, self.band)\n+        #output_obj.populate(self.layer_objects, self.ra, self.dec, self.mag, self.coadd, self._args.bandpass_value, normalize = True)\n+        #output_obj.write_to_fits(\"%s/Normalized_%s_%s_stackedPSF.fits\" % (self._args.outdir, self.tilename, self.band))\n+\n+\n+\n+    def setInstrumentFilter(self, se_image):\n+        \"\"\"\n+        extracts the instrument keyword from the image header of\n+        the single image and stores it for later use for the psf filters\n+        \"\"\"\n+        imghdu       = FitsServices.get_image_hdu(se_image)\n+        instrument   = FitsServices.read_key(se_image, imghdu, \"INSTRUME\").lower()\n+\n+        if \"decam\" in instrument:\n+            self.FilterType = FilterTypes.DES\n+        elif \"omegacam\" in instrument:\n+            self.FilterType = FilterTypes.KIDS\n+        elif \"LSST\" in instrument:\n+            self.FilterType = FilterTypes.LSST\n+        else: #add condition here. #LSST = ?\n+            self.FilterType = FilterTypes.NOS\n+        #\n+    #\n+\n+    def getInstrumentFilter(self):\n+        \"\"\"\n+        returns the instrument filter type that has been set\n+        with setInstrument.\n+        \"\"\"\n+        return self.FilterType\n+    #\n+\n+    def filter_bad_PSFs(self):\n+        \"\"\"\n+        Identifies the PSFs that cannot be fit with a gaussian shape\n+        and removes them from the output file\n+        \"\"\"\n+        temp_layer_objects = OrderedDict()\n+        X, Y  = self.project_positions_onto_coadd_pixel_space(self.ra, self.dec)\n+\n+        for i in range(len(self.layer_objects)):\n+            psfstack  = ExtractImage.ExtractImage(\n+                \"%s/layer%s/%s_%s.image.fits\" % (\n+                    self.tmpdir, i, self.tilename, self.band),\n+                verbose = 0)\n+            psfstack.getImage()\n+            data_dim = (psfstack.naxis2, psfstack.naxis1)\n+            temp = []\n+            for obj in self.layer_objects[str(i)]:\n+                x = X[obj]\n+                y = Y[obj]\n+                stamp  = Stamp(psfstack.image, self.get_stamp_size(), data_dim)\n+                stamp_image = stamp.get_stamp_at(x,y)\n+\n+                _max = abs(np.amax(stamp_image))\n+                _min = abs(np.amin(stamp_image))\n+                _sum = np.sum(stamp_image)\n+                #print(\" min, max \", _min, _max, \", same order of magnitude? \", (_max - _min)/_max < 0.9, \", sum \", _sum )\n+                if abs(_sum) < 0.0001 or _sum < 0:\n+                    print(\"sum of the values is too low. Reject.\", _sum)\n+                    continue\n+                try:\n+                    amplitude, center, sigma = ArrayUtils.fit_simple_gauss_2d(stamp_image)\n+                    #print(\"sigma \", sigma)\n+                    if amplitude < 0:\n+                        print(\"Negative fit amplitude. Reject.\")\n+                        continue\n+                    psf_dim = np.array(stamp_image.shape)\n+                    idxs = ArrayUtils.get_array_indices(psf_dim)\n+                    shape = stamp_image.shape\n+                    gauss = ArrayUtils.simple_gauss_2d(idxs, amplitude, center[1], center[0], sigma)\n+                    print(np.sqrt(np.mean((stamp_image.flatten() - gauss) ** 2.))/amplitude)\n+                    if np.sqrt(np.mean((stamp_image.flatten() - gauss) ** 2.))/amplitude < 0.05:\n+                        temp.append(obj)\n+                    else:\n+                        print('PSF centered at %.3f, %.3f skipped: bad PSF shape' % (x, y))\n+                        continue\n+                except:\n+                    print('PSF centered at %.3f, %.3f skipped: could not fit a gaussian' % (x, y))\n+            temp_layer_objects[str(i)] = temp\n+        self.layer_objects = temp_layer_objects\n+        print(\"  \", [len(self.layer_objects[x]) for x in self.layer_objects])\n+    \n+    \n+    def compute_non_empty_se_matches(self, se_matches = None):\n+        if se_matches is None:\n+            se_matches = self.se_matches\n+        non_empty = 0\n+        for key in se_matches:\n+            if len(se_matches[key].keys()) >0:\n+                non_empty = non_empty +1\n+            #\n+        #\n+        print(\"self.se_matches contains \", non_empty, \" keys with data.\")\n+\n+    def compute_total_object_number(self, se_matches):\n+        \"\"\"\n+        MW: deprecated, replace by compute_total_object_number_for_layer\n+        Computes the total number of objects contained in the layers\n+        included in the processing\n+        \"\"\"\n+        # Get the total number of objects\n+        totobj   = 0\n+        temp_layer_objects = OrderedDict()\n+        \n+        for i in self.layer_objects.keys():\n+            temp = []\n+            print(\"handle layer \", i)\n+            for x in self.layer_objects[i]:\n+                sematch = se_matches[\"%.10f:%.10f\" % (self.ra[x], self.dec[x])]\n+                \n+                #filter out funny mag values mag = sematch[key][0] for all keys\n+                if len(sematch) > 0 and any(\n+                    x <  90 for x in [sematch[key][0] for key in sematch]) and any(\n+                    x > -90 for x in [sematch[key][0] for key in sematch]):\n+                    temp.append(x)\n+            temp_layer_objects[i] = temp\n+            totobj += len(temp_layer_objects[i])\n+            print(\"added \", len(temp_layer_objects[i]), \" objects.\")\n+        self.layer_objects = temp_layer_objects\n+        \n+        return totobj\n+    \n+    \n+    def compute_total_object_number_for_layer(self, se_matches, layerid):\n+        \"\"\"\n+        Computes the total number of objects contained in the layers\n+        included in the processing\n+        \"\"\"\n+        # Get the total number of objects\n+        totobj   = 0\n+        temp_layer_objects = OrderedDict()\n+        \n+        temp = []\n+        print(\"handle layer \", layerid)\n+        for x in self.layer_objects[layerid]:\n+            sematch = se_matches[\"%.10f:%.10f\" % (self.ra[x], self.dec[x])]\n+                \n+            #filter out funny mag values mag = sematch[key][0] for all keys\n+            if len(sematch) > 0 and any(\n+                x <  90 for x in [sematch[key][0] for key in sematch]) and any(\n+                x > -90 for x in [sematch[key][0] for key in sematch]):\n+                temp.append(x)\n+        temp_layer_objects[layerid] = temp\n+        totobj += len(temp_layer_objects[layerid])\n+        print(\"added \", len(temp_layer_objects[layerid]), \" objects for layer \", layerid, \".\")\n+        self.layer_objects[layerid] = temp_layer_objects[layerid]\n+        \n+        return totobj\n+    \n+    \n+    def read_input_list(self):\n+        \"\"\"\n+        reads the input image list, stores the list with all\n+        auxinfo in self.inputdata, if it belongs to the requested band\n+        \"\"\"\n+        #store data in a dict: imagename:{img, zp, fwhm, psf, cat}\n+        self.inputdata = OrderedDict()\n+        for line in open(self._args.SElist, 'r'):\n+\n+            linesplit = line.split(\" \")\n+            if (self.band == linesplit[5]):\n+                self.inputdata[linesplit[0]] = {\"img\":linesplit[0],\"dummy\":linesplit[1], \"fwhm\": linesplit[2], \"zp\": float(linesplit[3]),\n+                \"zperr\": float(linesplit[4]), \"band\": linesplit[5], \"psf\":linesplit[6], \"cat\":linesplit[7]}\n+        #\n+        return self.inputdata\n+\n+\n+    def read_input_positions(self):\n+        \"\"\"\n+        Extract the positions on the sky on which we want to compute the\n+        stacked PSF, excluding the sources located at the edges of the coadd.\n+        Defines self.mag, containing the value self._args.default_zp for\n+        each input position.\n+        If the coadd catalogue is given as an input, matches the input\n+        positions with the coadd detections, and assigns to each position\n+        the value of magnitude corresponding to the detected object.\n+        Initializes  self.se_matches, a dictionary containing, for each object,\n+        a dictionary containing magnitude and position of the object in\n+        each single image in which the object is contained.\n+        \"\"\"\n+        #read the data from the VIS input catalog: ra, dec, mag\n+        ra, dec, mag = self.read_raw_positions_from_the_input_catalogue()\n+        X, Y        = self.project_positions_onto_coadd_pixel_space(ra, dec)\n+        idx_subset  = self.exclude_edge_sources(X, Y)\n+        self.ra     = ra[idx_subset]\n+        self.dec    = dec[idx_subset]\n+\n+        #MW: here we always put only the default value of 25 which is wrong\n+        #if an object is not found in the single epoch catalog, it will keep its\n+        #low magnitude and will dominate the coadd, even if the object is bright\n+        #in reality\n+        self.mag    = np.ones(len(self.ra)) * self._args.default_zp\n+\n+        #here we use the reference catalog magnitude, which can also be wrong.\n+        #best is to have a coadd catalog and initialize from there\n+        self.mag    = mag[idx_subset] #these are still reference (VIS) values\n+\n+        #now replace VIS with DECam magnitudes\n+        if self._args.coadd_cat is not None:\n+            self.read_magnitudes_from_coadd_catalogue()\n+\n+        #optionally, sort ra,dec,mag by magnitude\n+        if self._args.sort_catalog_by_mag:\n+            #convert the lists to one list of tuples\n+            def sort_table(table, col=0):\n+                #invert the table to get columns instead of rows\n+                sorted_table = sorted( list(zip(*table)), key=operator.itemgetter(col))\n+                return list( zip(*sorted_table) )\n+            #\n+            table = [self.ra,self.dec,self.mag]\n+            sorted_table = sort_table(table, 2)\n+\n+            self.ra = np.array(sorted_table[0])\n+            self.dec = np.array(sorted_table[1])\n+            self.mag = np.array(sorted_table[2])\n+\n+            #we have assigned self._args.default_zp as an default value\n+            #is it still in the maglist after coadd catalog matches have been assigned?\n+            self.writeLog(\"read_input_positions: number of default mags remaining in list \" + str( sorted_table[2].count(self._args.default_zp) ))\n+        #\n+\n+        #\n+        # Initialize the counter of single images in which we\n+        # find the given object\n+        se_matches = {}\n+        for radec in zip(self.ra, self.dec):\n+            se_matches['%.10f:%.10f' % (radec[0], radec[1])] = {}\n+        return self.ra, self.dec, se_matches\n+\n+    def read_raw_positions_from_the_input_catalogue(self):\n+        \"\"\"\n+        Reads the fields self._args.ra, self._args.dec\n+        from the input positions catalogue\n+\n+        MW: make sure, we read only objects to a depths, that the\n+        project under investigation can provide\n+        DES griz = [25.5, 25.2, 24.4, 23.8] for example.\n+        Uses mag_entry and ref_mag arguments\n+\n+        Optionally, sort the coordinates by mag.\n+        \"\"\"\n+        self.writeLog(\"Reading the position catalogue to extract the points\")\n+        self.writeLog(\"on which we want to measure the PSF\")\n+        cat_data = fitsio.FITS(self._args.position_cat)[-1]\n+\n+        ra     = cat_data[self._args.ra].read()\n+        dec    = cat_data[self._args.dec].read()\n+        mag    = cat_data[self._args.mag_entry].read()\n+\n+        idx = np.where( np.logical_and((mag >= self._args.min_mag), (mag <= self._args.ref_mag)) )\n+        print(\"positions from refcat \", len(ra), \", filtered by magnitude \", str(self._args.ref_mag), \" \", len(ra[idx]))\n+        return ra[idx], dec[idx], mag[idx]\n+\n+    def exclude_edge_sources(self, X, Y):\n+        \"\"\"\n+        Filter out the positions that do not overlap the coadd\n+        and the edge sources\n+        \"\"\"\n+        \"\"\"\n+        wo = np.where((X > (self.get_stamp_size() // 2)) &\n+                      (Y > (self.get_stamp_size() // 2)) &\n+                      (Y < self.coadd.naxis2 -\n+                       (self.get_stamp_size() // 2)) &\n+                      (X < self.coadd.naxis1 -\n+                          (self.get_stamp_size() // 2)))[0]\n+        \"\"\"\n+        wo = np.where(self.stamp_inside_image(X,Y,self.coadd_wcs.get_header()[\"NAXIS1\"],self.coadd_wcs.get_header()[\"NAXIS2\"] ))[0]\n+        return wo\n+\n+    def read_magnitudes_from_coadd_catalogue(self):\n+        \"\"\"\n+        Read the values of the magnitudes from the (optional)\n+        input coadd catalogue, ra/dec matching the coordinates\n+        of the objects with the input positions\n+        \"\"\"\n+        flagcat_data = fitsio.FITS(self._args.coadd_cat)[-1]\n+        flags = flagcat_data[\"flags\"].read()\n+        fields    = [self.coadd_x, self.coadd_y, 'flags',\n+                     'mag_auto', 'mag_psf', 'spread_model']\n+\n+        # Get the indexes of the objects that are detected in the coadd\n+        good_idx  = np.where((flags & 4  == 0) &\n+                                 (flags & 8  == 0) &\n+                                 (flags & 16 == 0) &\n+                                 (flags & 32 == 0))[0]\n+\n+        # Read the positions and magnitudes of the detected objects\n+        cox       = flagcat_data[self.coadd_x].read()[good_idx]\n+        coy       = flagcat_data[self.coadd_y].read()[good_idx]\n+        mag       = flagcat_data[\"mag_auto\"].read()[good_idx]\n+        mag_psf   = flagcat_data[\"mag_psf\"].read()[good_idx]\n+        spread    = flagcat_data[\"spread_model\"].read()[good_idx]\n+        star_idxs = np.where(np.abs(spread) < 0.002)[0]\n+\n+        #replace mag_auto for stars with mag_psf\n+        mag[star_idxs] = mag_psf[star_idxs]\n+\n+        #good      = np.where(mag < 90)[0]\n+        #for consistency...\n+        good = np.where( np.logical_and((mag >= self._args.min_mag), (mag <= self._args.ref_mag)) )\n+        mag       = mag[good]\n+        cox       = cox[good]\n+        coy       = coy[good]\n+\n+        # Project the detected object position to the sky to\n+        # compute the ra/dec positions\n+        #MW: retired\n+        #ite       = [iter(cox), iter(coy)]\n+        #pixcoo    = list(it.__next__() for it in itertools.cycle(ite))\n+        pixcoo    = OutputStackedPSF.prepare_sky_coordinates(cox, coy)\n+\n+        #MW: do this consistently with astropy\n+        #MW: no, as this crashes KIDS processing\n+        #investigate why\n+        #image_hdu = FitsServices.get_image_hdu(self.coadd.name)\n+        #hdul = fits.open(self.coadd.name)  # open a FITS file\n+        #h = hdul[image_hdu].header\n+        #WCSU = WcsUtils(h)\n+        \n+        cora, codec = self.coadd_wcs.py_pix2world(pixcoo[0::2], pixcoo[1::2])\n+        #skycoo    = _getWcsLibCorners.py_pix2world(\n+        #    self.coadd.wcs_params + pixcoo)\n+        #codec     = np.array(skycoo[1::2])\n+        #cora      = np.array(skycoo[0::2])\n+        if len(cora) > 0:\n+            self.writeLog(\"The provided coadd catalog is empty, it will not be used.\")\n+            idx, cidx = self.matchCoordinates(self.ra, self.dec, cora, codec)\n+            self.mag[idx]  = mag[cidx]\n+        #\n+\n+    def project_positions_onto_coadd_pixel_space(self, ra, dec):\n+        \"\"\"\n+        Project the input positions onto the coadd pixel space,\n+        and filter out the edge sources\n+        \"\"\"\n+        #get a extract image object of the coadd\n+        #self.coadd = self.getCoaddMetadata(self._args.coadd)\n+        skycoords  = OutputStackedPSF.prepare_sky_coordinates(ra, dec)\n+        X, Y       = OutputStackedPSF.getPixels(self._args.coadd, skycoords)\n+        \n+        return X, Y\n+    \n+    \n+    def init_image(self, image):\n+        \"\"\"\n+        Instantiates the ExtractImage class and reads the WCS parameters\n+        \"\"\"\n+        # Read the SE image and get WCS\n+        name       = image.split(\" \")[0]\n+        obj        = ExtractImage.ExtractImage(name, verbose = 0)\n+        obj.getWcsParams()\n+        return obj\n+\n+    def project_input_coordinates_into_the_single_image_pixel_space(self, single_epoch_image_header, layer_positions):\n+        \"\"\"\n+        Project the coordinates onto the se image pixel space\n+        \"\"\"\n+        #MW: in this method we add positions for all layer objects calculated from only one single image!\n+        #layer_positions must be copied and then thrown away\n+        temp_layer_pos = copy.deepcopy(layer_positions)\n+\n+        skycoords  = OutputStackedPSF.prepare_sky_coordinates(temp_layer_pos.ra, temp_layer_pos.dec)\n+        # Project the coordinates onto the se image pixel space\n+        WCSU       = WcsUtils(single_epoch_image_header)\n+        se_x, se_y   = WCSU.py_world2pix(skycoords)\n+\n+        #add the X,Y values for the ra, dec, mag values already in temp_layer_pos\n+        temp_layer_pos._add_y_list(np.array(se_y))\n+        temp_layer_pos._add_x_list(np.array(se_x))\n+        overlapping_positions = self.filter_out_objects_not_contained_in_the_single_image(single_epoch_image_header, temp_layer_pos)\n+        return overlapping_positions\n+\n+\n+    def filter_out_objects_not_contained_in_the_single_image(self, single_epoch_image_header : dict, objects : ObjectList):\n+        \"\"\"\n+        single_epoch_image: an ExtractImage object with single epoch image information\n+        objects: an ObjectList containing ra, dec, mag, X, Y information for objects on a layer\n+        \"\"\"\n+        # Get the objects that ovelap the single image\n+        \"\"\"\n+        #MW:here we take objects, whose center falls into the image\n+        idx = np.where((objects.X >= -(self.get_stamp_size() // 2)) &\n+                       (objects.Y >= -(self.get_stamp_size() // 2)) &\n+                       (objects.X <= single_epoch_image.naxis1 + (self.get_stamp_size() // 2)) &\n+                       (objects.Y <= single_epoch_image.naxis2 + (self.get_stamp_size() // 2)))[0]\n+        \"\"\"\n+        #MW:lets take all objects, whose stamp overlaps the image with at least one pixel\n+        idx = np.where(self.stamp_overlaps_image(objects.X, objects.Y, single_epoch_image_header[\"NAXIS1\"], single_epoch_image_header[\"NAXIS2\"]))[0]\n+\n+        objects._filter(idx)\n+        return objects\n+\n+    def get_object_positions_from_single_epoch_catalogue(self, sename):\n+        \"\"\"\n+        Reads the single epoch catalogue and extracts the relevant fields\n+\n+        return\n+        last hdu of the catalog (which is the one with the table data)\n+        \"\"\"\n+        # Read the single epoch catalogue\n+        psf_file_name, catname = self.get_auxiliary_file_names(sename)\n+        return fitsio.FITS(catname)[-1]\n+\n+\n+\n+    def flux_2_mag(self, fluxarray, zeropoint):\n+        \"\"\"\n+        converts arrays with fluxes into arrays with magnitudes\n+        if a flux value should be <=0, it will be set to math.pow(10, -28)\n+        to result in magnitudes > 90\n+        \"\"\"\n+        farray = np.array(fluxarray)\n+        farray[ np.where( farray <= 0.0 ) ] = math.pow(10, -28)\n+\n+        magarray = np.array([ (-2.5 * math.log10(flux) + zeropoint) for flux in farray])\n+        return magarray\n+    #\n+\n+\n+    def match_single_epoch_catalogue_objects_with_input_positions(self, overlapping_positions:ObjectList, cat_data, se_image_name, se_image_header):\n+        \"\"\"\n+        Match the catalogue objects with the coordinates that overlap the single image\n+        and re-order the matched fields\n+\n+        overlapping_positions: an ObjectList with the ra,dec,mag,x,y coordinates of the single image objects\n+        cat_data: the table hdu of the matching catalog of se_image_name\n+        se_image_name a single image\n+\n+        return\n+        ObjectList with the matched objects\n+        indexes: a dict with the ids of detected, non-detected and objects with good flags\n+        \"\"\"\n+        #Match the catalogue objects with the coordinates that overlap the single image\n+        #the single epoch catalogs contain only x,y positions\n+        #use the wcs from the header to generate ra,dec\n+        #DES data uses model fitting, KIDS aperture fitting. Add LSST later\n+        #possible_xy_catalog_values: list of catalog table names that apply (X_IMAGE, X_WIN etc.)\n+        xvalue = self.possible_xy_catalog_values[0][0]\n+        yvalue = self.possible_xy_catalog_values[0][1]\n+        for xy_values in self.possible_xy_catalog_values:\n+            if xy_values[0] in cat_data._colnames and xy_values[1] in cat_data._colnames:\n+                xvalue = xy_values[0]\n+                yvalue = xy_values[1]\n+                break\n+            #\n+        #\n+\n+        #from catalog X/Y and se-image WCS, get se-image ra/dec\n+        xlist = cat_data[xvalue].read()\n+        ylist = cat_data[yvalue].read()\n+\n+        WCSU = WcsUtils(se_image_header)\n+        ralist, declist = WCSU.py_pix2world(xlist, ylist)\n+\n+        #indexes of matching objects:\n+        #detected_objects_idx: indexes in overlapping positions\n+        #seIdx: indexes in catalog ra/dec. Apply to select from catalog\n+        detected_objects_idx, seIdx   =  self.matchCoordinates(\n+            overlapping_positions.ra, overlapping_positions.dec, ralist, declist)\n+\n+        # Re-order the matched fields\n+        #-> select from catalog\n+        #-> now sera, dec, spread have the same order as detected_objects\n+        #   from overlapping positions\n+        sera         = ralist[seIdx]\n+        sedec        = declist[seIdx]\n+        sespread     = cat_data[\"spread_model\"].read()[seIdx]\n+\n+        # We use mag_psf for stars, mag_auto for extended objects\n+        #MW: like above with the coordinates, fluxes should be used and transformed\n+        #with zeropoints - possibly updated in stage 2\n+        #prefer fluxes above magnitudes, if possible.\n+        #At the time of writing, DES does not come with mag_psf though\n+        #This code will be simplified, once all catalogs contain the required\n+        #flux values\n+        semag = np.array([])\n+        semag_psf = np.array([])\n+        if \"FLUX_AUTO\" in cat_data._colnames and \"FLUX_PSF\" in cat_data._colnames:\n+            semag        = cat_data[\"FLUX_AUTO\"].read()[seIdx] #kids flux_auto, des mag_auto\n+            semag_psf    = cat_data[\"FLUX_PSF\"].read()[seIdx] #kids: flux_psf, des mag-psf\n+\n+            #transform flux to magnitudes\n+            zp = float( self.inputdata[se_image_name][\"zp\"] )\n+            semag = self.flux_2_mag( semag, zp )\n+            semag_psf = self.flux_2_mag( semag_psf, zp )\n+        elif \"MAG_AUTO\" in cat_data._colnames and \"MAG_PSF\" in cat_data._colnames:\n+            semag        = cat_data[\"MAG_AUTO\"].read()[seIdx]\n+            semag_psf    = cat_data[\"MAG_PSF\"].read()[seIdx]\n+        elif \"MAG_PSF\" in cat_data._colnames:\n+            semag_psf    = cat_data[\"MAG_PSF\"].read()[seIdx]\n+            semag        = semag_psf\n+            self.logger.write(\"\\n%s\" % \"The is no MAG_AUTO in the catalog. Work with MAG_PSF only!\")\n+        #\n+        star_idxs    = np.where((np.abs(sespread) < 0.002) &\n+                                  (semag_psf < 90.))[0]\n+\n+        #replace the coadd catalog mags with the single epoch catalog mags?????? for stars\n+        semag[star_idxs] = semag_psf[star_idxs]\n+        \n+        flagnames = [\"flags\", \"FLAGS\"]\n+        for flagname in flagnames:\n+            if flagname in cat_data._colnames:\n+                break\n+            #\n+        #\n+        seflags      = cat_data[flagname].read()[seIdx]\n+        indexes      = self.define_single_epoch_indexes(detected_objects_idx, seflags, overlapping_positions._length)\n+\n+        se_matched_objects = ObjectList(semag, ra = sera, dec = sedec)\n+        se_matched_objects._add_attribute('flags', seflags)\n+        return se_matched_objects, indexes\n+\n+    def define_single_epoch_indexes(self, detected_objects_idx, seflags, total_object_number):\n+        \"\"\"\n+        Defines a dictionary containing the indexes of :\n+        - detected objects\n+        - good objects (where the flags are OK),\n+        - non-detected objects\n+        relative to the input set of positions overlapping the single image\n+        \"\"\"\n+        indexes = {}\n+        \"\"\"\n+        if self._args.keep_edge_obj:\n+            #MW: keep_edge_obj works for the coadd and the single images\n+            #there is no way to keep the edge objects of the single images and thus\n+            #guantee high quality mapping, but keep out the edge objects of the coadd\n+            #to avoid half-moon shaped psfs\n+            \n+            good_idx      = np.where((seflags & 4  == 0) &\n+                                    (seflags & 16 == 0) &\n+                                    (seflags & 32 == 0))[0]         \n+        else:\n+            good_idx      = np.where((seflags & 4  == 0) &\n+                                    (seflags & 8  == 0) &\n+                                    (seflags & 16 == 0) &\n+                                    (seflags & 32 == 0))[0]\n+        \"\"\"\n+        good_idx = np.logical_and(seflags,4) == 0\n+        good_idx = np.logical_and( good_idx, np.logical_and(seflags,16) == 0)\n+        good_idx = np.logical_and( good_idx, np.logical_and(seflags,32) == 0)     \n+        if not self._args.keep_edge_obj:\n+            good_idx = np.logical_and( good_idx, np.logical_and(seflags,8) == 0)\n+        good_idx = np.where(good_idx)[0]\n+        # Get the indexes of the objects that are not detected in the\n+        # single image, or for which the information in the single\n+        # epoch catalogue is not reliable (bad flags)\n+        # detected_objects_idx[good_idx]: values of detected_objects_idx at positions of good_idx\n+        non_detected_objects_idx        = [x for x in range(total_object_number) if x not in detected_objects_idx[good_idx]]\n+        indexes['detected_objects']     = detected_objects_idx #unaltered in this method. Was the match\n+                                                               #between se-image overlaps and se-catalog\n+        indexes['non_detected_objects'] = non_detected_objects_idx #se-image overlaps with bad flags\n+        indexes['good_objects']         = good_idx #all se-catalog flags as required\n+        \n+        return indexes\n+\n+\n+    def stamp_overlaps_image(self, x, y, naxis1, naxis2):\n+        \"\"\"\n+        Algorithm to define, if the stamp surrounding an object with coordinates x,y overlaps\n+        an image naxis1, naxis2.\n+\n+        x,y: numbers or arrays of x,y values\n+        naxis1, naxis2: wcs parameters\n+\n+        return\n+        boolean value or array of booleans, matching x,y format\n+        \"\"\"\n+        try:\n+            comparator = self.get_stamp_size()//2\n+            return (x > - (comparator - 0.5)) & \\\n+                (y > - (comparator - 0.5)) & \\\n+                (x < naxis1 + 0.5 + comparator) & \\\n+                (y < naxis2 + 0.5 + comparator)\n+        except Exception as e:\n+            print(e)\n+            print(\"comparator \", comparator, \", type \", type(comparator))\n+            print(\"naxis1 \", naxis1, \", type \", type(naxis1))\n+            print(\"naxis2 \", naxis2, \", type \", type(naxis2))\n+\n+    #\n+\n+    def stamp_inside_image(self, x, y, naxis1, naxis2):\n+        \"\"\"\n+        Algorithm to define, if the stamp of an object with coordinates x,y is\n+        completely inside an image naxis1, naxis2.\n+\n+        x,y: numbers or arrays of x,y values\n+        naxis1, naxis2: wcs parameters\n+\n+        return\n+        boolean value or array of booleans, matching x,y format\n+        \"\"\"\n+        comparator = self.get_stamp_size()//2\n+        return (x >= comparator + 0.5) & \\\n+           (y >= comparator + 0.5) & \\\n+           (x <= naxis1 - (comparator - 0.5)) & \\\n+           (y <= naxis2 - (comparator - 0.5))\n+    #\n+\n+\n+    def fill_se_matches(self, se_matches, overlapping_positions, se_matched_objects, se_object_name, se_object_header, indexes):\n+        \"\"\"\n+        Fills an entry in the self.se_matches for the objects that are\n+        not detected, using the value of the magnitude defined in the\n+        self.getSkyPosition method, and for those that are detected and\n+        are marked as 'good' (meaning that the flags are OK).\n+\n+        overlapping_positions: ra,dec,mag for the layer from the coadd catalogs\n+        se_matched_objects: data (ra,dec,mag) from single epoch catalogs\n+        seobj: ExtractImage Object, replace with se_object_name and se_object_header\n+        \"\"\"\n+        detected_objects_idx = indexes['detected_objects']\n+        if \"INSTRUME\" in se_object_header:\n+            instrument = se_object_header[\"INSTRUME\"] #we require this to be there\n+        else:\n+            instrument = \"LSST or KIDS\"\n+        #\n+        \n+        # Adding the 'good' objects contained in the coadd catalogue\n+        for i in indexes['good_objects']:\n+            # Fills an entry in the self.se_matches for each detected object\n+            # using the magnitude value from the single epoch catalogue\n+            key = \"%.10f:%.10f\" % (overlapping_positions.ra[detected_objects_idx[i]], overlapping_positions.dec[detected_objects_idx[i]])\n+            se_matches[key][se_object_name] = [\n+                    se_matched_objects.mag[i], se_matched_objects.ra[i], se_matched_objects.dec[i]]\n+        #\n+        if self._args.keep_edge_obj:\n+            # If keep_edge_obj option is given, we fill an entry for each object\n+            # whose stamp overlaps the single image\n+\n+            # Adding the non detected objects\n+            #they must always be added otherwise psf would miss in some images and thus\n+            #destroying the psf in the layer coadd\n+            #the mag could be set to -99 though in order to weight them out\n+            for i in indexes['non_detected_objects']:\n+                \"\"\"\n+                #MW:here we take objects, whose center falls into the image\n+                #this is duplicate code. See lines 1227 ff. correct\n+                if (overlapping_positions.X[i] >= -(self.get_stamp_size() // 2)) and (\n+                        overlapping_positions.Y[i] >= -(self.get_stamp_size() // 2)) and (\n+                            overlapping_positions.X[i] <= seobj.naxis1 + (self.get_stamp_size() // 2)) and (\n+                                overlapping_positions.Y[i] <= seobj.naxis2 + (self.get_stamp_size() // 2)):\n+                \"\"\"\n+\n+                magnitude = overlapping_positions.mag[i]\n+                #MW: DECAM is now calibrated\n+                #if \"DECam\" in instrument:\n+                #    #correct for incorrect DECam single epoch magnitudes\n+                #    overlapping_positions.mag[i] - self.inputdata[seobj.name][\"zp\"] + self._args.default_zp\n+                if self.stamp_overlaps_image(overlapping_positions.X[i], overlapping_positions.Y[i], se_object_header[\"NAXIS1\"], se_object_header[\"NAXIS1\"]):\n+                    key = \"%.10f:%.10f\" % (overlapping_positions.ra[i], overlapping_positions.dec[i])\n+                    se_matches[key][se_object_name] = [magnitude, overlapping_positions.ra[i], overlapping_positions.dec[i]]\n+            #\n+            # Adding the 'good' objects contained in the coadd catalogue\n+            #for i in indexes['good_objects']:\n+            #    # Fills an entry in the self.se_matches for each detected object\n+            #    # using the magnitude value from the single epoch catalogue\n+            #    self.se_matches[\"%.10f:%.10f\" % (\n+            #        overlapping_positions.ra[detected_objects_idx[i]], overlapping_positions.dec[detected_objects_idx[i]])][se_object_name] = [\n+            #            se_matched_objects.mag[i], se_matched_objects.ra[i], se_matched_objects.dec[i]]\n+            #\n+        else:\n+            # If keep_edge_obj option is not given, we fill an entry for each object\n+            # whose stamp is completely included in the single image\n+            # We assign the magnitude value of -99 to the edge objects,\n+            # in order to later mask them when creating the PSF stamps.\n+\n+            # Adding the non detected objects (i.e. the objects that are not included\n+            # in the single image catalogue\n+            for i in indexes['non_detected_objects']:\n+                \"\"\"\n+                if (overlapping_positions.X[i] > (self.get_stamp_size() // 2)) and (\n+                        overlapping_positions.Y[i] > (self.get_stamp_size() // 2)) and (\n+                            overlapping_positions.X[i] <= seobj.naxis1 - ((self.get_stamp_size() // 2) + 1)) and (\n+                                overlapping_positions.Y[i] <= seobj.naxis2 - ((self.get_stamp_size() // 2) + 1)):\n+                elif (overlapping_positions.X[i] >= -(self.get_stamp_size() // 2)) and (\n+                        overlapping_positions.Y[i] >= -(self.get_stamp_size() // 2)) and (\n+                            overlapping_positions.X[i] <= seobj.naxis1 + (self.get_stamp_size() // 2)) and (\n+                                overlapping_positions.Y[i] <= seobj.naxis2 + (self.get_stamp_size() // 2)):\n+                \"\"\"\n+                if self.stamp_inside_image(overlapping_positions.X[i], overlapping_positions.Y[i], se_object_header[\"NAXIS1\"], se_object_header[\"NAXIS2\"]):\n+                    key = \"%.10f:%.10f\" % (overlapping_positions.ra[i], overlapping_positions.dec[i])\n+                    se_matches[key][se_object_name] = [\n+                        overlapping_positions.mag[i] - self.inputdata[se_object_name][\"zp\"] + self._args.default_zp,\n+                        overlapping_positions.ra[i], overlapping_positions.dec[i]]\n+                    self.writeLog(\"fill_se_matches: overlapping_positions.mag[i] \" +str(overlapping_positions.mag[i]) +\", self.inputdata[seobj.name]['zp'] \" +str(self.inputdata[se_object_name]['zp']) +\", self._args.default_zp \" +str(self._args.default_zp) )\n+                elif self.stamp_overlaps_image(overlapping_positions.X[i], overlapping_positions.Y[i], se_object_header[\"NAXIS1\"], se_object_header[\"NAXIS2\"]):\n+                    key = \"%.10f:%.10f\" % (overlapping_positions.ra[i], overlapping_positions.dec[i])\n+                    se_matches[key][se_object_name] = [-99, overlapping_positions.ra[i], overlapping_positions.dec[i]]\n+            #\n+            # Adding the 'good' objects contained in the coadd catalogue\n+            #for i in indexes['good_objects']:\n+            #    # Fills an entry in the self.se_matches for each detected object\n+            #    # using the magnitude value from the single epoch catalogue\n+            #    self.se_matches[\"%.10f:%.10f\" % (\n+            #        overlapping_positions.ra[detected_objects_idx[i]], overlapping_positions.dec[detected_objects_idx[i]])][se_object_name] = [\n+            #            se_matched_objects.mag[i], se_matched_objects.ra[i], se_matched_objects.dec[i]]\n+        \n+        return se_matches\n+    #\n+\n+    def assertFalse(self, retval):\n+        \"\"\"\n+        print an error message if retval is not 0\n+        \"\"\"\n+        if retval !=0:\n+            self.writeLog(retval)\n+        #\n+    #\n+\n+    def get_auxiliary_file_names(self, sename):\n+        \"\"\"\n+        Get the name of the Single spoch catalogue and of the\n+        PSF model corresponding to an input single epoch image.\n+        \"\"\"\n+        return self.inputdata[sename][\"psf\"], self.inputdata[sename][\"cat\"]\n+\n+\n+    def compute_minimum_separation_for_objects_in_layer(self):\n+        \"\"\"\n+        Checks that all SE images in the SElist have an associated\n+        PSF model.\n+        Defines self.min_separation, containing the minimum distance\n+        for non-overlapping PSFs in all objects.\n+\n+        returns: the minimum separation in of 2 objects in arcseconds\n+        \"\"\"\n+        self.writeLog(\n+            \"Getting the minimum angular separation for non overlapping PSFs\")\n+        min_separation = 0.\n+        self.pixscale  = 0\n+        max_separation_in_pixels = 0\n+\n+        for seimage in self.inputdata:\n+            se_header = fitsio.FITS(seimage)[FitsServices.get_image_hdu(seimage)].read_header()\n+            if \"pixscal1\" in se_header and \"pixscal2\" in se_header:\n+                pixscal1 = float(se_header[\"pixscal1\"])\n+                pixscal2 = float(se_header[\"pixscal2\"])\n+            else:\n+                pixscal1 = math.sqrt( float(se_header[\"CD1_1\"]) **2 + float(se_header[\"CD2_1\"]) **2 ) *3600.0\n+                pixscal2 = math.sqrt( float(se_header[\"CD1_2\"]) **2 + float(se_header[\"CD2_2\"]) **2 ) *3600.0\n+            #\n+            ######################################################################\n+            if abs( max(pixscal1, pixscal2) - self.pixscale ) > 0.01:\n+                if self.pixscale == 0:\n+                    self.pixscale = max(pixscal1, pixscal2)\n+                    self.writeLog(\"Pixel scale = %s arcsec / pixel\" % self.pixscale)\n+                else:\n+                    self.writeLog(\"Error: some images have a different pixel scale: %s \" % max(pixscal1, pixscal2))\n+            #######################################################################\n+\n+            psf_file_name = self.inputdata[seimage][\"psf\"]\n+            catname = self.inputdata[seimage][\"cat\"]\n+\n+            \"\"\"\n+            psfobj        = PSFUtils.PSFUtils(psf_file_name)\n+            retval        = psfobj.file_exists(psf_file_name)\n+\n+            self.assertFalse(retval)\n+            psfnaxis        = psfobj.get_convolution_params()\n+            \"\"\"\n+            psfobj         = PsfModel.PsfModel(psf_file_name)\n+            psfnaxis       = psfobj.getStampsize()\n+\n+\n+            max_sep       = max(psfnaxis)\n+            if max_separation_in_pixels < max_sep:\n+                max_separation_in_pixels = max_sep\n+            sep = np.sqrt(2.) * max_sep * max(pixscal1, pixscal2)\n+\n+            if sep > min_separation:\n+                min_separation = sep\n+            #print(\"max_sep \", max_sep, \", max pixelsize \", max(pixscal1, pixscal2))\n+\n+        # We want the stamp size to be odd\n+        if max_separation_in_pixels % 2 == 0:\n+            max_separation_in_pixels = max_separation_in_pixels - 1\n+\n+        self.min_separation = min_separation\n+        self.writeLog(\"Min separation for non overlapping PSFs: %s arcsec\" % (\n+            self.min_separation))\n+        self.writeLog(\"Stampsize: %s pixels\" % (\n+            max_separation_in_pixels))\n+\n+        return min_separation, max_separation_in_pixels\n+\n+\n+    def readLayers(self):\n+        \"\"\"\n+        Reads the layers of non overlapping objects from a json file (DEBUG)\n+        \"\"\"\n+        lists              = OrderedDict()\n+        self.layer_objects = OrderedDict()\n+        with open(self._args.layers_file) as json_data:\n+            lists          = json.load(json_data)\n+        max_layers         = self._args.max_layer_num\n+        if max_layers <= 0 or max_layers > len(lists):\n+            max_layers     = len(lists)\n+\n+        missing_obj        = 0\n+        for layeridx in range(max_layers):\n+            self.layer_objects[str(layeridx)] = lists[str(layeridx)]\n+            self.writeLog(\"Layer %s contains %s objects\" % (\n+                layeridx, len(self.layer_objects[str(layeridx)])))\n+            layeridx      += 1\n+        for k in range(max_layers, len(lists)):\n+            missing_obj   += len(lists[str(k)])\n+            \n+        self.writeLog('The total number of layers is set to %s.' % max_layers)\n+        self.writeLog('You are missing the PSF for %s objects.' %\n+                      missing_obj)\n+    #\n+    \n+    def write_simulated_image_to_fits(self, layer, seimage, psfimage, psfweight):\n+        \"\"\"\n+        Write the PSF single image to a FITS file\n+        \"\"\"\n+        header = FitsServices.read_image_header(seimage)\n+        wheader = FitsServices.read_weight_header(seimage)\n+        \n+        seimage = seimage.rsplit(os.sep, 1)[-1]\n+        \n+        layerdir = self.tmpdir + os.sep + \"layer%s\" % layer\n+        tmp_out_name   = layerdir + os.sep + self.out_name % (\n+            os.path.basename(seimage.split(\".fits\")[0]))\n+        f = fitsio.FITS(tmp_out_name, 'rw', clobber = True)\n+        f.write(np.array(psfimage,       dtype = 'float32'),\n+                header = header,\n+                extver = 1)\n+        f.write(np.zeros(psfimage.shape, dtype = 'float32'),\n+                header = wheader,\n+                extver = 2)\n+        f.write(np.array(psfweight,      dtype = 'float32'),\n+                header = wheader,\n+                extver = 3)\n+        f.close()\n+    #\n+    \n+    \n+    def writeLog(self, message):\n+        \"\"\"\n+        prints the message and writes it in the log file\n+        \"\"\"\n+        print(\"\\n%s\" % datetime.datetime.now(), message)\n+        self.logger.write(\"\\n%s\" % message)\n+    #\n+    \n+    \n+    def gauss_fit_stamps(self, xlist, ylist, stampsize):\n+        \"\"\"\n+        for all positions: return a narrow gaussian\n+        \"\"\"\n+        stamplist = []\n+                         \n+        for i in range(len(xlist)):\n+            #add a gauss\n+            au = ArrayUtils() \n+            \n+            pixels = au.get_array_indices(np.empty([int(stampsize[0]),int(stampsize[1])]))\n+            \n+            \n+            parameters = {\n+                \"amplitude\": 1,\n+                \"center_row\": (stampsize[0] - 1)/2.0,\n+                \"center_col\": (stampsize[1] - 1)/2.0,\n+                \"sigma_row\": 0.95,\n+                \"sigma_col\": 0.95,\n+                \"angle\": 0,\n+                \"offset\": 0\n+            }\n+            \n+            stamplist.append( au.gauss_2d(pixels, **parameters).reshape([int(stampsize[0]),int(stampsize[1])]) )\n+        #\n+        return stamplist\n+    #\n+    \n+    \n+    def get_normalization_for_stamp(self, mag, zp):\n+        \"\"\"\n+        Evaluate the normalization for the PSF stamp\n+\n+        convert the magnitudes to flux using the default zp\n+        -> this should use the real zp of the image\n+\n+        If no mag is given, (the code sets it to -99 for objects\n+        not in the SE catalog), 0 is returned.\n+        \"\"\"\n+        if mag == -99:\n+            return 0.\n+        else:\n+            #convert the magnitudes to flux using the zp\n+            #-> this should use the real zp of the image\n+            #self.writeLog(\"get_normalization_for_stamp \" +str(mag) +\", zp \" +str(zp))\n+            #return 10 ** ((zp - mag) / 2.5)\n+            \n+            \n+            #MW: we decided to use the same magnitude for all objects\n+            #as it is constant for all objects of a stackset, it makes little sense to scale\n+            #but if a misdetection is in the stackset, it may introduce a large error\n+            return  10 ** ((zp - self.standard_magnitude_for_all_objects) / 2.5)\n+    #\n+    \n+    \n+    def center_objects_in_coadd_pixel(self, se_objects_ra, se_objects_dec, coadd_wcs, se_wcs):\n+        \"\"\"\n+        calculates the x,y position of an object psf with obj_ra, obj_dec so\n+        that it ends in the center of the pixel in the coadd.\n+        \n+        This avoids later a recentering of the coadded psf in the psf stamp\n+        \"\"\"\n+        coadd_x, coadd_y = coadd_wcs.py_world2pix_ra_dec(se_objects_ra, se_objects_dec)\n+        coadd_x_wanted = [round(_x) for _x in coadd_x]\n+        coadd_y_wanted = [round(_y) for _y in coadd_y]\n+        \n+        ra_wanted, dec_wanted = coadd_wcs.py_pix2world(coadd_x_wanted, coadd_y_wanted)\n+        \n+        se_x, se_y = se_wcs.py_world2pix_ra_dec(ra_wanted, dec_wanted)\n+        \n+        return se_x, se_y\n+    #\n+    \n+    \n+    def get_indexes_of_stamp_on_image_psfmodel_shift(self, pixel, stamp_dim):\n+        \"\"\"\n+        psfmodel_shift: psf is always centered in center pixel.\n+            psfmodel shifts like this:\n+            .0 : psf centered in center pixel\n+            .2 : psf shifted by 0.2 from center of center pixel\n+            .499 : psf shifted by .499 from center of center pixel (= between 2 pixel)\n+            .5 : psf shifted by -.5 from center of center pixel\n+            .8 : psf shifted by -.2 from center of center pixel\n+        \n+        \"\"\"\n+        x = pixel[0]\n+        y = pixel[1]\n+        stampSize = stamp_dim[0]\n+        \n+        #stampsize is always odd\n+        stampShift = (stampSize - 1) // 2\n+        \n+        \n+        #get the cuts\n+        #do it so that the shifted center is in the center of the cutout\n+        #enlarge by one, so interpolation does not generate voids\n+        \n+        #the psfmodel round switches one back starting from 0.5\n+        #round would then round(n.5) = n+1\n+        #but round(n.5) = n\n+        #so I need a manual round\n+        offset = {\"x\": x - math.floor(x), \"y\": y - math.floor(y)}\n+        for key in offset:\n+            if offset[key] < 0.5:\n+                offset[key] = 0\n+            else:\n+                offset[key] = 1\n+            #\n+        #\n+        startx = math.floor(x) - stampShift + offset[\"x\"]\n+        endx = math.floor(x) + stampShift + offset[\"x\"] +1 #+1: odd stampsize \n+        \n+        starty = math.floor(y) - stampShift + offset[\"y\"]\n+        endy = math.floor(y) + stampShift + offset[\"y\"] +1\n+        \n+        return [startx, starty], [endx, endy]\n+    #\n+    \n+    def get_indexes_of_stamp_on_image_scipy_shift(self, pixel, stamp_dim):\n+        \"\"\"\n+        scipy_shift: psf is always centered, independent of the pixel coordinate\n+                     shift is done by scipy\n+        \"\"\"\n+        x = pixel[0]\n+        y = pixel[1]\n+        stampSize = stamp_dim[0]\n+        \n+        #shift psf center at x,y to pixel center at .0,.0        \n+        interpolate_x = x - math.floor(x)\n+        interpolate_y = y - math.floor(y)\n+        \n+        #stampsize is always odd\n+        stampShift = (stampSize - 1) // 2\n+        \n+        \n+        #get the cuts\n+        #do it so that the shifted center is in the center of the cutout\n+        #enlarge by one, so interpolation does not generate voids\n+        startx = math.floor(x) - stampShift \n+        endx = math.floor(x) + stampShift +1 #+1: odd stampsize \n+        \n+        starty = math.floor(y) - stampShift\n+        endy = math.floor(y) + stampShift +1\n+        \n+        return [startx, starty], [endx, endy], [interpolate_x, interpolate_y]\n+    #\n+        \n+    def old_get_indexes_of_stamp_on_image(self, pixel, stamp_dim):\n+        \"\"\"\n+        Get the indexes of the image on which the stamp should be added\n+\n+        Parameters:\n+        pixel a set of (x,y) values in single image reference frame\n+        stamp_dim ndarray of stamp.shape\n+        \"\"\"\n+        #psf is centered in the central pixel of the stamp\n+        #which is coordinate (stampsize - 1)/2.0\n+        #\n+        #the center of a pixel are coordinates 1,1 in image space,\n+        #and 0.5,0.5 in pixel space\n+        #\n+        #We must distinguish between image pixel coordinate space, where the center of the first\n+        #pixel has coordinate 0.5, 0.5\n+        #and grid pixel space, where the first pixel is 0,0\n+\n+        #Center of the stamp falls into image pixel\n+        ix = int(pixel[0] +0.5)\n+        iy = int(pixel[1] +0.5)\n+\n+        #this is also the coordinate of the pixel after projection\n+        #so the necessary shift is\n+        shiftx = pixel[0] - ix\n+        shifty = pixel[1] - iy\n+\n+        #should the stamp have an even number of pixels,\n+        #the center is betwen pixels and it does not fall\n+        #into the center pixel of the image, but also between pixels\n+        #in this case, it must be shifted by another 0.5 pixel\n+        if (stamp_dim[0] & 1) == 0:\n+            shiftx+=0.5\n+        if (stamp_dim[1] & 1) == 0:\n+            shifty+=0.5\n+\n+        #first pixel for the stamp addition is\n+        startx = ix - stamp_dim[1]//2\n+        starty = iy - stamp_dim[0]//2\n+\n+        endx = startx + stamp_dim[1] #end coordinate is exclusive\n+        endy = starty + stamp_dim[0]\n+\n+        #return in col/row notation to avoid numpy chaos\n+        return [startx, starty], [endx, endy], [shiftx, shifty]\n+    #\n+\n+    def get_swarp_lists(self, layerdir):\n+        \"\"\"\n+        gets the input lists for swarp that are relevant for the current layer,\n+        stores them in the dictionary swarp_list ands returns it.\n+        \"\"\"\n+        #psflist: this is the list of the psfstamps\n+        psflist = [layerdir + os.sep + x for x in os.listdir(\n+            layerdir) if '_psfstamps' in x]\n+        names = [x.rsplit(os.sep, 1)[-1].split('_psfstamps')[0] for x in psflist]\n+\n+        #single_epoch_list_for_layer: the list of the real single epoch images\n+        #from the data directory\n+        #must be perfectly matched to the psflist\n+        single_epoch_srcpath = list(self.inputdata.keys())[0].rsplit(os.sep, 1)[0]\n+        single_epoch_list_for_layer = [self.exists(single_epoch_srcpath +os.sep +x +\".fits\") for x in names]\n+\n+        flux_scales    = self.write_flux_scales_list(layerdir, single_epoch_list_for_layer)\n+        imlist, wtlist = self.write_swarp_lists(psflist, 'psfSwarpList')\n+        swarp_lists    = {'images'       : imlist,\n+                          'weights'      : wtlist,\n+                          'flux_scales'  : flux_scales}\n+        return swarp_lists, single_epoch_list_for_layer, psflist\n+    #\n+\n+\n+    def swarp_call(self, coadd_header, band, swarp_lists, bg_sub):\n+        \"\"\"\n+        Define and return the swarp call\n+        \"\"\"\n+        swarpcall = [\n+            \"@%s\" % swarp_lists['images'],\n+            \"-c\",                \"%s/etc/default.swarp \" % os.environ[\n+                'EUCLID_CONF'],\n+            \"-PIXELSCALE_TYPE\",  \"MANUAL \",\n+            \"-PIXEL_SCALE\",      \"%s\"     % coadd_header[\"PIXSCALE\"],\n+            \"-CENTER_TYPE\",      \"MANUAL \",\n+            \"-CENTER\",           \"%s,%s \" % (coadd_header[\"CRVAL1\"], coadd_header[\"CRVAL2\"]),\n+            \"-IMAGE_SIZE\",       \"%s,%s \" % (coadd_header[\"NAXIS1\"], coadd_header[\"NAXIS2\"]),\n+            \"-SUBTRACT_BACK\",    \"%s\" % bg_sub,\n+            \"-BLANK_BADPIXELS\",  \"N \",\n+            \"-COPY_KEYWORDS\",    \"PSF_FWHM,PSF_BETA,FWHMHOMO \",\n+            \"-DELETE_TMPFILES\",  \"Y\",\n+            \"-FSCALASTRO_TYPE\",  \"NONE \",\n+            \"-FSCALE_DEFAULT\",   \"@%s\"   % swarp_lists['flux_scales'],\n+            \"-FSCALE_KEYWORD\",   \"nokey\",\n+            \"-RESAMPLE\",         \"Y\",\n+            \"-RESAMPLING_TYPE\",  self._args.swarp_resampling,\n+            \"-COMBINE\",          \"Y \",\n+            \"-WEIGHT_TYPE\",      \"MAP_WEIGHT \",\n+            #\"-WEIGHT_TYPE\",      \"NONE\",\n+            \"-WEIGHT_IMAGE\",     \"@%s\" % swarp_lists['weights'],\n+            \"-HEADER_ONLY\",      \"N \",\n+            \"-NTHREADS\",         str(self._args.multiprocesses),\n+            \"-COMBINE_TYPE\",     \"WEIGHTED \",\n+            #\"-COMBINE_TYPE\",     \"AVERAGE \",\n+            \"-WRITE_XML\",        \"Y\",\n+            \"-XML_NAME\",         \"%s/%s_%s_swarp.xml\" % (\n+                self.xmldir, self.tilename, band),\n+            \"-TILE_COMPRESS\",    \"N \", #MW: this creates correlated bg noise as the input is exactly 0\n+            \"-IMAGEOUT_NAME\",    \"%s_%s.image.fits\"   % (\n+                self.tilename, band),\n+            \"-WEIGHTOUT_NAME\",   \"%s_%s.weight.fits\"  % (\n+                self.tilename, band),\n+            \"-RESAMPLE_DIR\",     \"%s\" % self.resdir,\n+            \"-VMEM_DIR\",         \"%s\" % self.resdir +os.sep +\"VMEM_DIR\",\n+            \"-VMEM_MAX\",         \"4096 \",\n+            \"-MEM_MAX\",          \"6500 \",\n+            \"-COMBINE_BUFSIZE\",  \"6000 \"\n+        ]\n+        return swarpcall\n+\n+    def filter(self, ra, dec, mag, spread, flags, indexes):\n+        ra     = ra[indexes]\n+        dec    = dec[indexes]\n+        mag    = mag[indexes]\n+        spread = spread[indexes]\n+        flags  = flags[indexes]\n+        return ra, dec, mag, spread, flags\n+\n+    def matchCoordinates(self, ra1, dec1, ra2, dec2):\n+        \"\"\"\n+        Match the input coordinates, returns the lists of matched\n+        indexes for the two catalogues\n+\n+        Returns\n+        aidx: indices of matching coordinates in array ra1, dec1\n+        bidx: indices of matching coordinates in array r2, dec2\n+\n+        ra1[aidx[0]], dec1[aidx[0]] match ra2[bidx[0]], dec2[bidx[0]]\n+\n+        This code returns matches based on max distances of 2 pixels\n+        \"\"\"\n+        c       = SkyCoord(ra  = ra1  * u.degree,\n+                           dec = dec1 * u.degree)\n+        catalog = SkyCoord(ra  = ra2  * u.degree,\n+                           dec = dec2 * u.degree)\n+        #BIdx: indices of catalog that match c\n+        #d2d angular distance of the matched objects\n+        BIdx, d2d, d3d = c.match_to_catalog_sky(catalog)\n+        d2d     = d2d.hour * 15. * 3600.\n+\n+        AIdx    = np.arange(len(d2d))\n+        BIdx   = BIdx[d2d <=  self._args.max_separation_for_match]\n+        AIdx   = AIdx[d2d <=  self._args.max_separation_for_match]\n+        #BIdx    = BIdx[d2d <=  2 * self.pixscale]\n+        #AIdx    = AIdx[d2d <=  2 * self.pixscale]\n+        return AIdx, BIdx\n+\n+    def get_PSF_info(self, sename, layer_positions, se_matches):\n+        \"\"\"\n+        Extract the information about the objects in the se image from\n+        self.se_matches\n+        \"\"\"\n+        info  = np.array([\n+            se_matches[\n+                '%.10f:%.10f' % (key[0], key[1])][sename] for key in zip(\n+                layer_positions.ra, layer_positions.dec) if sename in se_matches[\n+                '%.10f:%.10f' % (key[0], key[1])]])\n+        mag    = np.array([x[0] for x in  info])\n+        se_ra  = np.array([x[1] for x in  info])\n+        se_dec = np.array([x[2] for x in  info])\n+        se_objects = ObjectList(mag, ra = se_ra, dec = se_dec)\n+        return se_objects\n+    #\n+\n+\n+    def create_working_directory_for_layer(self, layer):\n+        \"\"\"\n+        Creates the temporary layer directory\n+        \"\"\"\n+        layerdir = self.tmpdir + os.sep + \"layer%s\" % layer\n+        if os.path.isdir(layerdir):\n+            shutil.rmtree(layerdir)\n+        os.makedirs(layerdir)\n+\n+    def write_flux_scales_list(self, outdir, single_epoch_list_for_layer):\n+        \"\"\"\n+        Compute the scale corrections to be applied by swarp.\n+        Returns the filename containing the list of corrections.\n+        \"\"\"\n+        fscaleList = \"%s/%s_%s_fluxscale.list\" % (\n+            outdir, self.tilename, self.band)\n+        f = open(fscaleList, \"w+\")\n+        for sename in single_epoch_list_for_layer:\n+            zp = self.inputdata[sename.split(\" \")[0]][\"zp\"]\n+            #fluxscale for swarp: sets the coadd to default_zp\n+            #-> adapt that to coadd_zp?\n+            fluxscale = pow(10, ((self._args.coadd_zp - zp) / 2.5))\n+            f.write(\"%s\\n\" % fluxscale)\n+        f.close()\n+        return fscaleList\n+\n+    def write_swarp_lists(self, inlist, label):\n+        \"\"\"\n+        Defines two files containing the lists of images and weights\n+        for swarp. Returns the file names.\n+        \"\"\"\n+        iname = self.auxdir + os.sep + label + '_img.list'\n+        wname = self.auxdir + os.sep + label + '_wht.list'\n+        f = open(iname, 'w+')\n+        g = open(wname, 'w+')\n+        for sename in inlist:\n+            f.write(\"%s[0]\\n\" % sename)\n+            g.write(\"%s[2]\\n\" % sename)\n+        f.close()\n+        g.close()\n+        return iname, wname\n+\n+    def run_swarp(self, workdir, swarp_lists, bg_sub = 'N'):\n+        \"\"\"\n+        defines the swarp call and executes the program\n+\n+        returns the path to the output image\n+        \"\"\"\n+        pwd = os.getcwd()\n+        os.chdir(workdir)\n+\n+        swarp_args  = self.swarp_call(self.coadd_wcs.get_header(), self.band, swarp_lists, bg_sub)\n+        \n+        self.writeLog(\"Running Swarp:\")\n+        self.writeLog(\"CT_SWarp \" + \" \".join(swarp_args))\n+        self.execute(\"CT_SWarp\", swarp_args, wait = True)\n+\n+        #self.writeLog(\"%s/bin/swarp %s\" % (\n+        #    os.environ['EUCLID_PREREQ'], \" \".join(swarp_args)))\n+        #self.execute(\"%s/bin/swarp\" % os.environ['EUCLID_PREREQ'],\n+        #             swarp_args, wait = True)\n+        os.chdir(pwd)\n+        image_out = (workdir + os.sep +swarp_args[swarp_args.index(\"-IMAGEOUT_NAME\") +1]).split()[0]\n+        weight_out = (workdir + os.sep +swarp_args[swarp_args.index(\"-WEIGHTOUT_NAME\") +1]).split()[0]\n+        return image_out, weight_out\n+    #\n+\n+    def getCoaddMetadata(self, coaddname):\n+        \"\"\"\n+        @MW: deprecated, call commented out\n+        Get metadata from the coadd header\n+        \"\"\"\n+        self.writeLog(\"Getting the tile metadata\")\n+        coadd = self.init_image(coaddname)\n+\n+        coadd.getKey(\"crval1\")\n+        coadd.getKey(\"crval2\")\n+        coadd.getKey(\"cd1_1\")\n+        coadd.getKey(\"cd1_2\")\n+        coadd.getKey(\"cd2_1\")\n+        coadd.getKey(\"cd2_2\")\n+\n+        if coadd.compressed:\n+            coadd.getKey(\"znaxis1\")\n+            coadd.getKey(\"znaxis2\")\n+            setattr(coadd,\"naxis1\",coadd.znaxis1)\n+            setattr(coadd,\"naxis2\",coadd.znaxis2)\n+        else:\n+            coadd.getKey(\"naxis1\")\n+            coadd.getKey(\"naxis2\")\n+        ##calculate the pixel scale\n+        cdelt1   = np.sqrt(coadd.cd1_1 ** 2 + coadd.cd1_2 ** 2)\n+        cdelt2   = np.sqrt(coadd.cd2_1 ** 2 + coadd.cd2_2 ** 2)\n+        pixscale = 0.5 * (cdelt1 + cdelt2) * 3600.0\n+        setattr(coadd, 'pixscale', pixscale)\n+        return coadd\n+    #\n+\n+    def clean_up(self):\n+        \"\"\"\n+        Remove the temporary files, close the logger\n+        \"\"\"\n+        if self._args.clean_up:\n+            self.writeLog(\"Removing the temporary files\")\n+            shutil.rmtree(self.tmpdir)\n+        self.writeLog(\"Total computation time: %s s\" % (time.time() - self.starttime))\n+        self.logger.close()\n+        try:\n+            self.memlog.close()\n+            print(\"Mem logfile closed. Mem thread running \", t.is_alive())\n+        except:\n+            #those 2 might be None\n+            pass\n+    #\n+#\n+\n+\n+\n+\n+\n+if __name__ == \"__main__\":\n+    #prepare the command line\n+    argparser = argparse.ArgumentParser()\n+    argparser.add_argument(\"--position_cat\",  type = str, default = 'VIS.cat',\n+                           help = 'path to the catalogue containing the \\\n+                           positions on the sky on  which we want to compute \\\n+                           the stacked PSFs')\n+    argparser.add_argument(\"--sort_catalog_by_mag\", dest = 'sort_catalog_by_mag', action =\n+                           'store_const',     const = True,  default = False,\n+                           help = 'sorts the refence catalog by magnitude and thus puts \\\n+                           the brightest objects preferrably on the first layers.')\n+    argparser.add_argument(\"--ra\",            type = str, default = 'RA',\n+                           help = 'name of the catalogue field containing ra \\\n+                           positions')\n+    argparser.add_argument(\"--dec\",           type = str, default = 'DEC',\n+                           help = 'name of the catalogue field containing \\\n+                           dec positions')\n+    argparser.add_argument(\"--mag_entry\",  type = str, default = 'MAG_AUTO',\n+                           help = 'magnitude entry in the position catalog.')\n+    argparser.add_argument(\"--tilename\",      type = str, default = '',\n+                           help = 'tile name')\n+    argparser.add_argument(\"--band\",          type = str, default = '',\n+                           help = 'which band')\n+    argparser.add_argument(\"--SElist\",        type = str, default =\n+                           'SElist.fits',     help = 'path to the file \\\n+                           containing the list of SE image overlapping the \\\n+                           coadd')\n+    argparser.add_argument(\"--outdir\",        type = str, default = '~/tmp',\n+                           help = 'the output directory')\n+    argparser.add_argument(\"--ramdisk\",       type = str, default =\n+                           '/dev/shm/EuclidPipeline', help = 'the ramdisk \\\n+                           directory')\n+    argparser.add_argument(\"--resampledir\",       type = str, default =\n+                           '/dev/shm/EuclidPipeline', help = 'the swarp \\\n+                           resample directory')\n+    argparser.add_argument(\"--coadd\",         type = str, default = '',\n+                           help = 'path to the coadd tile. Needed only for \\\n+                           the metadata, can be replaced to a query to the \\\n+                           EAS ')\n+    argparser.add_argument(\"--coadd_cat\",     type = str, default = None,\n+                           help = 'path to the coadd catalogue, needed\\\n+                           to flag the edge sources, using the sextractor \\\n+                           edge flag (8). If left empty, the program will \\\n+                           skip the objects whose center lay within \\\n+                           (stamp_size // 2) + 1 pixels from the edge of  \\\n+                           the coadd, where stamp_size is the size in pixels \\\n+                           of the PSF model stamp. It can be replaced to a \\\n+                           query to the EAS.')\n+    argparser.add_argument(\"--coadd_x\",      type = str, default = 'X_IMAGE',\n+                           help = 'name of the coadd catalogue field \\\n+                           containing x positions in the coadd catalogues.')\n+    argparser.add_argument(\"--coadd_y\",     type = str, default = 'Y_IMAGE',\n+                           help = 'name of the coadd catalogue field \\\n+                           containing y positions in the coadd catalogues.')\n+    argparser.add_argument(\"--max_layer_num\", type = int, default = sys.maxsize,\n+                           help = 'Maximum number of non-overlapping layers \\\n+                           in the final PSF product. If set to 0, the code \\\n+                           will use the maximum value from the layer \\\n+                           computation')\n+    argparser.add_argument(\"--coadd_zp\", type = float, default = 30.0,\n+                           help = 'Value of the coadd zeropoint. This is where the \\\n+                           coadded simulated psfs are flux-scaled to')\n+    argparser.add_argument(\"--default_zp\",        type = float, default = 25.,\n+                           help = 'Value of the uncalibrated zero point')\n+    argparser.add_argument(\"--clean_up\",        type = int, default = True,\n+                           help = 'if this option is given, the ramdisk will \\\n+                           not be cleaned after the PSF object computation')\n+    argparser.add_argument(\"--keep_edge_objects\", dest = 'keep_edge_obj', action =\n+                           'store_const',     const = True,  default = True,\n+                           help = 'This option can be used to keep the objects \\\n+                           that are close to the edges of the single epoch \\\n+                           images in SElist (the default is to skip them)')\n+    argparser.add_argument(\"--normalize\", dest = 'normalize_psfs', action =\n+                           'store_const',     const = True,  default = False,\n+                           help = 'If used, the psfs are not normalized to \\\n+                           a flux value, but to max(psf) = 1 \\\n+                           to allow easy quality checks in debug mode.')\n+    argparser.add_argument(\"--max_separation_for_match\",  type = float, default = 1.,\n+                           help = 'Maximum distance in arcsecs for a match. \\\n+                           Default: 1.')\n+    argparser.add_argument(\"--multiprocesses\", type = int, default = 1,\n+                           help = 'Number of parallel processes. Default = 1.')\n+    argparser.add_argument(\"--bandpass_value\", type = float, default = 0.0,\n+                           help = 'The effective wavelength of the coadd. Default = 0.0')\n+    argparser.add_argument(\"--swarp_resampling\", type = str, default = \"LANCZOS3\",\n+                           help = 'One of swarps possible resampling types. Default = LANCZOS3')\n+    argparser.add_argument(\"--debug\", type = bool, default = False,\n+                           help = 'debug mode. Default = False.')\n+\n+\n+    if not len(sys.argv) > 1:\n+        argparser.print_help()\n+        sys.exit()\n+    args = argparser.parse_args()\n+\n+    psf  = StackPSF(args)\n+    a = time.time()\n+    psf.run()\n+    print(\"time for execution of stackpsf \", time.time() - a)\n+    psf.clean_up()\n+\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -768,7 +768,7 @@ class StackPSF(Pipeline):\n                 pass\n             counter+= 1\n         #\n-        print(\"added \", counter, \" psfstamps to image \")\n+        #print(\"added \", counter, \" psfstamps to image \")\n         #self.writeLog(\"size of stamplist after insert \" +str(TotalSizeOf.total_size(stamplist)))\n         \n         self.write_simulated_image_to_fits(layer, seimage, data, weight)\n@@ -911,7 +911,7 @@ class StackPSF(Pipeline):\n         \"\"\"\n         X, Y  = self.project_positions_onto_coadd_pixel_space(self.ra, self.dec)\n         \n-        print(\"produce_output_FITS_file: PsfStampFilter \")\n+        self.logger.write(\"produce_output_FITS_file: PsfStampFilter \")\n         psf = PsfStampFilter(self.get_stamp_size(), filtertype = self.getInstrumentFilter(), execfilter = self._args.execfilter)\n         self.layer_objects = psf.filter(self.layer_objects, self.layer_coadds, X, Y)\n         #self.layer_objects = filtered_layer_objects\n@@ -919,13 +919,13 @@ class StackPSF(Pipeline):\n         \n         # Initialize the FITS image and the FITS table in the output\n         # file extensions\n-        print(\"produce_output_FITS_file: OutputStackedPSF \")\n+        self.logger.write(\"produce_output_FITS_file: OutputStackedPSF \")\n         \n-        output_obj = OutputStackedPSF(total_object_number, self.get_stamp_size(), self.tmpdir, self.tilename, self.band)\n-        print(\"produce_output_FITS_file: populate \")\n+        output_obj = OutputStackedPSF(total_object_number, self.get_stamp_size(), self.tmpdir, self.tilename, self.band, self.logger)\n+        self.logger.write(\"produce_output_FITS_file: populate \")\n         \n         output_obj.populate(self.layer_objects, self.ra, self.dec, self.mag, self._args.coadd, self._args.bandpass_value)\n-        print(\"write this file \", self._args.outdir +os.sep +self.tilename +\"_\" +self.band +\"_\" +\"_stackedPSF.fits\" )\n+        self.logger.write(\"write this file \" + self._args.outdir +os.sep +self.tilename +\"_\" +self.band +\"_\" +\"_stackedPSF.fits\" )\n         output_obj.write_to_fits(\"%s/%s_%s_stackedPSF.fits\" % (self._args.outdir, self.tilename, self.band))\n \n         #write a second normalized image for quality control\n",
                            "log ramdisk and swarptemp size, improve stackpsf code",
                            "Michael",
                            "2023-06-16T10:48:31.000+02:00",
                            "d365302409e4a3268d1217c8fbf6ed9777bfa84b"
                        ],
                        [
                            "@@ -59,6 +59,7 @@ from EXT_PF1_GEN_P2.psf.WcsUtils import WcsUtils\n from EXT_PF1_GEN_P2.psf.ObjectList import ObjectList\n from EXT_PF1_GEN_P2.psf.Filter import FilterTypes, PsfStampFilter\n from EXT_PF1_GEN_P2.psf.OutputStackedPSF import OutputStackedPSF\n+from EXT_PF1_GEN_P2.psf import TotalSizeOf\n \n __author__='Thomas Vassallo'\n __email__ ='thomas.vassallo@physik.lmu.de'\n@@ -124,7 +125,69 @@ class StackPSF(Pipeline):\n         self.standard_magnitude_for_all_objects = 18.0\n         self.gauss_stamp = None #for testing and debugging\n         self.coadd_wcs = WcsUtils(FitsServices.read_image_header(args.coadd))\n+        print(\"PIXELSCALE from WcsUtils \", self.coadd_wcs.get_header()[\"PIXSCALE\"])\n+        \n+        \n+        mps = self._args.multiprocesses\n+        if mps > 1:\n+            #save one cpu for preparing the next step\n+            #while swarp does its job with mps - 1\n+            self._args.multiprocesses = mps - 1\n+        #\n+        \n+        #start a memory measuring daemon thread\n+        self.memthread, self.memlog = self.measure_memory(self.outdir)\n+    #\n+    \n+    \n+    def measure_memory(self, logdir, interval = 60):\n+        \"\"\"\n+        measure the use of the memory at the given intervals.\n+        Log the results in the log_folder.\n+        csv of time stamp [seconds], size [bytes]\n+        \"\"\"\n+        #import only on demand\n+        import psutil\n+        import threading\n+        \n+        #put the logs in the logdir, then it will automatically be saved later\n+        if not os.path.exists(logdir):\n+            os.makedirs(logdir)\n+        lname  = logdir +os.sep +\"stackpsf_mem.log\"\n+        logfile = open(lname, 'a')\n+        logfile.write(\"time, mem.total, mem.available, mem.percent\\n\")\n+        \n+        print(\"Mem logfile exists \", lname, os.path.exists(lname) )\n+        \n+        def log(_logfile):\n+            running = True\n+            while running:\n+                try:\n+                    mem = psutil.virtual_memory()\n+                    #svmem(total=10367352832, available=6472179712, percent=37.6, used=8186245120, free=2181107712\n+                    line = str(time.time()) +\",\" +str(mem.total) +\",\" +str(mem.available) +\",\" +str(mem.percent) +\"\\n\"\n+                    _logfile.write(line)\n+                    _logfile.flush()\n+                    \n+                    time.sleep(interval)\n+                except Exception as e:\n+                    print(e)\n+                    #when we close the logfile\n+                    running = False\n+                #\n+                #print(\"write to stackpsf_mem.log thread alive? \", running)\n+            #\n+        #\n+        t = threading.Thread(target=log, name='memory_use_thread', args=(logfile,))\n+        #set as daemon, so it does not prevent python to quit\n+        t.daemon = True\n+        t.start()\n+        \n+        print(\"created memory log file \", logdir +os.sep +\"stackpsf_mem.log\")\n+        print(\"mem thread running \", t.is_alive())\n \n+        return t, logfile\n+    #\n \n \n     def get_stamp_size(self):\n@@ -147,8 +210,8 @@ class StackPSF(Pipeline):\n         \"\"\"\n         RUN!\n         \"\"\"\n-        self.writeLog(\"init_run\")\n-        self.init_run()\n+        #self.writeLog(\"init_run\")\n+        #self.init_run()\n \n         self.writeLog(\"read_input_list\")\n         inputdata = self.read_input_list()\n@@ -171,19 +234,20 @@ class StackPSF(Pipeline):\n         layers = list(self.layer_objects.keys())\n         mp_queue = mp.Queue()\n         \n-        pm2 = ProcessManager.ProcessManager(1)\n+        self.writeLog(\"prepare_layer_processing for layer \" +str(layers[0]))\n+        pm2 = ProcessManager.ProcessManager(1, \"Prepare Initial Layer Process\")\n         pm2.add_process_and_start(self.prepare_layer_processing, args = (layers[0], se_matches, mp_queue))\n         \n+        \n         #Loop over the list of layers\n         self.writeLog(\"Loop over the list of layers\")\n-\n         \n         for _layer in range(len(layers)):\n             layer = layers[_layer]\n             \n-            #wait until the layer has been prepared\n-            #pm2.block_until_all_processes_finished()\n             outputs = mp_queue.get(block = True)\n+            #check, this should return instantly as the queue blocks\n+            pm2.block_until_all_processes_finished()\n             \n             print(\"got output from queue \", type(outputs))\n             print(\"queue empty? \", mp_queue.empty())\n@@ -192,43 +256,73 @@ class StackPSF(Pipeline):\n             layer_positions = outputs[\"layer_positions\"]\n             all_stamps_for_layer = outputs[\"all_stamps_for_layer\"]\n             se_matches = outputs[\"se_matches\"]\n-            \n+            self.writeLog(\"size of outputs \" +str(TotalSizeOf.total_size(outputs)))\n+            self.writeLog(\"size of layer_positions \" +str(TotalSizeOf.total_size(layer_positions)))\n+            self.writeLog(\"size of all_stamps_for_layer \" +str(TotalSizeOf.total_size(all_stamps_for_layer)))\n+            self.writeLog(\"size of se_matches \" +str(TotalSizeOf.total_size(se_matches)))\n             \n             # Loop over the list of SE images\n             start = time.time()\n-            pm = ProcessManager.ProcessManager(self._args.multiprocesses)\n+              \n+            pm = ProcessManager.ProcessManager(self._args.multiprocesses, \"Image Simulator Process\")\n             self.writeLog(\"Looping over the list of SE images\")\n             for seimage in self.inputdata:\n                 if seimage not in all_stamps_for_layer:\n                     continue\n                 ##### This part can be parallelized! (TBD) ###################\n-                self.writeLog(\"simulate_single_image\")\n+                #self.writeLog(\"simulate_single_image\")\n                 pm.add_process_and_start(self.simulate_single_image, args = (seimage, all_stamps_for_layer[seimage], layer))\n                 ##### End of the parallel part ##############################\n             \n             #while the images are simulated and swarp runs, prepare the next layer:\n+            \n             if len(layers) > _layer +1:\n-                pm2 = ProcessManager.ProcessManager(1)\n+                pm2 = ProcessManager.ProcessManager(1, \"Prepare Next Layer Process\")\n                 pm2.add_process_and_start(self.prepare_layer_processing, args = (layers[_layer +1], se_matches, mp_queue))\n             #\n             \n+            #wait with the swarp call until pm is ready\n             pm.block_until_all_processes_finished()\n             print(\"time for simulate_single_image for all images \",  time.time() - start)\n+            \n+            self.writeLog(\"sizes of objects before cleanup\")\n+            _keys = list(all_stamps_for_layer.keys())\n+            self.writeLog(\"lenght of all_stamps_for_layer list \" +str(len(_keys)) +\", \" +str(len(all_stamps_for_layer[_keys[0]])))\n+            self.writeLog(\"size of outputs \" +str(TotalSizeOf.total_size(outputs)))\n+            self.writeLog(\"size of layer_positions \" +str(TotalSizeOf.total_size(layer_positions)))\n+            self.writeLog(\"size of all_stamps_for_layer \" +str(TotalSizeOf.total_size(all_stamps_for_layer)))\n+            self.writeLog(\"size of se_matches \" +str(TotalSizeOf.total_size(se_matches)))\n+            self.writeLog(\"clean objects for next layer\")\n+            del outputs\n+            del layer_positions\n+            del all_stamps_for_layer\n+            #del se_matches\n+                \n             # Produce the stacked PSF image for the current layer\n             start = time.time()\n             self.writeLog(\"create_stacked_PSF_image_for_layer\")\n             self.create_stacked_PSF_image_for_layer(layer)\n             print(\"time for create_stacked_PSF_image \",  time.time() - start)\n-\n+            \n+            \n             # remove the temporary PSF images for the layer\n             start = time.time()\n             self.writeLog(\"clean_up_layer_directory\")\n             self.clean_up_layer_directory(layer)\n             print(\"time for clean_up_layer_directory \",  time.time() - start)\n+            \n+            #wait with the next layer until pm2 is ready\n+            #no don't as the queue needs to be empty to close the process\n+            #pm2._block_until_all_processes_finished()\n+            \n+            self.writeLog(\"compute_total_object_number_for_layer\")\n         #\n+        \n+        total_object_number = self.compute_total_object_number(se_matches)\n+\n         # Produce the output FITS file\n         self.writeLog(\"produce_output_FITS_file\")\n-        self.produce_output_FITS_file(se_matches)\n+        self.produce_output_FITS_file(total_object_number)\n     #\n     \n     \n@@ -265,11 +359,20 @@ class StackPSF(Pipeline):\n         #\n         \n         start = time.time()\n+        self.writeLog(\"all_psf_stamps_for_layer\")\n         all_stamps_for_layer = self.all_psf_stamps_for_layer(layer_positions, se_matches)\n-        print(\"time for all_psf_stamps_for_layer \", layer, \" in sequence \",  time.time() - start)\n+        self.writeLog(\"time for all_psf_stamps_for_layer \" + str(layer) + \" in sequence \" +str(time.time() - start))\n         \n+        self.writeLog(\"put results into the queue\")\n         mp_queue.put({\"layer_positions\" :layer_positions, \"all_stamps_for_layer\": all_stamps_for_layer, \"se_matches\": se_matches})\n-        self.compute_non_empty_se_matches(se_matches)\n+        self.writeLog(\"done put results into the queue \")\n+        \n+        self.writeLog(\"compute_non_empty_se_matches\")\n+        #this is the memory peak!\n+        #self.compute_non_empty_se_matches(se_matches)\n+        self.writeLog(\"finished compute_non_empty_se_matches \")\n+        \n+        return\n     #\n     \n     \n@@ -476,7 +579,8 @@ class StackPSF(Pipeline):\n         for se_image_name in self.inputdata:\n             try:\n                 #create an ExtractImage object\n-                single_epoch_image    = self.init_image(se_image_name)\n+                #MW this must go. Far too heavyweight for Panstarrs data\n+                #single_epoch_image    = self.init_image(se_image_name)\n                 \n                 se_image_header = FitsServices.read_image_header(se_image_name)\n                 overlapping_positions = self.project_input_coordinates_into_the_single_image_pixel_space(\n@@ -599,7 +703,14 @@ class StackPSF(Pipeline):\n         zp = self.inputdata[seimage][\"zp\"]\n         \n         counter = 0\n-        for stmp in stamplist:\n+        #for stmp in stamplist:\n+        \n+        #MW: loop from reverse, so the objects can be deleted \n+        #right after use without affecting the loop\n+        #self.writeLog(\"size of stamplist before insert \" +str(TotalSizeOf.total_size(stamplist)))\n+        for i in range(len(stamplist) -1, -1, -1):\n+            stmp = stamplist[i]\n+            \n             data, weight, shifted_stamp = self.add_PSF_stamp_to_simulated_image(data, weight, stmp[0], stmp[1], stmp[2], stmp[3], zp)\n \n             if self._args.debug:\n@@ -647,11 +758,24 @@ class StackPSF(Pipeline):\n                 #to deal with multiple psfs which it cannot\n                 data = np.zeros(shape=data.shape, dtype=float, order='F')\n             #\n-            \n+            #free mem as those are huge objects all together\n+            try:\n+                del shifted_stamp\n+                del stmp\n+                del stamplist[i] \n+            except:\n+                #don't fail in this case\n+                pass\n             counter+= 1\n         #\n         print(\"added \", counter, \" psfstamps to image \")\n+        #self.writeLog(\"size of stamplist after insert \" +str(TotalSizeOf.total_size(stamplist)))\n+        \n         self.write_simulated_image_to_fits(layer, seimage, data, weight)\n+        \n+        #MW; is this necessary?\n+        del data\n+        del weight\n     #\n     \n     \n@@ -713,7 +837,7 @@ class StackPSF(Pipeline):\n             if modify:\n                 stamp = stamp[int(stampcut[1]):int(stampcut[3]), int(stampcut[0]):int(stampcut[2]) ]\n             #\n-            #print(\"data \", int(start[1]),int(end[1]), int(start[0]),int(end[0]), \", stamp \", stamp.shape)\n+            #print(\"data positions \", int(start[1]),int(end[1]), int(start[0]),int(end[0]), \", stamp \", stamp.shape)\n \n             if np.all(np.array(stamp.shape) > 0):\n                 #if not, the stamp is outside of the array\n@@ -780,7 +904,7 @@ class StackPSF(Pipeline):\n                 if fname.endswith(\"_psfstamps.fits\"):\n                     os.remove(os.path.join(layerdir, fname))\n \n-    def produce_output_FITS_file(self, se_matches):\n+    def produce_output_FITS_file(self, total_object_number):\n         \"\"\"\n         Creates the output fits file containing all layers of non-overlapping\n         stacked PSFs, plus a table per extension containing the PSFs positions.\n@@ -792,23 +916,20 @@ class StackPSF(Pipeline):\n         self.layer_objects = psf.filter(self.layer_objects, self.layer_coadds, X, Y)\n         #self.layer_objects = filtered_layer_objects\n         \n-        print(\"produce_output_FITS_file: compute_total_object_number \")\n-        totobj = self.compute_total_object_number(se_matches)\n-        \n         \n         # Initialize the FITS image and the FITS table in the output\n         # file extensions\n         print(\"produce_output_FITS_file: OutputStackedPSF \")\n         \n-        output_obj = OutputStackedPSF(totobj, self.get_stamp_size(), self.tmpdir, self.tilename, self.band)\n+        output_obj = OutputStackedPSF(total_object_number, self.get_stamp_size(), self.tmpdir, self.tilename, self.band)\n         print(\"produce_output_FITS_file: populate \")\n         \n-        output_obj.populate(self.layer_objects, self.ra, self.dec, self.mag, self.coadd, self._args.bandpass_value)\n+        output_obj.populate(self.layer_objects, self.ra, self.dec, self.mag, self._args.coadd, self._args.bandpass_value)\n         print(\"write this file \", self._args.outdir +os.sep +self.tilename +\"_\" +self.band +\"_\" +\"_stackedPSF.fits\" )\n         output_obj.write_to_fits(\"%s/%s_%s_stackedPSF.fits\" % (self._args.outdir, self.tilename, self.band))\n \n         #write a second normalized image for quality control\n-        #output_obj = OutputStackedPSF(totobj, self.get_stamp_size(), self.tmpdir, self.tilename, self.band)\n+        #output_obj = OutputStackedPSF(total_object_number, self.get_stamp_size(), self.tmpdir, self.tilename, self.band)\n         #output_obj.populate(self.layer_objects, self.ra, self.dec, self.mag, self.coadd, self._args.bandpass_value, normalize = True)\n         #output_obj.write_to_fits(\"%s/Normalized_%s_%s_stackedPSF.fits\" % (self._args.outdir, self.tilename, self.band))\n \n@@ -906,6 +1027,7 @@ class StackPSF(Pipeline):\n \n     def compute_total_object_number(self, se_matches):\n         \"\"\"\n+        MW: deprecated, replace by compute_total_object_number_for_layer\n         Computes the total number of objects contained in the layers\n         included in the processing\n         \"\"\"\n@@ -932,6 +1054,33 @@ class StackPSF(Pipeline):\n         return totobj\n     \n     \n+    def compute_total_object_number_for_layer(self, se_matches, layerid):\n+        \"\"\"\n+        Computes the total number of objects contained in the layers\n+        included in the processing\n+        \"\"\"\n+        # Get the total number of objects\n+        totobj   = 0\n+        temp_layer_objects = OrderedDict()\n+        \n+        temp = []\n+        print(\"handle layer \", layerid)\n+        for x in self.layer_objects[layerid]:\n+            sematch = se_matches[\"%.10f:%.10f\" % (self.ra[x], self.dec[x])]\n+                \n+            #filter out funny mag values mag = sematch[key][0] for all keys\n+            if len(sematch) > 0 and any(\n+                x <  90 for x in [sematch[key][0] for key in sematch]) and any(\n+                x > -90 for x in [sematch[key][0] for key in sematch]):\n+                temp.append(x)\n+        temp_layer_objects[layerid] = temp\n+        totobj += len(temp_layer_objects[layerid])\n+        print(\"added \", len(temp_layer_objects[layerid]), \" objects for layer \", layerid, \".\")\n+        self.layer_objects[layerid] = temp_layer_objects[layerid]\n+        \n+        return totobj\n+    \n+    \n     def read_input_list(self):\n         \"\"\"\n         reads the input image list, stores the list with all\n@@ -1048,7 +1197,7 @@ class StackPSF(Pipeline):\n                       (X < self.coadd.naxis1 -\n                           (self.get_stamp_size() // 2)))[0]\n         \"\"\"\n-        wo = np.where(self.stamp_inside_image(X,Y,self.coadd.naxis1,self.coadd.naxis2 ))[0]\n+        wo = np.where(self.stamp_inside_image(X,Y,self.coadd_wcs.get_header()[\"NAXIS1\"],self.coadd_wcs.get_header()[\"NAXIS2\"] ))[0]\n         return wo\n \n     def read_magnitudes_from_coadd_catalogue(self):\n@@ -1100,8 +1249,8 @@ class StackPSF(Pipeline):\n         #hdul = fits.open(self.coadd.name)  # open a FITS file\n         #h = hdul[image_hdu].header\n         #WCSU = WcsUtils(h)\n-        WCSU      = WcsUtils(self.coadd.returnHeader())\n-        cora, codec = WCSU.py_pix2world(pixcoo[0::2], pixcoo[1::2])\n+        \n+        cora, codec = self.coadd_wcs.py_pix2world(pixcoo[0::2], pixcoo[1::2])\n         #skycoo    = _getWcsLibCorners.py_pix2world(\n         #    self.coadd.wcs_params + pixcoo)\n         #codec     = np.array(skycoo[1::2])\n@@ -1118,9 +1267,9 @@ class StackPSF(Pipeline):\n         and filter out the edge sources\n         \"\"\"\n         #get a extract image object of the coadd\n-        self.coadd = self.getCoaddMetadata(self._args.coadd)\n+        #self.coadd = self.getCoaddMetadata(self._args.coadd)\n         skycoords  = OutputStackedPSF.prepare_sky_coordinates(ra, dec)\n-        X, Y       = OutputStackedPSF.getPixels(self.coadd, skycoords)\n+        X, Y       = OutputStackedPSF.getPixels(self._args.coadd, skycoords)\n         \n         return X, Y\n     \n@@ -1849,7 +1998,7 @@ class StackPSF(Pipeline):\n     #\n \n \n-    def swarp_call(self, coadd, band, swarp_lists, bg_sub):\n+    def swarp_call(self, coadd_header, band, swarp_lists, bg_sub):\n         \"\"\"\n         Define and return the swarp call\n         \"\"\"\n@@ -1858,10 +2007,10 @@ class StackPSF(Pipeline):\n             \"-c\",                \"%s/etc/default.swarp \" % os.environ[\n                 'EUCLID_CONF'],\n             \"-PIXELSCALE_TYPE\",  \"MANUAL \",\n-            \"-PIXEL_SCALE\",      \"%s\"     % coadd.pixscale,\n+            \"-PIXEL_SCALE\",      \"%s\"     % coadd_header[\"PIXSCALE\"],\n             \"-CENTER_TYPE\",      \"MANUAL \",\n-            \"-CENTER\",           \"%s,%s \" % (coadd.crval1, coadd.crval2),\n-            \"-IMAGE_SIZE\",       \"%s,%s \" % (coadd.naxis1, coadd.naxis2),\n+            \"-CENTER\",           \"%s,%s \" % (coadd_header[\"CRVAL1\"], coadd_header[\"CRVAL2\"]),\n+            \"-IMAGE_SIZE\",       \"%s,%s \" % (coadd_header[\"NAXIS1\"], coadd_header[\"NAXIS2\"]),\n             \"-SUBTRACT_BACK\",    \"%s\" % bg_sub,\n             \"-BLANK_BADPIXELS\",  \"N \",\n             \"-COPY_KEYWORDS\",    \"PSF_FWHM,PSF_BETA,FWHMHOMO \",\n@@ -2001,8 +2150,8 @@ class StackPSF(Pipeline):\n         pwd = os.getcwd()\n         os.chdir(workdir)\n \n-        self.coadd  = self.getCoaddMetadata(self._args.coadd)\n-        swarp_args  = self.swarp_call(self.coadd, self.band, swarp_lists, bg_sub)\n+        swarp_args  = self.swarp_call(self.coadd_wcs.get_header(), self.band, swarp_lists, bg_sub)\n+        \n         self.writeLog(\"Running Swarp:\")\n         self.writeLog(\"CT_SWarp \" + \" \".join(swarp_args))\n         self.execute(\"CT_SWarp\", swarp_args, wait = True)\n@@ -2019,6 +2168,7 @@ class StackPSF(Pipeline):\n \n     def getCoaddMetadata(self, coaddname):\n         \"\"\"\n+        @MW: deprecated, call commented out\n         Get metadata from the coadd header\n         \"\"\"\n         self.writeLog(\"Getting the tile metadata\")\n@@ -2045,6 +2195,7 @@ class StackPSF(Pipeline):\n         pixscale = 0.5 * (cdelt1 + cdelt2) * 3600.0\n         setattr(coadd, 'pixscale', pixscale)\n         return coadd\n+    #\n \n     def clean_up(self):\n         \"\"\"\n@@ -2055,6 +2206,12 @@ class StackPSF(Pipeline):\n             shutil.rmtree(self.tmpdir)\n         self.writeLog(\"Total computation time: %s s\" % (time.time() - self.starttime))\n         self.logger.close()\n+        try:\n+            self.memlog.close()\n+            print(\"Mem logfile closed. Mem thread running \", t.is_alive())\n+        except:\n+            #those 2 might be None\n+            pass\n     #\n #\n \n",
                            "first attempt with memory improvement for stackpsf_v3",
                            "Michael",
                            "2023-06-09T09:51:34.000+02:00",
                            "7b8ba31d2dafe5229f54217bca17ba2a7b32bd71"
                        ],
                        [
                            "@@ -93,9 +93,12 @@ class StackPSF(Pipeline):\n         #\n         self.tmpdir     = os.path.abspath(\"%s/%s/%s/\"    % (\n             self._args.ramdisk, self.tilename, self.band))\n+        self.resampledir = os.path.abspath(\"%s/%s/%s/\"    % (\n+            self._args.resampledir, self.tilename, self.band))\n+            \n         self.auxdir     = os.path.abspath(\"%s/aux/\"      % self.outdir)\n-        self.xmldir     = os.path.abspath(\"%s/xml/\"      % self.tmpdir)\n-        self.resdir     = os.path.abspath(\"%s/resample/\" % self.tmpdir)\n+        self.xmldir     = os.path.abspath(\"%s/xml/\"      % self.resampledir)\n+        self.resdir     = os.path.abspath(\"%s/resample/\" % self.resampledir)\n         self.out_name   = \"%s_psfstamps.fits\"\n \n         #MW: remove existing dirs and recreate them\n@@ -2091,6 +2094,9 @@ if __name__ == \"__main__\":\n     argparser.add_argument(\"--ramdisk\",       type = str, default =\n                            '/dev/shm/EuclidPipeline', help = 'the ramdisk \\\n                            directory')\n+    argparser.add_argument(\"--resampledir\",       type = str, default =\n+                           '/dev/shm/EuclidPipeline', help = 'the swarp \\\n+                           resample directory')\n     argparser.add_argument(\"--coadd\",         type = str, default = '',\n                            help = 'path to the coadd tile. Needed only for \\\n                            the metadata, can be replaced to a query to the \\\n",
                            "add a separate folder for swarp temp (took it out of the general temp)",
                            "Michael",
                            "2023-05-16T20:42:42.000+02:00",
                            "c148ca2fc218a328e8f62903fa9cec79bd5a5489"
                        ],
                        [
                            "@@ -39,7 +39,7 @@ from EXT_PF1_GEN_P2_LIBS.file import ExtractImage\n from EXT_PF1_GEN_P2.Pipeline import SigmaClip\n from EXT_PF1_GEN_P2_LIBS.Pipeline.Pipeline import Pipeline\n #from EXT_PF1_GEN_P2.file import PSFUtils\n-from   EXT_STACK_PSF.psfmodel import PsfModel\n+from   EXT_PF1_GEN_P2.psfmodel import PsfModel\n import matplotlib.pyplot as plt\n \n \n",
                            "resolve a dependency to EXT_STACK_PSF",
                            "Michael",
                            "2023-05-11T17:01:33.000+02:00",
                            "7c1de7cb54eed2946858240bc4d1f5b3763d5630"
                        ],
                        [
                            "@@ -36,9 +36,10 @@ import  scipy.ndimage.measurements\n import file\n from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n from EXT_PF1_GEN_P2_LIBS.file import ExtractImage\n-from EXT_PF1_GEN_P2.file import PSFUtils\n from EXT_PF1_GEN_P2.Pipeline import SigmaClip\n from EXT_PF1_GEN_P2_LIBS.Pipeline.Pipeline import Pipeline\n+#from EXT_PF1_GEN_P2.file import PSFUtils\n+from   EXT_STACK_PSF.psfmodel import PsfModel\n import matplotlib.pyplot as plt\n \n \n@@ -557,9 +558,15 @@ class StackPSF(Pipeline):\n             \n             psf_file_name, catname = self.get_auxiliary_file_names(seimage)\n             \n+            \"\"\"\n             psfobj                 = PSFUtils.PSFUtils(psf_file_name)\n             \n             stamps[seimage] = list(zip( psfobj.evaluateAll( list(X +1), list(Y +1)), list(X), list(Y), list(se_overlapping_objects.mag)))\n+            \"\"\"\n+            psfobj = PsfModel.PsfModel(psf_file_name)\n+            \n+            stamps[seimage] = list(zip( psfobj.getAllPsfStamps( list(X +1), list(Y +1)), list(X), list(Y), list(se_overlapping_objects.mag)))\n+            \n         #\n         return stamps\n     #\n@@ -1526,12 +1533,16 @@ class StackPSF(Pipeline):\n             psf_file_name = self.inputdata[seimage][\"psf\"]\n             catname = self.inputdata[seimage][\"cat\"]\n \n+            \"\"\"\n             psfobj        = PSFUtils.PSFUtils(psf_file_name)\n             retval        = psfobj.file_exists(psf_file_name)\n \n             self.assertFalse(retval)\n-\n             psfnaxis        = psfobj.get_convolution_params()\n+            \"\"\"\n+            psfobj         = PsfModel.PsfModel(psf_file_name)\n+            psfnaxis       = psfobj.getStampsize()\n+\n \n             max_sep       = max(psfnaxis)\n             if max_separation_in_pixels < max_sep:\n@@ -1670,19 +1681,6 @@ class StackPSF(Pipeline):\n             return  10 ** ((zp - self.standard_magnitude_for_all_objects) / 2.5)\n     #\n     \n-    def evaluate_PSF_for_pixel(self, pixel, psfobj, norm):\n-        \"\"\"\n-        Evaluate the PSF from the PSF model for the given pixel\n-        pixel: the xy coordinate of the psf\n-        psfobj: the psf model objects\n-        norm: the flux of the objects (converted from mag and default_zp)\n-        \"\"\"\n-        # psfobj.evaluate wants the pixel expressed in the\n-        # IRAF reference frame\n-        psfdata = psfobj.evaluate(pixel[0] + 1, pixel[1] + 1)\n-        return  psfdata * norm\n-    #\n-    \n     \n     def center_objects_in_coadd_pixel(self, se_objects_ra, se_objects_dec, coadd_wcs, se_wcs):\n         \"\"\"\n@@ -1702,99 +1700,7 @@ class StackPSF(Pipeline):\n         return se_x, se_y\n     #\n     \n-    def add_PSF_stamp_to_image(self, data, gains, objects, object_index, stamp, psfnaxis):\n-        \"\"\"\n-        MW: retired\n-        Adds the PSF stamp to the image\n-        \"\"\"\n-        try:\n-            mag = objects.mag[object_index]\n-            # pixel is in the SE coordinate reference frame\n-            pixel     = (objects.X[object_index], objects.Y[object_index])\n-            #convert the magnitude to flux (=norm)\n-\n-            #MW here we need the real zp from the se_image of the objects origin\n-            zp = objects.zp[object_index]\n-            #self.writeLog(\"add_PSF_stamp_to_image: objects zp \" +str(zp) )\n-            norm = self.get_normalization_for_stamp(mag, zp)\n-\n-\n-            #which mags to we deal with?\n-            if abs(norm) > 1.e38 or mag > 90.:\n-                return data, gains, stamp\n-            #\n-            if self._args.normalize_psfs:\n-                #normalize!\n-                #this is a debug option\n-                stamp = stamp/max(stamp)\n-            else:\n-                #this is the usual way\n-                stamp = stamp*norm\n-            #\n-            #stamp     = self.evaluate_PSF_for_pixel(pixel, psfobj, norm)\n-            stamp.shape = ( int(psfnaxis[0]), int(psfnaxis[1]) )\n-            stamp_dim = np.array(stamp.shape)\n-\n-\n-            data_dim  = data.shape\n-            start, end  = self.get_indexes_of_stamp_on_image_psfmodel_shift(pixel, stamp_dim)\n-            \n-            #shift only, of psfexmodel does not shift\n-            #stamp = ndimage.shift(stamp, (shift[1], shift[0]), order=5)\n-\n-            #does the stamp completely overlap?\n-            #if not, adapt the indices\n-            stampcut = [0,0,psfnaxis[0],psfnaxis[1]]\n-            modify = False\n-            if start[0] < 0:\n-                #cut the stamp\n-                stampcut[0] = -start[0]\n-                start[0] = 0\n-                modify = True\n-            if start[1] < 0:\n-                stampcut[1] = -start[1]\n-                start[1] = 0\n-                modify = True\n-            if end[0] > data_dim[1]:\n-                #cut the stamp\n-                stampcut[2] = data_dim[1] - end[0]\n-                end[0] = data_dim[1]\n-                modify = True\n-            if end[1] > data_dim[0]:\n-                #cut the stamp\n-                stampcut[3] = data_dim[0] - end[1]\n-                end[1] = data_dim[0]\n-                modify = True\n-            if modify:\n-                stamp = stamp[int(stampcut[1]):int(stampcut[3]), int(stampcut[0]):int(stampcut[2]) ]\n-            #\n-            #print(\"data \", int(start[1]),int(end[1]), int(start[0]),int(end[0]), \", stamp \", stamp.shape)\n-\n-            if np.all(np.array(stamp.shape) > 0):\n-                #if not, the stamp is outside of the array\n-                #this may happen in some cases as the stampsize is the max of the psfnaxes\n-                if not modify:\n-                    #only add not modified stamps\n-                    data[int(start[1]):int(end[1]), int(start[0]):int(end[0])] += stamp\n-                else:\n-                    #if stamps are modified, set the gain to 0, so they don't contribute\n-                    gains[int(start[1]):int(end[1]), int(start[0]):int(end[0])] = 0\n-                #\n-                if norm == 0. or mag == -99:\n-                    #gains are set to 0, means also weights will be 0\n-                    gains[int(start[1]):int(end[1]), int(start[0]):int(end[0])] = 0\n-                #\n-            #\n-            return data, gains, stamp/norm\n-        #\n-        except Exception as e:\n-            #if there is a problem for one object,\n-            #don't fail. Report and go on.\n-            print(\"An object could not be added to the layer image. \")\n-            exec_info =  sys.exc_info()\n-            traceback.print_tb(exec_info[2])\n-        #\n-\n+    \n     def get_indexes_of_stamp_on_image_psfmodel_shift(self, pixel, stamp_dim):\n         \"\"\"\n         psfmodel_shift: psf is always centered in center pixel.\n",
                            "add panstarrs psfmodel",
                            "Michael",
                            "2023-05-08T18:42:21.000+02:00",
                            "4b5bc6cc5519400bbc5d668ca1d4812480ac6663"
                        ],
                        [
                            "@@ -1250,23 +1250,34 @@ class StackPSF(Pipeline):\n         semag = np.array([])\n         semag_psf = np.array([])\n         if \"FLUX_AUTO\" in cat_data._colnames and \"FLUX_PSF\" in cat_data._colnames:\n-            semag        = cat_data[\"flux_auto\"].read()[seIdx] #kids flux_auto, des mag_auto\n-            semag_psf    = cat_data[\"flux_psf\"].read()[seIdx] #kids: flux_psf, des mag-psf\n+            semag        = cat_data[\"FLUX_AUTO\"].read()[seIdx] #kids flux_auto, des mag_auto\n+            semag_psf    = cat_data[\"FLUX_PSF\"].read()[seIdx] #kids: flux_psf, des mag-psf\n \n             #transform flux to magnitudes\n             zp = float( self.inputdata[se_image_name][\"zp\"] )\n             semag = self.flux_2_mag( semag, zp )\n             semag_psf = self.flux_2_mag( semag_psf, zp )\n         elif \"MAG_AUTO\" in cat_data._colnames and \"MAG_PSF\" in cat_data._colnames:\n-            semag        = cat_data[\"mag_auto\"].read()[seIdx]\n-            semag_psf    = cat_data[\"mag_psf\"].read()[seIdx]\n+            semag        = cat_data[\"MAG_AUTO\"].read()[seIdx]\n+            semag_psf    = cat_data[\"MAG_PSF\"].read()[seIdx]\n+        elif \"MAG_PSF\" in cat_data._colnames:\n+            semag_psf    = cat_data[\"MAG_PSF\"].read()[seIdx]\n+            semag        = semag_psf\n+            self.logger.write(\"\\n%s\" % \"The is no MAG_AUTO in the catalog. Work with MAG_PSF only!\")\n         #\n         star_idxs    = np.where((np.abs(sespread) < 0.002) &\n                                   (semag_psf < 90.))[0]\n \n         #replace the coadd catalog mags with the single epoch catalog mags?????? for stars\n         semag[star_idxs] = semag_psf[star_idxs]\n-        seflags      = cat_data[\"flags\"].read()[seIdx]\n+        \n+        flagnames = [\"flags\", \"FLAGS\"]\n+        for flagname in flagnames:\n+            if flagname in cat_data._colnames:\n+                break\n+            #\n+        #\n+        seflags      = cat_data[flagname].read()[seIdx]\n         indexes      = self.define_single_epoch_indexes(detected_objects_idx, seflags, overlapping_positions._length)\n \n         se_matched_objects = ObjectList(semag, ra = sera, dec = sedec)\n@@ -1282,20 +1293,28 @@ class StackPSF(Pipeline):\n         relative to the input set of positions overlapping the single image\n         \"\"\"\n         indexes = {}\n+        \"\"\"\n         if self._args.keep_edge_obj:\n             #MW: keep_edge_obj works for the coadd and the single images\n             #there is no way to keep the edge objects of the single images and thus\n             #guantee high quality mapping, but keep out the edge objects of the coadd\n             #to avoid half-moon shaped psfs\n+            \n             good_idx      = np.where((seflags & 4  == 0) &\n                                     (seflags & 16 == 0) &\n-                                    (seflags & 32 == 0))[0]\n+                                    (seflags & 32 == 0))[0]         \n         else:\n             good_idx      = np.where((seflags & 4  == 0) &\n                                     (seflags & 8  == 0) &\n                                     (seflags & 16 == 0) &\n                                     (seflags & 32 == 0))[0]\n-\n+        \"\"\"\n+        good_idx = np.logical_and(seflags,4) == 0\n+        good_idx = np.logical_and( good_idx, np.logical_and(seflags,16) == 0)\n+        good_idx = np.logical_and( good_idx, np.logical_and(seflags,32) == 0)     \n+        if not self._args.keep_edge_obj:\n+            good_idx = np.logical_and( good_idx, np.logical_and(seflags,8) == 0)\n+        good_idx = np.where(good_idx)[0]\n         # Get the indexes of the objects that are not detected in the\n         # single image, or for which the information in the single\n         # epoch catalogue is not reliable (bad flags)\n",
                            "made flag extraction more robust",
                            "Michael",
                            "2023-05-02T19:37:26.000+02:00",
                            "7a7404383becdf96500bee81964825626c159add"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psfmodel/PanstarrsPsfModel.py": [
                        [
                            "@@ -0,0 +1,242 @@\n+from astropy.io import fits\n+import sys\n+import matplotlib.pyplot as plt\n+import numpy as np\n+from scipy import interpolate\n+from scipy import signal\n+import scipy\n+\n+class PanstarrsPsfModel():\n+    def __init__(self, filename):\n+        self.filename = filename\n+        f = fits.open(self.filename)\n+        #print( f.info())\n+        \n+        self.stampsize1 = int( f[\"PSF_MODEL\"].header[\"NAXIS1\"] )\n+        self.stampsize2 = int( f[\"PSF_MODEL\"].header[\"NAXIS2\"] )\n+        \n+        self.model_term = f[1].data['MODEL_TERM']\n+        self.x_power    = f[1].data['X_POWER']\n+        self.y_power    = f[1].data['Y_POWER']\n+        self.value      = f[1].data['VALUE']\n+        self.error      = f[1].data['ERROR']\n+        self.mask       = f[1].data['MASK']\n+\n+        self.imaxis1    = f[1].header['IMAXIS1']\n+        self.imaxis2    = f[1].header['IMAXIS2']\n+\n+        self.imref1    = f[1].header['IMREF1']\n+        self.imref2    = f[1].header['IMREF2']\n+\n+        # Load residuals & params\n+        self.model_residuals = f[2].data\n+        self.model_residuals_xcenter = f[2].header['XCENTER']\n+        self.model_residuals_ycenter = f[2].header['YCENTER']\n+\n+        # E0\n+        self.f_interp_e0, self.x_ref_e0, self.y_ref_e0, self.z_ref_e0 = \\\n+                           self._get_interp_function(4, f[1].header['PAR04_MD'],\n+                                                        f[1].header['PAR04_NX'],\n+                                                        f[1].header['PAR04_NY'])\n+\n+        # E1\n+        self.f_interp_e1, self.x_ref_e1, self.y_ref_e1, self.z_ref_e1 = \\\n+                           self._get_interp_function(5, f[1].header['PAR05_MD'],\n+                                                        f[1].header['PAR05_NX'],\n+                                                        f[1].header['PAR05_NY'])\n+\n+        # E2\n+        self.f_interp_e2, self.x_ref_e2, self.y_ref_e2, self.z_ref_e2 = \\\n+                           self._get_interp_function(6, f[1].header['PAR06_MD'],\n+                                                        f[1].header['PAR06_NX'],\n+                                                        f[1].header['PAR06_NY'])\n+\n+        # kappa\n+        self.f_interp_kappa, self.x_ref_kappa, self.y_ref_kappa, self.z_ref_kappa = \\\n+                              self._get_interp_function(7, f[1].header['PAR07_MD'],\n+                                                           f[1].header['PAR07_NX'],\n+                                                           f[1].header['PAR07_NY'])\n+        f.close()\n+\n+    \n+    def _get_interp_function(self, par_term, par_md, par_nx, par_ny):\n+        #print('')\n+        #print('PARxx_MD, PARxx_NX, PARxx_NY: %s %s %s' % (par_md, par_nx, par_ny))\n+        if par_md == 'MAP':\n+            dx = self.imaxis1/par_nx\n+            dy = self.imaxis2/par_ny\n+\n+            #print('dx, dy: %s %s' % (dx, dy))\n+            #print('par_nx, par_ny: %s %s' % (par_nx, par_ny))\n+\n+            w = np.where( (self.model_term == par_term) & (self.mask == 0))[0]\n+            x_ref = np.array(self.x_power[w]*dx+dx/2)\n+            y_ref = np.array(self.y_power[w]*dy+dy/2)\n+\n+            z_ref = np.array(self.value[w])\n+\n+            if par_nx == 1:\n+                nx_zz = par_nx + 1\n+                xx = np.hstack((0.,dx))\n+            else:\n+                nx_zz = par_nx\n+                xx = np.sort(list(dict.fromkeys(x_ref)))\n+\n+            if par_ny == 1:\n+                ny_zz = par_ny + 1\n+                yy = np.hstack((0.,dy))\n+            else:\n+                ny_zz = par_ny\n+                yy = np.sort(list(dict.fromkeys(y_ref)))\n+\n+            zz = np.zeros((nx_zz, ny_zz))\n+\n+            for i in range(par_nx):\n+                for j in range(par_ny):\n+                    w = np.where((self.model_term == par_term) & (self.mask == 0) & (self.x_power == i) & (self.y_power == j))[0]\n+                    zz[i,j] = self.value[w]\n+                    if par_ny == 1:\n+                        zz[i,j+1] = self.value[w]\n+                    if par_nx == 1:\n+                        zz[i+1,j] = self.value[w]\n+                        zz[i+1,j+1] = self.value[w]\n+\n+            #print('xx = ', xx)\n+            #print('yy = ', yy)\n+            #print('zz = ', zz)\n+\n+            f_interp = interpolate.RectBivariateSpline(xx, yy, zz, kx=1, ky=1)\n+        else:\n+            raise(\"Error: PAR_MD is not MAP\")\n+\n+        return f_interp, x_ref, y_ref, z_ref\n+\n+\n+    def _ellipse_pol_to_axes(self, e0, e1, e2, min_minor_axis):\n+        theta = 0.5*np.arctan2(e2, e1)\n+\n+        cs = np.cos(2*theta)\n+        sn = np.sin(2*theta)\n+\n+        if (cs > 0.707) | (cs < -0.707):\n+            ds = e1/cs\n+        else:\n+            ds = e2/sn\n+\n+        limit = min_minor_axis**2\n+        if e0 < limit:\n+            major = min_minor_axis\n+            minor = min_minor_axis\n+        else:\n+            if 2.0*(e0-ds) < limit:\n+                major = np.sqrt(e0 - limit)\n+                minor = np.sqrt(limit)\n+            else:\n+                major = np.sqrt(0.5*(e0 + ds))\n+                minor = np.sqrt(0.5*(e0 - ds))\n+\n+        if ~np.isfinite(major) | ~np.isfinite(minor) | ~np.isfinite(theta):\n+            raise ValueError(\"Infinite major, minor, or theta!\")\n+\n+        return major, minor, theta\n+\n+\n+    def _ellipse_axes_to_shape(self, major, minor, theta):\n+        if (minor <= 0) | (major <= 0):\n+            return np.nan, np.nan, np.nan\n+\n+        f1 = 1./minor**2 + 1./major**2\n+        f2 = 1./minor**2 - 1./major**2\n+\n+        sxr = 0.5*f1 - 0.5*f2*np.cos(2*theta)\n+        syr = 0.5*f1 + 0.5*f2*np.cos(2*theta)\n+\n+        sx  = 1./np.sqrt(sxr)\n+        sy  = 1./np.sqrt(syr)\n+        sxy = -0.5*f2*np.sin(2*theta)\n+\n+        return sx, sy, sxy\n+\n+\n+    # make postage stamp\n+    def make_stamp_analytical(self, x0, y0, size, center_offset_x, center_offset_y):\n+        e0    = self.f_interp_e0(x0, y0)\n+        e1    = self.f_interp_e1(x0, y0)\n+        e2    = self.f_interp_e2(x0, y0)\n+        kappa = self.f_interp_kappa(x0, y0)\n+\n+        major, minor, theta = self._ellipse_pol_to_axes(e0, e1, e2, 0.00001)\n+        sx, sy, sxy = self._ellipse_axes_to_shape(major, minor, theta)\n+        #print('sx, sy, sxy:', sx, sy, sxy)\n+\n+        x = np.linspace(0, size-1, size)\n+        y = np.linspace(0, size-1, size)\n+        (x, y) = np.meshgrid(x, y)\n+\n+        xx = x - size//2 + center_offset_x\n+        yy = y - size//2 + center_offset_y\n+\n+        zz = xx**2/(2*sx**2) + yy**2/(2*sy**2) + sxy*xx*yy\n+\n+        #ff = 1./(1 + kappa*zz + zz**1.67) + 0. # from paper\n+        ff = 1./(1 + kappa*zz + zz**1.665) + 0. # from code (pmModel_PS1_V1.c)\n+\n+        return x, y, ff\n+\n+    def add_residuals(self, stamp, x, y):\n+        ny, nx = stamp.shape\n+        xc = nx//2\n+        yc = ny//2\n+\n+        _, ny_resid, nx_resid = self.model_residuals.shape\n+\n+        xmin = yc-self.model_residuals_xcenter\n+        ymin = yc-self.model_residuals_ycenter\n+\n+        xmax = xmin + nx_resid\n+        ymax = ymin + ny_resid\n+\n+        resid0    = np.zeros((ny,nx))\n+        resid1    = np.zeros((ny,nx))\n+        resid2    = np.zeros((ny,nx))\n+        flag      = np.zeros((ny,nx)) + 2.\n+\n+        offset = 0\n+        resid0[ymin+offset:ymax+offset, xmin+offset:xmax+offset] = self.model_residuals[0,:,:]\n+        resid1[ymin+offset:ymax+offset, xmin+offset:xmax+offset] = self.model_residuals[1,:,:]\n+        resid2[ymin+offset:ymax+offset, xmin+offset:xmax+offset] = self.model_residuals[2,:,:]\n+        flag[ymin+offset:ymax+offset, xmin+offset:xmax+offset] = self.model_residuals[3,:,:]\n+\n+        # enlarge flagged area to be consistent with IPP solution\n+        #kernel = np.array([[1,1,1],[1,1,1],[1,1,1]], dtype=np.int)\n+        #flag_conv = scipy.signal.convolve2d(flag, kernel, mode='same')\n+\n+        resid0[flag > 0] = 0.\n+        resid1[flag > 0] = 0.\n+        resid2[flag > 0] = 0.\n+\n+        #res = stamp + (resid0 + resid1*x + resid2*x) # wrong formulae, for comparison with IPP code only\n+        res = stamp + (resid0 + resid1*x + resid2*y)\n+\n+        return res, flag, (resid0 + resid1*x + resid2*y)\n+\n+\n+    def get_stamp(self, x, y, stamp_size=65, center_offset_x = 0.0, center_offset_y = 0.0):\n+        xs, ys, stmp_an = self.make_stamp_analytical(x, y, stamp_size, center_offset_x, center_offset_y)\n+\n+        stmp_tot, stmp_flag, _ = self.add_residuals(stmp_an, x, y)\n+\n+        return stmp_tot, stmp_flag\n+\n+    def get_stampsize(self):\n+        #return the size of the psfmodel\n+        return [self.stampsize1, self.stampsize2]\n+\n+\n+\n+if __name__ == \"__main__\":\n+    fname = sys.argv[1]\n+    ps = PanstarrsPsfModel.PanstarrsPsfModel(fname)\n+\n+    stmp, flag = ps.get_stamp(1000, 1000)\n+\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/psfmodel/PsfModel.py": [
                        [
                            "@@ -0,0 +1,121 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+from EXT_PF1_GEN_P2.file import PSFUtils\n+from EXT_PF1_GEN_P2.psfmodel import PanstarrsPsfModel\n+\n+\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+\n+import os\n+import copy\n+import numpy as np\n+from astropy.io import fits\n+\n+\n+class PsfModel():\n+    \n+    def __init__(self, psfmodelfile):\n+        self.model = self._getModelInterface(psfmodelfile)\n+    #\n+    \n+    def _getModelInterface(self, psfmodelfile):\n+        hdul = fits.open(psfmodelfile)\n+        names = []\n+        for hdu in hdul:\n+            names.append(hdu.name)\n+        #\n+        hdul.close()\n+        \n+        if (\"SkyChip.hdr\" in names) and (\"SkyChip.psf_model\" in names):\n+            return PanstarrsModelInterface(psfmodelfile)\n+        elif (\"PSF_DATA\" in names) and len(hdul) == 2:\n+            return PsfexModelInterface(psfmodelfile)\n+        #\n+        \n+    #\n+    \n+    def getPsfStamp(self, x:float, y:float):\n+        return self.model.getPsfStamp(x,y)\n+    #\n+    \n+    def getAllPsfStamps(self, x:list, y:list):\n+        return self.model.getAllPsfStamps(x,y)\n+    #\n+    \n+    \n+    def getStampsize(self):\n+        return self.model.getStampsize()\n+    #\n+#\n+\n+\n+\n+class PsfexModelInterface():\n+    \n+    def __init__(self, psfmodelfile):\n+        self.psfobj = PSFUtils.PSFUtils(psfmodelfile)\n+    #\n+    \n+    def getPsfStamp(self, x:float, y:float):\n+        return self.psfobj.evaluate( x, y)\n+    #\n+    \n+    def getAllPsfStamps(self, x:list, y:list):\n+        return self.psfobj.evaluateAll( x, y)\n+    #\n+    \n+    def getStampsize(self):\n+        return self.psfobj.get_convolution_params()\n+    #\n+#\n+\n+\n+\n+class PanstarrsModelInterface():\n+    \n+    def __init__(self, psfmodelfile):\n+        self.psfobj = PanstarrsPsfModel.PanstarrsPsfModel(psfmodelfile)\n+    #\n+    \n+    def getPsfStamp(self, x:float, y:float):\n+        #1. determine the offset\n+        offsets = {\"off_x\": x - int(x), \"off_y\": y - int(y)}\n+        \n+        for off in offsets:\n+            if 0 <=offsets[off] and offsets[off] < 0.5:\n+                offsets[off] = -offsets[off]\n+            else:\n+                offsets[off] = 1.0 - offsets[off]\n+            #\n+        #\n+        \n+        stamp = self.psfobj.get_stamp(x, y, stamp_size=65, center_offset_x = offsets[\"off_x\"], center_offset_y = offsets[\"off_y\"])[0]\n+        stamp = stamp/stamp.sum() #normalize\n+        return stamp.flatten()\n+    #\n+    \n+    def getAllPsfStamps(self, x:list, y:list):\n+        stamps = []\n+        for i in range(len(x)):\n+            stamps.append(self.getPsfStamp(x[i], y[i]))\n+        return stamps\n+    #\n+    \n+    \n+    def getStampsize(self):\n+        return self.psfobj.get_stampsize()\n+    #\n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/pybin/IAL_Pipeline_Assembler_coadd_stage2.py": [
                        [
                            "@@ -20,6 +20,7 @@ import os, sys\n import logging\n import shutil\n import stat\n+import re\n \n import healpy\n import numpy as np\n@@ -30,28 +31,30 @@ from astropy.table import vstack\n \n from ST_DM_TileUtils.ST_DM_TileXML import ST_DM_TileXML\n \n-from EXT_PF1_Coadd.Pipeline import RDisk_coadd_stage2 as rdisk_coadd\n+from EXT_PF1_GEN_P2.Pipeline import RDisk_coadd_stage2 as rdisk_coadd\n from EXT_PF1_GEN_P2_LIBS.pybin import IAL_Pipeline_Assembler_coadd\n-from EXT_PF1_Coadd.pybin import IAL_Pipeline_Assembler_coadd_stage2\n-from EXT_PF1_Coadd.AstrometricValidation import AstrometricValidation\n-from EXT_PF1_GEN_P2_LIBS.dxml.ExtValidationReportExporter import ExtValidationReportExporter\n-from EXT_PF1_GEN_P2_LIBS.dxml.ExtStackedFrameExporter import ExtStackedFrameExporter\n-from EXT_PF1_GEN_P2_LIBS.dxml.ExtSourceCatalogExporter import ExtSourceCatalogExporter\n+from EXT_PF1_GEN_P2.pybin import IAL_Pipeline_Assembler_coadd_stage2\n+from EXT_PF1_GEN_P2.AstrometricValidation import AstrometricValidation\n+from EXT_PF1_GEN_P2_LIBS.exml.ExtValidationReportTExporter import ExtValidationReportTExporter\n+from EXT_PF1_GEN_P2_LIBS.exml.ExtStackedFrameTExporter import ExtStackedFrameTExporter\n+from EXT_PF1_GEN_P2_LIBS.exml.ExtCoaddSourceCatalogTExporter import ExtCoaddSourceCatalogTExporter\n \n from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n from EXT_PF1_GEN_P2_LIBS.file import CatalogDuplicates\n \n-        \n+from EXT_WCS_TOOLS import Sip_2_tpv\n+\n+\n \n class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n     \"\"\"\n-    Reads the configuration files necessary to execute a pipeline and \n+    Reads the configuration files necessary to execute a pipeline and\n     puts together all values necessary to call PipelineExec\n     \"\"\"\n-    \n+\n     def __init__(self):\n         IAL_Pipeline_Assembler_coadd.IPA_coadd.__init__(self)\n-        \n+\n         self.get_parser().add_argument('--mer_star_cat', type=str, help='reference catalog for the tile to find stackPSF positions.', default = '')\n         self.get_parser().add_argument('--mer_galaxy_cat', type=str, help='reference catalog for the tile to find stackPSF positions.', default = '')\n         self.get_parser().add_argument('--coadd_props_file', type=str, help='property file from EAS.', default = '')\n@@ -63,170 +66,276 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n         self.vis_position_cat_type = (\"DpdVisAuxDataCatalogue\", \"DpdVisCatalog\", \"DpdVisStackedFrameCatalog\")\n         self.mer_star_cat_type = (\"DpdMerTrueUniverseStarCatalog\")\n         self.mer_galaxy_cat_type = (\"DpdMerTrueUniverseGalaxyCatalog\")\n-        IAL_Pipeline_Assembler_coadd_stage2.set_stage2__exporter_classes(self)\n-     \n+\n     #\n-    \n-    \n+\n+\n     def get_sourcelist_tags(self, workdir):\n         #tags for stage 2\n-        return  [('Data', 'DataStorage', 'DataContainer', 'FileName'), ('dummy',), \n-                 (\"Data\", \"FWHM\"), \n-                 ('Data', 'ZeroPoint', 'Value'), \n-                 ('Data', 'ZeroPoint', 'Error'), \n+        return  [('Data', 'DataStorage', 'DataContainer', 'FileName'), ('dummy',),\n+                 (\"Data\", \"FWHM\"),\n+                 ('Data', 'ZeroPoint', 'Value'),\n+                 ('Data', 'ZeroPoint', 'Error'),\n                   self.getFilterTags(workdir, self.single_epoch_science_type)]\n     #\n-    \n+\n     def undo_euclid_filenaming_convention(self, filename):\n         \"\"\"\n-        This is done within stage 1 for DES data, as exposure and ccd number are \n+        This is done within stage 1 for DES data, as exposure and ccd number are\n         encoded in the filename.\n         In stage 2 this should remain empty\n         \"\"\"\n         return filename\n     #\n     \n+    def extract_gain_from_xml(self, xml_contents):\n+        tagvalues = self.parse_xml_file_for_tags(xml_contents, [(\"Data\",\"Gain\")])\n+        gain_settings = list()\n+        \n+        for section in tagvalues[0].split(\"<Product>\")[1:]:\n+            tagvalues = self.parse_xml_file_for_tags(\"<Product>\" +section, \n+                            [(\"Product\",\"AmpName\"), (\"Product\",\"GainValue\"), \n+                             (\"Product\",\"XRange\", \"Start\"), (\"Product\",\"XRange\", \"End\"),\n+                             (\"Product\",\"YRange\", \"Start\"), (\"Product\",\"YRange\", \"End\")])\n+            d = {\"AmpName\": tagvalues[0], \"GainValue\":tagvalues[1], \"XRange\":[tagvalues[2], tagvalues[3]], \"YRange\":[tagvalues[4], tagvalues[5]]}\n+            gain_settings.append(d)\n+        #\n+        return gain_settings\n+    #        \n     \n-    def deprecated_copy_from_xml_to_header(self, workdir, xml_contents):\n+    def copy_from_xml_to_header(self, fitsfile, workdir, target_dir, xml_contents):\n         \"\"\"\n-        matches the xml inputs and their fits contents and \n-        moves the wcs from xml to fits, if there is a difference\n+        copies fitsfile to target_dir\n+        reads wcs data from the xml_contents\n+        writes the wcs data (plus some other relevant xml data) to the copied fits files header\n+\n+        returns modified fitsfile in the target_dir\n         \"\"\"\n-        #unfortunate to open the images extra for this again\n-        if not os.path.exists(workdir):\n-            return ''\n-        #\n-        \n+\n         inputdir = workdir +os.sep +'inputs'\n         datadir =  workdir +os.sep +'data'\n-        if not os.path.exists(inputdir) or not os.path.exists(datadir):\n-            #hm, what?\n-            pass\n+\n+        if os.sep not in fitsfile:\n+            fitsfile = datadir + os.sep +fitsfile\n         #\n-        tagvalues = self.parse_xml_file_for_tags(xml_contents, [(\"Data\", \"DataStorage\", \"DataContainer\", \"FileName\")])\n-        fitsfile = datadir + os.sep +tagvalues[0]\n+\n         if not os.path.exists(fitsfile):\n-            fitsfile = datadir + os.sep +super().undo_euclid_filenaming_convention(tagvalues[0])\n-            if not os.path.exists(fitsfile):\n-                return\n+            fitsfile = datadir + os.sep +super().undo_euclid_filenaming_convention(fitsfile)\n+        #\n+        target_filename = target_dir +os.sep +fitsfile.rsplit(os.sep, 1)[-1]\n+        \n+        #copy the fitsfile to the target_dir, so we can modify\n+        try:\n+            logging.debug(\"copy %s to %s.\", fitsfile, target_filename)\n+            fitsfile = shutil.copyfile(fitsfile, target_filename, follow_symlinks=True) #dereference links\n+            print(\"copy_from_xml_to_header \", fitsfile)\n+            os.chmod(fitsfile, stat.S_IWUSR | stat.S_IRUSR | stat.S_IWGRP | stat.S_IRGRP)\n+            \n+            #and funpack, if necessary\n+            print(\"FitsServices.get_comptype(fitsfile).lower() \", FitsServices.get_comptype(fitsfile))\n+            comptype = FitsServices.get_comptype(fitsfile)\n+            if (comptype is not None) and (\"rice\" in comptype.lower()):\n+                os.system(\"funpack \" +fitsfile)\n+                \n+                #make a flavourcheck\n+                tmpfile = fitsfile.rsplit(\".fits\", 1)[0] +\".fits\"\n+                if os.path.exists(tmpfile):\n+                    os.remove(fitsfile)\n+                    fitsfile = tmpfile\n+                #\n             #\n+        except:\n+            import traceback\n+            exec_info =  sys.exc_info()\n+            traceback.print_tb(exec_info[2])\n+            return\n         #\n-        wcs_keys = FitsServices.get_wcs_keys()\n         \n-        #read the fitsfile\n-        image_hdu = FitsServices.get_image_hdu(fitsfile)\n-        wcs_dict = FitsServices.read_wcs(fitsfile, image_hdu, wcs_keys)\n         \n-        #update its wcs in all headers\n-        for wcs_card in wcs_dict:\n-            tags = (\"Data\", \"WCS\", wcs_card['name'])\n-            if wcs_card['name'].startswith(\"PV\"):\n-                tags = (\"Data\", \"WCS\", \"NonLinearCoeffs\", \"NonLinearTPVAstromCoeffs\", wcs_card['name'])\n-            if wcs_card['name'].startswith(\"CTYPE\"):\n-                continue #datamodel is garbage here. We don't change CTYPE anyway(?)\n-            #\n-            tagvalues = self.parse_xml_file_for_tags(xml_contents, [tags])\n-            wcs_card['value'] = wcs_card['value'].__class__(tagvalues[0])\n-            #\n+        header_dict_list = list()\n+        header_dict_list, header_keys_to_remove = self.get_wcs_values(header_dict_list, xml_contents, fitsfile)\n+        \n+        \n         #add a few more values to improve xml export later\n         tagvalues = self.parse_xml_file_for_tags(xml_contents, [(\"Data\",\"Instrument\",\"InstrumentName\"),\n-                                                                (\"Data\",\"Instrument\",\"TelescopeName\"), \n+                                                                (\"Data\",\"Instrument\",\"TelescopeName\"),\n                                                                 (\"Data\",\"Instrument\",\"Longitude\"),\n                                                                 (\"Data\",\"Instrument\",\"Latitude\"),\n                                                                 (\"Data\",\"Instrument\",\"Elevation\"),\n                                                                 (\"Data\",\"Filter\",\"Name\")])\n-        print(\"copy_from_xml_to_header: found header values \", tagvalues)\n-        wcs_dict.append( {'name':'INSTRUME', 'value': tagvalues[0], 'comment': '' } )\n-        wcs_dict.append( {'name':'TELESCOP', 'value': tagvalues[1], 'comment': '' } )\n-        wcs_dict.append( {'name':'OBS-LONG', 'value': tagvalues[2], 'comment': '' } )\n-        wcs_dict.append( {'name':'OBS-LAT', 'value': tagvalues[3], 'comment': '' } )\n-        wcs_dict.append( {'name':'OBS-ELEV', 'value': tagvalues[4], 'comment': '' } )\n-        wcs_dict.append( {'name':'FILTER', 'value': tagvalues[5], 'comment': '' } )\n+        logging.debug(\"copy_from_xml_to_header: found header values %s.\", str(tagvalues))\n+        header_dict_list.append( {'name':'INSTRUME', 'value': tagvalues[0], 'comment': '' } )\n+        header_dict_list.append( {'name':'TELESCOP', 'value': tagvalues[1], 'comment': '' } )\n+        header_dict_list.append( {'name':'OBS-LONG', 'value': tagvalues[2], 'comment': '' } )\n+        header_dict_list.append( {'name':'OBS-LAT', 'value': tagvalues[3], 'comment': '' } )\n+        header_dict_list.append( {'name':'OBS-ELEV', 'value': tagvalues[4], 'comment': '' } )\n+        header_dict_list.append( {'name':'FILTER', 'value': tagvalues[5], 'comment': '' } )\n+                                                                             #those values will be moved into the metadata of\n+                                                                             #the coadd, coadd catalog and validation report\n+        header_dict_list.append( {'name':'DETECTOR', 'value': \"unknown\", 'comment': '' } ) #for those, the values don't make sense\n+        header_dict_list.append( {'name':'CCDNUM', 'value': \"unknown\", 'comment': '' } ) #-> reset\n+        \n+        #gain\n+        #this puzzles stackpsf_v2. Check carefully! \n+        \"\"\"\n+        gain_settings = self.extract_gain_from_xml(xml_contents)\n         \n-        FitsServices.write_keys(fitsfile, image_hdu, wcs_dict)\n-        #print(\"updated wcs \", wcs_dict, \" in header of \", fitsfile, \" hdu \", image_hdu)\n+        if len(gain_settings) > 0:\n+            AMPNAMES = gain_settings[0][\"AmpName\"]\n+            for gain in gain_settings[1:]:\n+                AMPNAMES = AMPNAMES +\",\" +gain[\"AmpName\"]\n+            #\n+            header_dict_list.append( {'name':'AMPNAMES', 'value': \"comment\", 'comment': AMPNAMES } )\n+            gaincounter = 0\n+            for gain in gain_settings:\n+                header_dict_list.append( {'name': gain[\"AmpName\"], 'value': gain[\"GainValue\"], 'comment': \"Value for gain \" +gain[\"AmpName\"] } )\n+                header_dict_list.append( {'name':\"CCDSEC\" +str(gaincounter), 'value': str(gain[\"XRange\"] + gain[\"YRange\"]), 'comment': \"CCDSEC for gain \" +gain[\"AmpName\"] } )\n+                gaincounter = gaincounter +1\n+            #\n+        #\n+        \"\"\"\n+        image_hdu = FitsServices.get_image_hdu(fitsfile)\n+        FitsServices.write_keys(fitsfile, image_hdu, header_dict_list, header_keys_to_remove)\n+\n+        return fitsfile\n     #\n     \n     \n-    def copy_from_xml_to_header(self, fitsfile, workdir, target_dir, xml_contents):\n-        \"\"\"\n-        matches the xml inputs and their fits contents and \n-        moves the wcs from xml to fits, if there is a difference\n-        \"\"\"\n-        #unfortunate to open the images extra for this again\n-        if not os.path.exists(workdir):\n-            return ''\n-        #\n+    def get_wcs_values(self, header_dict_list, xml_contents, fitsfile):\n+        if \"<ProjectionType>TAN-SIP</ProjectionType>\" in xml_contents:\n+            return self.get_sip_header(header_dict_list, xml_contents, fitsfile)\n+        elif \"<ProjectionType>TPV</ProjectionType>\" in xml_contents:\n+            return self.get_tpv_header(header_dict_list, xml_contents)\n+        else:\n+            return header_dict_list, xml_contents\n+    #\n+    \n+    \n+    def get_sip_header(self, header_dict_list, xml_contents, fitsfile):\n+        temp_header_dict_list = list()\n+        wcs_keys = FitsServices.get_linear_wcs_keys()\n         \n-        inputdir = workdir +os.sep +'inputs'\n-        datadir =  workdir +os.sep +'data'\n+        #the SIP keys are extracted directly from the xml\n+        tags = (\"WCS\", \"NonLinearCoeffs\", \"NonLinearSIPAstromCoeffs\")\n+        tagvalues = self.parse_xml_file_for_tags(xml_contents, [tags])[0]\n         \n-        if os.sep not in fitsfile:\n-            fitsfile = datadir + os.sep +fitsfile\n+        #get the tags, not the end-tags\n+        pattern = re.compile(r'(<[A-Z0-9]+[_]+[0-9]+[_]+[0-9]>)')\n+        \n+        coeffs = pattern.findall(tagvalues)\n+        \n+        header_keys_to_remove = []\n+        non_linear_keys = []\n+        for coeff in coeffs:\n+            non_linear_keys.append(coeff[1:-1])\n+            header_keys_to_remove.append(coeff[1:-1])\n+            \n+        non_linear_keys.append(\"A_ORDER\")\n+        non_linear_keys.append(\"B_ORDER\")\n+        non_linear_keys.append(\"AP_ORDER\")\n+        non_linear_keys.append(\"BP_ORDER\")\n+        \n+        header_keys_to_remove.append(\"A_ORDER\")\n+        header_keys_to_remove.append(\"B_ORDER\")\n+        header_keys_to_remove.append(\"AP_ORDER\")\n+        header_keys_to_remove.append(\"BP_ORDER\")\n+        \n+        #update the header with the xml SIP values\n+        for wcs_key in wcs_keys:\n+            if wcs_key.startswith(\"CTYPE\"):\n+                continue\n+            #\n+            tagvalues = self.parse_xml_file_for_tags(xml_contents, [(\"Data\", \"WCS\", wcs_key)])\n+            if tagvalues[0] != wcs_key: #if the value is not in the xml, the key is returned\n+                #we have strings and floats\n+                try:\n+                    value = float(tagvalues[0])\n+                except:\n+                    value = tagvalues[0] #string\n+                #\n+                temp_header_dict_list.append( {'name':wcs_key, 'value': value, 'comment': '' } )\n+            #\n         #\n-        if not os.path.exists(inputdir) or not os.path.exists(datadir):\n-            #hm, what?\n-            pass\n+        \n+        for wcs_key in non_linear_keys:\n+            \n+            tagvalues = self.parse_xml_file_for_tags(xml_contents, [(\"WCS\", \"NonLinearCoeffs\", \"NonLinearSIPAstromCoeffs\", wcs_key)])\n+            if tagvalues[0] != wcs_key: #if the value is not in the xml, the key is returned\n+                #we have strings and floats\n+                try:\n+                    value = float(tagvalues[0])\n+                except:\n+                    value = tagvalues[0] #string\n+                #\n+                temp_header_dict_list.append( {'name':wcs_key, 'value': value, 'comment': '' } )\n+            #\n         #\n         \n-        if not os.path.exists(fitsfile):\n-            fitsfile = datadir + os.sep +super().undo_euclid_filenaming_convention(fitsfile)\n-            if not os.path.exists(fitsfile):\n-                return\n+        \n+        #update the header with the xml SIP values\n+        image_head = FitsServices.read_image_header(fitsfile)\n+        for card in temp_header_dict_list:\n+            if card['name'] in image_head:\n+                image_head[ card['name'] ] = card['value']\n             #\n         #\n-        #copy the fitsfile to the target_dir, so we can modify\n-        target_filename = target_dir +os.sep +fitsfile.rsplit(os.sep, 1)[-1]\n-        if os.path.exists( target_filename ):\n-            #all this has been done already, don't do it again\n-            return target_filename\n+        #convert SIP to TPV\n+        tpv_values = Sip_2_tpv.sip2tpv(image_head)\n+        \n+        #write the TPV header values into a new header_dict_list\n+        #don't forget the ProjectionType!        \n+        for wcs_key in tpv_values:\n+            header_dict_list.append( {'name':wcs_key, 'value': tpv_values[wcs_key], 'comment': '' } )\n+            #\n         #\n         \n-        logging.debug(\"copy \", fitsfile, \" to \", target_filename)\n-        fitsfile = shutil.copy(fitsfile, target_filename, follow_symlinks=True) #dereference links\n-        os.chmod(fitsfile, stat.S_IWUSR | stat.S_IRUSR | stat.S_IWGRP | stat.S_IRGRP) \n         \n-        wcs_keys = FitsServices.get_wcs_keys()\n+        #since we have the header open, check a few more values to remove\n+        #this is a bit brute force, but simpler than finding out the order\n+        for i in range(9):\n+            for j in range(9):\n+                prefix = [\"A_\", \"B_\", \"AP_\", \"BP_\"]\n+                for p in prefix:\n+                    value = p+str(i) +\"_\" +str(j)\n+                    if (value in image_head) and (value not in header_keys_to_remove):\n+                        header_keys_to_remove.append(value)\n+                    #\n+                #\n+            #\n+        #\n+        print(\"header_keys_to_remove \", header_keys_to_remove)\n+        \n+        \n+        #return the new header_dict_list, header_keys_to_remove\n+        return header_dict_list, header_keys_to_remove\n+    #\n+    \n+    \n+    def get_tpv_header(self, header_dict_list, xml_contents):\n+        wcs_keys = FitsServices.get_wcs_tpv_keys()\n         \n-        #read the fitsfile\n-        image_hdu = FitsServices.get_image_hdu(fitsfile)\n-        wcs_dict = FitsServices.read_wcs(fitsfile, image_hdu, wcs_keys)\n         \n-        #update its wcs in all headers\n-        for wcs_card in wcs_dict:\n-            tags = (\"Data\", \"WCS\", wcs_card['name'])\n-            if wcs_card['name'].startswith(\"PV\"):\n-                tags = (\"Data\", \"WCS\", \"NonLinearCoeffs\", \"NonLinearTPVAstromCoeffs\", wcs_card['name'])\n-            if wcs_card['name'].startswith(\"CTYPE\"):\n-                continue #datamodel is garbage here. We don't change CTYPE anyway(?)\n+        #read the values from xml\n+        for wcs_key in wcs_keys:\n+            tags = (\"Data\", \"WCS\", wcs_key)\n+            if wcs_key.startswith(\"PV\"):\n+                tags = (\"Data\", \"WCS\", \"NonLinearCoeffs\", \"NonLinearTPVAstromCoeffs\", wcs_key)\n+            elif wcs_key.startswith(\"CTYPE\"):\n+                continue\n             #\n             tagvalues = self.parse_xml_file_for_tags(xml_contents, [tags])\n-            wcs_card['value'] = wcs_card['value'].__class__(tagvalues[0])\n+            if tagvalues[0] != wcs_key: #if the value is not in the xml, the key is returned\n+                #we have strings and floats\n+                try:\n+                    value = float(tagvalues[0])\n+                except:\n+                    value = tagvalues[0] #string\n+                #\n+                header_dict_list.append( {'name':wcs_key, 'value': value, 'comment': '' } )\n             #\n-        #add a few more values to improve xml export later\n-        tagvalues = self.parse_xml_file_for_tags(xml_contents, [(\"Data\",\"Instrument\",\"InstrumentName\"),\n-                                                                (\"Data\",\"Instrument\",\"TelescopeName\"), \n-                                                                (\"Data\",\"Instrument\",\"Longitude\"),\n-                                                                (\"Data\",\"Instrument\",\"Latitude\"),\n-                                                                (\"Data\",\"Instrument\",\"Elevation\"),\n-                                                                (\"Data\",\"Filter\",\"Name\")])\n-        logging.debug(\"copy_from_xml_to_header: found header values \", tagvalues)\n-        wcs_dict.append( {'name':'INSTRUME', 'value': tagvalues[0], 'comment': '' } )\n-        wcs_dict.append( {'name':'TELESCOP', 'value': tagvalues[1], 'comment': '' } )\n-        wcs_dict.append( {'name':'OBS-LONG', 'value': tagvalues[2], 'comment': '' } )\n-        wcs_dict.append( {'name':'OBS-LAT', 'value': tagvalues[3], 'comment': '' } )\n-        wcs_dict.append( {'name':'OBS-ELEV', 'value': tagvalues[4], 'comment': '' } )\n-        wcs_dict.append( {'name':'FILTER', 'value': tagvalues[5], 'comment': '' } )\n-                                                                             #those values will be moved into the metadata of \n-                                                                             #the coadd, coadd catalog and validation report\n-        wcs_dict.append( {'name':'DETECTOR', 'value': \"unknown\", 'comment': '' } ) #for those, the values don't make sense\n-        wcs_dict.append( {'name':'CCDNUM', 'value': \"unknown\", 'comment': '' } ) #-> reset\n-        \n-        \n-        FitsServices.write_keys(fitsfile, image_hdu, wcs_dict)\n-        #print(\"updated wcs \", wcs_dict, \" in header of \", fitsfile, \" hdu \", image_hdu)\n+        #\n         \n-        return fitsfile\n+        return header_dict_list, []\n     #\n     \n     \n@@ -234,42 +343,42 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n         \"\"\"\n         returns the path to the property template\n         This overwrites code in stage 1 IAL_Pipeline_Assembler as\n-        stage 2 finds its instrument specific templates in the \n+        stage 2 finds its instrument specific templates in the\n         stage2 conf folder.\n-        \n+\n         \"\"\"\n         #extract instrument\n         single_epoch_science_files = self.find_file_of_type_in_dir(workdir+os.sep+\"inputs\", self.single_epoch_science_type)\n-        \n+\n         of = open(single_epoch_science_files[0], 'r')\n         xml_content = of.read()\n         of.close()\n-        \n+\n         try:\n             #those tags are shared in all 3 science type xml files\n             instrument = self.parse_xml_file_for_tags(xml_content, [('Data', 'Instrument', 'InstrumentName')])[0]\n         except:\n             raise Exception(\"No instrument could be found in \" +single_epoch_science_files[0])\n         #\n-        \n-        #1st way: \n+\n+        #1st way:\n         #the conf file comes with the EAS query.\n-        #In that case there is an xml file in input and the conf file is \n+        #In that case there is an xml file in input and the conf file is\n         #in the data folder\n         confpath = workdir + os.sep +\"data\" +os.sep\n         properties = [confpath +f for f in os.listdir(confpath) if f.startswith(\"coadd_template\") and f.endswith(\".properties\")]\n-        \n-        \n+\n+\n         #2nd way:\n         #find the stage2 conf folder\n         #and in it the conf file\n         #is there a more elegant way?\n         path = os.path.dirname(IAL_Pipeline_Assembler_coadd_stage2.__file__)\n-        rootpath = path.rsplit(\"python/EXT_PF1_Coadd/pybin\", 1)[0]\n-        confpath = rootpath +os.sep +\"conf\" +os.sep +\"EXT_PF1_Coadd\" +os.sep\n-        \n+        rootpath = path.rsplit(\"python/EXT_PF1_GEN_P2/pybin\", 1)[0]\n+        confpath = rootpath +os.sep +\"conf\" +os.sep +\"EXT_PF1_GEN_P2\" +os.sep\n+\n         properties = properties + [confpath +f for f in os.listdir(confpath) if f.startswith(\"coadd_template\") and f.endswith(\".properties\")]\n-        \n+\n         #get the template for that instrument\n         #first is now the data path, second the default install conf path\n         #if something is in data, it will be taken\n@@ -283,15 +392,15 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n         #here we did not find a property template\n         raise Exception(\"No property template found for \" +instrument)\n     #\n-    \n-    \n+\n+\n     def prepare_coadd_input_files(self, workdir, pixelscale, check_available_fits_files, run_dir):\n         \"\"\"\n-        We have a history of changing datamodels and input image types.                        \n-        This method checks, which image type we are dealing with and \n+        We have a history of changing datamodels and input image types.\n+        This method checks, which image type we are dealing with and\n         the decides either to call the current method (for SingleEpochFrames) or\n         the deprecated method (for all other image types).\n-        \n+\n         Thus ensuring backwards compatibility\n         \"\"\"\n         all_xml_files = [_f for _f in os.listdir(workdir + os.sep +\"inputs\") if _f.startswith(self.single_epoch_science_frame)]\n@@ -299,8 +408,8 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n             return self.se_frames_prepare_coadd_input_files(workdir, run_dir, pixelscale, check_available_fits_files)\n         #\n     #\n-    \n-    \n+\n+\n     ###########################################\n     #\n     # code used for all input types\n@@ -309,7 +418,7 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n     def select_tu_sources(self, catalog_file_names, tile, ra_list = [\"RA_MAG\",\"RA\",\"X_WORLD\"], dec_list = [\"DEC_MAG\",\"DEC\",\"Y_WORLD\"]):\n         \"\"\"Selects sources from the list of input catalogs that fall inside the\n         tile inner area.\n-        \n+\n         Parameters\n         ----------\n         catalog_file_names: list\n@@ -317,35 +426,26 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n             path.xwq\n         tile: object\n             A tile instance.\n-        \n-        \n+\n+\n         Returns\n         -------\n         object\n              A catalog with the selected sources.\n-        \n+\n         \"\"\"\n-        # Get the tile healpix indices\n-        tile_healpix = tile.get_inner_healpix_indices()[1]\n-        \n-        # Get the tile MOC order\n-        moc_order = tile.get_data_element().MocOrder\n-        \n-        # Calculate the tile nside parameter\n-        nside = healpy.order2nside(moc_order)\n-        \n         # Loop over the catalog files and select the overlapping sources\n         merged_catalog = None\n-        \n+\n         #with the MER catalogs, the sorting becomes important\n         #galaxy catalogs have Ra and Ra_Mag, and RA_Mag must be taken\n         #star catalogs only contain Ra\n         #-> Ra_Mag, Dec_Mag must be looked for first\n-        \n+\n         for i, catalog_file_name in enumerate(catalog_file_names):\n             # Read the catalog\n             catalog = Table.read(catalog_file_name, format=\"fits\", hdu = -1) #\"LDAC_OBJECTS\"\n-            \n+\n             # Get the source (RA, Dec) coodinates\n             for i in range(len(ra_list)):\n                 if ra_list[i] in catalog.colnames:\n@@ -354,18 +454,18 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n                     break\n                 #\n             #\n-            \n+\n             # Convert the units from degrees to radians\n             theta = np.pi / 2 - np.pi * dec / 180\n             phi = np.pi * ra / 180\n-            \n+\n             # Get the healpix indices\n             sources_healpix = healpy.pixelfunc.ang2pix(\n-                nside, theta, phi, nest=True)\n-            \n+                healpy.order2nside(tile.get_data_element().MocOrder), theta, phi, nest=True)\n+\n             # Check which sources fall inside the tile\n-            inside = np.in1d(sources_healpix, tile_healpix, assume_unique=False)\n-            \n+            inside = np.in1d(sources_healpix, tile.get_inner_healpix_indices()[1], assume_unique=False)\n+\n             if np.any(inside):\n                 # Add the selected sources to the merged catalog\n                 if merged_catalog is None:\n@@ -375,7 +475,7 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n                 #\n         return merged_catalog\n     #\n-                                      \n+\n     def read_file(self, xml_file):\n         of = open(xml_file, 'r')\n         xml_content = of.read()\n@@ -387,8 +487,8 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n             print(xml_file, \" \", xml_content)\n         return self.parse_xml_file_for_tags(xml_content, tags), xml_content\n     #\n-    \n-    \n+\n+\n     ###########################################\n     #\n     # input files for single epoch frames\n@@ -398,7 +498,7 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n         \"\"\"\n         prepare the same coaddswarp.list as in stage 1 but then add\n         the catalogs and psfs, if present\n-        \n+\n         Overwrites the same method in IPA_coadd\n         \"\"\"\n         single_epoch_frames_id_tags = [\n@@ -411,80 +511,80 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n                    (\"Data\", \"ObservationDateTime\", \"UTCObservationDateTime\"),\n                    (\"Data\", \"Filter\", \"Name\"),\n                    (\"Data\", \"Detector\")]\n-        \n+\n         #call method on parent class\n         coadd_files, input_dir = super(IPA_coadd_stage2, self).prepare_coadd_input_files(workdir, pixelscale, check_available_fits_files)\n         logging.info(\"se_frames_prepare_coadd_input_files\")\n         #change the pipeline script\n-        coadd_files[\"scriptname\"] = \"rdisk_coadd_stage2_pipeline.py\"\n-        \n+        coadd_files[\"scriptname\"] = \"rdisk_coadd_stage2_pipeline.pscript\"\n+\n         #add the mer position catalogs\n         catalog_filename = workdir +os.sep +\"data\" +os.sep +\"merged_mer_catalog.fits\"\n         logging.info(\"merge_mer_star_galaxy_catalogs\")\n-        self.merge_mer_star_galaxy_catalogs(workdir, coadd_files, catalog_filename, magnitude_cut = 26)\n+        catalog_filename = self.merge_mer_star_galaxy_catalogs(workdir, coadd_files, catalog_filename, magnitude_cut = 26)\n         coadd_files[\"vis_position_cat\"] = catalog_filename\n         #\n-        \n+\n         #use a dict to store extracted values from the xml\n         img_dict = dict() #image name with [psf, catalog, zp, zperr, FWHM]\n-        \n+\n         all_xml_files = [_f for _f in os.listdir(input_dir) if _f.startswith(self.single_epoch_science_frame) and _f.endswith(\".xml\")]\n-        \n+\n         logging.info(\"loop xml files\")\n         data_header_update_dir = workdir +os.sep +'header_updated_data' +os.sep\n         os.makedirs(data_header_update_dir, mode=0o777, exist_ok=True)\n-        \n+\n         for _f in all_xml_files:\n             values, xml_contents = self.parse_tags(input_dir +os.sep +_f, single_epoch_frames_id_tags)\n-            \n+\n             if not self.is_valid(xml_contents):\n                 #print(\"Found INVALID. Drop it.\")\n                 continue\n             #\n-            \n+\n             #here we have the xml_contents open. So update the wcs\n             filename = values[0] #initialize\n             try:\n                 filename = self.copy_from_xml_to_header(values[0], workdir, data_header_update_dir, xml_contents)\n             except Exception as e:\n-                logging.info(e)       \n+                logging.info(e)\n                 import traceback\n                 exec_info =  sys.exc_info()\n                 traceback.print_tb(exec_info[2])\n                 #continue\n             #\n-            \n+\n             #img filename\n             img_dict[values[0]] = dict()\n-            \n+\n             #img in data_header_update_dir directory\n             #for aestetic reasons, remove //\n-            \n+\n             img_dict[values[0]][\"img\"] = filename.replace(os.sep+os.sep, os.sep)\n-            \n+\n             #psf\n             img_dict[values[0]][\"psf\"] = values[1]\n-            \n+\n             #se-catalog\n             img_dict[values[0]][\"cat\"] = values[2]\n-            \n+\n             #zp\n             img_dict[values[0]][\"zp\"] = values[3]\n-            \n+\n             #zp-err\n             img_dict[values[0]][\"zp-err\"] = values[4]\n-            \n+\n             #FWHM\n             img_dict[values[0]][\"FWHM\"] = values[5]\n         #\n-        \n+\n         #move the values into the coaddswarplist\n         #read the coaddswarp.list\n         #the full path is in coadd_files[\"coaddswarp\"]\n         of = open(coadd_files[\"coaddswarp\"], 'r')\n         lines = of.readlines()\n         of.close()\n-        \n+\n         new_lines = list()\n         print(\"original coaddswarp file length \", len(lines))\n         for i in range(len(lines)):\n@@ -494,25 +594,25 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n                     line = line[:-1]\n                 #for aestetic reasons, remove //\n                 line = line.replace(os.sep+os.sep, os.sep)\n-                \n+\n                 #get the filepath and name\n                 _file = line.split()[0]\n                 f = _file.rsplit(os.sep, 1)\n-                \n+\n                 if img_dict[f[1]][\"img\"] in line:\n                     #2nd run, coaddswarplist is ready\n                     #don't run again\n                     print(\"coaddswarp.list is up to date.\")\n                     return coadd_files, input_dir\n                 #\n-                \n+\n                 #img in data_header_update_dir\n                 line = line.replace(_file, img_dict[f[1]][\"img\"])\n-                \n+\n                 #psf and cat\n                 psf = f[0] +os.sep +img_dict[f[1]][\"psf\"]\n                 cat = f[0] +os.sep +img_dict[f[1]][\"cat\"]\n-                \n+\n                 #fill in the values\n                 line = line.replace(\"FWHM\", img_dict[f[1]][\"FWHM\"])\n                 line = line.replace(\"FullWidthHalfMaximum\", img_dict[f[1]][\"FWHM\"])\n@@ -523,7 +623,7 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n                 line = line.replace(\"Value\", img_dict[f[1]][\"zp\"])\n                 line = line.replace(\"Error\", img_dict[f[1]][\"zp-err\"])\n                 if psf not in line and cat not in line:\n-                    #if this is a second run, don't redo the \n+                    #if this is a second run, don't redo the\n                     #already existing list\n                     line = line +\" \" +psf +\" \" +cat +\"\\n\"\n                 else:\n@@ -531,7 +631,7 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n                     return coadd_files, input_dir\n                 #\n                 if line in new_lines:\n-                    #don't add! \n+                    #don't add!\n                     #Raise exception and jump over the line adding code\n                     raise Exception(\"Double entry in the coaddswarp list eliminated.\")\n                 #\n@@ -550,24 +650,62 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n                 print(\"line \", line)\n             #\n         #\n-        \n+\n         of = open(coadd_files[\"coaddswarp\"], 'w')\n         of.writelines(new_lines)\n         of.close()\n-        \n+\n         #self.printout_db_ids(xml_matches)\n         return coadd_files, input_dir\n     #\n-    \n-    \n+\n+\n     def merge_mer_star_galaxy_catalogs(self, workdir, coadd_files, catalog_filename, magnitude_cut = 26):\n         \"\"\"\n         treat both types of the mer  catalogs:\n         find them, merge them, remove duplicates, select ra,dec,mag, rename columns X_WORLD, Y_WORLD, MAG_AUTO\n         convert flxes in micro-jansky to AB magnitudes\n-        \n+\n         finally, merge star and galaxy catalogs into one catalog\n         \"\"\"\n+        def extract_reference_catalog_columns():\n+            \"\"\"\n+            get all entries of Reference_Catalog_Calibration_band\n+            and put them into a dict by band\n+            \"\"\"\n+            pt = self.get_property_template_path(workdir)\n+            f = open(pt)\n+            lines = f.readlines()\n+            f.close()\n+\n+            Reference_Catalog_Columns = dict()\n+            for line in lines:\n+                if line.startswith(\"Reference_Catalog_Calibration_\"):\n+                    splits = line.split(\"=\")\n+                    key = splits[0].strip()\n+                    value = splits[1].strip().split(\",\")\n+                    value = [x.strip() for x in value]\n+                    Reference_Catalog_Columns[key.split(\"Reference_Catalog_Calibration_\", 1)[-1]] = value\n+                #\n+            #\n+            return Reference_Catalog_Columns\n+        #\n+        def get_columns_to_keep(reference_catalog_columns, merged_mer_star_catalog):\n+            \"\"\"\n+            get the columns from reference_catalog_columns and check, which are in\n+            merged_mer_star_catalog.\n+            Return a new dict(band, column_name_to_keep)\n+            \"\"\"\n+            columns_to_keep = dict()\n+            for key in reference_catalog_columns:\n+                for column in reference_catalog_columns[key]: #this is a list\n+                    if column in merged_mer_star_catalog.colnames:\n+                        columns_to_keep[key] = column\n+                    #\n+                #\n+            #\n+            return columns_to_keep\n+        #\n         def read_catalog_file_from_xml(catalog_xml_files):\n             postion_cat_fitsfiles = []\n             for catalog_xml_file in catalog_xml_files:\n@@ -582,151 +720,182 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n         #\n         def convert_jansky_mag_and_cut(table, jansky_column, cutmagnitude):\n             \"\"\"\n-            Convert the jansky column to abmag and cut away everything \n+            Convert the jansky column to abmag and cut away everything\n             darker than cutmagnitude\n             \"\"\"\n             t = table[jansky_column].data\n-            \n+\n             #make a default out with mag > cutmagnitude, so it gets filtered out\n             magout = (cutmagnitude + 1.0)*np.ones(t.shape, dtype = np.float64)\n-            \n+\n             #convert, only where flux > 0, else magout remains (cutmagnitude + 1)\n             abmag = 2.5 * (23 - np.log10(t, where=t > 0.0, out = magout)) - 48.6\n             table[jansky_column] = Column(data=abmag, name=jansky_column, dtype=float)\n-            \n+\n             return table[abmag <= cutmagnitude ]\n-        #    \n-        \n+        #\n+\n         input_dir = workdir + os.sep + 'inputs'\n         tile = ST_DM_TileXML(coadd_files['tileinfo_xml_file'], logger = None)\n-        \n-        #treat the mer star catalogs:        \n+\n+        #treat the mer star catalogs:\n         #find them, merge them, remove duplicates, select ra,dec,mag, rename columns X_WORLD, Y_WORLD, MAG_AUTO\n-        mer_position_cat_xml_files = self.find_file_of_type_in_dir(input_dir, self.mer_star_cat_type)\n-        mer_postion_cat_fitsfiles =  read_catalog_file_from_xml(mer_position_cat_xml_files)\n-                \n+        mer_position_cat_star_xml_files = self.find_file_of_type_in_dir(input_dir, self.mer_star_cat_type)\n+        mer_position_cat_galaxy_xml_files = self.find_file_of_type_in_dir(input_dir, self.mer_galaxy_cat_type)\n+\n+        if len(mer_position_cat_star_xml_files) == 0 or len(mer_position_cat_galaxy_xml_files) ==0:\n+            #we have no catalog inputs\n+            #use the coadd catalog as position cat instead\n+            return \"coadd_cat\"\n+\n+        mer_postion_cat_fitsfiles =  read_catalog_file_from_xml(mer_position_cat_star_xml_files)\n+        reference_catalog_columns = extract_reference_catalog_columns()\n+\n+        print(\"reference_catalog_columns \", reference_catalog_columns)\n+\n         merged_mer_star_catalog = self.select_tu_sources(mer_postion_cat_fitsfiles, tile, ra_list = [\"RA\"], dec_list = [\"DEC\"])\n-        merged_mer_star_catalog.keep_columns([\"RA\", \"DEC\", \"TU_FNU_G_GAIA\"])\n-        merged_mer_star_catalog.rename_column(\"RA\", \"X_WORLD\")\n-        merged_mer_star_catalog.rename_column(\"DEC\", \"Y_WORLD\")\n-        merged_mer_star_catalog.rename_column(\"TU_FNU_G_GAIA\", \"MAG_AUTO\")\n-        \n+        columns_to_keep = get_columns_to_keep(reference_catalog_columns, merged_mer_star_catalog)\n+\n+        #make a new catalog:\n+        new_mer_star_catalog = Table()\n+        new_mer_star_catalog.add_column( Column(merged_mer_star_catalog[\"RA\"].data,  name =\"X_WORLD\") )\n+        new_mer_star_catalog.add_column( Column(merged_mer_star_catalog[\"DEC\"].data,  name =\"Y_WORLD\") )\n+        for key in columns_to_keep:\n+            new_mer_star_catalog.add_column( Column(merged_mer_star_catalog[columns_to_keep[key]].data,  name =key) )\n+        #\n+\n         #treat the mer galaxy catalogs:\n         #find them, merge them, remove duplicates, select ra_mag,dec_mag,mag, rename columns X_WORLD, Y_WORLD, MAG_AUTO\n-        mer_position_cat_xml_files = self.find_file_of_type_in_dir(input_dir, self.mer_galaxy_cat_type)\n-        mer_postion_cat_fitsfiles =  read_catalog_file_from_xml(mer_position_cat_xml_files)\n-                \n+        mer_postion_cat_fitsfiles =  read_catalog_file_from_xml(mer_position_cat_galaxy_xml_files)\n+\n         merged_mer_galaxy_catalog = self.select_tu_sources(mer_postion_cat_fitsfiles, tile, ra_list = [\"RA_MAG\"], dec_list = [\"DEC_MAG\"])\n-        merged_mer_galaxy_catalog.keep_columns([\"RA_MAG\", \"DEC_MAG\", \"TU_FNU_G_GAIA_MAG\"])\n-        merged_mer_galaxy_catalog.rename_column(\"RA_MAG\", \"X_WORLD\")\n-        merged_mer_galaxy_catalog.rename_column(\"DEC_MAG\", \"Y_WORLD\")\n-        merged_mer_galaxy_catalog.rename_column(\"TU_FNU_G_GAIA_MAG\", \"MAG_AUTO\")\n-        \n-        merged_catalog = vstack([merged_mer_star_catalog, merged_mer_galaxy_catalog])\n-        \n-        \n-        merged_catalog = convert_jansky_mag_and_cut(merged_catalog, \"MAG_AUTO\", magnitude_cut)\n+        columns_to_keep = get_columns_to_keep(reference_catalog_columns, merged_mer_galaxy_catalog)\n+\n+        #make a new catalog\n+        new_mer_galaxy_catalog = Table()\n+        new_mer_galaxy_catalog.add_column( Column(merged_mer_galaxy_catalog[\"RA_MAG\"].data,  name =\"X_WORLD\") )\n+        new_mer_galaxy_catalog.add_column( Column(merged_mer_galaxy_catalog[\"DEC_MAG\"].data,  name =\"Y_WORLD\") )\n+        for key in columns_to_keep:\n+            new_mer_galaxy_catalog.add_column( Column(merged_mer_galaxy_catalog[columns_to_keep[key]].data,  name =key) )\n+        #\n+\n+        merged_catalog = vstack([new_mer_star_catalog, new_mer_galaxy_catalog])\n+\n+        for key in columns_to_keep:\n+            merged_catalog = convert_jansky_mag_and_cut(merged_catalog, key, magnitude_cut)\n+        #\n+\n         merged_catalog.write(catalog_filename, format='fits', overwrite=True)\n+        \n+        #MW: remove ext = -1 and put a valid number instead. -1 is deprecated!\n         fits.setval(catalog_filename, 'extname', value='LDAC_OBJECTS', ext=-1)\n+        #fits.setval(catalog_filename, 'extname', value='LDAC_OBJECTS', ext=1)\n+        \n         return catalog_filename\n     #\n-    \n-    \n+\n+\n     def merge_reference_catalogs(self, workdir, vis_postion_cat_fitsfiles, tileinfo_xml_file):\n         tile = ST_DM_TileXML(tileinfo_xml_file, logger = None)\n         merged_vis_catalog = self.select_tu_sources(vis_postion_cat_fitsfiles, tile)\n-        \n-                \n+\n+\n         #write it to a fits file and return the file name\n         filename = workdir +os.sep +\"data\" +os.sep +\"merged_vis_catalog.fits\"\n         merged_vis_catalog.write(filename, format='fits', overwrite=True)\n         fits.setval(filename, 'extname', value='LDAC_OBJECTS', ext=-1)\n-        \n+\n         cd = CatalogDuplicates.CatalogDuplicates(filename)\n         nodups = workdir +os.sep +\"data\" +os.sep +\"merged_vis_catalog_no_duplicates.fits\"\n         #take the precision of VIS astrometry between catalogs to be 0.3 arcsecs = 3 pixels\n         cd.remove_duplicates_based_on_subset(nodups, cat_columns = [\"X_WORLD\", \"Y_WORLD\", \"MAG_AUTO\"], rejection_limits  = [0.3/3600.0, 0.3/3600.0, 0.3])\n-        \n+\n         return nodups\n     #\n-    \n-    \n-    def printout_db_ids(self, xml_matches, band = \"ALL\"):\n+\n+\n+    def printout_db_ids(self, xml_matches):\n         \"\"\"\n         as a diagnostic tool to allow streamlined ppo creation,\n-        read the swarplist and printout the database ids, so a \n-        ppo can easily be created as long as COORs in not available \n+        read the swarplist and printout the database ids, so a\n+        ppo can easily be created as long as COORs in not available\n         (means probably: for ever)\n-        \n+\n         xml_matches: a pylist containing matching tupels of path to (science_xml, catalog_xml)\n         \"\"\"\n-        \n+\n         #sort the matches according to band\n         banddict = dict()\n         tags = [(\"Header\", \"ProductId\"), (\"Data\", \"Filter\", \"Name\")]\n-        \n+\n         for xml_match in xml_matches:\n             sci_values = self.parse_tags(xml_match[0], tags)\n             cat_values = self.parse_tags(xml_match[1], tags)\n-            \n+\n             if sci_values[1] not in banddict:\n                 banddict[sci_values[1]] = []\n             banddict[sci_values[1]].append( (sci_values[0],cat_values[0]) )\n         #\n-        \n-        for band in banddict:\n-            print(\"band \", band)\n-            for t in banddict[band]:\n+\n+        for _band in banddict:\n+            print(\"band \", _band)\n+            for t in banddict[_band]:\n                 print(\"science id: \", t[0],\" catalog id: \", t[1])\n             #\n         #\n         sys.exit(0)\n     #\n-    \n-    \n+\n+\n     def write_outputs(self, ial_outputs, pipeline_outputs, workdir, exitcode = 0):\n         \"\"\"\n         stage 2 coadd has the validation images as a special, so\n         treat them here separately and the call the parent method\n         \"\"\"\n-        \n+\n         file_list = []\n+\n+        #tilename\n+        #get the listfile in the datadir\n+        tiledef = [_f for _f in os.listdir(workdir + os.sep +'data') if _f.endswith('.list')][0]\n+\n+        #extract the tilename\n+        openfile = open(workdir + os.sep +'data' +os.sep +tiledef, 'r')\n+        tilename = openfile.readlines()[0].split(' ')[0]\n+        openfile.close()\n+\n+        #instrument\n+        single_image_fitsfile = pipeline_outputs['se_image'][ list( pipeline_outputs['se_image'].keys() ) [0] ]\n+\n+        se_head = FitsServices.read_header(single_image_fitsfile, FitsServices.get_image_hdu(single_image_fitsfile))\n+        instrument = se_head[ 'INSTRUME' ]\n+\n+        #run\n+        run = self.get_run_number(workdir +os.sep +'inputs', self.single_epoch_science_type, return_type = self.ial_run)\n+\n         if 'validation_images' in pipeline_outputs.keys():\n             json_val_list = []\n-            \n-            #tilename\n-            datadir = workdir + os.sep +'data'\n-            tiledef = [_f for _f in os.listdir(datadir) if _f.endswith('.list')][0]\n-            openfile = open(datadir +os.sep +tiledef, 'r')\n-            tilename = openfile.readlines()[0].split(' ')[0]\n-            openfile.close()\n-        \n-            #instrument\n-            single_image_dict = pipeline_outputs['se_image']\n-            single_image_fitsfile = single_image_dict[ list( single_image_dict.keys() ) [0] ]\n-            \n-            se_head = FitsServices.read_header(single_image_fitsfile, FitsServices.get_image_hdu(single_image_fitsfile))\n-            instrument = se_head[ 'INSTRUME' ]\n-            \n-            #run\n-            run = self.get_run_number(workdir +os.sep +'inputs', self.single_epoch_science_type, return_type = self.ial_run)\n-            \n+\n             #xml name\n             coadd_fitsfile = pipeline_outputs['coadds'][ list(pipeline_outputs['coadds'].keys() )[0]]\n             path = coadd_fitsfile.rsplit(os.sep, 1)[0]\n             xml_file = path +os.sep +self.coadd_xml_validation_prefix +instrument +\"_\" +tilename +'.xml'\n-            \n+\n             description = \"Validation plots for: tile: \" +tilename \\\n                          +\"\\ninstrument: \" +instrument \\\n                          +\"\\nall processed bands. \" \\\n                          +\"The plots show the distance of the input images \" \\\n                          +\"objects to the matched objects of the reference catalog and thus test the astrometry of the input images.\"\n-            \n-            evr = ExtValidationReportExporter()\n-            #logging.debug('coadd export fitsfile %s, xml_file %s, run %s', fitsfile, xml_file, run)\n-            success, header, file_list = evr.fill(pipeline_outputs['validation_images'], xml_file, single_image_fitsfile, run, module_name = AstrometricValidation.get_module_name(), description = description, exitcode = 0)\n-    \n-            json_val_list = [xml_file.split(workdir, 1)[-1]]\n+\n+            evr = ExtValidationReportTExporter(pipeline_outputs['validation_images'], description)\n+            file_list = evr.set_all_values(single_image_fitsfile, AstrometricValidation.get_module_name(), run)\n+            evr.saveProductMetadata(xml_file)\n+\n+            json_file = xml_file.split(workdir, 1)[-1]\n+            #make it a relative path\n+            if json_file.startswith(os.sep):\n+                json_file = json_file[1:]\n+            json_val_list = [json_file]\n             if 'validationreport' not in ial_outputs:\n                 ial_outputs['validationreport'] = 'coadd_stage2_pipeline/validationreport.json'\n             #\n@@ -734,35 +903,132 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n             json.dump(json_val_list, open_file)\n             open_file.close()\n         #\n+        if 'background_images' in pipeline_outputs.keys():\n+            description = \"Background images for: tile: \" +tilename \\\n+                         +\"\\ninstrument: \" +instrument \\\n+                         +\"\\nall processed bands. \"\n+\n+            evr = ExtValidationReportTExporter(pipeline_outputs['background_images'], description)\n+            file_list = file_list + evr.set_all_values(single_image_fitsfile, AstrometricValidation.get_module_name(), run)\n+\n+            xml_file = path +os.sep +self.coadd_xml_validation_prefix +\"background_files_\" +tilename +'.xml'\n+            evr.saveProductMetadata(xml_file)\n+\n+            json_file_2 = xml_file.split(workdir, 1)[-1]\n+            open_file = open(self.assure_dirs_exist(workdir +os.sep +ial_outputs['validationreport']), 'w')\n+            json_val_list = [json_file,json_file_2]\n+            json.dump(json_val_list, open_file)\n+            open_file.close()\n+        #\n+\n         #the tilename is hardcoded as tilename_prefix + tileindex\n         #so de-assembling it here is acceptable?\n         tileindex = tilename\n         if len(self.tilename_prefix) > 0:\n             tileindex = tilename.split(self.tilename_prefix)[-1]\n         all_outputfiles = super(IPA_coadd_stage2, self).write_outputs(ial_outputs, pipeline_outputs, workdir, stage = 2, tileindex = tileindex)\n-        \n-        #add the renamed validation file list to all outputs\n+        #all_outputfiles = self.write_coadd_outputs(ial_outputs, pipeline_outputs, workdir, run, tileindex)\n+\n+        #add the validation file list to all outputs\n         all_outputfiles = all_outputfiles + file_list\n-        \n+\n         return all_outputfiles\n     #\n-    \n-    \n+\n+    #relocated to EXT_PF1_GEN_P2_LIBS\n+    def write_coadd_outputs(self, ial_outputs, pipeline_outputs, workdir, run, tileindex):\n+        \"\"\"\n+        find matching outputs and write them to files\n+\n+        \"\"\"\n+        #MER requirements:\n+        #turn weights into rms and store uncompressed\n+        pipeline_object = rdisk_coadd.RDisk_coadd()\n+        pipeline_object.setLogger(open(os.devnull,\"w\"))\n+        pipeline_outputs['coadds'] = pipeline_object.add_rms_and_mask(pipeline_outputs['coadds'], compress = False)\n+\n+        exitcode = pipeline_outputs['exitcode']\n+\n+        #cd into the workdir, so relative paths will work\n+        json_coadd_list = []\n+        json_catalog_list = []\n+        all_outputfiles = []\n+\n+        for band in pipeline_outputs['coadds']:\n+            #create an xml instance in the same directory as the fitsfile\n+            fitsfile = pipeline_outputs['coadds'][band]\n+            new_fitsname = fitsfile #initialize for catalog, if the coadd xml is not created\n+\n+            if band in pipeline_outputs['psfs']:\n+                dce = ExtStackedFrameTExporter(fitsfile)\n+                new_fitsname, new_psf_fitsname = dce.set_all_values(pipeline_outputs['se_image'][band], pipeline_outputs['psfs'][band], tileindex, run)\n+\n+                path = fitsfile.rsplit(os.sep, 1)[0]\n+                xml_file = path +os.sep +dce.product_type +\"_\" +band +'.xml'\n+                dce.saveProductMetadata(xml_file)\n+\n+                json_coadd_list.append(xml_file.split(workdir, 1)[-1])\n+                all_outputfiles.append(new_fitsname)\n+                all_outputfiles.append(new_psf_fitsname)\n+            #\n+\n+            #catalogs\n+            if band in pipeline_outputs['coadd_catalogs']:\n+                dcce = ExtCoaddSourceCatalogTExporter(pipeline_outputs['coadd_catalogs'][band])\n+                new_cat_fitsname = dcce.set_all_values(pipeline_outputs['se_image'][band], new_fitsname, run)\n+\n+                path = pipeline_outputs['coadd_catalogs'][band].rsplit(os.sep, 1)[0]\n+                xml_file = path +os.sep +dcce.product_type +\"_\" +band +'.xml'\n+                dcce.saveProductMetadata(xml_file)\n+\n+                json_catalog_list.append(xml_file.split(workdir, 1)[-1])\n+                all_outputfiles.append(new_cat_fitsname)\n+            #\n+        #\n+\n+        #write the json_coadd_list into the workdir\n+        #json output must be a string as the pipeline runner\n+        #tries to apply string methods to it\n+        if 'coadds' not in ial_outputs:\n+            ial_outputs['coadds'] = 'coadd_stage2_pipeline/coadds.json'\n+        #\n+        open_file = open(self.assure_dirs_exist(workdir +os.sep +ial_outputs['coadds']), 'w')\n+        json.dump(json_coadd_list, open_file)\n+        open_file.close()\n+\n+\n+        if 'coadd_catalogs' not in ial_outputs:\n+            #this may happen in stage 2\n+            ial_outputs['coadd_catalogs'] = 'coadd_stage2_pipeline/coadd_catalogs.json'\n+        #\n+        if len(json_catalog_list) > 0:\n+            open_file = open(self.assure_dirs_exist(workdir +os.sep +ial_outputs['coadd_catalogs']), 'w')\n+            json.dump(json_catalog_list, open_file)\n+            open_file.close()\n+        #\n+\n+        print(\"outputlists written to file \", path)\n+        print(\"catalogs \", json_catalog_list)\n+        print(\"coadds \", json_coadd_list)\n+\n+        return all_outputfiles\n+    #\n+\n     def xml_export(self, rundir):\n         \"\"\"\n-        The method starts a manual xml export of the output files and \n+        The method starts a manual xml export of the output files and\n         moves them to the ial output directories.\n-        \n+\n         If the xml export at the end of the pipeline fails for whatever reason, the pipeline should not be run\n         again, only to debug the code or generate the xml files.\n-        Especially, if more than one exposure is executed, failure of the xml export \n+        Especially, if more than one exposure is executed, failure of the xml export\n         at the end of the processing cannot justify reprocessing.\n-        \n+\n         parameters:\n-        rundir: the DES runtime directory. It is one level below the ial workdir. Since the method infers the workdir \n-        from the rundir, it is essential that the relation is kept (which is guaranteed, if the data has previously \n+        rundir: the DES runtime directory. It is one level below the ial workdir. Since the method infers the workdir\n+        from the rundir, it is essential that the relation is kept (which is guaranteed, if the data has previously\n         been processed with the pipeline and only the xml export is done manually).\n-        \n+\n         returns:\n         nothing\n         \"\"\"\n@@ -780,59 +1046,64 @@ class IPA_coadd_stage2(IAL_Pipeline_Assembler_coadd.IPA_coadd):\n         openfile = open(datadir +os.sep +tiledef, 'r')\n         tilename = openfile.readlines()[0].split(' ')[0]\n         openfile.close()\n-        \n+\n         #get the bands from the coadds\n-        bands = [_f.split(tilename +'_')[-1].split('.fits')[0] for _f in os.listdir(coadd_out) if _f.startswith(tilename +'_') and (_f.endswith('.fits.fz') or _f.endswith('.fits')) and 'stackedPSF' not in _f and 'cat.fits' not in _f and '_det.fits' not in _f ]\n+        bands = [_f.split(tilename +'_')[-1].split('.fits')[0] for _f in os.listdir(coadd_out) if _f.startswith(tilename +'_') and (_f.endswith('.fits.fz') or _f.endswith('.fits')) \\\n+            and 'stackedPSF' not in _f and 'cat.fits' not in _f and '_det.fits' not in _f and '_mobjects.fits' not in _f]\n         print(\"bands \", bands)\n         logging.debug('coadd_out %s, tilename %s, bands %s', coadd_out, tilename, bands)\n-        \n+\n         #browse the coadd - outdir for products\n         pipeline_object = rdisk_coadd.RDisk_coadd_stage2()\n         pipeline_object.setLogger(open(os.devnull,\"w\"))\n         coadds, catalogs, psfs = pipeline_object.get_outputs(coadd_out, tilename, bands, stage = 2)\n-        \n+        print(\"\\ngot outputs:\")\n+        print(\"coadds \", coadds)\n+        print(\"catalogs \", catalogs)\n+        print(\"psfs \", psfs)\n+        print()\n+\n         #get an se image\n         bandlist = pipeline_object.separate_band_from_src(workdir +os.sep +\"coaddswarp.list\", bandindex = 1, maxsplit=3)\n         se_image = pipeline_object.get_se_images(bandlist)\n-        #se_image = datadir + os.sep + [_f for _f in os.listdir(datadir) if (_f.startswith('DECam_') or _f.startswith('EUC_EXT_DPDEXTSINGLEEPOCHFRAME') or _f.startswith(\"KID_\")) and (_f.endswith(\".fits.fz\") or _f.endswith(\".fits\")) and not _f.endswith(\"_cat.fits\") ][0]\n         print(\"se_image = \", se_image)\n-        \n+\n         #and the validation images\n-        #validation_images = [ rundir + os.sep +\"validation_plots\" + os.sep +jpg for jpg in os.listdir( rundir + os.sep +\"validation_plots\") if jpg.endswith(\".tar.gz\")]\n         validation_images = pipeline_object.get_validation_data(rundir + os.sep +\"validation_plots\")\n-        \n+\n         pipeline_outputs = pipeline_object.test_output_integrity({'coadds':coadds, 'coadd_catalogs': catalogs, 'psfs':psfs})\n         coaddswarp = workdir +os.sep +\"coaddswarp.list\"\n         bandlists = pipeline_object.separate_band_from_src(coaddswarp, bandindex = 1, maxsplit=3)\n-        \n+\n         pipeline_outputs['se_image'] = pipeline_object.get_se_images( bandlists)\n         pipeline_outputs['validation_images'] = validation_images\n         pipeline_outputs['exitcode'] = 0\n-        \n+        pipeline_outputs = pipeline_object.put_background_data_to_outputs(rundir +os.sep +\"bgsub\", pipeline_outputs, tar = True, gzip = True, save_backgrounds_for_validation = save_backgrounds_for_validation)\n+\n         \"\"\"\n-        pipeline_outputs = {'coadds': coadds, \n-                         'det_image': \"\", \n-                    'coadd_catalogs': catalogs, \n-                              'psfs': psfs, \n+        pipeline_outputs = {'coadds': coadds,\n+                         'det_image': \"\",\n+                    'coadd_catalogs': catalogs,\n+                              'psfs': psfs,\n                           'se_image': se_image,\n                           'validation_images': validation_images,\n                           'exitcode':0} #well, assume ...\n         \"\"\"\n         #2. ial_outputs are static\n-        ial_outputs = {'coadds':'coadd_pipeline/coadds.json', 'coadd_catalogs':'coadd_pipeline/coadd_catalogs.json'}        \n-        \n-        \n+        ial_outputs = {'coadds':'coadd_pipeline/coadds.json', 'coadd_catalogs':'coadd_pipeline/coadd_catalogs.json'}\n+\n+\n         #log the results\n         logging.debug('workdir = %s', workdir)\n         logging.debug('ial_outputs = %s', ial_outputs)\n-        logging.debug('pipeline outputs', pipeline_outputs)\n-        \n-        \n+        logging.debug('pipeline outputs %s', pipeline_outputs)\n+\n+\n         print('workdir = ', workdir)\n         print('ial_outputs = ', ial_outputs)\n         print('pipeline outputs', pipeline_outputs)\n-        \n-        \n+\n+\n         #all_outputs = self.write_outputs(ial_outputs, pipeline_outputs, workdir, stage = 2)\n         #self.move_outputs_to_ial_output(all_outputs, workdir)\n         return self.write_outputs(ial_outputs, pipeline_outputs, workdir)\n@@ -851,13 +1122,6 @@ def mainMethod(args):\n #\n \n \n-def set_stage2__exporter_classes(_class):\n-    _class.coadd_exporter_class = ExtStackedFrameExporter\n-    _class.coadd_xml_prefix = \"DpdExtStackedFrame_\"\n-    _class.catalog_exporter_class = ExtSourceCatalogExporter\n-    _class.catalog_xml_prefix = \"DpdExtSourceCatalog_\"\n-#\n-\n if __name__ == '__main__':\n     ipa = IPA_coadd_stage2()\n     ipa.execute()\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/pybin/StackPsfExec.py": [
                        [
                            "@@ -3,7 +3,7 @@\n import sys\n import os\n import argparse\n-from EXT_PF1_Coadd.Pipeline import stackPSF\n+from EXT_PF1_GEN_P2.psf import stackPSF\n \n \n \n@@ -36,15 +36,18 @@ def defineSpecificProgramOptions():\n                            help = 'path to the catalogue containing the \\\n                            positions on the sky on  which we want to compute \\\n                            the stacked PSFs')\n-    #argparser.add_argument(\"--sort_catalog_by_mag\", dest = 'sort_catalog_by_mag', action = \n-    #                       'store_const',     const = True,  default = False,  \n-    #                       help = 'sorts the refence catalog by magnitude and thus puts \\\n-    #                       the brightest objects preferrably on the first layers.')\n+    argparser.add_argument(\"--sort_catalog_by_mag\", dest = 'sort_catalog_by_mag', action = \n+                           'store_const',     const = True,  default = False,  \n+                           help = 'sorts the refence catalog by magnitude and thus puts \\\n+                           the brightest objects preferrably on the first layers.')\n     argparser.add_argument(\"--mag_entry\",  type = str, default = 'MAG_AUTO', \n                            help = 'magnitude entry in the position catalog.')\n-    argparser.add_argument(\"--ref_mag\",  type = float, default = 25, \n+    argparser.add_argument(\"--ref_mag\",  type = float, default = 31.0, \n                            help = 'magnitude, where the position catalog is cut as \\\n                            this projects data is at the depths limit.')\n+    argparser.add_argument(\"--min_mag\",  type = float, default = 12.0, \n+                           help = 'magnitude, where the position catalog is cut at \\\n+                           the bright end to exclude possible artefacts.')                       \n     argparser.add_argument(\"--ra\",            type = str, default = 'RA', \n                            help = 'name of the catalogue field containing ra \\\n                            positions')\n@@ -64,6 +67,9 @@ def defineSpecificProgramOptions():\n     argparser.add_argument(\"--ramdisk\",       type = str, default = \n                            '/dev/shm/EuclidPipeline', help = 'the ramdisk \\\n                            directory')\n+    argparser.add_argument(\"--resampledir\",       type = str, default =\n+                           '/dev/shm/EuclidPipeline', help = 'the swarp \\\n+                           resample directory')\n     argparser.add_argument(\"--coadd\",         type = str, default = '', \n                            help = 'path to the coadd tile. Needed only for \\\n                            the metadata, can be replaced to a query to the \\\n@@ -88,10 +94,9 @@ def defineSpecificProgramOptions():\n                            in the final PSF product. If set to 0, the code \\\n                            will use the maximum value from the layer \\\n                            computation')\n-    argparser.add_argument(\"--min_zp\",        type = float, default = 24.5, \n-                           help = 'Minimum value of the zero point to be used \\\n-                           in the analysis. Single images with lower zero \\\n-                           points will be dropped')    \n+    argparser.add_argument(\"--coadd_zp\", type = float, default = 30.0, \n+                           help = 'Value of the coadd zeropoint. This is where the \\\n+                           coadded simulated psfs are flux-scaled to')\n     argparser.add_argument(\"--max_zp\",        type = float, default = 25.5, \n                            help = 'Maximum value of the zero point to be used \\\n                            in the analysis. Single images with higher zero \\\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -67,6 +67,9 @@ def defineSpecificProgramOptions():\n     argparser.add_argument(\"--ramdisk\",       type = str, default = \n                            '/dev/shm/EuclidPipeline', help = 'the ramdisk \\\n                            directory')\n+    argparser.add_argument(\"--resampledir\",       type = str, default =\n+                           '/dev/shm/EuclidPipeline', help = 'the swarp \\\n+                           resample directory')\n     argparser.add_argument(\"--coadd\",         type = str, default = '', \n                            help = 'path to the coadd tile. Needed only for \\\n                            the metadata, can be replaced to a query to the \\\n",
                            "add a separate folder for swarp temp (took it out of the general temp)",
                            "Michael",
                            "2023-05-16T20:42:42.000+02:00",
                            "c148ca2fc218a328e8f62903fa9cec79bd5a5489"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/pybin/__init__.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestAstrometricValidation_test.py": [
                        [
                            "@@ -14,9 +14,11 @@ You should have received a copy of the GNU Lesser General Public License along w\n the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n \n \"\"\"\n-from EXT_PF1_Coadd.AstrometricValidation import AstrometricValidation\n-from EXT_PF1_Coadd.Pipeline import RDisk_coadd_stage2\n+from EXT_PF1_GEN_P2.AstrometricValidation import AstrometricValidation\n+from EXT_PF1_GEN_P2.Pipeline import RDisk_coadd_stage2\n from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+from EXT_PF1_GEN_P2_LIBS.test import Testutils\n+\n from astropy.io import fits\n from astropy.wcs import WCS\n from astropy.table import Table\n@@ -49,6 +51,26 @@ class TestAstrometricValidation(unittest.TestCase):\n         self.assertTrue(1)\n     def test_plot_distances(self):\n         self.assertTrue(1)\n+    def test_collect_catalogs(self):\n+        #make a catalog file\n+        cat = os.getcwd() +os.sep +\"catalog_prefix_andmore.fits\"\n+        f = open(cat, 'w')\n+        f.write(\"\")\n+        f.close()\n+        \n+        av = AstrometricValidation.AstrometricValidation(self.se, self.se)\n+        av.catalog_prefix = \"catalog_prefix\"\n+        \n+        catalogs = av.collect_catalogs(os.getcwd())\n+        self.assertTrue(cat in catalogs)\n+        \n+        os.remove(cat)\n+    def test_match_names(self):\n+        files = [\"file1_postix.fits\", \"file2_postix.fits\"]\n+        matches = AstrometricValidation.AstrometricValidation.match_names(files, reference_files = [\"file1\"])\n+        self.assertTrue(\"file1_postix.fits\" in matches)\n+        self.assertTrue(\"file2_postix.fits\" not in matches)\n+    #\n     def manual_only_test_add_ra_dec_to_table(self):\n         se_cat = Table.read(self.se, \"LDAC_OBJECTS\", format=\"fits\")\n         av = AstrometricValidation.AstrometricValidation(self.se, self.se)\n@@ -76,6 +98,39 @@ class TestAstrometricValidation(unittest.TestCase):\n         \n         self.assertTrue(len(paths) > 0)\n     #  \n+    def test_astropy_validation(self):\n+        #make a se_cat\n+        se_cat = Table()\n+        se_cat[\"X\"] = [131.8720020368789,24.050439198883033,13.43767590792685,143.96421519542167,174.9245910295104,233.1334865496713,206.3554889082032]\n+        se_cat[\"Y\"] = [1751.8178512754296,1931.3853403774167,2942.03078639144,3716.9960341111178,3349.8325332090835,3912.4864711834507,3927.676488919787]\n+        Testutils.makeFitsTable(\"se_cat.fits\", se_cat)\n+                                                                                                                                               \n+        ref_cat = Table()\n+        ref_cat[\"X_WORLD\"] = [10.66989630211617,10.683786064286625,10.761788011917952,10.821594132016973,10.793228691717383,10.836675783082441,10.837854617137639]\n+        ref_cat[\"Y_WORLD\"] = [-19.040391491464295,-19.032618255727733,-19.032379480504652,-19.04229166390956,-19.044376517994188,-19.04889997807353,-19.046947715039966]\n+        ref_cat[\"MAG_AUTO\"] = len(ref_cat[\"Y_WORLD\"])*[18.0]\n+        Testutils.makeFitsTable(\"ref_cat.fits\", ref_cat)\n+        \n+        image_header = {\"CTYPE1\":'RA---TAN', \"CTYPE2\":'DEC--TAN',\n+                        \"CRVAL1\": 11.35107900487, \"CRVAL2\": -18.53241834894,\n+                        \"CRPIX1\":  -6866.4, \"CRPIX2\": 10566.67,\n+                        \"CD1_1\": -1.336910078583e-07, \"CD1_2\": 7.285929972108e-05,\n+                        \"CD2_1\":-7.285479866869e-05, \"CD2_2\":-1.294589545437e-07,\n+                        \"NAXIS1\":2048, \"NAXIS2\":4096}\n+        \n+        Testutils.makeFitsImage(path = \"./image.fits\")\n+        Testutils.updateFitsHeader(\"./image.fits\", image_header, hdu=0)\n+        \n+        \n+        av = AstrometricValidation.AstrometricValidation(\"se_cat.fits\", \"ref_cat.fits\")\n+        av.range = [1.0,5.0] #enlarge allowed distances for this example\n+        d2d = av.astropy_validation(\"X\", \"Y\", \"./image.fits\", band = \"MAG_AUTO\")\n+        \n+        self.assertTrue(len(d2d) > 0)\n+        os.remove(\"se_cat.fits\")\n+        os.remove(\"ref_cat.fits\")\n+        os.remove(\"./image.fits\")\n+    #  \n #\n \n if __name__ == '__main__':\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestCatalogMatchFinder_test.py": [
                        [
                            "@@ -0,0 +1,48 @@\n+import os\n+import numpy as np\n+from astropy.table import Table, Column\n+from EXT_PF1_GEN_P2.Fwhm import CatalogMatchFinder\n+import unittest\n+\n+class TestCatalogMatchFinder(unittest.TestCase):\n+    def setUp(self):\n+        self.cmf = CatalogMatchFinder.CatalogMatchFinder()\n+        self.tableids = [[0,0,[],\"2\", \"matched ra\", \"matched dec\", \"distance arcsecs\", \"stackpsf_idx\", \"catalog object id\"],\n+                    [0,0,[],\"2\", \"matched ra\", \"matched dec\", \"distance arcsecs\", \"stackpsf_idx\", \"catalog object id\"],\n+                    [0.1,89.0,[],\"3\", \"matched ra\", \"matched dec\", \"distance arcsecs\", \"stackpsf_idx\", \"catalog object id\"],\n+                    [0.2,30.0,[],\"3\", \"matched ra\", \"matched dec\", \"distance arcsecs\", \"stackpsf_idx\", \"catalog object id\"],\n+                    [0.3,31.0,[],\"3\", \"matched ra\", \"matched dec\", \"distance arcsecs\", \"stackpsf_idx\", \"catalog object id\"],\n+                    [0.3,45.0,[],\"3\", \"matched ra\", \"matched dec\", \"distance arcsecs\", \"stackpsf_idx\", \"catalog object id\"]\n+                    ]\n+        #make a layertable    \n+        self.ra = [0.099]\n+        self.dec = [87.0]                   \n+        self._id = [21]\n+        c = Column(data = [self.ra,self.dec,self._id], name = \"3\")\n+        self.layertable = Table()\n+        self.layertable.add_column(c, name = \"3\")\n+    def tearDown(self):\n+        pass\n+    def test_find_matches(self):\n+        tids = self.cmf.find_matches(self.layertable, self.tableids)\n+                             \n+        self.assertTrue( tids[2][4] == self.ra[0])\n+        self.assertTrue( tids[2][5] == self.dec[0])\n+        self.assertTrue( tids[2][7] == self._id[0])\n+    def test_write(self):\n+        outfile = \"./outfile.fits\"\n+        self.cmf.write(self.tableids, outfile)\n+        self.assertTrue(os.path.exists(outfile))\n+        os.remove(outfile)\n+    def test_find_nearest_matches_only(self):\n+        tids = self.cmf.find_nearest_matches_only(self.layertable, self.tableids, _write = False)\n+        self.assertTrue( tids[\"nearest stackpsf ra\"].data[0] == self.ra[0])\n+        self.assertTrue( tids[\"nearest stackpsf dec\"].data[0] == self.dec[0])\n+        \n+        samestackset = tids[\"same stackset\"].data\n+        for i in range(samestackset.shape[0]):\n+            self.assertTrue( samestackset[i] == (self.tableids[i][3] == self.layertable.colnames[0]) )\n+        #\n+    #\n+#\n+#\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestCatalogPsfExtractor_test.py": [
                        [
                            "@@ -0,0 +1,55 @@\n+import os\n+import numpy as np\n+from astropy.io.fits import HDUList\n+from EXT_PF1_GEN_P2.Fwhm import CatalogPsfExtractor\n+import unittest\n+\n+class TestCatalogPsfExtractor(unittest.TestCase):\n+    def setUp(self):\n+        self.cpe = CatalogPsfExtractor.CatalogPsfExtractor()\n+        self.tableids = [(0,0,0,0,\"2\"), (1,0,0,0,\"2\"), (2,0.1,89.0,0,\"3\"), (3,0.1,89.0,0,\"3\"), (4,0.1,89.0,0,\"3\"), (5,0.1,89.0,0,\"3\")]\n+        \n+    def tearDown(self):\n+        pass\n+    def test_maximum_number_of_objects_on_layer(self):\n+        self.assertTrue( self.cpe.maximum_number_of_objects_on_layer(self.tableids) == 4)\n+    #\n+    def test_make_tables(self):\n+        table_dict = self.cpe.make_tables(self.tableids)\n+        keys= list(table_dict.keys())\n+        self.assertTrue( len(keys) == 1)        \n+        self.assertTrue( len(table_dict[keys[0]].colnames) == 2)\n+                \n+        data2 = table_dict[keys[0]][\"2\"].data\n+        self.assertTrue( len(data2[0]) == 2)\n+        self.assertTrue( data2[0][0] == 0)\n+        self.assertTrue( data2[1][0] == 0)\n+        \n+        \n+        data3 = table_dict[keys[0]][\"3\"].data\n+        self.assertTrue( len(data3[0]) == 4)\n+        self.assertTrue( data3[0][0] == 0.1)\n+        self.assertTrue( data3[1][0] == 89.0)\n+    def test_write(self):\n+        outfile = \"./outfile.fits\"\n+        self.cpe.write(HDUList(), self.cpe.make_tables(self.tableids), outfile)\n+        self.assertTrue(os.path.exists(outfile))\n+        os.remove(outfile)\n+    def test_read(self):\n+        outfile = \"./outfile.fits\"\n+        self.cpe.write(HDUList(), self.cpe.make_tables(self.tableids), outfile)\n+        wcs_dict, layertable = self.cpe.read(outfile)\n+        os.remove(outfile)\n+        \n+        #we have no wcs\n+        self.assertTrue(len(wcs_dict) == 0)\n+        \n+        #redo the test from make_tables\n+        data3 = layertable[\"3\"].data\n+        self.assertTrue( len(data3[0]) == 4)\n+        self.assertTrue( data3[0][0] == 0.1)\n+        self.assertTrue( data3[1][0] == 89.0)\n+    #\n+\n+\n+#\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestGenerator_test.py": [
                        [
                            "@@ -1,6 +1,7 @@\n import os\n from importlib import __import__\n import inspect\n+import EXT_PF1_GEN_P2\n \n class TestGenerator:\n     \"\"\"\n@@ -99,7 +100,7 @@ class TestGenerator:\n         for ff in f:\n             print(ff[0], ff[1].__module__)\n             try:\n-                if (type(_class).__name__   in ff[1].__module__ or _class.__module__ in ff[1].__module__) and ff[0] is not \"__init__\": \n+                if (type(_class).__name__   in ff[1].__module__ or _class.__module__ in ff[1].__module__) and ff[0] != \"__init__\": \n                     method_names.append(ff[0])\n                     print(ff[0], ff[1].__module__)\n                 #\n@@ -111,8 +112,7 @@ class TestGenerator:\n \n \n def instantiate_stackpsf():\n-    import EXT_PF1_Coadd\n-    from EXT_PF1_Coadd.Pipeline import stackPSF\n+    from EXT_PF1_GEN_P2.psf import stackPSF\n     class args:\n         def __init__(self):\n             pass\n@@ -135,66 +135,60 @@ def instantiate_stackpsf():\n         SElist = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC456/coadd_40012_20190314_des/coaddswarp.list\"\n         position_cat = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC456/coadd_40012_20190314_des//data/EUC_VIS_SWL-CAT-000-000000-0000000__20190330T021335.8Z_00.00_26mag.fits\"\n     #\n-    _class = EXT_PF1_Coadd.Pipeline.stackPSF.StackPSF(args)\n+    _class = EXT_PF1_GEN_P2.Pipeline.stackPSF.StackPSF(args)\n     return _class\n #\n def instantiate_WcsUtils():\n-    import EXT_PF1_Coadd\n-    from EXT_PF1_Coadd.Pipeline import stackPSF\n+    from EXT_PF1_GEN_P2.psf import stackPSF\n     \n-    _class = EXT_PF1_Coadd.Pipeline.stackPSF.WcsUtils({\"key\":\"value\"})\n+    _class = EXT_PF1_GEN_P2.Pipeline.stackPSF.WcsUtils({\"key\":\"value\"})\n     return _class\n #\n \n def instantiate_OutputStackedPSF():\n-    import EXT_PF1_Coadd\n-    from EXT_PF1_Coadd.Pipeline import stackPSF\n+    from EXT_PF1_GEN_P2.psf import stackPSF\n     \n-    _class = EXT_PF1_Coadd.Pipeline.stackPSF.OutputStackedPSF(1, 50, \"./\", \"\", \"g\")\n+    _class = EXT_PF1_GEN_P2.Pipeline.stackPSF.OutputStackedPSF(1, 50, \"./\", \"\", \"g\")\n     return _class\n #\n def instantiate_ObjectList():\n-    import EXT_PF1_Coadd\n-    from EXT_PF1_Coadd.Pipeline import stackPSF\n+    from EXT_PF1_GEN_P2.psf import stackPSF\n     \n-    _class = EXT_PF1_Coadd.Pipeline.stackPSF.ObjectList([25])\n+    _class = EXT_PF1_GEN_P2.Pipeline.stackPSF.ObjectList([25])\n     return _class\n def instantiate_PsfStampFilter():\n-    import EXT_PF1_Coadd\n-    from EXT_PF1_Coadd.Pipeline import stackPSF\n+    from EXT_PF1_GEN_P2.psf import stackPSF\n     \n-    _class = EXT_PF1_Coadd.Pipeline.stackPSF.PsfStampFilter(1)\n+    _class = EXT_PF1_GEN_P2.Pipeline.stackPSF.PsfStampFilter(1)\n     return _class\n def instantiate_IAL_Pipeline_Assembler_coadd_stage2():\n-    import EXT_PF1_Coadd\n-    from EXT_PF1_Coadd import pybin\n-    from EXT_PF1_Coadd.pybin import IAL_Pipeline_Assembler_coadd_stage2\n-    return EXT_PF1_Coadd.pybin.IAL_Pipeline_Assembler_coadd_stage2.IPA_coadd_stage2()\n-\n+    from EXT_PF1_GEN_P2.pybin import IAL_Pipeline_Assembler_coadd_stage2\n+    return EXT_PF1_GEN_P2.pybin.IAL_Pipeline_Assembler_coadd_stage2.IPA_coadd_stage2()\n+def instantiate_ObjectList():\n+    from EXT_PF1_GEN_P2.psf import ObjectList\n+    return EXT_PF1_GEN_P2.psf.ObjectList.ObjectList(None)\n+    \n if __name__ == '__main__':\n     tg = TestGenerator(\"Pipeline\")\n-    \n-    import EXT_PF1_Coadd\n-    from EXT_PF1_Coadd import Pipeline\n-    from EXT_PF1_Coadd.Pipeline import RDisk_coadd_stage2\n-    _class = EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2.RDisk_coadd_stage2()\n+    \"\"\"\n+    from EXT_PF1_GEN_P2.Pipeline import RDisk_coadd_stage2\n+    _class = EXT_PF1_GEN_P2.Pipeline.RDisk_coadd_stage2.RDisk_coadd_stage2()\n     classname = \"RDisk_coadd_stage2\"\n     \n-    from EXT_PF1_Coadd.Pipeline import SigmaClip\n-    _class = EXT_PF1_Coadd.Pipeline.SigmaClip.SigmaClip([],[])\n+    from EXT_PF1_GEN_P2.Pipeline import SigmaClip\n+    _class = EXT_PF1_GEN_P2.Pipeline.SigmaClip.SigmaClip([],[])\n     classname = \"SigmaClip\"\n     \n-    \n-    \"\"\"\n-    from EXT_PF1_Coadd import AstrometricValidation\n-    from EXT_PF1_Coadd.AstrometricValidation import AstrometricValidation\n+    from  import AstrometricValidation\n+    from .AstrometricValidation import AstrometricValidation\n     _class = AstrometricValidation.AstrometricValidation(\"\",\"\")\n     classname = \"AstrometricValidation\"\n     \n-    from EXT_PF1_Coadd.file import PSFUtils\n+    from .file import PSFUtils\n     _class = PSFUtils.PSFUtils(\" \")\n     classname = \"PSFUtils\"\n     \"\"\"\n+        \n     \n     _class = instantiate_stackpsf()\n     _class = instantiate_WcsUtils()\n@@ -202,6 +196,7 @@ if __name__ == '__main__':\n     _class = instantiate_ObjectList()\n     _class = instantiate_PsfStampFilter()\n     _class = instantiate_IAL_Pipeline_Assembler_coadd_stage2()\n+    _class = instantiate_ObjectList()\n     testclass = \"Test\" +type(_class).__name__ +\".py\"\n     method_names = tg.list_methods_in_class(_class)\n     print(method_names)\n@@ -209,13 +204,13 @@ if __name__ == '__main__':\n     \n     testclasses = [f for f in os.listdir(os.getcwd()) if f.startswith(\"Test\") and f.endswith('py')]\n     if testclass in testclasses:\n-        code = tg.add_to_class(testclass, classname, method_names)\n+        code = tg.add_to_class(testclass, str(_class), method_names)\n     else:\n-        code = tg.build_class(classname, method_names)\n+        code = tg.build_class( str(_class), method_names)\n     f = open(testclass, 'w')\n     f.write(code)\n     f.close()\n-    #tg.list_classes_in_package(\"EXT_PF1_Coadd\", _from = [\"EXT_PF1_Coadd.Pipeline.RDisk_coadd_stage2\", \"RDisk_coadd_stage2\"])\n+    #tg.list_classes_in_package(\"\", _from = [\".Pipeline.RDisk_coadd_stage2\", \"RDisk_coadd_stage2\"])\n     \n #\n \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestIPA_coadd_stage2.py": [
                        [
                            "@@ -1,33 +0,0 @@\n-import unittest\n-import os\n-from EXT_PF1_Coadd.pybin import IAL_Pipeline_Assembler_coadd_stage2\n-\n-class TestIPA_coadd_stage2(unittest.TestCase):\n-    def setUp(self):\n-        self.ipa = IAL_Pipeline_Assembler_coadd_stage2.IPA_coadd_stage2()\n-    def tearDown(self):\n-        pass\n-    def test_copy_wcs_from_xml_to_header(self):\n-        self.assertTrue(1)\n-    def test_get_property_template_path(self):\n-        self.assertTrue(1)\n-    def test_get_sourcelist_tags(self):\n-        self.assertTrue(1)\n-    def test_merge_vis_catalogs(self):\n-        self.assertTrue(1)\n-    def test_prepare_coadd_input_files(self):\n-        self.assertTrue(1)\n-    def test_printout_db_ids(self):\n-        self.assertTrue(1)\n-    def test_undo_euclid_filenaming_convention(self):\n-        self.assertTrue(1)\n-    def test_write_outputs(self):\n-        self.assertTrue(1)\n-    def test_xml_export(self):\n-        self.assertTrue(1)\n-    def sleep_test_merge_mer_star_galaxy_catalogs(self):\n-        workdir = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/launcher/coadd__20200805181035_testcopy/\"\n-        coadd_files = {'tileinfo_xml_file':\"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/launcher/coadd__20200805181035_testcopy/inputs/Tile_40012.xml\"}\n-        catalog_filename = \"merged_mer_catalog.fits\"\n-        self.ipa.merge_mer_star_galaxy_catalogs(workdir, coadd_files, catalog_filename, magnitude_cut = 26)\n-    #\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestIPA_coadd_stage2_test.py": [
                        [
                            "@@ -0,0 +1,182 @@\n+import unittest\n+import os\n+from EXT_PF1_GEN_P2_LIBS.test import Testutils\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+\n+from EXT_PF1_GEN_P2.pybin import IAL_Pipeline_Assembler_coadd_stage2\n+\n+class TestIPA_coadd_stage2(unittest.TestCase):\n+    def setUp(self):\n+        self.ipa = IAL_Pipeline_Assembler_coadd_stage2.IPA_coadd_stage2()\n+        \n+        self.xml_contents = \"<Data> \\\n+          <ImgType> \\\n+           <Category>SCIENCE</Category> \\\n+           <FirstType>OBJECT</FirstType> \\\n+           <SecondType>SKY</SecondType> \\\n+           <ThirdType>DEEP</ThirdType> \\\n+           <Technique>IMAGE</Technique> \\\n+          </ImgType> \\\n+          <ImgNumber>1</ImgNumber> \\\n+          <AxisNumber>2</AxisNumber> \\\n+          <AxisLengths>2048 4096</AxisLengths> \\\n+          <DataSize>8</DataSize> \\\n+          <DataLength>8388608</DataLength> \\\n+          <Instrument> \\\n+           <InstrumentName>DECAM</InstrumentName> \\\n+           <TelescopeName>BLANCO</TelescopeName> \\\n+           <ObservatoryName>CTIO</ObservatoryName> \\\n+           <Longitude>70.81489</Longitude> \\\n+           <Latitude>-30.16606</Latitude> \\\n+           <Elevation>2215.0</Elevation> \\\n+           <Timezone>-4.0</Timezone> \\\n+          </Instrument> \\\n+          <Filter> \\\n+           <Name>DECAM_g</Name> \\\n+          </Filter> \\\n+          <Detector>DECam_ccd_39</Detector> \\\n+          <WCS> \\\n+           <CTYPE1> \\\n+            <CoordinateType>RA</CoordinateType> \\\n+            <ProjectionType>TPV</ProjectionType> \\\n+           </CTYPE1> \\\n+           <CTYPE2> \\\n+            <CoordinateType>DEC</CoordinateType> \\\n+            <ProjectionType>TPV</ProjectionType> \\\n+           </CTYPE2> \\\n+           <CRVAL1>227.50678926</CRVAL1> \\\n+           <CRVAL2>26.3874745713</CRVAL2> \\\n+           <CRPIX1>-2357.6</CRPIX1> \\\n+           <CRPIX2>12696.33</CRPIX2> \\\n+           <CD1_1>-1.3382525475e-07</CD1_1> \\\n+           <CD1_2>7.28593436368e-05</CD1_2> \\\n+           <CD2_1>-7.28547806884e-05</CD2_1> \\\n+           <CD2_2>-1.29434379809e-07</CD2_2> \\\n+          </WCS> \\\n+          <Zeropoint> \\\n+           <Value>30.1899452209</Value> \\\n+           <Error>0.00767249520868</Error> \\\n+          </Zeropoint> \\\n+          <FWHM>6.13621902</FWHM> \\\n+          <Gain> \\\n+            <Product> \\\n+                <AmpName>Gain_A</AmpName> \\\n+                <GainValue>0</GainValue> \\\n+                <XRange> \\\n+                    <Start>0</Start> \\\n+                    <End>1024</End> \\\n+                    <Step>1</Step> \\\n+                </XRange> \\\n+                <YRange> \\\n+                    <Start>0</Start> \\\n+                    <End>2047</End> \\\n+                    <Step>1</Step> \\\n+                </YRange> \\\n+            </Product> \\\n+            <Product> \\\n+                <AmpName>Gain_B</AmpName> \\\n+                <GainValue>1</GainValue> \\\n+                <XRange> \\\n+                    <Start>1025</Start> \\\n+                    <End>2048</End> \\\n+                    <Step>1</Step> \\\n+                </XRange> \\\n+                <YRange> \\\n+                    <Start>2048</Start> \\\n+                    <End>4096</End> \\\n+                    <Step>1</Step> \\\n+                </YRange> \\\n+            </Product> \\\n+        </Gain> \\\n+        </Data>\"\n+        \n+    def tearDown(self):\n+        pass\n+    def test_copy_wcs_from_xml_to_header(self):\n+        \n+        fitsfile = Testutils.makeFitsImage(path = \"./test.fits\", size = (10,10), hdus = {'image':0}, compressed = False)\n+        header = {\"NAXIS1\":1024, \"NAXIS2\":2048, \"CTYPE1\":\"RA-TAN\", \"CTYPE2\":\"DEC_TAN\", \"CD2_1\":0.0}\n+        Testutils.updateFitsHeader(fitsfile, header, hdu = 0)\n+        \n+        targetdir = os.getcwd() +os.sep +\"xmltest\"\n+        if not os.path.exists(targetdir):\n+            os.mkdir(targetdir)\n+        fitsfile = self.ipa.copy_from_xml_to_header(fitsfile, os.getcwd(), targetdir, self.xml_contents)\n+        print(\"fitsfile \", fitsfile)\n+        h = FitsServices.read_image_header(fitsfile)        \n+        \n+        #check a few values      \n+        self.assertTrue(h[\"CD2_1\"] == -7.28547806884e-05)\n+        self.assertTrue(h[\"CRVAL1\"] == 227.50678926)\n+        self.assertTrue(h[\"CRPIX1\"] == -2357.6)\n+        self.assertTrue(\"DECAM\" in  h[\"INSTRUME\"] )\n+        \n+        os.remove(\"./test.fits\")\n+        os.remove(fitsfile)\n+        os.rmdir(targetdir)\n+    def test_extract_gain_from_xml(self):\n+        gain_settings = self.ipa.extract_gain_from_xml(self.xml_contents)    \n+        \n+        self.assertTrue(len(gain_settings) == 2)\n+        self.assertTrue(gain_settings[0][\"AmpName\"] == \"Gain_A\")\n+        self.assertTrue(gain_settings[1][\"AmpName\"] == \"Gain_B\")\n+        \n+    def test_get_property_template_path(self):\n+        #prepare a workdir\n+        try:\n+            os.mkdir(os.getcwd() +os.sep +\"inputs\")\n+        except:\n+            pass #folder exists\n+        try:\n+            os.mkdir(os.getcwd() +os.sep +\"data\")\n+        except:\n+            pass #folder exists\n+        xml_file = os.getcwd() +os.sep +\"inputs\" +os.sep +self.ipa.single_epoch_science_type[0] +\".xml\"\n+        f = open(xml_file, 'w')\n+        f.write(\"<Data><Instrument><InstrumentName>DECAM</Data></Instrument></InstrumentName>\")\n+        f.close()\n+        p = self.ipa.get_property_template_path(os.getcwd())\n+        \n+        os.remove(xml_file)\n+        os.rmdir(os.getcwd() +os.sep +\"inputs\")\n+        os.rmdir(os.getcwd() +os.sep +\"data\")\n+        \n+        self.assertTrue(\"conf\" in p)\n+        self.assertTrue(p.endswith(\"properties\"))\n+        self.assertTrue(\"DECAM\" in p)\n+    def test_read_file(self):\n+        xml_file = os.getcwd() +\"xml_test.xml\"\n+        f = open(xml_file, 'w')\n+        text = \"<Data><Instrument><InstrumentName>DECAM</Data></Instrument></InstrumentName>\"\n+        f.write(text)\n+        f.close()\n+        self.assertTrue(self.ipa.read_file(xml_file) == text)\n+        os.remove(xml_file)\n+    def test_parse_tags(self):\n+        xml_file = os.getcwd() +\"xml_test.xml\"\n+        f = open(xml_file, 'w')\n+        text = \"<Data><Instrument><InstrumentName>DECAM</Data></Instrument></InstrumentName>\"\n+        f.write(text)\n+        f.close()\n+        self.assertTrue(self.ipa.parse_tags(xml_file, [(\"Data\",\"Instrument\",\"InstrumentName\")])[0][0] == \"DECAM\")\n+        os.remove(xml_file)\n+    def test_get_sourcelist_tags(self):\n+        self.assertTrue(1)\n+    def test_merge_vis_catalogs(self):\n+        self.assertTrue(1)\n+    def test_prepare_coadd_input_files(self):\n+        self.assertTrue(1)\n+    def test_printout_db_ids(self):\n+        self.assertTrue(1)\n+    def test_undo_euclid_filenaming_convention(self):\n+        self.assertTrue(1)\n+    def test_write_outputs(self):\n+        self.assertTrue(1)\n+    def test_xml_export(self):\n+        self.assertTrue(1)\n+    def local_test_merge_mer_star_galaxy_catalogs(self):\n+        workdir = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/launcher/coadd__20200805181035_testcopy/\"\n+        coadd_files = {'tileinfo_xml_file':\"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC8/launcher/coadd__20200805181035_testcopy/inputs/Tile_40012.xml\"}\n+        catalog_filename = \"merged_mer_catalog.fits\"\n+        self.ipa.merge_mer_star_galaxy_catalogs(workdir, coadd_files, catalog_filename, magnitude_cut = 26)\n+    #\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestLayerIDs_test.py": [
                        [
                            "@@ -0,0 +1,30 @@\n+import os\n+import numpy as np\n+from astropy.io.fits import HDUList\n+from EXT_PF1_GEN_P2.Fwhm import LayerIds\n+import unittest\n+\n+class TestLayerIDs(unittest.TestCase):\n+    def setUp(self):\n+        self.lid = LayerIds.LayerIds()\n+        self.tableids = [(0,0,0,[],\"2\"), (1,0,[],\"2\"), (2,0.1,89.0,0,[],\"3\"), (3,0.1,89.0,0,[],\"3\"), (4,0.1,89.0,0,[],\"3\"), (5,0.1,89.0,0,[],\"3\")]\n+        \n+    def tearDown(self):\n+        pass\n+    def test_generate_layer_id(self):\n+        self.assertTrue( self.lid.generate_layer_id([1,2,3]) == str( hash(\"1_2_3\")).replace(\"-\", \"_\"))\n+    #\n+    def test_encode_layer_ids(self):\n+        table_ids = [[[3,2,1], \"\"], [[7,8,2,5], \"\"]]\n+        tids = self.lid.encode_layer_ids(table_ids, 0)\n+        \n+        self.assertTrue( tids[0][1] == str( hash(\"1_2_3\")).replace(\"-\", \"_\"))\n+        self.assertTrue( tids[1][1] == str( hash(\"2_5_7_8\")).replace(\"-\", \"_\"))\n+    def test_extract_layer_ids(self):\n+        wcs_dict = {1:{\"contains\": [1,2,4]}, 2:{\"contains\": [3,2,4]}, 3:{\"contains\": [1,2]}, 4:{\"contains\": [1]}, 5:{\"contains\": [3,5]}}\n+        tids = self.lid.extract_layer_ids(wcs_dict, self.tableids)\n+        #print(tids)\n+    #\n+\n+\n+#\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestMasks_test.py": [
                        [
                            "@@ -1,5 +1,5 @@\n import numpy as np\n-from EXT_PF1_Coadd.Masks import Masks\n+from EXT_PF1_GEN_P2.Masks import Masks\n import unittest\n \n class TestMasks(unittest.TestCase):\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestObjectList.py": [
                        [
                            "@@ -1,15 +0,0 @@\n-import unittest\n-import os\n-class TestSigmaClip(unittest.TestCase):\n-    def setUp(self):\n-        pass\n-    def tearDown(self):\n-        pass\n-    def test__add_attribute(self):\n-        self.assertTrue(1)\n-    def test__add_x_list(self):\n-        self.assertTrue(1)\n-    def test__add_y_list(self):\n-        self.assertTrue(1)\n-    def test__filter(self):\n-        self.assertTrue(1)\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestObjectList_test.py": [
                        [
                            "@@ -0,0 +1,28 @@\n+import unittest\n+import os\n+import numpy as np\n+from EXT_PF1_GEN_P2.psf import ObjectList\n+\n+class TestObjectList(unittest.TestCase):\n+    def setUp(self):\n+        self.objl = ObjectList.ObjectList([20.0, 21.0], [25.4, 24.9], [10.5, 11.5], [-75.0, -74.0]) #mag, zp, ra,dec\n+    def tearDown(self):\n+        pass\n+    def test__add_attribute(self):\n+        self.assertTrue(self.objl.ra == [10.5, 11.5])\n+        self.assertTrue(self.objl.dec == [-75.0, -74.0])\n+        self.assertTrue(self.objl.mag == [20.0, 21.0])\n+        self.assertTrue(self.objl.zp == [25.4, 24.9])\n+        self.assertTrue(self.objl._length == 2)\n+    def test__add_x_list(self):\n+        self.objl._add_x_list(np.array([50, 52]))\n+        self.assertTrue( all( np.array(self.objl.X) == np.array([50,52]) ) )\n+    def test__add_y_list(self):\n+        self.objl._add_y_list(np.array([89, 90]))\n+        self.assertTrue( all( np.array(self.objl.Y) == np.array([89,90]) ) )\n+    def test__filter(self):\n+        self.objl._filter([1]) #indices of the desired objects\n+        self.assertTrue(self.objl.ra== [11.5])\n+        self.assertTrue(self.objl.dec == [-74.0])\n+        self.assertTrue(self.objl.mag == [21.0])\n+        self.assertTrue(self.objl._length == 1)\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestOutputStackedPSF.py": [
                        [
                            "@@ -1,23 +0,0 @@\n-import unittest\n-import os\n-class TestSigmaClip(unittest.TestCase):\n-    def setUp(self):\n-        pass\n-    def tearDown(self):\n-        pass\n-    def test_add_layer_objects_stamps_to_grid_image(self):\n-        self.assertTrue(1)\n-    def test_add_layer_objects_to_info_table(self):\n-        self.assertTrue(1)\n-    def test_add_stamps_to_output_image(self):\n-        self.assertTrue(1)\n-    def test_get_stamp_size(self):\n-        self.assertTrue(1)\n-    def test_populate(self):\n-        self.assertTrue(1)\n-    def test_prepare_sky_coordinates(self):\n-        self.assertTrue(1)\n-    def test_set_stamp_size(self):\n-        self.assertTrue(1)\n-    def test_write_to_fits(self):\n-        self.assertTrue(1)\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestPSFModel_test.py": [
                        [
                            "@@ -0,0 +1,52 @@\n+\n+import EXT_PF1_GEN_P2_LIBS.Pipeline\n+#from EXT_PF1_GEN_P2.psfmodel import PsfModel\n+import unittest\n+import os\n+from astropy.io.fits import HDUList, ImageHDU, PrimaryHDU, Header\n+\n+\n+class TestPsfModel(unittest.TestCase):\n+    def setUp(self):\n+        #make a generic fits file with a header\n+        self.psfex = \"/data/euclid/u/drwstein/sim_archive/sim_workdir/DES/DES_EDFS/DES_EDFS_v4/analysis/PPO_WIDE_DES_EDFS_v4_SDC-DE_TEST_DECAM_TILE_100021013_3/data/EUC_EXT_PSFMODELSTORAGE_DECam_00901883_60_PSFCAT_20221013T195632.917000Z.psf\"\n+        self.panstarrs = \"/data/euclid/u/drwstein/Work/Projects/EDEN-3.0/EXT_PF1_GEN_P2/EXT_PF1_GEN_P2/tests/python/EUC_EXT_PSFMODEL_FDE2B5FF9E83C6AA0435F823_20221222T153203.405849Z_00.01.fits\"\n+        #self.panstarrs = \"/data/euclid/u/drwstein/Work/Projects/EDEN-3.0/EXT_PF1_GEN_P2/EXT_PF1_GEN_P2/tests/python/o7761g0580o.1182392.wrp.2104032.skycell.197.257.psf\"\n+        #self.panstarrs = '/data/euclid/u/drwstein/Work/Projects/EDEN-3.0/EXT_PF1_GEN_P2/EXT_PF1_GEN_P2/tests/python/EUC_EXT_PSFMODEL_E8D92750A5E7D3AEBAFE69D4_20221222T154314.703668Z_00.01.fits'\n+        \n+    def tearDown(self):\n+        pass\n+    def sleeping_test_init(self):\n+        psm = PsfModel.PsfModel(self.psfex)\n+        self.assertTrue( psm.model.__eq__(PsfModel.PsfexModelInterface(self.psfex)) )\n+        \n+        psm = PsfModel.PsfModel(self.panstarrs)\n+        self.assertTrue( psm.model.__eq__(  PsfModel.PanstarrsModelInterface(self.panstarrs)) )\n+    def sleeping_test_getStampsize(self):\n+        psm = PsfModel.PsfModel(self.psfex)\n+        self.assertTrue( len(psm.getStampsize())  == 2)\n+        self.assertTrue( psm.getStampsize()[0] > 0.0)\n+        \n+        psm = PsfModel.PsfModel(self.panstarrs)\n+        self.assertTrue( len(psm.getStampsize())  == 2)\n+        self.assertTrue( psm.getStampsize()[0] > 0.0)\n+    def sleeping_test_getPsfStamp(self):\n+        psm = PsfModel.PsfModel(self.psfex)\n+        self.assertTrue( psm.getPsfStamp(100,100).shape[0] == psm.getStampsize()[0] * psm.getStampsize()[1]  )\n+        self.assertTrue( psm.getPsfStamp(100,100).sum() > 0.0  )\n+        \n+        psm = PsfModel.PsfModel(self.panstarrs)\n+        print(\"shape \", psm.getPsfStamp(100,100).shape)\n+        self.assertTrue( psm.getPsfStamp(100,100).shape[0] == psm.getStampsize()[0] * psm.getStampsize()[1]  )\n+        self.assertTrue( psm.getPsfStamp(100,100).sum() > 0.0  )\n+    def sleeping_test_getAllPsfStamp(self):\n+        psm = PsfModel.PsfModel(self.psfex)\n+        self.assertTrue( len( psm.getAllPsfStamps([100., 101.],[100., 50.]) ) == 2  )\n+        \n+        psm = PsfModel.PsfModel(self.panstarrs)\n+        self.assertTrue( len( psm.getAllPsfStamps([100., 101.],[100., 50.]) ) == 2  )\n+        \n+    #\n+    #\n+    \n+#\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -1,6 +1,6 @@\n \n import EXT_PF1_GEN_P2_LIBS.Pipeline\n-from EXT_PF1_GEN_P2.psfmodel import PsfModel\n+#from EXT_PF1_GEN_P2.psfmodel import PsfModel\n import unittest\n import os\n from astropy.io.fits import HDUList, ImageHDU, PrimaryHDU, Header\n@@ -16,13 +16,13 @@ class TestPsfModel(unittest.TestCase):\n         \n     def tearDown(self):\n         pass\n-    def test_init(self):\n+    def sleeping_test_init(self):\n         psm = PsfModel.PsfModel(self.psfex)\n         self.assertTrue( psm.model.__eq__(PsfModel.PsfexModelInterface(self.psfex)) )\n         \n         psm = PsfModel.PsfModel(self.panstarrs)\n         self.assertTrue( psm.model.__eq__(  PsfModel.PanstarrsModelInterface(self.panstarrs)) )\n-    def test_getStampsize(self):\n+    def sleeping_test_getStampsize(self):\n         psm = PsfModel.PsfModel(self.psfex)\n         self.assertTrue( len(psm.getStampsize())  == 2)\n         self.assertTrue( psm.getStampsize()[0] > 0.0)\n@@ -30,7 +30,7 @@ class TestPsfModel(unittest.TestCase):\n         psm = PsfModel.PsfModel(self.panstarrs)\n         self.assertTrue( len(psm.getStampsize())  == 2)\n         self.assertTrue( psm.getStampsize()[0] > 0.0)\n-    def test_getPsfStamp(self):\n+    def sleeping_test_getPsfStamp(self):\n         psm = PsfModel.PsfModel(self.psfex)\n         self.assertTrue( psm.getPsfStamp(100,100).shape[0] == psm.getStampsize()[0] * psm.getStampsize()[1]  )\n         self.assertTrue( psm.getPsfStamp(100,100).sum() > 0.0  )\n@@ -39,7 +39,7 @@ class TestPsfModel(unittest.TestCase):\n         print(\"shape \", psm.getPsfStamp(100,100).shape)\n         self.assertTrue( psm.getPsfStamp(100,100).shape[0] == psm.getStampsize()[0] * psm.getStampsize()[1]  )\n         self.assertTrue( psm.getPsfStamp(100,100).sum() > 0.0  )\n-    def test_getAllPsfStamp(self):\n+    def sleeping_test_getAllPsfStamp(self):\n         psm = PsfModel.PsfModel(self.psfex)\n         self.assertTrue( len( psm.getAllPsfStamps([100., 101.],[100., 50.]) ) == 2  )\n         \n",
                            "update to DM 9.1.5",
                            "Michael",
                            "2023-03-20T17:50:13.000+01:00",
                            "212424335002e366458e456f1de475c2507f8922"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestPSFUtils_test.py": [
                        [
                            "@@ -1,37 +1,60 @@\n \n import EXT_PF1_GEN_P2_LIBS.Pipeline\n-from EXT_PF1_Coadd.file import PSFUtils\n+from EXT_PF1_GEN_P2.file import PSFUtils\n from astropy.io import fits\n from astropy.wcs import WCS\n import numpy as np\n import math\n import unittest\n import os\n+from astropy.io.fits import HDUList, ImageHDU, PrimaryHDU, Header\n+\n \n class TestPSFUtils(unittest.TestCase):\n-    def setUp(self):        \n-        self.pfu = PSFUtils.PSFUtils(\"/data/euclid/u/drwstein/sim_archive/sim_workdir/SC456/coadd_40012_20190314_des/data/EUC_EXT_DPDEXTPSF_DECAM-90001200-49-PSFCAT_20190312T152617.5Z_00.00.psf\")\n+    def setUp(self):\n+        #make a generic fits file with a header\n+        self.header = {\"testvalue\":5}\n+        self.hdul = HDUList([PrimaryHDU(), \n+                        ImageHDU(data = np.zeros((1000, 1000)), header = Header(self.header), name = \"IMAGE\")])\n+        self.psf_fitsfile = os.getcwd() +os.sep +\"psffile.psf\"\n+        self.hdul.writeto(self.psf_fitsfile, overwrite=True)        \n+        self.pfu = PSFUtils.PSFUtils(self.psf_fitsfile)\n     def tearDown(self):\n-        pass\n+        os.remove(self.psf_fitsfile)\n     def test_close(self):\n-        self.assertTrue(1)\n+        self.pfu.close()\n+        self.assertTrue(self.pfu.closed)\n     def test_evaluate(self):\n         self.assertTrue(1)\n     def test_evaluateAll(self):\n         self.assertTrue(1)\n     def test_fileHasHDUs(self):\n-        self.assertTrue(1)\n+        retval = self.pfu.fileHasHDUs()\n+        self.assertTrue(retval == 0) #True\n+        self.assertTrue(self.pfu.hdu_num == len(self.hdul) )\n     def test_file_exists(self):\n-        self.assertTrue(1)\n+        retvalue = self.pfu.file_exists(self.psf_fitsfile)\n+        self.assertTrue(retvalue == 0) #file exists\n+        \n+        retvalue = self.pfu.file_exists(\"/non/existing/file\")\n+        self.assertTrue(type(retvalue) == type(\"string\")) \n     def test_getConvolutionParams(self):\n         self.assertTrue(1)\n     def test_getEpsilon(self):\n-        self.assertTrue(1)\n+        self.assertTrue(self.pfu.get_epsilon(None) == 0)\n     def test_getHeader(self):\n-        self.assertTrue(1)\n+        h = self.pfu.get_header()\n+        self.assertTrue(h is not None)\n     def test_getKey(self):\n-        self.assertTrue(1)\n+        self.pfu.get_key(\"testvalue\")\n+        \n+        self.assertTrue(self.pfu.testvalue == self.header[\"testvalue\"])\n+        \n+        self.pfu.get_key(\"nokey\")\n+        self.assertTrue(self.pfu.nokey is None)\n     def test_open(self):\n+        self.pfu.open()\n+        self.assertTrue(not self.pfu.closed)\n         self.assertTrue(1)\n     def sleeping_test_evaluate(self):\n         stamp = self.pfu.evaluate(10,10)\n@@ -61,4 +84,34 @@ class TestPSFUtils(unittest.TestCase):\n         print(\"time for all processing \", time.time() - s)\n         \n     # \n+    def sleeping_test_evaluate(self):\n+        from EXT_PF1_GEN_P2_LIBS.astronomy.ArrayUtils import ArrayUtils\n+        au = ArrayUtils()\n+        \n+        self.pfu = PSFUtils.PSFUtils(\"EUC_EXT_DPDEXTPSFMODEL_LSST-3261190-R41-S00_20200901T011349.332Z_LSST-SWF1-0710.fits\")\n+        shape = self.pfu.get_convolution_params()\n+        print(\"shape \", shape)\n+        sshape = [int(shape[0]), int(shape[1])]\n+        \n+        stamp1 = self.pfu.evaluate(321.0,321.0)\n+        results = au.fit_gauss_2d(stamp1.reshape(sshape))\n+        print(\"results 321.0 \", results)\n+        \n+        stamp2 = self.pfu.evaluate(321.2,321.2)\n+        results = au.fit_gauss_2d(stamp2.reshape(sshape))\n+        print(\"results 321.2 \", results)\n+        \n+        stamp2 = self.pfu.evaluate(321.499,321.499)\n+        results = au.fit_gauss_2d(stamp2.reshape(sshape))\n+        print(\"results 321.499 \", results)\n+        \n+        stamp2 = self.pfu.evaluate(321.5,321.5)\n+        results = au.fit_gauss_2d(stamp2.reshape(sshape))\n+        print(\"results 321.5 \", results)\n+\n+        stamp2 = self.pfu.evaluate(321.8,321.8)\n+        results = au.fit_gauss_2d(stamp2.reshape(sshape))\n+        print(\"results 321.8 \", results)\n+        \n+    # \n #\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestProcessManager.py": [
                        [
                            "@@ -0,0 +1,50 @@\n+import time\n+from EXT_PF1_GEN_P2.Pipeline import ProcessManager\n+import unittest\n+\n+class TestProcessManager(unittest.TestCase):\n+    def setUp(self):\n+        pass\n+    def tearDown(self):\n+        pass\n+    def test_free(self):\n+        pm = ProcessManager.ProcessManager(5)\n+        self.assertTrue( len(pm.free()) == 5 )\n+    def test_add(self):\n+        def sleep(_time):\n+            time.sleep(_time)\n+        #    \n+        \n+        processes = 1\n+        extime = 2.0\n+        pm = ProcessManager.ProcessManager(processes)\n+        \n+        s = time.time()\n+        for i in range(processes):\n+            pm.add_process_and_start(sleep, (extime,)) \n+        #\n+        \n+        #process returns only after runtime\n+        pt = time.time() - s\n+        self.assertTrue( pt > extime*processes )\n+        self.assertTrue( pt < extime*processes +1.0 )\n+        \n+    def test_block(self):\n+        def sleep(_time):\n+            time.sleep(_time)\n+        #    \n+        \n+        processes = 5\n+        extime = 2.0\n+        pm = ProcessManager.ProcessManager(processes)\n+        \n+        s = time.time()\n+        for i in range(processes):\n+            pm.add_process_and_start(sleep, (extime,))\n+        #\n+        pm.block_until_free_process()\n+        \n+        pt = time.time() - s\n+        self.assertTrue( pt > extime*processes )\n+        self.assertTrue( pt < extime*processes +1.0 )\n+        \n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestPsfStampFilter_test.py": [
                        [
                            "",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestPsfid_test.py": [
                        [
                            "@@ -0,0 +1,87 @@\n+import unittest\n+import math\n+import os\n+\n+from EXT_PF1_GEN_P2.psf import Psfid\n+from astropy.table import Table\n+from astropy.io.fits import HDUList, PrimaryHDU, ImageHDU, Header\n+\n+import numpy as np\n+\n+class TestPsfid(unittest.TestCase):\n+    def setUp(self):\n+         #wcs from header\n+        header = {}\n+        header[\"NAXiS\"] = 2\n+        header[\"NAXIS1\"] = 200\n+        header[\"NAXIS2\"] = 100\n+        header[\"CRPIX1\"] = 0.0\n+        header[\"CRPIX2\"] = 0.0\n+        header[\"CRVAL1\"] = 0.0\n+        header[\"CRVAL2\"] = 0.0\n+        #1 arcsec per pixel\n+        header[\"CD1_1\"] = 2.6603E-06\n+        header[\"CD1_2\"] = 7.10338E-05\n+        header[\"CD2_1\"] = 7.10338E-05\n+        header[\"CD2_2\"] = -2.6603E-06\n+        \n+        #make a numpy array with 5 psfid sections\n+        psfid = np.zeros( (100,200))  #y, x = dec, ra\n+        psfid[0:50, 0:100] = math.pow(1.1, 1)\n+        psfid[0:50, 100:200] = math.pow(1.1, 2)\n+        psfid[50:100, 0:100] = math.pow(1.1, 3)\n+        psfid[50:100, 100:200] = math.pow(1.1, 4)\n+        \n+        #and add an irregular section\n+        psfid[13:17, 0:100] = math.pow(1.1, 5)\n+        \n+        hdul = HDUList( [PrimaryHDU(), ImageHDU(np.zeros((100,200)), header = Header(header), name = \"IMAGE\"), ImageHDU(psfid, name = \"PSFID\")] )\n+        self.coaddname = os.getcwd() +os.sep +\"psfid_coadd.fits\"\n+        hdul.writeto(self.coaddname, overwrite = True)\n+        hdul.close\n+        \n+        self.array = psfid\n+        self.psfid = Psfid.Psfid(self.coaddname)\n+        \n+    def tearDown(self):\n+        os.remove(self.coaddname)\n+    def test_get_number_of_unique_psfids(self):\n+        \n+        len_uniq, uniq = self.psfid.get_number_of_unique_psfids()\n+        self.assertTrue(len_uniq == 5)\n+        \n+        for i in range(1,6):\n+            self.assertTrue(math.pow(1.1, i) in uniq)\n+        #\n+    def test_get_psfid_array(self):\n+        self.assertTrue( (self.psfid.get_psfid_array() == self.array).all() )\n+    def test_regular_psf_grid(self):\n+        array, master_cat = self.psfid.regular_psf_grid(self.psfid.get_psfid_array(), gridspacing = 50, markvalue = -5)\n+        self.assertTrue( len(master_cat[0]) == 2 )\n+        self.assertTrue( array.flatten()[array.flatten() == -5].size == 2)\n+    def test_get_psf_points_cm_add(self):\n+        cm, replaced = self.psfid.get_psf_points_cm_add( self.array, self.array.copy(), math.pow(1.1, 5), markvalue = -5)\n+        \n+        self.assertTrue(13 < cm[0] < 17)\n+        self.assertTrue(45 < cm[1] < 55)\n+        \n+        self.assertTrue(replaced)\n+    def test_set_psf_points_within_a_psfid_section(self):\n+        master_cat = self.psfid.set_psf_points_within_a_psfid_section()\n+        self.assertTrue(len(master_cat[0]) == 5)\n+    def test_make_catalog(self):\n+        master_cat = self.psfid.set_psf_points_within_a_psfid_section()\n+        catalog_path = os.getcwd() + os.sep +\"psfid_catalog.fits\"\n+        self.psfid.make_catalog(master_cat, catalog_path, mag = 20.0, ra_name = \"RA\", dec_name = \"DEC\", mag_name = \"MAG\")\n+        \n+        td = Table.read(catalog_path)\n+        \n+        self.assertTrue(\"RA\" in td.colnames)\n+        self.assertTrue(\"DEC\" in td.colnames)\n+        self.assertTrue(\"MAG\" in td.colnames)\n+        \n+        self.assertTrue( td[\"DEC\"].data.size == 5)\n+        self.assertTrue( td[\"MAG\"].data[0] == 20.0)\n+        \n+        os.remove(catalog_path)\n+    #\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestRDisk_coadd_stage2.py": [
                        [
                            "@@ -1,92 +0,0 @@\n-import unittest\n-import numpy as np\n-import os\n-from EXT_PF1_GEN_P2_LIBS.Pipeline import PipelineControl\n-import EXT_PF1_Coadd\n-from EXT_PF1_Coadd.Pipeline import RDisk_coadd_stage2\n-from EXT_PF1_GEN_P2_LIBS.test import Testutils\n-from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n-\n-class TestRDisk_coadd_stage2(unittest.TestCase):\n-    def setUp(self):\n-        self.rdc = RDisk_coadd_stage2.RDisk_coadd_stage2()\n-    def tearDown(self):\n-        pass\n-    def test_astrometric_validation(self):\n-        self.assertTrue(1)\n-    def test_compose_validation_images(self):\n-        self.assertTrue(1)\n-    def test_getName(self):\n-        self.assertTrue(1)\n-    def test_get_validator(self):\n-        self.assertTrue(1)\n-    def test_match_src_and_cat(self):\n-        self.assertTrue(1)\n-    def test_add_mask_2_weight(self):\n-        Testutils.makeFitsImage(path = \"./test.fits\", size = (10,10), hdus = {'image':0, 'mask':0, 'weight':5.0}, compressed = False)\n-        \n-        mask = np.zeros((10,10), dtype = np.dtype('i4') )\n-        mask[5,5] = 1\n-        mask[6,5] = 8\n-        \n-        FitsServices.write_mask_data(\"./test.fits\", mask)\n-        \n-        self.rdc.add_mask_2_weight(\"./test.fits\", [0])\n-        \n-        self.assertTrue(FitsServices.read_weight_data(\"./test.fits\")[5,5] == 0.0)\n-        self.assertFalse(FitsServices.read_weight_data(\"./test.fits\")[6,5] == 0.0)\n-        \n-        self.rdc.add_mask_2_weight(\"./test.fits\", [3])\n-        self.assertTrue(FitsServices.read_weight_data(\"./test.fits\")[6,5] == 0.0)\n-        os.remove(\"./test.fits\")\n-    def test_set_maskbits_in_mask(self):    \n-        Testutils.makeFitsImage(path = \"./test.fits\", size = (10,10), hdus = {'image':0, 'mask':0, 'weight':5.0}, compressed = False)\n-        \n-        mask = np.zeros((10,10), dtype = np.dtype('i4') )\n-        mask[5,5] = 1\n-        mask[6,5] = 8\n-        \n-        FitsServices.write_mask_data(\"./test.fits\", mask)\n-    \n-        self.rdc.set_maskbits_in_mask(\"./test.fits\", [0])\n-        mask = FitsServices.read_mask_data(\"./test.fits\")\n-        self.assertTrue(mask[5,5] == 1)\n-        self.assertTrue(mask[6,5] == 0)\n-        \n-        \n-        mask[5,5] = 1\n-        mask[6,5] = 8\n-        FitsServices.write_mask_data(\"./test.fits\", mask)\n-        \n-        self.rdc.set_maskbits_in_mask(\"./test.fits\", [3])\n-        mask = FitsServices.read_mask_data(\"./test.fits\")\n-        self.assertTrue(mask[5,5] == 0)\n-        self.assertTrue(mask[6,5] == 8)\n-        \n-        mask[5,5] = 1\n-        mask[6,5] = 8\n-        FitsServices.write_mask_data(\"./test.fits\", mask)\n-        \n-        self.rdc.set_maskbits_in_mask(\"./test.fits\", [3], [9])\n-        mask = FitsServices.read_mask_data(\"./test.fits\")\n-        self.assertTrue(mask[5,5] == 0)\n-        self.assertTrue(mask[6,5] == 512)\n-        \n-        mask[5,5] = 1\n-        mask[6,5] = 8\n-        FitsServices.write_mask_data(\"./test.fits\", mask)\n-        \n-        self.rdc.set_maskbits_in_mask(\"./test.fits\", [4])\n-        mask = FitsServices.read_mask_data(\"./test.fits\")\n-        self.assertTrue(mask.sum() == 0)\n-        os.remove(\"./test.fits\")\n-    def test_extract_properties(self):\n-        pc = PipelineControl.PipelineControl()\n-        propfile = EXT_PF1_Coadd.__file__.rsplit(os.sep, 1)[0] +\"/../../conf/EXT_PF1_Coadd/coadd_template_LSST.properties\"\n-        props = pc.load_props(propfile)\n-        self.rdc = RDisk_coadd_stage2.RDisk_coadd_stage2()\n-        dd = self.rdc.extract_properties(props, -1)\n-        self.assertTrue(\"_se_catalog_x\" in dd)\n-        self.assertTrue(\"_se_catalog_y\" in dd)\n-    #\n-        \n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestRDisk_coadd_stage2_test.py": [
                        [
                            "@@ -0,0 +1,359 @@\n+import unittest\n+import numpy as np\n+import os, sys, math\n+from tarfile import TarFile\n+import shutil\n+\n+import EXT_PF1_GEN_P2_LIBS\n+from EXT_PF1_GEN_P2_LIBS.Pipeline import PipelineControl\n+from EXT_PF1_GEN_P2.Pipeline import RDisk_coadd_stage2\n+from EXT_PF1_GEN_P2_LIBS.test import Testutils\n+from EXT_PF1_GEN_P2_LIBS.file import FitsServices\n+\n+from astropy.io.fits import Header, PrimaryHDU, ImageHDU, BinTableHDU, HDUList\n+from astropy.table import Table, Column\n+\n+from astropy.table import Table\n+\n+class TestRDisk_coadd_stage2(unittest.TestCase):\n+    def setUp(self):\n+        self.rdc = RDisk_coadd_stage2.RDisk_coadd_stage2()\n+    def tearDown(self):\n+        pass\n+    def test_astrometric_validation(self):\n+        #make a se_cat\n+        se_cat = Table()\n+        se_cat[\"X\"] = [131.8720020368789,24.050439198883033,13.43767590792685,143.96421519542167,174.9245910295104,233.1334865496713,206.3554889082032]\n+        se_cat[\"Y\"] = [1751.8178512754296,1931.3853403774167,2942.03078639144,3716.9960341111178,3349.8325332090835,3912.4864711834507,3927.676488919787]\n+        Testutils.makeFitsTable(\"se_cat.fits\", se_cat)\n+                                                                                                                                               \n+        ref_cat = Table()\n+        ref_cat[\"X_WORLD\"] = [10.66989630211617,10.683786064286625,10.761788011917952,10.821594132016973,10.793228691717383,10.836675783082441,10.837854617137639]\n+        ref_cat[\"Y_WORLD\"] = [-19.040391491464295,-19.032618255727733,-19.032379480504652,-19.04229166390956,-19.044376517994188,-19.04889997807353,-19.046947715039966]\n+        ref_cat[\"MAG_AUTO\"] = len(ref_cat[\"Y_WORLD\"])*[18.0]\n+        Testutils.makeFitsTable(\"ref_cat.fits\", ref_cat)\n+        \n+        image_header = {\"CTYPE1\":'RA---TAN', \"CTYPE2\":'DEC--TAN',\n+                        \"CRVAL1\": 11.35107900487, \"CRVAL2\": -18.53241834894,\n+                        \"CRPIX1\":  -6866.4, \"CRPIX2\": 10566.67,\n+                        \"CD1_1\": -1.336910078583e-07, \"CD1_2\": 7.285929972108e-05,\n+                        \"CD2_1\":-7.285479866869e-05, \"CD2_2\":-1.294589545437e-07,\n+                        \"NAXIS1\":2048, \"NAXIS2\":4096}\n+        \n+        Testutils.makeFitsImage(path = \"./image.fits\")\n+        Testutils.updateFitsHeader(\"./image.fits\", image_header, hdu=0)\n+        \n+        skip_astrometric_validation = self.rdc.astrometric_validation(\"se_cat.fits\", \"ref_cat.fits\", os.getcwd(), \"./image.fits\", se_catalog_x = \"X\", se_catalog_y = \"Y\", band = \"MAG_AUTO\")\n+        self.assertFalse(skip_astrometric_validation)\n+        \n+        skip_astrometric_validation = self.rdc.astrometric_validation(\"se_cat.fits\", \"non_existing_ref_cat.fits\", os.getcwd(), \"./image.fits\", se_catalog_x = \"X\", se_catalog_y = \"Y\", band = \"MAG_AUTO\")\n+        self.assertTrue(skip_astrometric_validation)\n+        \n+        os.remove(\"se_cat.fits\")\n+        os.remove(\"ref_cat.fits\")\n+        os.remove(\"./image.fits\")\n+    #\n+    def test_compose_validation_images(self):\n+        self.assertTrue(1)\n+    def test_getName(self):\n+        self.assertTrue(1)\n+    def test_get_validator(self):\n+        self.assertTrue(1)\n+    def test_match_src_and_cat(self):\n+        srcfile = os.getcwd() + os.sep +\"srcfile.txt\"\n+        f = open(srcfile, 'w')\n+        f.write(\"image band psf, catalog\\n\")\n+        f.close()\n+        \n+        d = self.rdc.match_src_and_cat(srcfile)\n+        self.assertTrue(list(d.keys())[0] == \"image\")\n+        self.assertTrue(d[\"image\"] == \"catalog\")\n+        os.remove(srcfile)\n+    def test_add_mask_2_weight(self):\n+        Testutils.makeFitsImage(path = \"./test.fits\", size = (10,10), hdus = {'image':0, 'mask':0, 'weight':5.0}, compressed = False)\n+        \n+        mask = np.zeros((10,10), dtype = np.dtype('i4') )\n+        mask[5,5] = 1\n+        mask[6,5] = 8\n+        \n+        FitsServices.write_mask_data(\"./test.fits\", mask)\n+        \n+        self.rdc.add_mask_2_weight(\"./test.fits\", [0])\n+        \n+        self.assertTrue(FitsServices.read_weight_data(\"./test.fits\")[5,5] == 0.0)\n+        self.assertFalse(FitsServices.read_weight_data(\"./test.fits\")[6,5] == 0.0)\n+        \n+        self.rdc.add_mask_2_weight(\"./test.fits\", [3])\n+        self.assertTrue(FitsServices.read_weight_data(\"./test.fits\")[6,5] == 0.0)\n+        os.remove(\"./test.fits\")\n+    def test_set_maskbits_in_mask(self):    \n+        Testutils.makeFitsImage(path = \"./test.fits\", size = (10,10), hdus = {'image':0, 'mask':0, 'weight':5.0}, compressed = False)\n+        \n+        mask = np.zeros((10,10), dtype = np.dtype('i4') )\n+        mask[5,5] = 1\n+        mask[6,5] = 8\n+        \n+        FitsServices.write_mask_data(\"./test.fits\", mask)\n+    \n+        self.rdc.set_maskbits_in_mask(\"./test.fits\", [0])\n+        mask = FitsServices.read_mask_data(\"./test.fits\")\n+        self.assertTrue(mask[5,5] == 1)\n+        self.assertTrue(mask[6,5] == 0)\n+        \n+        \n+        mask[5,5] = 1\n+        mask[6,5] = 8\n+        FitsServices.write_mask_data(\"./test.fits\", mask)\n+        \n+        self.rdc.set_maskbits_in_mask(\"./test.fits\", [3])\n+        mask = FitsServices.read_mask_data(\"./test.fits\")\n+        self.assertTrue(mask[5,5] == 0)\n+        self.assertTrue(mask[6,5] == 8)\n+        \n+        mask[5,5] = 1\n+        mask[6,5] = 8\n+        FitsServices.write_mask_data(\"./test.fits\", mask)\n+        \n+        self.rdc.set_maskbits_in_mask(\"./test.fits\", [3], [9])\n+        mask = FitsServices.read_mask_data(\"./test.fits\")\n+        self.assertTrue(mask[5,5] == 0)\n+        self.assertTrue(mask[6,5] == 512)\n+        \n+        mask[5,5] = 1\n+        mask[6,5] = 8\n+        FitsServices.write_mask_data(\"./test.fits\", mask)\n+        \n+        self.rdc.set_maskbits_in_mask(\"./test.fits\", [4])\n+        mask = FitsServices.read_mask_data(\"./test.fits\")\n+        self.assertTrue(mask.sum() == 0)\n+        os.remove(\"./test.fits\")\n+    def test_fill_bandpasses(self):\n+        fitsfile = self.rdc.fill_bandpasses(\"12345\", os.getcwd(), 1024, 2048)\n+        bp = FitsServices.read_data(fitsfile, 0)\n+        self.assertTrue(bp.sum() == 1024*2048*12345)\n+        os.remove(fitsfile)\n+    def test_extract_properties(self):\n+        pc = PipelineControl.PipelineControl()\n+        propfile = RDisk_coadd_stage2.__file__.rsplit(os.sep, 1)[0] +\"/../../../conf/EXT_PF1_GEN_P2/coadd_template_LSST.properties\"\n+        print(propfile)\n+        props = pc.load_props(propfile)\n+        self.rdc = RDisk_coadd_stage2.RDisk_coadd_stage2()\n+        dd = self.rdc.extract_properties(props, -1)\n+        self.assertTrue(\"_se_catalog_x\" in dd)\n+        self.assertTrue(\"_se_catalog_y\" in dd)\n+    def test_getName(self):\n+        self.assertTrue(self.rdc.getName() in str(self.rdc.__class__))\n+    def test_getValidator(self):\n+        self.assertTrue(self.rdc.get_validator() == self.rdc)\n+    def test_add_config_file_to_validation(self):\n+        #make a property file\n+        prop = os.getcwd() +os.sep +\"pipeline.properties\"\n+        f = open(prop, \"w\")\n+        f.write(\"some stuff so the size gets larger than 0\")\n+        f.close()\n+        data = os.getcwd() +os.sep +\"data\" +os.sep\n+        os.makedirs(data, exist_ok=True)\n+        \n+        validation_data = self.rdc.add_config_file_to_validation([], os.getcwd())\n+\n+        self.assertTrue(data +\"pipeline_property_file.properties\" in validation_data)\n+        self.assertTrue(\"pipeline_property_file.properties\" in os.listdir(data))\n+        \n+        os.remove(prop)\n+        shutil.rmtree(data)\n+    def test_put_background_data_to_outputs(self):\n+        #make a bg file\n+        bgfile1 = os.getcwd() +os.sep +\"file1_background.fits\"\n+        bgfile2 = os.getcwd() +os.sep +\"file2_background.fits\"\n+        \n+        f = open(bgfile1, \"w\")\n+        f.close()\n+        f = open(bgfile2, \"w\")\n+        f.close()\n+        \n+        outputs = {}\n+        outputs = self.rdc.put_background_data_to_outputs(os.getcwd(), outputs, tar = True, gzip = True, save_backgrounds_for_validation = 0)\n+        self.assertTrue(len(outputs.keys()) == 0)\n+        \n+        outputs = self.rdc.put_background_data_to_outputs(os.getcwd(), outputs, tar = True, gzip = True, save_backgrounds_for_validation = 1)\n+        self.assertTrue(len(outputs.keys()) == 1)\n+        self.assertTrue(len(outputs[\"background_images\"]) == 1)\n+        self.assertTrue(outputs[\"background_images\"][0].endswith(\".tar.gz\"))\n+        \n+        tf = TarFile.open(outputs[\"background_images\"][0])\n+        tcontents = tf.getnames()\n+        tf.close()\n+        \n+        self.assertTrue(bgfile1.rsplit(os.sep, 1)[-1] in tcontents)\n+        self.assertTrue(bgfile2.rsplit(os.sep, 1)[-1] in tcontents)\n+        \n+        os.remove(outputs[\"background_images\"][0])\n+        \n+        outputs = {}\n+        outputs = self.rdc.put_background_data_to_outputs(os.getcwd(), outputs, tar = False, gzip = False, save_backgrounds_for_validation = 1)\n+        self.assertTrue(len(outputs.keys()) == 1)\n+        self.assertTrue(len(outputs[\"background_images\"]) == 2)\n+        \n+        os.remove(bgfile1)\n+        os.remove(bgfile2)\n+    def test_coadd_list_2_bgsublist(self):\n+        #make a coaddswarplist\n+        coaddswarp = os.getcwd() +os.sep +\"coaddswarp.list\"\n+        f = open(coaddswarp, \"w\")\n+        f.writelines([\"/some/path/filename1.fits more stuff\\n\", \"/some/path/filename2.fits more stuff\\n\"])\n+        f.close()\n+        \n+        #make one of the files\n+        os.makedirs(\"./bgsubdir/\", exist_ok=True)\n+        f = open(\"./bgsubdir/filename2.fits\", \"w\")\n+        f.write(\"something so the file gets a size > 0\")\n+        f.close()\n+        \n+        bgsub = self.rdc.coadd_list_2_bgsublist(coaddswarp, \"./bgsubdir\")\n+        f = open(bgsub)\n+        lines = f.readlines()\n+        f.close()\n+        \n+        self.assertTrue(len(lines) > 0)\n+        self.assertTrue(\"./bgsubdir/\" in lines[0])\n+        self.assertTrue(\"filename2.fits\" in lines[0])\n+        self.assertTrue(\" more \" in lines[0])\n+        self.assertTrue(\" stuff\" in lines[0])\n+        \n+        os.remove(coaddswarp)\n+        os.remove(bgsub)\n+        os.remove(\"./bgsubdir/filename2.fits\")\n+        shutil.rmtree(\"./bgsubdir/\")\n+    def test_make_sourceXtractor_background(self):\n+        #make an image fitsfile\n+        hdul = HDUList([PrimaryHDU(), \n+                        ImageHDU(data = np.zeros((1000, 1000)), header = Header({}), name = \"IMAGE\"),\n+                        ImageHDU(data = np.ones((1000, 1000)), header = Header({}), name = \"WEIGHT\")])\n+        image_fitsfile = os.getcwd() +os.sep +\"image_g_band.fits\"\n+        hdul.writeto(image_fitsfile, overwrite=True)\n+        \n+        swarp_parameters_sourceXtractor = EXT_PF1_GEN_P2_LIBS.__path__[0].rsplit(\"/python/\")[0] +\"/conf/etc/default.sex\"\n+        \n+        logfile = \"./logfile.log\"\n+        self.rdc.setLogger(open(logfile, \"w\"))\n+        \n+        \n+        self.rdc.make_background(image_fitsfile, swarp_parameters_sourceXtractor, os.getcwd(), subtract_bg = 1, _which = self.rdc.SExtractor_bg)\n+        \n+        #file exists: image_fitsfile.rsplit(\".fits\")[0] +\"_background.fits\"\n+        bgfile = image_fitsfile.rsplit(\".fits\")[0] +\"_background.fits\"\n+        self.assertTrue(os.path.exists(bgfile))\n+        bgarray = FitsServices.read_data(bgfile, 0)\n+        self.assertTrue(bgarray.sum()< 1e-5)\n+        \n+        os.remove(bgfile)\n+        os.remove(image_fitsfile)\n+        os.remove(logfile)\n+    def test_make_swarp_background(self):\n+        #make an image fitsfile\n+        hdul = HDUList([PrimaryHDU(), \n+                        ImageHDU(data = np.zeros((1000, 1000)), header = Header({}), name = \"IMAGE\"),\n+                        ImageHDU(data = np.ones((1000, 1000)), header = Header({}), name = \"WEIGHT\")])\n+        image_fitsfile = os.getcwd() +os.sep +\"image_g_band.fits\"\n+        hdul.writeto(image_fitsfile, overwrite=True)\n+        \n+        swarp_parameters = EXT_PF1_GEN_P2_LIBS.__path__[0].rsplit(\"/python/\")[0] +\"/conf/etc/default.swarp\"\n+        \n+        logfile = \"./logfile.log\"\n+        self.rdc.setLogger(open(logfile, \"w\"))\n+        \n+        \n+        self.rdc.make_background(image_fitsfile, swarp_parameters, os.getcwd(), subtract_bg = 1, _which = self.rdc.Swarp_bg)\n+        \n+        #file exists: image_fitsfile.rsplit(\".fits\")[0] +\"_background.fits\"\n+        bgfile = image_fitsfile.rsplit(\".fits\")[0] +\"_background.fits\"\n+        self.assertTrue(os.path.exists(bgfile))\n+        bgarray = FitsServices.read_data(bgfile, 0)\n+        self.assertTrue(bgarray.sum()< 1e-5)\n+        \n+        os.remove(bgfile)\n+        os.remove(image_fitsfile)\n+        os.remove(logfile)\n+    def test_output_integrity(self):\n+        #make two stackpsfs with info tables\n+        psf1 = \"stackpsf1.fits\"\n+        psf2 = \"stackpsf2.fits\"\n+        \n+        t = Table(Column([], name = \"empty\"))\n+        hdul = HDUList([PrimaryHDU(), BinTableHDU(t, name = \"INFO\")])\n+        hdul.writeto(psf1, overwrite=True)\n+        \n+        t = Table(Column([1,2], name = \"not empty\"))\n+        hdul = HDUList([PrimaryHDU(), BinTableHDU(t, name = \"INFO\")])\n+        hdul.writeto(psf2, overwrite=True)\n+        \n+        #3 coadds, 2 psfs: will all be removed\n+        outputs = {\"psfs\": {\"g\":psf1, \"r\":psf2}, \"coadds\": {\"g\":\"coadd_g\", \"r\":\"coadd_r\", \"i\":\"coadd_i\" }}\n+        \n+        tested_outputs = self.rdc.test_output_integrity(outputs)\n+        self.assertTrue(len(tested_outputs[\"psfs\"]) == 0 and len(tested_outputs[\"coadds\"]) == 0)\n+        \n+        #2 coadds, 2 psfs: test INFO table\n+        outputs = {\"psfs\": {\"g\":psf1, \"r\":psf2}, \"coadds\": {\"g\":\"coadd_g\", \"r\":\"coadd_r\" }}\n+        tested_outputs = self.rdc.test_output_integrity(outputs)\n+        self.assertTrue(len(tested_outputs[\"psfs\"]) == 1 and psf2 in tested_outputs[\"psfs\"][\"r\"])\n+                \n+        os.remove(psf1)\n+        os.remove(psf2)\n+    def test_psfid_2_catalog(self):\n+        #wcs from header\n+        header = {}\n+        header[\"NAXiS\"] = 2\n+        header[\"NAXIS1\"] = 100\n+        header[\"NAXIS2\"] = 100\n+        header[\"CRPIX1\"] = 0.0\n+        header[\"CRPIX2\"] = 0.0\n+        header[\"CRVAL1\"] = 0.0\n+        header[\"CRVAL2\"] = 0.0\n+        #1 arcsec per pixel\n+        header[\"CD1_1\"] = 2.6603E-06\n+        header[\"CD1_2\"] = 7.10338E-05\n+        header[\"CD2_1\"] = 7.10338E-05\n+        header[\"CD2_2\"] = -2.6603E-06\n+        \n+        #make a numpy array with 5 psfid sections\n+        psfid = np.zeros( (100,100))\n+        psfid[0:50, 0:50] = math.pow(1.1, 1)\n+        psfid[0:50, 50:100] = math.pow(1.1, 2)\n+        psfid[50:100, 0:50] = math.pow(1.1, 3)\n+        psfid[50:100, 50:100] = math.pow(1.1, 4)\n+        \n+        #and add an irregular section\n+        psfid[13:17, 0:100] = math.pow(1.1, 5)\n+        \n+        hdul = HDUList( [PrimaryHDU(), ImageHDU(np.zeros((100,100)), header = Header(header), name = \"IMAGE\"), ImageHDU(psfid, name = \"PSFID\")] )\n+        coaddname = os.getcwd() +os.sep +\"psfid_coadd.fits\"\n+        hdul.writeto(coaddname, overwrite = True)\n+        hdul.close()\n+        \n+        catalog_outputfile = os.getcwd() +os.sep +\"psfid_catalog.fits\"\n+        \n+        catalog_outputfile, ra_name, dec_name, mag_name = self.rdc.psfid_2_catalog(coaddname, catalog_outputfile, gridspacing = 50, ra_name = \"RA\", dec_name = \"DEC\")\n+        \n+        os.remove(coaddname)\n+        os.remove(catalog_outputfile)\n+        \n+    def test_randomly_cut_catalog(self):\n+        ref_cat = Table()\n+        ref_cat[\"X_WORLD\"] = range(0,100)\n+        ref_cat[\"Y_WORLD\"] = range(0,100)\n+        ref_cat[\"MAG_AUTO\"] = range(0,100)\n+        \n+        catfile = os.getcwd() +os.sep +\"random_catalog.fits\"\n+        ref_cat.write(catfile, format = \"fits\", overwrite=True)\n+        \n+        new_cat = self.rdc.randomly_cut_catalog(catfile, 80)\n+        \n+        t = Table.read(new_cat, format = \"fits\")\n+        \n+        self.assertTrue(len(t) == 0.8*len(ref_cat[\"X_WORLD\"]))\n+        \n+        os.remove(catfile)\n+        os.remove(new_cat)\n+    #\n+        \n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestSigmaClip.py": [
                        [
                            "@@ -1,11 +0,0 @@\n-import unittest\n-import os\n-class TestSigmaClip(unittest.TestCase):\n-    def setUp(self):\n-        pass\n-    def tearDown(self):\n-        pass\n-    def test_clipIteration(self):\n-        self.assertTrue(1)\n-    def test_getStatistics(self):\n-        self.assertTrue(1)\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestSigmaClip_test.py": [
                        [
                            "@@ -0,0 +1,33 @@\n+import unittest\n+import os\n+\n+from EXT_PF1_GEN_P2.Pipeline import SigmaClip\n+import numpy as np\n+\n+class TestSigmaClip(unittest.TestCase):\n+    def setUp(self):\n+        pass\n+    def tearDown(self):\n+        pass\n+    def test_clipIteration(self):\n+        testvalue = 20.0\n+        inarray = np.zeros((10,10))\n+        inarray[5,5] =testvalue\n+        inweight = np.ones((10,10))\n+        s = SigmaClip.SigmaClip(inarray, inweight, 0)\n+        mean, rms = s.get_statistics(inarray)\n+        \n+        inarr, inweight, mean , new_mean, new_rms  = s.clip_iteration(inarray, inweight, mean, rms)\n+        \n+        self.assertTrue(inarr.shape[0] == 99)\n+        self.assertTrue(inarr.sum() == 0.0)\n+        \n+    def test_get_statistics(self):\n+        testvalue = 20.0\n+        inarray = np.zeros((100,100))\n+        inarray[50,50] =testvalue\n+        inweight = np.ones((100,100))\n+        s = SigmaClip.SigmaClip(inarray, inweight, 0)\n+        mean, rms = s.get_statistics(inarray)\n+        \n+        self.assertTrue(mean == testvalue/(inarray.shape[0]*inarray.shape[1]))\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestStackPsfAndCatalogDebugger_test.py": [
                        [
                            "@@ -0,0 +1,94 @@\n+import os\n+from astropy.table import Table, Column\n+import numpy\n+from EXT_PF1_GEN_P2.Fwhm import StackPsfAndCatalogDebugger\n+import unittest\n+\n+class TestStackPsfAndCatalogDebugger(unittest.TestCase):\n+    def setUp(self):\n+        self.spcd = StackPsfAndCatalogDebugger.StackPsfAndCatalogDebugger()\n+    def tearDown(self):\n+        pass\n+    def test_clean_reference_catalog(self):\n+        c1 = Column([1,2,3,4,5,6,7,8,9,-1, -1, -1], name = \"stackpsf index\")\n+        c2 = Column([11,22,33,44,55,66,77,88,99,100, 101, 102], name = \"catalog object id\")\n+        c3 = Column([11,22,33,44,55,66,77,88,99,100, 101, 102], name = \"OBJECT_ID\")\n+        \n+        match_catalog = Table()   \n+        match_catalog.add_column(c1, name = \"stackpsf index\")\n+        match_catalog.add_column(c2, name = \"catalog object id\")\n+        \n+        ref_cat = Table()\n+        ref_cat.add_column(c3, name = \"OBJECT_ID\")\n+        \n+        rc, mc = self.spcd.clean_reference_catalog(ref_cat, match_catalog)\n+        \n+        self.assertFalse( 100 in rc[\"OBJECT_ID\"].data)\n+        self.assertFalse( 101 in rc[\"OBJECT_ID\"].data)\n+        self.assertFalse( 102 in rc[\"OBJECT_ID\"].data)\n+        \n+        self.assertFalse( -1 in mc[\"stackpsf index\"].data)\n+        self.assertFalse( 100 in mc[\"catalog object id\"].data)\n+        self.assertFalse( 101 in mc[\"catalog object id\"].data)\n+        self.assertFalse( 102 in mc[\"catalog object id\"].data)\n+    def test_rebuild_stackpsf(self):\n+        #make an image with stampsize 49 img = y,x\n+        img = numpy.zeros((98,98))\n+        img[0:49,0:49] = 0.0\n+        img[0:49,49:] = 1.0\n+        img[49:,0:49] = 2.0\n+        img[49:,49:] = 3.0\n+        \n+        \n+        matchcat = Table()\n+        matchcat.add_column(Column([]), name = \"catalog ra\")\n+        matchcat.add_column(Column([]), name = \"catalog dec\")\n+        matchcat.add_column(Column([]), name = \"stackpsf index\")\n+        matchcat.add_column(Column([]), name = \"distance arcsec\")\n+        matchcat.add_row((4.1, 5.1, 3, 0.3))\n+        matchcat.add_row((2.1, 3.1, 1, 0.1))\n+        matchcat.add_row((3.1, 4.1, 2, 0.2))\n+        matchcat.add_row((1.0, 2.0, 0, 0.0))\n+        matchcat.add_row((1.1, 2.1, 3, 0.11))\n+        \n+        \n+        stackpsf_infotable = Table()\n+        stackpsf_infotable.add_column(Column([]), name = \"RA\")\n+        stackpsf_infotable.add_column(Column([]), name = \"DEC\")\n+        stackpsf_infotable.add_column(Column([]), name = \"X\")\n+        stackpsf_infotable.add_column(Column([]), name = \"Y\")\n+        stackpsf_infotable.add_column(Column([]), name = \"GRIDX\")\n+        stackpsf_infotable.add_column(Column([]), name = \"GRIDY\")\n+        stackpsf_infotable.add_row((3.0, 4.0, 1.0, 2.0,25.0, 25.0))\n+        stackpsf_infotable.add_row((2.0, 3.0, 1.0, 2.0,74.0, 25.0))\n+        stackpsf_infotable.add_row((4.0, 5.0, 1.0, 2.0,25.0, 74.0))\n+        stackpsf_infotable.add_row((1.0, 2.0, 1.0, 2.0,74.0, 74.0))\n+        \n+        \n+        header = dict()\n+        header[\"STMPSIZE\"] = 49\n+        header[\"NAXIS1\"] = 2048\n+        header[\"NAXIS2\"] = 4098\n+        header[\"CRPIX1\"] = 1024\n+        header[\"CRPIX2\"] = 2048\n+        header[\"CRVAL1\"] = 1024\n+        header[\"CRVAL2\"] = 2048\n+        header[\"CTYPE1\"] = \"RA-TAN\"\n+        header[\"CTYPE2\"] = \"DEC-TAN\"\n+        header[\"CD1_1\"] = -7.277777777778E-05\n+        header[\"CD1_2\"] = 0.0\n+        header[\"CD2_1\"] = 0.0\n+        header[\"CD2_2\"] = 7.277777777778E-05\n+        \n+        new_image, new_infotable = self.spcd.rebuild_stackpsf(matchcat, stackpsf_infotable, img, header)\n+        \n+        \n+        self.assertTrue(new_image[0:49,0:49].sum() == 49*49*0)\n+        self.assertTrue(new_image[0:49,49:98].sum() == 49*49*1)\n+        self.assertTrue(new_image[0:49,98:].sum() == 49*49*2)\n+        self.assertTrue(new_image[49:98,0:49].sum() == 49*49*3)\n+        self.assertTrue(new_image[49:98,49:98].sum() == 49*49*3)\n+        self.assertTrue(new_image[49:98,98:].sum() == 49*49*0)\n+    #\n+\n+#\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestStackPsfStamp_test.py": [
                        [
                            "@@ -0,0 +1,82 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+from EXT_PF1_GEN_P2 import Pipeline\n+from EXT_PF1_GEN_P2.psf import stackPSF\n+from EXT_PF1_GEN_P2.psf import Stamp\n+\n+from EXT_PF1_GEN_P2_LIBS.astronomy import ArrayUtils\n+\n+from astropy.io import fits\n+from astropy.wcs import WCS\n+import numpy as np\n+import math\n+import unittest\n+import os\n+from matplotlib import pyplot as plt\n+\n+\n+class TestStackPsfStamp(unittest.TestCase):\n+    def setUp(self):\n+        pass\n+    def tearDown(self):\n+        pass\n+    def test_get_stamp_at(self):\n+        data = np.zeros(shape=(4096,2048), dtype=float, order='F')\n+        gains = np.ones((4096,2048), dtype = np.float64)\n+\n+        au = ArrayUtils.ArrayUtils()\n+        pixels = au.get_array_indices(np.empty(data.shape))\n+        fwhm = 2.25\n+        \n+        parameters = {\n+            \"amplitude\": 1,\n+            \"center_row\": 625.3,\n+            \"center_col\": 203.8,\n+            \"sigma_row\": fwhm,                   \n+            \"sigma_col\": fwhm,\n+            \"angle\": 0,\n+            \"offset\": 0\n+        }\n+        img =  au.gauss_2d(pixels, **parameters).reshape(data.shape)\n+        results = au.fit_gauss_2d(img)\n+\n+        #is the stamp fit correct?\n+        print(\"results \", results)\n+        self.assertTrue( abs(results[1][0] - 625.3) < 0.001) #center\n+        self.assertTrue( abs(results[1][1] - 203.8) < 0.001)\n+\n+        self.assertTrue( abs(results[2][1] - fwhm) < 0.001) #width 10exp-3 of a pixel\n+        self.assertTrue( abs(results[2][0] - fwhm) < 0.001)\n+        \n+        stamp = Stamp.Stamp(img, 49, data.shape)\n+        stampfit = stamp.get_stamp_at(203.8, 625.3)\n+        \n+        stmp_results = au.fit_gauss_2d(stampfit)\n+        print(\"stmp_results \", stmp_results)\n+        \n+        print(\"flux \", stampfit.sum(), img.sum())\n+        self.assertTrue( abs(stampfit.sum() - img.sum())/stampfit.sum() < 0.0001 )\n+\n+        #self.assertTrue(newimg.shape == (49,49))\n+        #self.assertTrue(newimg.sum() == 0.0)        \n+    #\n+    \n+    \n+#\n+\n+if __name__ == '__main__':\n+    unittest.main()\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestWcsUtils.py": [
                        [
                            "@@ -1,11 +0,0 @@\n-import unittest\n-import os\n-class TestWcsUtils(unittest.TestCase):\n-    def setUp(self):\n-        pass\n-    def tearDown(self):\n-        pass\n-    def test_py_pix2world(self):\n-        self.assertTrue(1)\n-    def test_py_world2pix(self):\n-        self.assertTrue(1)                                           \n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "EXT_PF1_GEN_P2/tests/python/TestWcsUtils_test.py": [
                        [
                            "@@ -0,0 +1,33 @@\n+import unittest\n+import os\n+\n+from EXT_PF1_GEN_P2.psf import WcsUtils\n+\n+class TestWcsUtils(unittest.TestCase):\n+    def setUp(self):\n+         #wcs from header\n+        header = {}\n+        header[\"NAXiS\"] = 2\n+        header[\"NAXIS1\"] = 1000\n+        header[\"NAXIS2\"] = 1000\n+        header[\"CRPIX1\"] = 0.0\n+        header[\"CRPIX2\"] = 0.0\n+        header[\"CRVAL1\"] = 0.0\n+        header[\"CRVAL2\"] = 0.0\n+        #1 arcsec per pixel\n+        header[\"CD1_1\"] = 2.6603E-06\n+        header[\"CD1_2\"] = 7.10338E-05\n+        header[\"CD2_1\"] = 7.10338E-05\n+        header[\"CD2_2\"] = -2.6603E-06\n+        self.header = header\n+        self.wcsu = WcsUtils.WcsUtils(header)\n+    def tearDown(self):\n+        pass\n+    def test_py_pix2world(self):\n+        world = self.wcsu.py_pix2world(self.header[\"CRPIX1\"], self.header[\"CRPIX2\"])\n+        self.assertTrue(self.header[\"CRVAL1\"] ==  world[0])\n+        self.assertTrue(self.header[\"CRVAL2\"] ==  world[1])\n+    def test_py_world2pix(self):\n+        pix = self.wcsu.py_world2pix([self.header[\"CRVAL1\"], self.header[\"CRVAL2\"]])\n+        self.assertTrue(self.header[\"CRPIX1\"] ==  pix[0])\n+        self.assertTrue(self.header[\"CRPIX2\"] ==  pix[1])\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ],
                    "Makefile": [
                        [
                            "@@ -44,148 +44,19 @@\n #\n ################################################################################\n \n-# settings\n-CMAKE := cmake\n-CTEST := ctest\n-NINJA := $(shell which ninja-build 2> /dev/null)\n-ifeq ($(NINJA),)\n-  NINJA := $(shell which ninja 2> /dev/null)\n-endif\n-\n-\n-# Looking for the ToolChain\n+ELEMENTS_MAKE_LIB := Elements.mk\n \n-TOOLCHAIN_NAME := ElementsToolChain.cmake\n-\n-ifneq ($(wildcard $(CURDIR)/cmake/$(TOOLCHAIN_NAME)),)\n-  TOOLCHAIN_FILE := $(CURDIR)/cmake/$(TOOLCHAIN_NAME)\n+ifneq ($(wildcard $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)),)\n+  ELEMENTS_MAKE_LIB_FILE := $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)\n else\n   ifneq ($(CMAKE_PREFIX_PATH),)\n     PREFIX_LIST := $(subst :, ,$(CMAKE_PREFIX_PATH))\n-    TOOLCHAIN_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/lib*/cmake/ElementsProject/$(TOOLCHAIN_NAME) $(dir)/$(TOOLCHAIN_NAME)))\n-    TOOLCHAIN_FILE := $(firstword $(TOOLCHAIN_LIST))\n-  endif\n-endif\n-\n-override ALL_CMAKEFLAGS := -Wno-dev --no-warn-unused-cli\n-\n-ifneq ($(TOOLCHAIN_FILE),)\n-  # A toolchain has been found. Lets use it.\n-  override ALL_CMAKEFLAGS += -DCMAKE_TOOLCHAIN_FILE=$(TOOLCHAIN_FILE)\n-endif\n-\n-\n-BUILD_PREFIX_NAME := build\n-\n-override ALL_CMAKEFLAGS += -DUSE_LOCAL_INSTALLAREA=ON -DBUILD_PREFIX_NAME:STRING=$(BUILD_PREFIX_NAME)\n-override ALL_CMAKEFLAGS += -DUSE_VERSIONED_LIBRARIES=OFF\n-\n-ifndef BINARY_TAG\n-  ifdef CMAKECONFIG\n-    BINARY_TAG := ${CMAKECONFIG}\n-  else\n-    ifdef CMTCONFIG\n-      BINARY_TAG := ${CMTCONFIG}\n-    endif\n+    ELEMENTS_MAKE_LIB_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/share/Elements/make/$(ELEMENTS_MAKE_LIB) $(dir)/../make/$(ELEMENTS_MAKE_LIB)))\n   endif\n+  ELEMENTS_MAKE_LIB_LIST += /usr/share/Elements/make/$(ELEMENTS_MAKE_LIB)\n+  ELEMENTS_MAKE_LIB_FILE := $(firstword $(ELEMENTS_MAKE_LIB_LIST))\n endif\n \n-ifdef BINARY_TAG\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME).$(BINARY_TAG)\n-else\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME)\n-endif\n-BUILDDIR := $(CURDIR)/$(BUILD_SUBDIR)\n-\n-# build tool\n-\n-ifneq ($(USE_NINJA),)\n-  # enable Ninja\n-  override ALL_CMAKEFLAGS += -GNinja\n-  BUILD_CONF_FILE := build.ninja\n-  BUILDFLAGS := $(NINJAFLAGS)\n-  ifneq ($(VERBOSE),)\n-    BUILDFLAGS := -v $(BUILDFLAGS)\n-  endif\n-else\n-  BUILD_CONF_FILE := Makefile\n-endif\n-BUILD_CMD := $(CMAKE) --build $(BUILD_SUBDIR) --target\n-\n-\n-# Use environment variable for extra flags\n-\n-# Replace the \":\" from eclipse variable list to spaces  \n-ifneq ($(EXPAND_FLAGS),)\n-  CMAKEFLAGS := $(subst :-, -,$(CMAKEFLAGS))\n-endif\n-\n-ifneq ($(CMAKEFLAGS),)\n-  override ALL_CMAKEFLAGS += $(CMAKEFLAGS)\n-endif\n-\n-# default target\n-all:\n-\n-# deep clean\n-purge:\n-\t$(RM) -r $(BUILDDIR) $(CURDIR)/InstallArea/$(BINARY_TAG)\n-\tfind $(CURDIR) \"(\" -name \"InstallArea\" -prune -o -name \"*.pyc\" -o -name \"*.pyo\" \")\" -a -type f -exec $(RM) -v \\{} \\;\n-\tfind $(CURDIR) -depth -type d -name \"__pycache__\" -exec $(RM) -rv \\{} \\;\n-\n-# delegate any target to the build directory (except 'purge')\n-ifneq ($(MAKECMDGOALS),purge)\n-%: $(BUILDDIR)/$(BUILD_CONF_FILE) FORCE\n-\t+$(BUILD_CMD) $* -- $(BUILDFLAGS)\n-endif\n-\n-# aliases\n-.PHONY: configure tests FORCE\n-ifneq ($(wildcard $(BUILDDIR)/$(BUILD_CONF_FILE)),)\n-configure: rebuild_cache\n-else\n-configure: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-endif\n-\t@ # do not delegate further\n-\n-\n-# This wrapping around the test target is used to ensure the generation of\n-# the XML output from ctest.\n-test: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) HTMLSummary\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-\n-# This target ensures that the \"all\" target is called before\n-# running the tests (unlike the \"test\" default target of CMake)\n-tests: all\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) HTMLSummary\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-ifeq ($(VERBOSE),)\n-# less verbose install\n-# (emulate the default CMake install target)\n-install: all\n-\tcd $(BUILDDIR) && $(CMAKE) -P cmake_install.cmake | grep -v \"^-- Up-to-date:\"\n-endif\n-\n-# ensure that the target are always passed to the CMake Makefile\n-FORCE:\n-\t@ # dummy target\n-\n-# Makefiles are used as implicit targets in make, but we should not consider\n-# them for delegation.\n-$(MAKEFILE_LIST):\n-\t@ # do not delegate further\n-\n-\n-# trigger CMake configuration\n-$(BUILDDIR)/$(BUILD_CONF_FILE): | $(BUILDDIR)\n-\tcd $(BUILDDIR) && $(CMAKE) $(ALL_CMAKEFLAGS) $(CURDIR)\n+$(info Using the $(ELEMENTS_MAKE_LIB_FILE) make library)\n+include $(ELEMENTS_MAKE_LIB_FILE)\n \n-$(BUILDDIR):\n-\tmkdir -p $(BUILDDIR)\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ],
                        [
                            "@@ -44,170 +44,19 @@\n #\n ################################################################################\n \n-# settings\n-CMAKE := cmake\n-CTEST := ctest\n-NINJA := $(shell which ninja-build 2> /dev/null)\n-ifeq ($(NINJA),)\n-  NINJA := $(shell which ninja 2> /dev/null)\n-endif\n-\n-# Looking for the Custom make library\n-\n-CUSTOM_MAKE_LIB := Custom.mk\n-\n-ifneq ($(wildcard $(CURDIR)/make/$(CUSTOM_MAKE_LIB)),)\n-  CUSTOM_MAKE_LIB_FILE := $(CURDIR)/make/$(CUSTOM_MAKE_LIB)\n-else\n-  ifneq ($(CMAKE_PREFIX_PATH),)\n-    PREFIX_LIST := $(subst :, ,$(CMAKE_PREFIX_PATH))\n-    CUSTOM_MAKE_LIB_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/share/Elements/make/$(CUSTOM_MAKE_LIB) $(dir)/../make/$(CUSTOM_MAKE_LIB)))\n-  endif\n-  CUSTOM_MAKE_LIB_LIST += /usr/share/Elements/make/$(CUSTOM_MAKE_LIB)\n-  CUSTOM_MAKE_LIB_FILE := $(firstword $(CUSTOM_MAKE_LIB_LIST))\n-endif\n-\n-# Looking for the ToolChain\n-\n-TOOLCHAIN_NAME := ElementsToolChain.cmake\n+ELEMENTS_MAKE_LIB := Elements.mk\n \n-ifneq ($(wildcard $(CURDIR)/cmake/$(TOOLCHAIN_NAME)),)\n-  TOOLCHAIN_FILE := $(CURDIR)/cmake/$(TOOLCHAIN_NAME)\n+ifneq ($(wildcard $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)),)\n+  ELEMENTS_MAKE_LIB_FILE := $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)\n else\n   ifneq ($(CMAKE_PREFIX_PATH),)\n     PREFIX_LIST := $(subst :, ,$(CMAKE_PREFIX_PATH))\n-    TOOLCHAIN_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/lib*/cmake/ElementsProject/$(TOOLCHAIN_NAME) $(dir)/$(TOOLCHAIN_NAME)))\n-    TOOLCHAIN_FILE := $(firstword $(TOOLCHAIN_LIST))\n-  endif\n-endif\n-\n-override ALL_CMAKEFLAGS := -Wno-dev --no-warn-unused-cli\n-\n-ifneq ($(TOOLCHAIN_FILE),)\n-  # A toolchain has been found. Lets use it.\n-  override ALL_CMAKEFLAGS += -DCMAKE_TOOLCHAIN_FILE=$(TOOLCHAIN_FILE)\n-endif\n-\n-\n-BUILD_PREFIX_NAME := build\n-\n-override ALL_CMAKEFLAGS += -DUSE_LOCAL_INSTALLAREA=ON -DBUILD_PREFIX_NAME:STRING=$(BUILD_PREFIX_NAME)\n-override ALL_CMAKEFLAGS += -DUSE_VERSIONED_LIBRARIES=OFF\n-\n-ifndef BINARY_TAG\n-  ifdef CMAKECONFIG\n-    BINARY_TAG := ${CMAKECONFIG}\n-  else\n-    ifdef CMTCONFIG\n-      BINARY_TAG := ${CMTCONFIG}\n-    endif\n-  endif\n-endif\n-\n-ifdef BINARY_TAG\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME).$(BINARY_TAG)\n-else\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME)\n-endif\n-BUILDDIR := $(CURDIR)/$(BUILD_SUBDIR)\n-\n-# build tool\n-\n-ifneq ($(USE_NINJA),)\n-  # enable Ninja\n-  override ALL_CMAKEFLAGS += -GNinja\n-  BUILD_CONF_FILE := build.ninja\n-  BUILDFLAGS := $(NINJAFLAGS)\n-  ifneq ($(VERBOSE),)\n-    BUILDFLAGS := -v $(BUILDFLAGS)\n+    ELEMENTS_MAKE_LIB_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/share/Elements/make/$(ELEMENTS_MAKE_LIB) $(dir)/../make/$(ELEMENTS_MAKE_LIB)))\n   endif\n-else\n-  BUILD_CONF_FILE := Makefile\n-endif\n-BUILD_CMD := $(CMAKE) --build $(BUILD_SUBDIR) --target\n-\n-\n-# Use environment variable for extra flags\n-\n-# Replace the \":\" from eclipse variable list to spaces  \n-ifneq ($(EXPAND_FLAGS),)\n-  CMAKEFLAGS := $(subst :-, -,$(CMAKEFLAGS))\n-endif\n-\n-ifneq ($(CMAKEFLAGS),)\n-  override ALL_CMAKEFLAGS += $(CMAKEFLAGS)\n+  ELEMENTS_MAKE_LIB_LIST += /usr/share/Elements/make/$(ELEMENTS_MAKE_LIB)\n+  ELEMENTS_MAKE_LIB_FILE := $(firstword $(ELEMENTS_MAKE_LIB_LIST))\n endif\n \n-# default target\n-all:\n-\n-# deep clean\n-purge:\n-\t$(RM) -r $(BUILDDIR) $(CURDIR)/InstallArea/$(BINARY_TAG)\n-\tfind $(CURDIR) \"(\" -name \"InstallArea\" -prune -o -name \"*.pyc\" -o -name \"*.pyo\" \")\" -a -type f -exec $(RM) -v \\{} \\;\n-\tfind $(CURDIR) -depth -type d -name \"__pycache__\" -exec $(RM) -rv \\{} \\;\n-\n-# Remove all the possible directories and the whole InstallArea as well\n-mrproper:\n-\t$(RM) -r $(CURDIR)/build $(CURDIR)/build.* $(CURDIR)/InstallArea\n-\tfind $(CURDIR) \"(\" -name \"*.pyc\" -o -name \"*.pyo\" \")\" -a -type f -exec $(RM) -v \\{} \\;\n-\tfind $(CURDIR) -depth -type d -name \"__pycache__\" -exec $(RM) -rv \\{} \\;\n-\n-# delegate any target to the build directory (except 'purge')\n-ifneq ($(MAKECMDGOALS),purge)\n-ifneq ($(MAKECMDGOALS),mrproper)\n-%: $(BUILDDIR)/$(BUILD_CONF_FILE) FORCE\n-\t+$(BUILD_CMD) $* -- $(BUILDFLAGS)\n-endif\n-endif\n-\n-# aliases\n-.PHONY: configure tests FORCE\n-ifneq ($(wildcard $(BUILDDIR)/$(BUILD_CONF_FILE)),)\n-configure: rebuild_cache\n-else\n-configure: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-endif\n-\t@ # do not delegate further\n-\n-\n-# This wrapping around the test target is used to ensure the generation of\n-# the XML output from ctest.\n-test: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-\n-# This target ensures that the \"all\" target is called before\n-# running the tests (unlike the \"test\" default target of CMake)\n-tests: all\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-ifeq ($(VERBOSE),)\n-# less verbose install\n-# (emulate the default CMake install target)\n-install: all\n-\tcd $(BUILDDIR) && $(CMAKE) -P cmake_install.cmake | grep -v \"^-- Up-to-date:\"\n-endif\n-\n-# import the library to look for a custom Makefile\n--include $(CUSTOM_MAKE_LIB_FILE)\n-\n-# ensure that the target are always passed to the CMake Makefile\n-FORCE: ;\n-\n-# Makefiles are used as implicit targets in make, but we should not consider\n-# them for delegation.\n-$(MAKEFILE_LIST): ;\n-\n-\n-# trigger CMake configuration\n-$(BUILDDIR)/$(BUILD_CONF_FILE): | $(BUILDDIR)\n-\tcd $(BUILDDIR) && $(CMAKE) $(ALL_CMAKEFLAGS) $(CURDIR)\n-\n-$(BUILDDIR):\n-\tmkdir -p $(BUILDDIR)\n+$(info Using the $(ELEMENTS_MAKE_LIB_FILE) make library)\n+include $(ELEMENTS_MAKE_LIB_FILE)\n \n",
                            "updated Makefile for elements 6.1.1",
                            "Michael",
                            "2023-05-12T11:20:47.000+02:00",
                            "66988119178a3031ea1186dd755a991fc9d3a984"
                        ]
                    ],
                    "README.md": [
                        [
                            "@@ -7,9 +7,16 @@ Custodian: Michael Wetzstein (drwstein@mpe.mpg.de)\n This code repository consists of:\n * the following directories underneath directory [EXT_PF1_GEN_P2](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2):\n    * [conf](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/conf): the pipeline configuration files that are input to the script that executes the pipeline, consisting of \n-      * the files [`coadd_template_<CAMERA_NAME>.properties`](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/blob/develop/EXT_PF1_GEN_P2/conf/EXT_PF1_Coadd/coadd_template_GPC.properties) are input to the [PipelineScript_StackedFrameProductionPipeline](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines/-/blob/release-2.65/EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Pipeline/PipelineScript.py), namely via the argument `coadd_props_file`\n-      * the files [`DpdExtConfigurationSet_<CAMERA_NAME>.xml`](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/blob/develop/EXT_PF1_GEN_P2/conf/EXT_PF1_Coadd/DpdExtConfigurationSet_HSC.xml) are input to TBW\n-   * [python/EXT_PF1_Coadd](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_Coadd) contains the algorithms of the processing\n+      * the files [`coadd_template_<CAMERA_NAME>.properties`](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2) are input to the [PipelineScript_StackedFrameProductionPipeline](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines/-/blob/release-2.65/EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Pipeline/PipelineScript.py), namely via the argument `coadd_props_file`\n+      * the files [`DpdExtConfigurationSet_<CAMERA_NAME>.xml`](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/conf/EXT_PF1_GEN_P2) are input to TBW\n+   * [python/EXT_PF1_GEN_P2](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2) contains the algorithms of the processing\n+   * [python/EXT_PF1_GEN_P2/AstrometricValidation](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/AstrometricValidation) Contains code to evaluate the astrometry of the input catalogs against a reference catalog\n+   * [python/EXT_PF1_GEN_P2/file](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/file) Code to access psfmodels provided by PSFEX\n+   * [python/EXT_PF1_GEN_P2/Fwhm](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Fwhm) Code to measure and validate the fwhm of the stacked psfs against the fwhm of the coadds\n+   * [python/EXT_PF1_GEN_P2/Masks](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Masks) Class to handle and transfer masks of images and coadds\n+   * [python/EXT_PF1_GEN_P2/Pipeline](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/Pipeline) Contains the pipeline script and its helper classes\n+   * [python/EXT_PF1_GEN_P2/pybin](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/pybin) Executables of this pipeline\n+   * [python/EXT_PF1_GEN_P2/RMS](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/python/EXT_PF1_GEN_P2/RMS) A class to measure the rms of an image (including masking of the objects)\n    * [src/lib](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/src/lib) : contains the Python wrapper around the stackedPSF model code\n    * [tests/python](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/tree/develop/EXT_PF1_GEN_P2/tests/python): contains unit tests for the StackedFrame Production Pipeline\n    * [`CMakeLists.txt`](https://gitlab.euclid-sgs.uk/PF-EXT/ext_pf1_gen_p2/-/blob/develop/EXT_PF1_GEN_P2/CMakeLists.txt) : configuration file for Cmake, the build tool used within Euclid's [Elements](https://euclid.roe.ac.uk/projects/codeen-users/wiki/User_Bui_Too): the build software system for  C++ code.\n@@ -22,3 +29,5 @@ Handy links\n * [StackedFrame Production Pipeline work package redmine ticket (#13771)](https://euclid.roe.ac.uk/issues/13771)\n * [stackedPSF model code repository (EXT_STACK_PSF)](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_STACK_PSF/-/tree/develop)\n * [pipeline specification repository (EXT_IAL_PIPELINES)](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines/-/blob/develop/README.md)\n+* [Data Model gitlab project](https://gitlab.euclid-sgs.uk/ST-DM/ST_DataModel)\n+* [Astromatic github](https://github.com/astromatic)\n",
                            "Merge branch 'develop' into gvk/StackedFrameProductionTour",
                            "Gijs Verdoes Kleijn",
                            "2023-06-28T15:52:39.000+02:00",
                            "6f09405abb77b7dcbe3dc27906f9db63d4c734f0"
                        ]
                    ]
                },
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": [
                    {
                        "name": "0.78.0",
                        "created_at": "2023-03-13T16:11:43.000+01:00",
                        "author_name": "Michael"
                    },
                    {
                        "name": "0.9.0",
                        "created_at": "2023-07-04T12:48:58.000+02:00",
                        "author_name": "Michael"
                    }
                ]
            },
            "PF-EXT/EXT_PF1_GEN_P2_LIBS": {
                "start date": "2023-03-06T10:10:48",
                "end date": "2023-07-03T17:42:16",
                "start tag": "0.78.0",
                "end tag": "0.9.0",
                "count_files_modified": "25",
                "modifications_by_file": {
                    ".jenkinsFile": [
                        [
                            "@@ -1,3 +1,3 @@\n #!groovy\n @Library('integration-library@release-10') _\n-pipelineElements(name:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1')\n+pipelineElements(name:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1', skipQuality:true)\n",
                            ", skipQuality:true",
                            "Michael",
                            "2023-07-03T17:42:16.000+02:00",
                            "f6dc7f777c860c1fab16e95476690bc4b12b4e6a"
                        ],
                        [
                            "@@ -1,3 +1,3 @@\n #!groovy\n @Library('integration-library@release-10') _\n-pipelineElements(name:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1', repository:'eden.3.1')\n+pipelineElements(name:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1')\n",
                            ".jenkins",
                            "Michael",
                            "2023-06-27T20:52:56.000+02:00",
                            "d1bedd370f4d82cf08a6f5e2af3620ca613abd3b"
                        ],
                        [
                            "@@ -1,3 +1,3 @@\n #!groovy\n @Library('integration-library@release-10') _\n-pipelineElements(artifactId:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1', repository:'eden.3.1')\n+pipelineElements(name:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1', repository:'eden.3.1')\n",
                            ".jenkins",
                            "Michael",
                            "2023-06-27T20:50:01.000+02:00",
                            "edca351af3285eb788ab800873632c96881c7c3d"
                        ],
                        [
                            "@@ -1,3 +1,3 @@\n #!groovy\n-@Library('integration-library@release-9') _\n+@Library('integration-library@release-10') _\n pipelineElements(artifactId:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1', repository:'eden.3.1')\n",
                            "to DM 9.2",
                            "Michael",
                            "2023-06-27T20:47:56.000+02:00",
                            "bed5118abc8a4396671fb47ec2326b0b02a4ab78"
                        ],
                        [
                            "@@ -1,3 +1,3 @@\n #!groovy\n @Library('integration-library@release-9') _\n-pipelineElements(artifactId:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.0', repository:'eden.3.0')\n+pipelineElements(artifactId:\"EXT_PF1_GEN_P2_LIBS\", component:'eden.3.1', repository:'eden.3.1')\n",
                            "to DM 9.2",
                            "Michael",
                            "2023-06-27T20:39:19.000+02:00",
                            "9e5bbe92e0cebd02c082d234b1f022ae73cdd438"
                        ]
                    ],
                    "CMakeLists.txt": [
                        [
                            "@@ -8,6 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-project(EXT_PF1_GEN_P2_LIBS)\n elements_project(EXT_PF1_GEN_P2_LIBS 0.9.0 USE Elements 6.2.1 ST_DataModel 9.2.0  ST_DataModelTools 9.2.1 EL_PSFExModel 10.2.0)\n \n",
                            ", skipQuality:true",
                            "Michael",
                            "2023-07-03T17:42:16.000+02:00",
                            "f6dc7f777c860c1fab16e95476690bc4b12b4e6a"
                        ],
                        [
                            "@@ -8,5 +8,6 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.9 USE Elements 6.2.1 ST_DataModel 9.2.0  ST_DataModelTools 9.2.1 EL_PSFExModel 10.2)\n+project(EXT_PF1_GEN_P2_LIBS)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.9.0 USE Elements 6.2.1 ST_DataModel 9.2.0  ST_DataModelTools 9.2.1 EL_PSFExModel 10.2.0)\n \n",
                            "release 0.9",
                            "Michael",
                            "2023-07-03T16:21:03.000+02:00",
                            "d8dd78d68ff45fd8da3498c0a7467f18cdb2a94c"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.9 USE Elements 6.2.1 ST_DataModel 9.2  ST_DataModelTools 9.2 EL_PSFExModel 10.2)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.9 USE Elements 6.2.1 ST_DataModel 9.2.0  ST_DataModelTools 9.2.1 EL_PSFExModel 10.2)\n \n",
                            "9.2/9.2 to 9.2.0/9.2.1",
                            "Michael",
                            "2023-06-28T15:57:43.000+02:00",
                            "87c1620d8a8954a3069d89f67bb5fe07dcfeaacc"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.80 USE Elements 6.1.1 ST_DataModel 9.1.5  ST_DataModelTools 9.1.2 EL_PSFExModel 10.1)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.9 USE Elements 6.2.1 ST_DataModel 9.2  ST_DataModelTools 9.2 EL_PSFExModel 10.2)\n \n",
                            "to DM 9.2",
                            "Michael",
                            "2023-06-27T20:39:19.000+02:00",
                            "9e5bbe92e0cebd02c082d234b1f022ae73cdd438"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.79 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.80 USE Elements 6.1.1 ST_DataModel 9.1.5  ST_DataModelTools 9.1.2 EL_PSFExModel 10.1)\n \n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.80 USE Elements 6.1.1 ST_DataModel 9.1.5  ST_DataModelTools 9.1.2 EL_PSFExModel 10.1)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.79 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n \n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.80 USE Elements 6.1.1 ST_DataModel 9.1.5  ST_DataModelTools 9.1.2 EL_PSFExModel 10.1)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.79 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n \n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.79 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.80 USE Elements 6.1.1 ST_DataModel 9.1.5  ST_DataModelTools 9.1.2 EL_PSFExModel 10.1)\n \n",
                            "update to DM 9.1.5",
                            "Michael",
                            "2023-03-20T17:25:32.000+01:00",
                            "d7fa36f74bb28c14b1afdd7446e95bc1949503e1"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.78 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.79 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n \n",
                            "count up version. Improved ProcessManager",
                            "Michael",
                            "2023-03-13T13:52:35.000+01:00",
                            "30b6fdedd75433b62e05d99c8b27c4d428d77d1d"
                        ],
                        [
                            "@@ -8,5 +8,5 @@ find_package(ElementsProject)\n # Declare project name and version\n #dev version: 2 digits\n #release: 3 digits\n-elements_project(EXT_PF1_GEN_P2_LIBS 0.78 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n+elements_project(EXT_PF1_GEN_P2_LIBS 0.79 USE Elements 6.0.1 ST_DataModel 9.0.3  ST_DataModelTools 9.0.3 EL_PSFExModel 9.0.1)\n \n",
                            "Set version to 0.79",
                            "Jelte de Jong",
                            "2023-03-13T10:06:53.000+01:00",
                            "a11cd41f309a6eae2d7111943cd3b66c5f93aa38"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/Pipeline/Pipeline.py": [
                        [
                            "@@ -777,12 +777,9 @@ class Pipeline(lm):\n             d['_ingestion'] = int(xmlcontents.get('ingestion'))\n         except:\n             d['_ingestion'] = 0\n-        try:\n-            from EXT_PF1_GEN_P2_LIBS.Pipeline import Postprocessing\n-            d['_Postprocessing'] = Postprocessing.Postprocessing(xmlcontents)\n-        except:\n-            #for example, if the ingest package is not available\n-            d['_Postprocessing'] = DummyCall()\n+        \n+        #for example, if the ingest package is not available\n+        d['_Postprocessing'] = DummyCall()\n         #\n         \n         try:\n",
                            "release 0.9",
                            "Michael",
                            "2023-07-03T16:59:41.000+02:00",
                            "fc5c52b3e1ddaf8862ac763d19c3bbca967c4a2b"
                        ],
                        [
                            "@@ -690,7 +690,8 @@ class Pipeline(lm):\n         try:\n             d['_swarp_temp'] = xmlcontents['swarp_temp']\n         except:\n-            d['_swarp_temp'] = xmlcontents['_ramdisk']\n+            d['_swarp_temp'] = xmlcontents['ramdisk']\n+        #\n         exclude_list = []\n                 \n         #exclude list\n",
                            "corrected a typo",
                            "Michael",
                            "2023-05-24T15:17:47.000+02:00",
                            "0ae44fe14e0293929ce08c172bf32148123beae2"
                        ],
                        [
                            "@@ -687,6 +687,10 @@ class Pipeline(lm):\n         \"\"\" \n         d['PipelineTypes'] = PipelineTypes\n         d['_ramdisk'] = xmlcontents['ramdisk']\n+        try:\n+            d['_swarp_temp'] = xmlcontents['swarp_temp']\n+        except:\n+            d['_swarp_temp'] = xmlcontents['_ramdisk']\n         exclude_list = []\n                 \n         #exclude list\n",
                            "add a separate folder for swarp temp (took it out of the general temp)",
                            "Michael",
                            "2023-05-16T20:43:23.000+02:00",
                            "c69fbbe0776176fa8e39d372dab931a65257f1dc"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/Pipeline/Postprocessing.py": [
                        [
                            "@@ -1,61 +0,0 @@\n-\"\"\"\n-Copyright (C) 2012-2020 Euclid Science Ground Segment\n-\n-This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n-Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n-any later version.\n-\n-This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n-warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n-details.\n-\n-You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n-the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n-\n-\"\"\"\n-from builtins import object\n-\n-from ingest import Metadata_Ingestor\n-import os\n-\n-class Postprocessing(object):\n-    \n-    def __init__(self, props):\n-        \"\"\"\n-        props: properties of the pipeline, like important paths (ramdsk, rundir etc.)\n-        \"\"\"\n-        self.props = props\n-        self.metadata_ingestion = \"metadata_ingestion\"\n-        self.qa_merger = \"qa_merger\"\n-        try:\n-            self.ingestion = props['ingestion']\n-        except:\n-            self.ingestion = 0\n-        #\n-        #print 'Postprocessing: I will ingest ', self.ingestion\n-        \n-    #\n-        \n-    \n-    def execute(self, modules):\n-        \"\"\"\n-        modules: a list of modules\n-        \"\"\"\n-        for module in modules:\n-            if module ==  self.metadata_ingestion and self.ingestion:\n-                self.ingest()\n-            elif module == self.qa_merger and self.props['qa_execution']:\n-                from EXT_PF1_GEN_P2_LIBS.Pipeline import QaStuff\n-                qas = QaStuff.QaSettings(self.props)\n-                qas.merge_qa_database()\n-        #\n-    #\n-    \n-    \n-    def ingest(self):\n-        mi = Metadata_Ingestor.Metadata_Ingestor(self.props)\n-        mi.ingest()\n-    #\n-    \n-#\n-\n",
                            "release 0.9",
                            "Michael",
                            "2023-07-03T16:59:41.000+02:00",
                            "fc5c52b3e1ddaf8862ac763d19c3bbca967c4a2b"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/tests/python/TestPostprocessing_test.py": [
                        [
                            "@@ -1,35 +0,0 @@\n-\"\"\"\n-Copyright (C) 2012-2020 Euclid Science Ground Segment\n-\n-This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n-Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n-any later version.\n-\n-This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n-warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n-details.\n-\n-You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n-the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n-\n-\"\"\"\n-\n-import unittest\n-import os\n-\n-\n-class TestPostprocessing(unittest.TestCase):\n-    def setUp(self):\n-        try:\n-            from EXT_PF1_GEN_P2_LIBS.Pipeline import Postprocessing\n-            ppr = Postprocessing.Postprocessing({'ingestion':0})\n-        except:\n-            pass #ingst module missing at SDC\n-    def tearDown(self):\n-        pass\n-    #\n-#\n-\n-if __name__ == '__main__':\n-    unittest.main()\n-#\n",
                            "release 0.9",
                            "Michael",
                            "2023-07-03T16:59:41.000+02:00",
                            "fc5c52b3e1ddaf8862ac763d19c3bbca967c4a2b"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/tests/python/TestPipelineControl_test.py": [
                        [
                            "@@ -112,7 +112,7 @@ class TestPipelineControl(unittest.TestCase):\n         \n         os.remove(logfile.name)\n         size = []\n-        for l in log:\n+        for l in log[1:]:\n             size.append(l.split(\",\")[-1][:-1])\n         for s in size:\n             self.assertTrue(size[0] == s)\n",
                            "to DM 9.2",
                            "Michael",
                            "2023-06-27T20:39:19.000+02:00",
                            "9e5bbe92e0cebd02c082d234b1f022ae73cdd438"
                        ]
                    ],
                    "Makefile": [
                        [
                            "",
                            "to DM 9.2",
                            "Michael",
                            "2023-06-27T20:39:19.000+02:00",
                            "9e5bbe92e0cebd02c082d234b1f022ae73cdd438"
                        ],
                        [
                            "@@ -44,170 +44,19 @@\n #\n ################################################################################\n \n-# settings\n-CMAKE := cmake\n-CTEST := ctest\n-NINJA := $(shell which ninja-build 2> /dev/null)\n-ifeq ($(NINJA),)\n-  NINJA := $(shell which ninja 2> /dev/null)\n-endif\n-\n-# Looking for the Custom make library\n-\n-CUSTOM_MAKE_LIB := Custom.mk\n-\n-ifneq ($(wildcard $(CURDIR)/make/$(CUSTOM_MAKE_LIB)),)\n-  CUSTOM_MAKE_LIB_FILE := $(CURDIR)/make/$(CUSTOM_MAKE_LIB)\n-else\n-  ifneq ($(CMAKE_PREFIX_PATH),)\n-    PREFIX_LIST := $(subst :, ,$(CMAKE_PREFIX_PATH))\n-    CUSTOM_MAKE_LIB_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/share/Elements/make/$(CUSTOM_MAKE_LIB) $(dir)/../make/$(CUSTOM_MAKE_LIB)))\n-  endif\n-  CUSTOM_MAKE_LIB_LIST += /usr/share/Elements/make/$(CUSTOM_MAKE_LIB)\n-  CUSTOM_MAKE_LIB_FILE := $(firstword $(CUSTOM_MAKE_LIB_LIST))\n-endif\n-\n-# Looking for the ToolChain\n-\n-TOOLCHAIN_NAME := ElementsToolChain.cmake\n+ELEMENTS_MAKE_LIB := Elements.mk\n \n-ifneq ($(wildcard $(CURDIR)/cmake/$(TOOLCHAIN_NAME)),)\n-  TOOLCHAIN_FILE := $(CURDIR)/cmake/$(TOOLCHAIN_NAME)\n+ifneq ($(wildcard $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)),)\n+  ELEMENTS_MAKE_LIB_FILE := $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)\n else\n   ifneq ($(CMAKE_PREFIX_PATH),)\n     PREFIX_LIST := $(subst :, ,$(CMAKE_PREFIX_PATH))\n-    TOOLCHAIN_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/lib*/cmake/ElementsProject/$(TOOLCHAIN_NAME) $(dir)/$(TOOLCHAIN_NAME)))\n-    TOOLCHAIN_FILE := $(firstword $(TOOLCHAIN_LIST))\n-  endif\n-endif\n-\n-override ALL_CMAKEFLAGS := -Wno-dev --no-warn-unused-cli\n-\n-ifneq ($(TOOLCHAIN_FILE),)\n-  # A toolchain has been found. Lets use it.\n-  override ALL_CMAKEFLAGS += -DCMAKE_TOOLCHAIN_FILE=$(TOOLCHAIN_FILE)\n-endif\n-\n-\n-BUILD_PREFIX_NAME := build\n-\n-override ALL_CMAKEFLAGS += -DUSE_LOCAL_INSTALLAREA=ON -DBUILD_PREFIX_NAME:STRING=$(BUILD_PREFIX_NAME)\n-override ALL_CMAKEFLAGS += -DUSE_VERSIONED_LIBRARIES=OFF\n-\n-ifndef BINARY_TAG\n-  ifdef CMAKECONFIG\n-    BINARY_TAG := ${CMAKECONFIG}\n-  else\n-    ifdef CMTCONFIG\n-      BINARY_TAG := ${CMTCONFIG}\n-    endif\n-  endif\n-endif\n-\n-ifdef BINARY_TAG\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME).$(BINARY_TAG)\n-else\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME)\n-endif\n-BUILDDIR := $(CURDIR)/$(BUILD_SUBDIR)\n-\n-# build tool\n-\n-ifneq ($(USE_NINJA),)\n-  # enable Ninja\n-  override ALL_CMAKEFLAGS += -GNinja\n-  BUILD_CONF_FILE := build.ninja\n-  BUILDFLAGS := $(NINJAFLAGS)\n-  ifneq ($(VERBOSE),)\n-    BUILDFLAGS := -v $(BUILDFLAGS)\n+    ELEMENTS_MAKE_LIB_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/share/Elements/make/$(ELEMENTS_MAKE_LIB) $(dir)/../make/$(ELEMENTS_MAKE_LIB)))\n   endif\n-else\n-  BUILD_CONF_FILE := Makefile\n-endif\n-BUILD_CMD := $(CMAKE) --build $(BUILD_SUBDIR) --target\n-\n-\n-# Use environment variable for extra flags\n-\n-# Replace the \":\" from eclipse variable list to spaces  \n-ifneq ($(EXPAND_FLAGS),)\n-  CMAKEFLAGS := $(subst :-, -,$(CMAKEFLAGS))\n-endif\n-\n-ifneq ($(CMAKEFLAGS),)\n-  override ALL_CMAKEFLAGS += $(CMAKEFLAGS)\n+  ELEMENTS_MAKE_LIB_LIST += /usr/share/Elements/make/$(ELEMENTS_MAKE_LIB)\n+  ELEMENTS_MAKE_LIB_FILE := $(firstword $(ELEMENTS_MAKE_LIB_LIST))\n endif\n \n-# default target\n-all:\n-\n-# deep clean\n-purge:\n-\t$(RM) -r $(BUILDDIR) $(CURDIR)/InstallArea/$(BINARY_TAG)\n-\tfind $(CURDIR) \"(\" -name \"InstallArea\" -prune -o -name \"*.pyc\" -o -name \"*.pyo\" \")\" -a -type f -exec $(RM) -v \\{} \\;\n-\tfind $(CURDIR) -depth -type d -name \"__pycache__\" -exec $(RM) -rv \\{} \\;\n-\n-# Remove all the possible directories and the whole InstallArea as well\n-mrproper:\n-\t$(RM) -r $(CURDIR)/build $(CURDIR)/build.* $(CURDIR)/InstallArea\n-\tfind $(CURDIR) \"(\" -name \"*.pyc\" -o -name \"*.pyo\" \")\" -a -type f -exec $(RM) -v \\{} \\;\n-\tfind $(CURDIR) -depth -type d -name \"__pycache__\" -exec $(RM) -rv \\{} \\;\n-\n-# delegate any target to the build directory (except 'purge')\n-ifneq ($(MAKECMDGOALS),purge)\n-ifneq ($(MAKECMDGOALS),mrproper)\n-%: $(BUILDDIR)/$(BUILD_CONF_FILE) FORCE\n-\t+$(BUILD_CMD) $* -- $(BUILDFLAGS)\n-endif\n-endif\n-\n-# aliases\n-.PHONY: configure tests FORCE\n-ifneq ($(wildcard $(BUILDDIR)/$(BUILD_CONF_FILE)),)\n-configure: rebuild_cache\n-else\n-configure: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-endif\n-\t@ # do not delegate further\n-\n-\n-# This wrapping around the test target is used to ensure the generation of\n-# the XML output from ctest.\n-test: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-\n-# This target ensures that the \"all\" target is called before\n-# running the tests (unlike the \"test\" default target of CMake)\n-tests: all\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-ifeq ($(VERBOSE),)\n-# less verbose install\n-# (emulate the default CMake install target)\n-install: all\n-\tcd $(BUILDDIR) && $(CMAKE) -P cmake_install.cmake | grep -v \"^-- Up-to-date:\"\n-endif\n-\n-# import the library to look for a custom Makefile\n--include $(CUSTOM_MAKE_LIB_FILE)\n-\n-# ensure that the target are always passed to the CMake Makefile\n-FORCE: ;\n-\n-# Makefiles are used as implicit targets in make, but we should not consider\n-# them for delegation.\n-$(MAKEFILE_LIST): ;\n-\n-\n-# trigger CMake configuration\n-$(BUILDDIR)/$(BUILD_CONF_FILE): | $(BUILDDIR)\n-\tcd $(BUILDDIR) && $(CMAKE) $(ALL_CMAKEFLAGS) $(CURDIR)\n-\n-$(BUILDDIR):\n-\tmkdir -p $(BUILDDIR)\n+$(info Using the $(ELEMENTS_MAKE_LIB_FILE) make library)\n+include $(ELEMENTS_MAKE_LIB_FILE)\n \n",
                            "add a separate folder for swarp temp (took it out of the general temp)",
                            "Michael",
                            "2023-05-16T20:43:23.000+02:00",
                            "c69fbbe0776176fa8e39d372dab931a65257f1dc"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/Pipeline/PipelineControl.py": [
                        [
                            "@@ -98,7 +98,7 @@ class PipelineControl(object):\n             #set the alive signal\n             heartbeat_thread = self.start_heartbeat(options.ramdisk)\n             rd = options.ramdisk\n-            if \"swarp_temp\" in props and os.isdir(props[\"swarp_temp\"]):\n+            if \"swarp_temp\" in props and os.path.isdir(props[\"swarp_temp\"]):\n                 rd = props[\"swarp_temp\"]\n             ramdisk_measure_thread, ramdisk_logfile = self.measure_ramdisk(rd, props[\"log_dir\"])\n             \n",
                            "fix typo",
                            "Michael",
                            "2023-06-17T12:35:09.000+02:00",
                            "b63a769dbbfa509fbe6083cee2e7585f217d58a0"
                        ],
                        [
                            "@@ -97,7 +97,10 @@ class PipelineControl(object):\n             \n             #set the alive signal\n             heartbeat_thread = self.start_heartbeat(options.ramdisk)\n-            ramdisk_measure_thread, ramdisk_logfile = self.measure_ramdisk(options.ramdisk, props[\"log_dir\"])\n+            rd = options.ramdisk\n+            if \"swarp_temp\" in props and os.isdir(props[\"swarp_temp\"]):\n+                rd = props[\"swarp_temp\"]\n+            ramdisk_measure_thread, ramdisk_logfile = self.measure_ramdisk(rd, props[\"log_dir\"])\n             \n             \n             #start the qa database, if qa is available\n@@ -723,6 +726,7 @@ class PipelineControl(object):\n         if not os.path.exists(logdir):\n             os.makedirs(logdir)\n         logfile = open(logdir +os.sep +\"ramdisk.log\", 'a')\n+        logfile.write(\"measure \" +ramdisk +\"\\n\")\n         \n         def log(logfile, ramdisk):\n             running = True\n",
                            "log ramdisk and swarptemp size",
                            "Michael",
                            "2023-06-16T10:45:32.000+02:00",
                            "86f3e6f717e5549036ad2d49a3eb40e2d86c09bd"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/Pipeline/ProcessManager.py": [
                        [
                            "@@ -3,9 +3,10 @@ import time\n \n class ProcessManager:\n \n-    def __init__(self, number_of_Processes):\n+    def __init__(self, number_of_Processes, process_name = \"Process\"):\n         self.number_of_Processes = number_of_Processes\n-\n+        self.process_name = process_name\n+        \n         #initialize process dict\n         self.processes = dict()\n         for i in range(self.number_of_Processes):\n@@ -44,7 +45,7 @@ class ProcessManager:\n \n     def block_until_free_process(self):            \n         while len(self.free()) == 0:\n-            print(\"blocking until free process is available\")\n+            #print(\"blocking until free process is available\")\n             time.sleep(5)\n         #\n         return\n@@ -60,7 +61,6 @@ class ProcessManager:\n                 #\n             #\n             if alive > 0:\n-                print(\"waiting for all parallel threads to finish. \", alive , \" still working.\")\n                 time.sleep(5)\n             else:\n                 wait = False\n@@ -73,8 +73,9 @@ class ProcessManager:\n         for k in self.processes.keys():\n             if self.processes[k] is not None and self.processes[k].is_alive():\n             \t#found one still active\n-                print(\"...waiting for all parallel threads to finish. \")\n+                print(self.process_name, \":...waiting for all parallel threads to finish. \")\n                 self.processes[k].join()\n+                print(self.process_name, \": finished. \")\n             #\n         #\n         return\n",
                            "log ramdisk and swarptemp size",
                            "Michael",
                            "2023-06-16T10:45:32.000+02:00",
                            "86f3e6f717e5549036ad2d49a3eb40e2d86c09bd"
                        ],
                        [
                            "@@ -14,13 +14,14 @@ class ProcessManager:\n     #\n \n     def add_process_and_start(self, method, args):\n+        self.block_until_free_process()\n+        \n         pnumbers = self.free()\n         if len(pnumbers) > 0:\n             p = mp.Process(target=method, args=args)\n             p.start()\n             self.processes[ pnumbers[0] ] = p\n \n-            self.block_until_free_process()\n             return True\n         #\n         return False\n@@ -41,11 +42,42 @@ class ProcessManager:\n         return pnumbers\n     #\n \n-    def block_until_free_process(self):\n+    def block_until_free_process(self):            \n         while len(self.free()) == 0:\n+            print(\"blocking until free process is available\")\n             time.sleep(5)\n         #\n         return\n \n-\n+    def _block_until_all_processes_finished(self):\n+        wait = True\n+        while wait:\n+            alive = 0\n+            for k in self.processes.keys():\n+                if self.processes[k] is not None and self.processes[k].is_alive():\n+                    #found one still active\n+                    alive = alive +1\n+                #\n+            #\n+            if alive > 0:\n+                print(\"waiting for all parallel threads to finish. \", alive , \" still working.\")\n+                time.sleep(5)\n+            else:\n+                wait = False\n+            #\n+        #\n+        return\n+    #\n+    \n+    def block_until_all_processes_finished(self):\n+        for k in self.processes.keys():\n+            if self.processes[k] is not None and self.processes[k].is_alive():\n+            \t#found one still active\n+                print(\"...waiting for all parallel threads to finish. \")\n+                self.processes[k].join()\n+            #\n+        #\n+        return\n+    #\n+#\n \n",
                            "count up version. Improved ProcessManager",
                            "Michael",
                            "2023-03-13T13:52:35.000+01:00",
                            "30b6fdedd75433b62e05d99c8b27c4d428d77d1d"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/pybin/IAL_Pipeline_Assembler.py": [
                        [
                            "@@ -32,6 +32,8 @@ import re\n import shutil\n import time\n import logging\n+import bisect \n+\n \n class IPA(object):\n     \"\"\"                                                    \n@@ -170,7 +172,7 @@ class IPA(object):\n     #\n     \n     \n-    def get_exec_options_from_props(self, propfile, ial_tmpdir):\n+    def get_exec_options_from_props(self, propfile):\n         \"\"\"\n         return a namespace object list of the parameters in params_dict\n         \"\"\"                                                  \n@@ -186,15 +188,7 @@ class IPA(object):\n         for key in ['ramdisk', 'xml', 'scriptname', 'pipeline_class', 'binpath', 'pybinpath', 'debug']:\n             __key = '--' +key\n             args.append(__key)\n-            if key == 'ramdisk' and ial_tmpdir != \"\":\n-                if not ial_tmpdir.endswith(os.sep):\n-                    ial_tmpdir = ial_tmpdir +os.sep\n-                #\n-                value = ial_tmpdir +\"EuclidPipeline\"\n-                print(\"using ial tempdir \", value)\n-            else:\n-                value = proplib.assembleCmdLine(pcontents, key)\n-            #\n+            value = proplib.assembleCmdLine(pcontents, key)\n             args.append(value)\n             #the value may contain references to the environment\n             value = self.replace_environ(value)\n@@ -364,7 +358,7 @@ class IPA(object):\n     #\n     \n     \n-    def prepare_property_template(self, props_dict, workdir):\n+    def prepare_property_template(self, props_dict, workdir, ial_tmpdir):\n         \"\"\"\n         takes a dict, that has been filled with paths (values) and names (keys) of \n         all pipeline input data files and lists.\n@@ -387,6 +381,20 @@ class IPA(object):\n         props_dict['euclid_home'] = os.environ['EUCLID_HOME']\n         props_dict['euclid_conf'] = os.environ['EUCLID_CONF']\n         \n+        if ial_tmpdir != \"\":\n+            #a ial tmpdir has been defined.\n+            #use it for swarp temp\n+            if not ial_tmpdir.endswith(os.sep):\n+                ial_tmpdir = ial_tmpdir +os.sep\n+            #\n+            swarp_temp = ial_tmpdir +\"EuclidPipeline\"\n+            props_dict['swarp_temp'] = swarp_temp\n+            template_contents_line = \"swarp_temp = \" +swarp_temp\n+            \n+            bisect.insort(template_contents, template_contents_line) \n+            logging.info('Use special swarp resample dir %s', swarp_temp)\n+        #\n+        \n         new_props = []\n         for line in template_contents:\n             if line.startswith(tuple(props_dict.keys())):\n@@ -809,13 +817,13 @@ class IPA(object):\n             logging.info('IAL_Pipeline_Assembler outputs .... %s', ial_defined_outputs)\n             \n             logging.info('Prepare property template')\n-            props_file = self.prepare_property_template(props_dict, options.workdir)\n+            props_file = self.prepare_property_template(props_dict, options.workdir, options.tmpdir)\n             \n             #write the output of the last step into the next step\n             #self.outputs_to_xml(params_dict, ial_defined_outputs)\n             \n             #exec_options, _non_string_args = self.get_exec_options(props_file)\n-            exec_options = self.get_exec_options_from_props(props_file, options.tmpdir)\n+            exec_options = self.get_exec_options_from_props(props_file)\n             logging.info('parsed exec options = %s', exec_options)\n             \n             logging.info('Launch Pipeline Control')\n",
                            "add a separate folder for swarp temp (took it out of the general temp)",
                            "Michael",
                            "2023-05-16T20:43:23.000+02:00",
                            "c69fbbe0776176fa8e39d372dab931a65257f1dc"
                        ],
                        [
                            "@@ -98,6 +98,7 @@ class IPA(object):\n         #define the common inputs\n         self.parser.add_argument(\"--workdir\", help=\"Workdir.\", default=\".\")\n         self.parser.add_argument(\"--logdir\", help=\"Logdir.\", default=\"./logdir\")\n+        self.parser.add_argument(\"--tmpdir\", help=\"tempdir where temp processing is done.\", default=\"\")\n         self.parser.add_argument('--stage', type=int, help=\"determine the kind of in and outputs. Either for stage 2 (default) or stage 1.\", default = 2)\n         \n         \n@@ -169,7 +170,7 @@ class IPA(object):\n     #\n     \n     \n-    def get_exec_options_from_props(self, propfile):\n+    def get_exec_options_from_props(self, propfile, ial_tmpdir):\n         \"\"\"\n         return a namespace object list of the parameters in params_dict\n         \"\"\"                                                  \n@@ -185,7 +186,15 @@ class IPA(object):\n         for key in ['ramdisk', 'xml', 'scriptname', 'pipeline_class', 'binpath', 'pybinpath', 'debug']:\n             __key = '--' +key\n             args.append(__key)\n-            value = proplib.assembleCmdLine(pcontents, key)\n+            if key == 'ramdisk' and ial_tmpdir != \"\":\n+                if not ial_tmpdir.endswith(os.sep):\n+                    ial_tmpdir = ial_tmpdir +os.sep\n+                #\n+                value = ial_tmpdir +\"EuclidPipeline\"\n+                print(\"using ial tempdir \", value)\n+            else:\n+                value = proplib.assembleCmdLine(pcontents, key)\n+            #\n             args.append(value)\n             #the value may contain references to the environment\n             value = self.replace_environ(value)\n@@ -806,7 +815,7 @@ class IPA(object):\n             #self.outputs_to_xml(params_dict, ial_defined_outputs)\n             \n             #exec_options, _non_string_args = self.get_exec_options(props_file)\n-            exec_options = self.get_exec_options_from_props(props_file)\n+            exec_options = self.get_exec_options_from_props(props_file, options.tmpdir)\n             logging.info('parsed exec options = %s', exec_options)\n             \n             logging.info('Launch Pipeline Control')\n",
                            "added ial tmpdir",
                            "Michael",
                            "2023-03-17T12:24:52.000+01:00",
                            "eb200fa15038d2099237f1c567b651cd701ef44b"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/EXT_PF1_GEN_P2_LIBS/ExtCoaddCatalogTemplate.xml": [
                        [
                            "@@ -0,0 +1,79 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<so:DpdExtCoaddCatalog xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:fits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:sqf=\"http://ecdm.euclid-ec.org/schema/bas/dqc/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:so=\"http://ecdm.euclid-ec.org/schema/dpd/ext/sourcecatalog\"\n+ xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/sourcecatalog file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtCoaddCatalog.xsd\">\n+    <Header>\n+        <ProductId>ProductId0</ProductId>\n+        <ProductType>DpdExtCoaddCatalog</ProductType>\n+        <SoftwareName>SoftwareName0</SoftwareName>\n+        <SoftwareRelease>0.0.0</SoftwareRelease>\n+        <ProdSDC>ProdSDC0</ProdSDC>\n+        <DataSetRelease>DataSetRelease0</DataSetRelease>\n+        <Purpose>DATA_RELEASE</Purpose>\n+        <PlanId>PlanId0</PlanId>\n+        <PPOId>PPOId0</PPOId>\n+        <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n+        <PpoStatus>PpoStatus0</PpoStatus>\n+        <ManualValidationStatus>VALID</ManualValidationStatus>\n+        <Curator>Curator0</Curator>\n+        <CreationDate>2006-05-04T18:13:51.0</CreationDate>\n+    </Header>\n+    <Data>\n+        <CatalogCoverage unit=\"deg\">\n+            <Polygon>\n+                <Vertex>\n+                    <C1>0</C1>\n+                    <C2>0</C2>\n+                </Vertex>\n+                <Vertex>\n+                    <C1>0</C1>\n+                    <C2>0</C2>\n+                </Vertex>\n+            </Polygon>\n+        </CatalogCoverage>\n+        <ProcessParams>\n+        </ProcessParams>\n+        <QualityParams>\n+        </QualityParams>\n+        <ObservationDateTime>\n+        </ObservationDateTime>\n+        <Instrument>\n+            <InstrumentName>DECAM</InstrumentName>\n+            <TelescopeName>BLANCO</TelescopeName>\n+            <Longitude>0</Longitude>\n+            <Latitude>0</Latitude>\n+            <Elevation>0</Elevation>\n+            <Timezone>-4.0</Timezone>\n+        </Instrument>\n+        <Filter>\n+            <Name>DECAM_g</Name>\n+        </Filter>\n+        <Detector>Detector0</Detector>\n+        <DataStorage format=\"ext.sourceCatalog\" version=\"0.1\">\n+            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue0\">\n+                <FileName>FileName0.fits</FileName>\n+            </DataContainer>\n+        </DataStorage>\n+    </Data>\n+    <QualityFlags>\n+        <NumberOfDetectedSourcesMinExceeded xpath=\"/*/Data/QualityParams/NumberOfDetectedSources\" minLimit=\"0\" flagged=\"false\">false</NumberOfDetectedSourcesMinExceeded>\n+        <NumberOfDetectedSourcesMaxExceeded xpath=\"/*/Data/QualityParams/NumberOfDetectedSources\" maxLimit=\"0\" flagged=\"false\">false</NumberOfDetectedSourcesMaxExceeded>\n+    </QualityFlags>\n+</so:DpdExtCoaddCatalog>\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -1,79 +0,0 @@\n-<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<so:DpdExtCoaddCatalog xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n- xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n- xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n- xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n- xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n- xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n- xmlns:fits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n- xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n- xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n- xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n- xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n- xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n- xmlns:sqf=\"http://ecdm.euclid-ec.org/schema/bas/dqc/ext\"\n- xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n- xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n- xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n- xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n- xmlns:so=\"http://ecdm.euclid-ec.org/schema/dpd/ext/sourcecatalog\"\n- xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/sourcecatalog file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtCoaddCatalog.xsd\">\n-    <Header>\n-        <ProductId>ProductId0</ProductId>\n-        <ProductType>DpdExtCoaddCatalog</ProductType>\n-        <SoftwareName>SoftwareName0</SoftwareName>\n-        <SoftwareRelease>0.0.0</SoftwareRelease>\n-        <ProdSDC>ProdSDC0</ProdSDC>\n-        <DataSetRelease>DataSetRelease0</DataSetRelease>\n-        <Purpose>DATA_RELEASE</Purpose>\n-        <PlanId>PlanId0</PlanId>\n-        <PPOId>PPOId0</PPOId>\n-        <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PpoStatus>PpoStatus0</PpoStatus>\n-        <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <Curator>Curator0</Curator>\n-        <CreationDate>2006-05-04T18:13:51.0</CreationDate>\n-    </Header>\n-    <Data>\n-        <CatalogCoverage unit=\"deg\">\n-            <Polygon>\n-                <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n-                </Vertex>\n-                <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n-                </Vertex>\n-            </Polygon>\n-        </CatalogCoverage>\n-        <ProcessParams>\n-        </ProcessParams>\n-        <QualityParams>\n-        </QualityParams>\n-        <ObservationDateTime>\n-        </ObservationDateTime>\n-        <Instrument>\n-            <InstrumentName>DECAM</InstrumentName>\n-            <TelescopeName>BLANCO</TelescopeName>\n-            <Longitude>0</Longitude>\n-            <Latitude>0</Latitude>\n-            <Elevation>0</Elevation>\n-            <Timezone>-4.0</Timezone>\n-        </Instrument>\n-        <Filter>\n-            <Name>DECAM_g</Name>\n-        </Filter>\n-        <Detector>Detector0</Detector>\n-        <DataStorage format=\"ext.sourceCatalog\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue0\">\n-                <FileName>FileName0.fits</FileName>\n-            </DataContainer>\n-        </DataStorage>\n-    </Data>\n-    <QualityFlags>\n-        <NumberOfDetectedSourcesMinExceeded xpath=\"/*/Data/QualityParams/NumberOfDetectedSources\" minLimit=\"0\" flagged=\"false\">false</NumberOfDetectedSourcesMinExceeded>\n-        <NumberOfDetectedSourcesMaxExceeded xpath=\"/*/Data/QualityParams/NumberOfDetectedSources\" maxLimit=\"0\" flagged=\"false\">false</NumberOfDetectedSourcesMaxExceeded>\n-    </QualityFlags>\n-</so:DpdExtCoaddCatalog>\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -1,79 +0,0 @@\n-<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<so:DpdExtCoaddCatalog xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n- xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n- xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n- xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n- xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n- xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n- xmlns:fits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n- xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n- xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n- xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n- xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n- xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n- xmlns:sqf=\"http://ecdm.euclid-ec.org/schema/bas/dqc/ext\"\n- xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n- xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n- xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n- xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n- xmlns:so=\"http://ecdm.euclid-ec.org/schema/dpd/ext/sourcecatalog\"\n- xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/sourcecatalog file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtCoaddCatalog.xsd\">\n-    <Header>\n-        <ProductId>ProductId0</ProductId>\n-        <ProductType>DpdExtCoaddCatalog</ProductType>\n-        <SoftwareName>SoftwareName0</SoftwareName>\n-        <SoftwareRelease>0.0.0</SoftwareRelease>\n-        <ProdSDC>ProdSDC0</ProdSDC>\n-        <DataSetRelease>DataSetRelease0</DataSetRelease>\n-        <Purpose>DATA_RELEASE</Purpose>\n-        <PlanId>PlanId0</PlanId>\n-        <PPOId>PPOId0</PPOId>\n-        <PipelineDefinitionId>PipelineDefinitionId0</PipelineDefinitionId>\n-        <PpoStatus>PpoStatus0</PpoStatus>\n-        <ManualValidationStatus>VALID</ManualValidationStatus>\n-        <Curator>Curator0</Curator>\n-        <CreationDate>2006-05-04T18:13:51.0</CreationDate>\n-    </Header>\n-    <Data>\n-        <CatalogCoverage unit=\"deg\">\n-            <Polygon>\n-                <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n-                </Vertex>\n-                <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n-                </Vertex>\n-            </Polygon>\n-        </CatalogCoverage>\n-        <ProcessParams>\n-        </ProcessParams>\n-        <QualityParams>\n-        </QualityParams>\n-        <ObservationDateTime>\n-        </ObservationDateTime>\n-        <Instrument>\n-            <InstrumentName>DECAM</InstrumentName>\n-            <TelescopeName>BLANCO</TelescopeName>\n-            <Longitude>0</Longitude>\n-            <Latitude>0</Latitude>\n-            <Elevation>0</Elevation>\n-            <Timezone>-4.0</Timezone>\n-        </Instrument>\n-        <Filter>\n-            <Name>DECAM_g</Name>\n-        </Filter>\n-        <Detector>Detector0</Detector>\n-        <DataStorage format=\"ext.sourceCatalog\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue0\">\n-                <FileName>FileName0.fits</FileName>\n-            </DataContainer>\n-        </DataStorage>\n-    </Data>\n-    <QualityFlags>\n-        <NumberOfDetectedSourcesMinExceeded xpath=\"/*/Data/QualityParams/NumberOfDetectedSources\" minLimit=\"0\" flagged=\"false\">false</NumberOfDetectedSourcesMinExceeded>\n-        <NumberOfDetectedSourcesMaxExceeded xpath=\"/*/Data/QualityParams/NumberOfDetectedSources\" maxLimit=\"0\" flagged=\"false\">false</NumberOfDetectedSourcesMaxExceeded>\n-    </QualityFlags>\n-</so:DpdExtCoaddCatalog>\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/EXT_PF1_GEN_P2_LIBS/ExtSingleEpochFrameTemplate.xml": [
                        [
                            "@@ -1,24 +1,24 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<si:DpdExtSingleEpochFrame xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:sqf=\"http://euclid.esa.org/schema/bas/dqc/ext\"\n- xmlns:si=\"http://euclid.esa.org/schema/dpd/ext/singleepochframe\"\n+<si:DpdExtSingleEpochFrame xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:sqf=\"http://ecdm.euclid-ec.org/schema/bas/dqc/ext\"\n+ xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:si=\"http://ecdm.euclid-ec.org/schema/dpd/ext/singleepochframe\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/singleepochframe file:/Users/michael/Euclid/git/Eden-3.0/ST_DataModel/InstallArea/x86_64-conda_cos6-gcc93-o2g/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtSingleEpochFrame.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/singleepochframe file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtSingleEpochFrame.xsd\">\n     <Header>\n         <ProductId>ProductId0</ProductId>\n         <ProductType>DpdDqcDynamicFlags</ProductType>\n@@ -60,29 +60,24 @@\n         <Instrument>\n             <InstrumentName>DECAM</InstrumentName>\n             <TelescopeName>BLANCO</TelescopeName>\n-            <ObservatoryName>CTIO</ObservatoryName>\n             <Longitude>70.81489</Longitude>\n             <Latitude>-30.16606</Latitude>\n             <Elevation>2215.0</Elevation>\n             <Timezone>-4.0</Timezone>\n         </Instrument>\n         <Filter>\n-            <Name>DECAM_z</Name>\n+            <Name>DECAM_g</Name>\n         </Filter>\n         <Detector>DECam_ccd_18</Detector>\n-        <ImgSpatialFootprint>\n+        <ImgSpatialFootprint unit=\"deg\">\n             <Polygon>\n                 <Vertex>\n-                    <Position>\n-                        <C1>0</C1>\n-                        <C2>0</C2>\n-                    </Position>\n+                    <C1>0</C1>\n+                    <C2>0</C2>\n                 </Vertex>\n                 <Vertex>\n-                    <Position>\n-                        <C1>0</C1>\n-                        <C2>0</C2>\n-                    </Position>\n+                    <C1>0</C1>\n+                    <C2>0</C2>\n                 </Vertex>\n             </Polygon>\n             <BBox>\n@@ -93,17 +88,17 @@\n             </BBox>\n         </ImgSpatialFootprint>\n         <DataStorage format=\"ext.singleEpochFrame\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\">\n+            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue0\">\n                 <FileName>FileName0</FileName>\n             </DataContainer>\n         </DataStorage>\n         <PsfModelStorage format=\"ext.psfModel\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\">\n+            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue1\">\n                 <FileName>FileName1</FileName>\n             </DataContainer>\n         </PsfModelStorage>\n-        <CatalogDataStorage format=\"ext.sourceCatalog\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\">\n+        <CatalogDataStorage format=\"ext.singleEpochFrameCatalog\" version=\"0.1\">\n+            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue2\">\n                 <FileName>FileName2</FileName>\n             </DataContainer>\n         </CatalogDataStorage>\n@@ -126,28 +121,28 @@\n             <CD2_2>0</CD2_2>\n             <NonLinearCoeffs>\n                 <NonLinearTPVAstromCoeffs>\n-                  <PV1_0>-0.0069431577523</PV1_0>\n-                  <PV1_1>1.0278322459</PV1_1>\n-                  <PV1_2>0.00722806693008</PV1_2>\n-                  <PV1_3>0.0</PV1_3>\n-                  <PV1_4>-0.0288213527774</PV1_4>\n-                  <PV1_5>-0.0193808754041</PV1_5>\n-                  <PV1_6>-0.0048232168311</PV1_6>\n-                  <PV1_7>0.00968149386763</PV1_7>\n-                  <PV1_8>0.0114830792391</PV1_8>\n-                  <PV1_9>0.00413901118889</PV1_9>\n-                  <PV1_10>0.000733289577585</PV1_10>\n-                  <PV2_0>-0.00360745021673</PV2_0>\n-                  <PV2_1>1.0091374769</PV2_1>\n-                  <PV2_2>0.0100713758737</PV2_2>\n-                  <PV2_3>0.0</PV2_3>\n-                  <PV2_4>0.00287330476473</PV2_4>\n-                  <PV2_5>-0.0169822473983</PV2_5>\n-                  <PV2_6>-0.0103873129566</PV2_6>\n-                  <PV2_7>-0.00749644062862</PV2_7>\n-                  <PV2_8>0.00571547693739</PV2_8>\n-                  <PV2_9>0.00691276995575</PV2_9>\n-                  <PV2_10>0.00361777051678</PV2_10>\n+                    <PV1_0>0</PV1_0>\n+                    <PV1_1>0</PV1_1>\n+                    <PV1_2>0</PV1_2>\n+                    <PV1_3>0</PV1_3>\n+                    <PV1_4>0</PV1_4>\n+                    <PV1_5>0</PV1_5>\n+                    <PV1_6>0</PV1_6>\n+                    <PV1_7>0</PV1_7>\n+                    <PV1_8>0</PV1_8>\n+                    <PV1_9>0</PV1_9>\n+                    <PV1_10>0</PV1_10>\n+                    <PV2_0>0</PV2_0>\n+                    <PV2_1>0</PV2_1>\n+                    <PV2_2>0</PV2_2>\n+                    <PV2_3>0</PV2_3>\n+                    <PV2_4>0</PV2_4>\n+                    <PV2_5>0</PV2_5>\n+                    <PV2_6>0</PV2_6>\n+                    <PV2_7>0</PV2_7>\n+                    <PV2_8>0</PV2_8>\n+                    <PV2_9>0</PV2_9>\n+                    <PV2_10>0</PV2_10>\n                 </NonLinearTPVAstromCoeffs>\n             </NonLinearCoeffs>\n         </WCS>\n@@ -155,7 +150,7 @@\n             <Value>0</Value>\n             <Error>0</Error>\n         </Zeropoint>\n-        <FWHM>5.23851967</FWHM>\n+        <FWHM>50</FWHM>\n         <Gain>\n             <Product>\n                 <AmpName>AmpName0</AmpName>\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -1,24 +1,24 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<si:DpdExtSingleEpochFrame xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n- xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n- xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n- xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n- xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n- xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n- xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n- xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n- xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n- xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n- xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n- xmlns:sqf=\"http://ecdm.euclid-ec.org/schema/bas/dqc/ext\"\n- xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n- xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n- xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n- xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n- xmlns:si=\"http://ecdm.euclid-ec.org/schema/dpd/ext/singleepochframe\"\n+<si:DpdExtSingleEpochFrame xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n+ xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n+ xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n+ xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n+ xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n+ xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n+ xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n+ xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n+ xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n+ xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n+ xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n+ xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n+ xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n+ xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n+ xmlns:sqf=\"http://euclid.esa.org/schema/bas/dqc/ext\"\n+ xmlns:si=\"http://euclid.esa.org/schema/dpd/ext/singleepochframe\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/singleepochframe file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtSingleEpochFrame.xsd\">\n+ xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/singleepochframe file:/Users/michael/Euclid/git/Eden-3.0/ST_DataModel/InstallArea/x86_64-conda_cos6-gcc93-o2g/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtSingleEpochFrame.xsd\">\n     <Header>\n         <ProductId>ProductId0</ProductId>\n         <ProductType>DpdDqcDynamicFlags</ProductType>\n@@ -60,24 +60,29 @@\n         <Instrument>\n             <InstrumentName>DECAM</InstrumentName>\n             <TelescopeName>BLANCO</TelescopeName>\n+            <ObservatoryName>CTIO</ObservatoryName>\n             <Longitude>70.81489</Longitude>\n             <Latitude>-30.16606</Latitude>\n             <Elevation>2215.0</Elevation>\n             <Timezone>-4.0</Timezone>\n         </Instrument>\n         <Filter>\n-            <Name>DECAM_g</Name>\n+            <Name>DECAM_z</Name>\n         </Filter>\n         <Detector>DECam_ccd_18</Detector>\n-        <ImgSpatialFootprint unit=\"deg\">\n+        <ImgSpatialFootprint>\n             <Polygon>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position>\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position>\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n             </Polygon>\n             <BBox>\n@@ -88,17 +93,17 @@\n             </BBox>\n         </ImgSpatialFootprint>\n         <DataStorage format=\"ext.singleEpochFrame\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue0\">\n+            <DataContainer filestatus=\"PROPOSED\">\n                 <FileName>FileName0</FileName>\n             </DataContainer>\n         </DataStorage>\n         <PsfModelStorage format=\"ext.psfModel\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue1\">\n+            <DataContainer filestatus=\"PROPOSED\">\n                 <FileName>FileName1</FileName>\n             </DataContainer>\n         </PsfModelStorage>\n-        <CatalogDataStorage format=\"ext.singleEpochFrameCatalog\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue2\">\n+        <CatalogDataStorage format=\"ext.sourceCatalog\" version=\"0.1\">\n+            <DataContainer filestatus=\"PROPOSED\">\n                 <FileName>FileName2</FileName>\n             </DataContainer>\n         </CatalogDataStorage>\n@@ -121,28 +126,28 @@\n             <CD2_2>0</CD2_2>\n             <NonLinearCoeffs>\n                 <NonLinearTPVAstromCoeffs>\n-                    <PV1_0>0</PV1_0>\n-                    <PV1_1>0</PV1_1>\n-                    <PV1_2>0</PV1_2>\n-                    <PV1_3>0</PV1_3>\n-                    <PV1_4>0</PV1_4>\n-                    <PV1_5>0</PV1_5>\n-                    <PV1_6>0</PV1_6>\n-                    <PV1_7>0</PV1_7>\n-                    <PV1_8>0</PV1_8>\n-                    <PV1_9>0</PV1_9>\n-                    <PV1_10>0</PV1_10>\n-                    <PV2_0>0</PV2_0>\n-                    <PV2_1>0</PV2_1>\n-                    <PV2_2>0</PV2_2>\n-                    <PV2_3>0</PV2_3>\n-                    <PV2_4>0</PV2_4>\n-                    <PV2_5>0</PV2_5>\n-                    <PV2_6>0</PV2_6>\n-                    <PV2_7>0</PV2_7>\n-                    <PV2_8>0</PV2_8>\n-                    <PV2_9>0</PV2_9>\n-                    <PV2_10>0</PV2_10>\n+                  <PV1_0>-0.0069431577523</PV1_0>\n+                  <PV1_1>1.0278322459</PV1_1>\n+                  <PV1_2>0.00722806693008</PV1_2>\n+                  <PV1_3>0.0</PV1_3>\n+                  <PV1_4>-0.0288213527774</PV1_4>\n+                  <PV1_5>-0.0193808754041</PV1_5>\n+                  <PV1_6>-0.0048232168311</PV1_6>\n+                  <PV1_7>0.00968149386763</PV1_7>\n+                  <PV1_8>0.0114830792391</PV1_8>\n+                  <PV1_9>0.00413901118889</PV1_9>\n+                  <PV1_10>0.000733289577585</PV1_10>\n+                  <PV2_0>-0.00360745021673</PV2_0>\n+                  <PV2_1>1.0091374769</PV2_1>\n+                  <PV2_2>0.0100713758737</PV2_2>\n+                  <PV2_3>0.0</PV2_3>\n+                  <PV2_4>0.00287330476473</PV2_4>\n+                  <PV2_5>-0.0169822473983</PV2_5>\n+                  <PV2_6>-0.0103873129566</PV2_6>\n+                  <PV2_7>-0.00749644062862</PV2_7>\n+                  <PV2_8>0.00571547693739</PV2_8>\n+                  <PV2_9>0.00691276995575</PV2_9>\n+                  <PV2_10>0.00361777051678</PV2_10>\n                 </NonLinearTPVAstromCoeffs>\n             </NonLinearCoeffs>\n         </WCS>\n@@ -150,7 +155,7 @@\n             <Value>0</Value>\n             <Error>0</Error>\n         </Zeropoint>\n-        <FWHM>50</FWHM>\n+        <FWHM>5.23851967</FWHM>\n         <Gain>\n             <Product>\n                 <AmpName>AmpName0</AmpName>\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -1,24 +1,24 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<si:DpdExtSingleEpochFrame xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n- xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n- xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n- xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n- xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n- xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n- xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n- xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n- xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n- xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n- xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n- xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n- xmlns:sqf=\"http://ecdm.euclid-ec.org/schema/bas/dqc/ext\"\n- xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n- xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n- xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n- xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n- xmlns:si=\"http://ecdm.euclid-ec.org/schema/dpd/ext/singleepochframe\"\n+<si:DpdExtSingleEpochFrame xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n+ xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n+ xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n+ xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n+ xmlns:impfits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n+ xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n+ xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n+ xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n+ xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n+ xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n+ xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n+ xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n+ xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n+ xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n+ xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n+ xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n+ xmlns:sqf=\"http://euclid.esa.org/schema/bas/dqc/ext\"\n+ xmlns:si=\"http://euclid.esa.org/schema/dpd/ext/singleepochframe\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/singleepochframe file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtSingleEpochFrame.xsd\">\n+ xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/singleepochframe file:/Users/michael/Euclid/git/Eden-3.0/ST_DataModel/InstallArea/x86_64-conda_cos6-gcc93-o2g/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtSingleEpochFrame.xsd\">\n     <Header>\n         <ProductId>ProductId0</ProductId>\n         <ProductType>DpdDqcDynamicFlags</ProductType>\n@@ -60,24 +60,29 @@\n         <Instrument>\n             <InstrumentName>DECAM</InstrumentName>\n             <TelescopeName>BLANCO</TelescopeName>\n+            <ObservatoryName>CTIO</ObservatoryName>\n             <Longitude>70.81489</Longitude>\n             <Latitude>-30.16606</Latitude>\n             <Elevation>2215.0</Elevation>\n             <Timezone>-4.0</Timezone>\n         </Instrument>\n         <Filter>\n-            <Name>DECAM_g</Name>\n+            <Name>DECAM_z</Name>\n         </Filter>\n         <Detector>DECam_ccd_18</Detector>\n-        <ImgSpatialFootprint unit=\"deg\">\n+        <ImgSpatialFootprint>\n             <Polygon>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position>\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position>\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n             </Polygon>\n             <BBox>\n@@ -88,17 +93,17 @@\n             </BBox>\n         </ImgSpatialFootprint>\n         <DataStorage format=\"ext.singleEpochFrame\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue0\">\n+            <DataContainer filestatus=\"PROPOSED\">\n                 <FileName>FileName0</FileName>\n             </DataContainer>\n         </DataStorage>\n         <PsfModelStorage format=\"ext.psfModel\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue1\">\n+            <DataContainer filestatus=\"PROPOSED\">\n                 <FileName>FileName1</FileName>\n             </DataContainer>\n         </PsfModelStorage>\n-        <CatalogDataStorage format=\"ext.singleEpochFrameCatalog\" version=\"0.1\">\n-            <DataContainer filestatus=\"PROPOSED\" checksumMethod=\"MD5\" checksumValue=\"checksumValue2\">\n+        <CatalogDataStorage format=\"ext.sourceCatalog\" version=\"0.1\">\n+            <DataContainer filestatus=\"PROPOSED\">\n                 <FileName>FileName2</FileName>\n             </DataContainer>\n         </CatalogDataStorage>\n@@ -121,28 +126,28 @@\n             <CD2_2>0</CD2_2>\n             <NonLinearCoeffs>\n                 <NonLinearTPVAstromCoeffs>\n-                    <PV1_0>0</PV1_0>\n-                    <PV1_1>0</PV1_1>\n-                    <PV1_2>0</PV1_2>\n-                    <PV1_3>0</PV1_3>\n-                    <PV1_4>0</PV1_4>\n-                    <PV1_5>0</PV1_5>\n-                    <PV1_6>0</PV1_6>\n-                    <PV1_7>0</PV1_7>\n-                    <PV1_8>0</PV1_8>\n-                    <PV1_9>0</PV1_9>\n-                    <PV1_10>0</PV1_10>\n-                    <PV2_0>0</PV2_0>\n-                    <PV2_1>0</PV2_1>\n-                    <PV2_2>0</PV2_2>\n-                    <PV2_3>0</PV2_3>\n-                    <PV2_4>0</PV2_4>\n-                    <PV2_5>0</PV2_5>\n-                    <PV2_6>0</PV2_6>\n-                    <PV2_7>0</PV2_7>\n-                    <PV2_8>0</PV2_8>\n-                    <PV2_9>0</PV2_9>\n-                    <PV2_10>0</PV2_10>\n+                  <PV1_0>-0.0069431577523</PV1_0>\n+                  <PV1_1>1.0278322459</PV1_1>\n+                  <PV1_2>0.00722806693008</PV1_2>\n+                  <PV1_3>0.0</PV1_3>\n+                  <PV1_4>-0.0288213527774</PV1_4>\n+                  <PV1_5>-0.0193808754041</PV1_5>\n+                  <PV1_6>-0.0048232168311</PV1_6>\n+                  <PV1_7>0.00968149386763</PV1_7>\n+                  <PV1_8>0.0114830792391</PV1_8>\n+                  <PV1_9>0.00413901118889</PV1_9>\n+                  <PV1_10>0.000733289577585</PV1_10>\n+                  <PV2_0>-0.00360745021673</PV2_0>\n+                  <PV2_1>1.0091374769</PV2_1>\n+                  <PV2_2>0.0100713758737</PV2_2>\n+                  <PV2_3>0.0</PV2_3>\n+                  <PV2_4>0.00287330476473</PV2_4>\n+                  <PV2_5>-0.0169822473983</PV2_5>\n+                  <PV2_6>-0.0103873129566</PV2_6>\n+                  <PV2_7>-0.00749644062862</PV2_7>\n+                  <PV2_8>0.00571547693739</PV2_8>\n+                  <PV2_9>0.00691276995575</PV2_9>\n+                  <PV2_10>0.00361777051678</PV2_10>\n                 </NonLinearTPVAstromCoeffs>\n             </NonLinearCoeffs>\n         </WCS>\n@@ -150,7 +155,7 @@\n             <Value>0</Value>\n             <Error>0</Error>\n         </Zeropoint>\n-        <FWHM>50</FWHM>\n+        <FWHM>5.23851967</FWHM>\n         <Gain>\n             <Product>\n                 <AmpName>AmpName0</AmpName>\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/EXT_PF1_GEN_P2_LIBS/ExtStackedFrameTemplate.xml": [
                        [
                            "@@ -1,24 +1,23 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<st:DpdExtStackedFrame xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n- xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n- xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n- xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n- xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n- xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n- xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n- xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n- xmlns:fits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n- xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n- xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n- xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n- xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n- xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n- xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n- xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n- xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n- xmlns:st=\"http://euclid.esa.org/schema/dpd/ext/stackedframe\"\n+<st:DpdExtStackedFrame xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n+ xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n+ xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n+ xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n+ xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n+ xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n+ xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n+ xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n+ xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n+ xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n+ xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n+ xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n+ xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n+ xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n+ xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n+ xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n+ xmlns:st=\"http://ecdm.euclid-ec.org/schema/dpd/ext/stackedframe\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/stackedframe file:/home/dave/Euclid/git/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtStackedFrame.xsd\">\n+ xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/stackedframe file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtStackedFrame.xsd\">\n     <Header>\n         <ProductId>ProductId0</ProductId>\n         <ProductType>DpdExtStackedFrame</ProductType>\n@@ -52,16 +51,16 @@\n             <TelescopeName>BLANCO</TelescopeName>\n         </Instrument>\n         <Filter>\n-            <Name>VIS</Name>\n+            <Name>DECAM_g</Name>\n         </Filter>\n         <WCS>\n             <CTYPE1>\n                 <CoordinateType>RA</CoordinateType>\n-                <ProjectionType></ProjectionType>\n+                <ProjectionType>PLA</ProjectionType>\n             </CTYPE1>\n             <CTYPE2>\n                 <CoordinateType>RA</CoordinateType>\n-                <ProjectionType></ProjectionType>\n+                <ProjectionType>PLA</ProjectionType>\n             </CTYPE2>\n             <CRVAL1>0</CRVAL1>\n             <CRVAL2>0</CRVAL2>\n@@ -76,19 +75,15 @@\n             <Value>0</Value>\n             <Error>0</Error>\n         </ZeroPoint>\n-        <ImgSpatialFootprint>\n+        <ImgSpatialFootprint unit=\"deg\">\n             <Polygon>\n                 <Vertex>\n-                    <Position coordUnit=\"m\">\n-                        <C1>0</C1>\n-                        <C2>0</C2>\n-                    </Position>\n+                    <C1>0</C1>\n+                    <C2>0</C2>\n                 </Vertex>\n                 <Vertex>\n-                    <Position coordUnit=\"m\">\n-                        <C1>0</C1>\n-                        <C2>0</C2>\n-                    </Position>\n+                    <C1>0</C1>\n+                    <C2>0</C2>\n                 </Vertex>\n             </Polygon>\n         </ImgSpatialFootprint>\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -1,23 +1,24 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<st:DpdExtStackedFrame xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n- xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n- xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n- xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n- xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n- xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n- xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n- xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n- xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n- xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n- xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n- xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n- xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n- xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n- xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n- xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n- xmlns:st=\"http://ecdm.euclid-ec.org/schema/dpd/ext/stackedframe\"\n+<st:DpdExtStackedFrame xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n+ xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n+ xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n+ xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n+ xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n+ xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n+ xmlns:fits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n+ xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n+ xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n+ xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n+ xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n+ xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n+ xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n+ xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n+ xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n+ xmlns:st=\"http://euclid.esa.org/schema/dpd/ext/stackedframe\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/stackedframe file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtStackedFrame.xsd\">\n+ xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/stackedframe file:/home/dave/Euclid/git/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtStackedFrame.xsd\">\n     <Header>\n         <ProductId>ProductId0</ProductId>\n         <ProductType>DpdExtStackedFrame</ProductType>\n@@ -51,16 +52,16 @@\n             <TelescopeName>BLANCO</TelescopeName>\n         </Instrument>\n         <Filter>\n-            <Name>DECAM_g</Name>\n+            <Name>VIS</Name>\n         </Filter>\n         <WCS>\n             <CTYPE1>\n                 <CoordinateType>RA</CoordinateType>\n-                <ProjectionType>PLA</ProjectionType>\n+                <ProjectionType></ProjectionType>\n             </CTYPE1>\n             <CTYPE2>\n                 <CoordinateType>RA</CoordinateType>\n-                <ProjectionType>PLA</ProjectionType>\n+                <ProjectionType></ProjectionType>\n             </CTYPE2>\n             <CRVAL1>0</CRVAL1>\n             <CRVAL2>0</CRVAL2>\n@@ -75,15 +76,19 @@\n             <Value>0</Value>\n             <Error>0</Error>\n         </ZeroPoint>\n-        <ImgSpatialFootprint unit=\"deg\">\n+        <ImgSpatialFootprint>\n             <Polygon>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position coordUnit=\"m\">\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position coordUnit=\"m\">\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n             </Polygon>\n         </ImgSpatialFootprint>\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -1,23 +1,24 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<st:DpdExtStackedFrame xmlns:dqc=\"http://ecdm.euclid-ec.org/schema/bas/dqc\"\n- xmlns:img=\"http://ecdm.euclid-ec.org/schema/bas/img\"\n- xmlns:dss=\"http://ecdm.euclid-ec.org/schema/sys/dss\"\n- xmlns:cot=\"http://ecdm.euclid-ec.org/schema/bas/cot\"\n- xmlns:fit=\"http://ecdm.euclid-ec.org/schema/bas/fit\"\n- xmlns:ins=\"http://ecdm.euclid-ec.org/schema/ins\"\n- xmlns:par=\"http://ecdm.euclid-ec.org/schema/bas/ppr/par\"\n- xmlns:dtd=\"http://ecdm.euclid-ec.org/schema/bas/dtd\"\n- xmlns:imp=\"http://ecdm.euclid-ec.org/schema/bas/imp\"\n- xmlns:utd=\"http://ecdm.euclid-ec.org/schema/bas/utd\"\n- xmlns:stc=\"http://ecdm.euclid-ec.org/schema/bas/imp/stc\"\n- xmlns:sys=\"http://ecdm.euclid-ec.org/schema/sys\"\n- xmlns:ext=\"http://ecdm.euclid-ec.org/schema/pro/ext\"\n- xmlns:ppr=\"http://ecdm.euclid-ec.org/schema/bas/ppr\"\n- xmlns:impfits=\"http://ecdm.euclid-ec.org/schema/bas/imp/fits\"\n- xmlns:eso=\"http://ecdm.euclid-ec.org/schema/bas/imp/eso\"\n- xmlns:st=\"http://ecdm.euclid-ec.org/schema/dpd/ext/stackedframe\"\n+<st:DpdExtStackedFrame xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n+ xmlns:dtd=\"http://euclid.esa.org/schema/bas/dtd\"\n+ xmlns:ext=\"http://euclid.esa.org/schema/pro/ext\"\n+ xmlns:dqc=\"http://euclid.esa.org/schema/bas/dqc\"\n+ xmlns:ppr=\"http://euclid.esa.org/schema/bas/ppr\"\n+ xmlns:cot=\"http://euclid.esa.org/schema/bas/cot\"\n+ xmlns:stc=\"http://euclid.esa.org/schema/bas/imp/stc\"\n+ xmlns:eso=\"http://euclid.esa.org/schema/bas/imp/eso\"\n+ xmlns:fits=\"http://euclid.esa.org/schema/bas/imp/fits\"\n+ xmlns:ins=\"http://euclid.esa.org/schema/ins\"\n+ xmlns:msk=\"http://euclid.esa.org/schema/bas/msk\"\n+ xmlns:utd=\"http://euclid.esa.org/schema/bas/utd\"\n+ xmlns:imp=\"http://euclid.esa.org/schema/bas/imp\"\n+ xmlns:img=\"http://euclid.esa.org/schema/bas/img\"\n+ xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n+ xmlns:fit=\"http://euclid.esa.org/schema/bas/fit\"\n+ xmlns:par=\"http://euclid.esa.org/schema/bas/ppr/par\"\n+ xmlns:st=\"http://euclid.esa.org/schema/dpd/ext/stackedframe\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n- xsi:schemaLocation=\"http://ecdm.euclid-ec.org/schema/dpd/ext/stackedframe file:/Users/michael/Euclid/eucgu94/Work/Projects/EDEN-3.0/release/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-ext-ExtStackedFrame.xsd\">\n+ xsi:schemaLocation=\"http://euclid.esa.org/schema/dpd/ext/stackedframe file:/home/dave/Euclid/git/ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/dpd/ext/euc-test-ext-ExtStackedFrame.xsd\">\n     <Header>\n         <ProductId>ProductId0</ProductId>\n         <ProductType>DpdExtStackedFrame</ProductType>\n@@ -51,16 +52,16 @@\n             <TelescopeName>BLANCO</TelescopeName>\n         </Instrument>\n         <Filter>\n-            <Name>DECAM_g</Name>\n+            <Name>VIS</Name>\n         </Filter>\n         <WCS>\n             <CTYPE1>\n                 <CoordinateType>RA</CoordinateType>\n-                <ProjectionType>PLA</ProjectionType>\n+                <ProjectionType></ProjectionType>\n             </CTYPE1>\n             <CTYPE2>\n                 <CoordinateType>RA</CoordinateType>\n-                <ProjectionType>PLA</ProjectionType>\n+                <ProjectionType></ProjectionType>\n             </CTYPE2>\n             <CRVAL1>0</CRVAL1>\n             <CRVAL2>0</CRVAL2>\n@@ -75,15 +76,19 @@\n             <Value>0</Value>\n             <Error>0</Error>\n         </ZeroPoint>\n-        <ImgSpatialFootprint unit=\"deg\">\n+        <ImgSpatialFootprint>\n             <Polygon>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position coordUnit=\"m\">\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n                 <Vertex>\n-                    <C1>0</C1>\n-                    <C2>0</C2>\n+                    <Position coordUnit=\"m\">\n+                        <C1>0</C1>\n+                        <C2>0</C2>\n+                    </Position>\n                 </Vertex>\n             </Polygon>\n         </ImgSpatialFootprint>\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/EXT_PF1_GEN_P2_LIBS/ExtValidationReportTemplate.xml": [
                        [
                            "@@ -20,13 +20,15 @@\n \t\t<Instrument>\n \t\t\t<InstrumentName>MEGACAM</InstrumentName>\n \t\t\t<TelescopeName>CFHT</TelescopeName>\n-\t\t\t<Longitude>-155.47015</Longitude>\n-\t\t\t<Latitude>19.82525</Latitude>\n-\t\t\t<Elevation>4204.0</Elevation>\n+\t\t\t<Longitude>0</Longitude>\n+\t\t\t<Latitude>0</Latitude>\n+\t\t\t<Elevation>0</Elevation>\n \t\t\t<Timezone>-4.0</Timezone>\n \t\t</Instrument>\n-\t\t<Description> </Description>\n-\t\t<ModuleName> </ModuleName>\n+\t\t<Description> \n+\t\t</Description>\n+\t\t<ModuleName> \n+\t\t</ModuleName>\n \t\t<ValidationFiles>\n \t\t\t<FileContainer filestatus=\"PROPOSED\">\n \t\t\t\t<FileName>FileName0</FileName>\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -20,15 +20,13 @@\n \t\t<Instrument>\n \t\t\t<InstrumentName>MEGACAM</InstrumentName>\n \t\t\t<TelescopeName>CFHT</TelescopeName>\n-\t\t\t<Longitude>0</Longitude>\n-\t\t\t<Latitude>0</Latitude>\n-\t\t\t<Elevation>0</Elevation>\n+\t\t\t<Longitude>-155.47015</Longitude>\n+\t\t\t<Latitude>19.82525</Latitude>\n+\t\t\t<Elevation>4204.0</Elevation>\n \t\t\t<Timezone>-4.0</Timezone>\n \t\t</Instrument>\n-\t\t<Description> \n-\t\t</Description>\n-\t\t<ModuleName> \n-\t\t</ModuleName>\n+\t\t<Description> </Description>\n+\t\t<ModuleName> </ModuleName>\n \t\t<ValidationFiles>\n \t\t\t<FileContainer filestatus=\"PROPOSED\">\n \t\t\t\t<FileName>FileName0</FileName>\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -20,15 +20,13 @@\n \t\t<Instrument>\n \t\t\t<InstrumentName>MEGACAM</InstrumentName>\n \t\t\t<TelescopeName>CFHT</TelescopeName>\n-\t\t\t<Longitude>0</Longitude>\n-\t\t\t<Latitude>0</Latitude>\n-\t\t\t<Elevation>0</Elevation>\n+\t\t\t<Longitude>-155.47015</Longitude>\n+\t\t\t<Latitude>19.82525</Latitude>\n+\t\t\t<Elevation>4204.0</Elevation>\n \t\t\t<Timezone>-4.0</Timezone>\n \t\t</Instrument>\n-\t\t<Description> \n-\t\t</Description>\n-\t\t<ModuleName> \n-\t\t</ModuleName>\n+\t\t<Description> </Description>\n+\t\t<ModuleName> </ModuleName>\n \t\t<ValidationFiles>\n \t\t\t<FileContainer filestatus=\"PROPOSED\">\n \t\t\t\t<FileName>FileName0</FileName>\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/etc/default_flag.swarp": [
                        [
                            "@@ -92,12 +92,12 @@ BACK_FILTTHRESH        0.0             # Threshold above which the background-\n #------------------------------ Memory management -----------------------------\n  \n VMEM_DIR               .               # Directory path for swap files\n+VMEM_MAX               6047            # Maximum amount of virtual memory (MB)\n+MEM_MAX                6000             # Maximum amount of usable RAM (MB)\n+COMBINE_BUFSIZE        6000            # RAM dedicated to co-addition(MB)\n #VMEM_MAX               2047            # Maximum amount of virtual memory (MB)\n-#MEM_MAX                512             # Maximum amount of usable RAM (MB)\n-#COMBINE_BUFSIZE        512             # RAM dedicated to co-addition(MB)\n-VMEM_MAX\t\t      6047\t\t       # Maximum amount of virtual memory (MB)\n-MEM_MAX\t\t\t      7500\t\t       # Maximum amount of usable RAM (MB)\n-COMBINE_BUFSIZE\t\t  6000\t\t       # RAM dedicated to co-addition(MB)\n+#MEM_MAX                256             # Maximum amount of usable RAM (MB)\n+#COMBINE_BUFSIZE        256             # RAM dedicated to co-addition(MB)\n  \n #------------------------------ Miscellaneous ---------------------------------\n  \n@@ -118,5 +118,5 @@ NODE_INDEX             0               # Node index (for clusters)\n NTHREADS               1               # Number of simultaneous threads for\n                                        # the SMP version of SWarp\n                                        # 0 = automatic\n-NOPENFILES_MAX         2048            # Maximum number of files opened by SWarp\n+NOPENFILES_MAX         6000            # Maximum number of files opened by SWarp\n TILE_COMPRESS          N               # Write tile compressed output image (Y/N)?\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -92,12 +92,12 @@ BACK_FILTTHRESH        0.0             # Threshold above which the background-\n #------------------------------ Memory management -----------------------------\n  \n VMEM_DIR               .               # Directory path for swap files\n-VMEM_MAX               2047            # Maximum amount of virtual memory (MB)\n-MEM_MAX                512             # Maximum amount of usable RAM (MB)\n-COMBINE_BUFSIZE        512             # RAM dedicated to co-addition(MB)\n #VMEM_MAX               2047            # Maximum amount of virtual memory (MB)\n-#MEM_MAX                256             # Maximum amount of usable RAM (MB)\n-#COMBINE_BUFSIZE        256             # RAM dedicated to co-addition(MB)\n+#MEM_MAX                512             # Maximum amount of usable RAM (MB)\n+#COMBINE_BUFSIZE        512             # RAM dedicated to co-addition(MB)\n+VMEM_MAX\t\t      6047\t\t       # Maximum amount of virtual memory (MB)\n+MEM_MAX\t\t\t      7500\t\t       # Maximum amount of usable RAM (MB)\n+COMBINE_BUFSIZE\t\t  6000\t\t       # RAM dedicated to co-addition(MB)\n  \n #------------------------------ Miscellaneous ---------------------------------\n  \n@@ -118,5 +118,5 @@ NODE_INDEX             0               # Node index (for clusters)\n NTHREADS               1               # Number of simultaneous threads for\n                                        # the SMP version of SWarp\n                                        # 0 = automatic\n-NOPENFILES_MAX         512             # Maximum number of files opened by SWarp\n+NOPENFILES_MAX         2048            # Maximum number of files opened by SWarp\n TILE_COMPRESS          N               # Write tile compressed output image (Y/N)?\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -92,12 +92,12 @@ BACK_FILTTHRESH        0.0             # Threshold above which the background-\n #------------------------------ Memory management -----------------------------\n  \n VMEM_DIR               .               # Directory path for swap files\n-VMEM_MAX               2047            # Maximum amount of virtual memory (MB)\n-MEM_MAX                512             # Maximum amount of usable RAM (MB)\n-COMBINE_BUFSIZE        512             # RAM dedicated to co-addition(MB)\n #VMEM_MAX               2047            # Maximum amount of virtual memory (MB)\n-#MEM_MAX                256             # Maximum amount of usable RAM (MB)\n-#COMBINE_BUFSIZE        256             # RAM dedicated to co-addition(MB)\n+#MEM_MAX                512             # Maximum amount of usable RAM (MB)\n+#COMBINE_BUFSIZE        512             # RAM dedicated to co-addition(MB)\n+VMEM_MAX\t\t      6047\t\t       # Maximum amount of virtual memory (MB)\n+MEM_MAX\t\t\t      7500\t\t       # Maximum amount of usable RAM (MB)\n+COMBINE_BUFSIZE\t\t  6000\t\t       # RAM dedicated to co-addition(MB)\n  \n #------------------------------ Miscellaneous ---------------------------------\n  \n@@ -118,5 +118,5 @@ NODE_INDEX             0               # Node index (for clusters)\n NTHREADS               1               # Number of simultaneous threads for\n                                        # the SMP version of SWarp\n                                        # 0 = automatic\n-NOPENFILES_MAX         512             # Maximum number of files opened by SWarp\n+NOPENFILES_MAX         2048            # Maximum number of files opened by SWarp\n TILE_COMPRESS          N               # Write tile compressed output image (Y/N)?\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/exml/ExtCoaddSourceCatalogTExporter.py": [
                        [
                            "@@ -27,10 +27,10 @@ class ExtCoaddSourceCatalogTExporter(HeaderExporter):\n     \n     def __init__(self, catalog_fits_file):\n         HeaderExporter.__init__(self)\n-        self.product_type = \"DpdExtSourceCatalog\"\n+        self.product_type = \"DpdExtCoaddCatalog\"\n         \n         template = inspect.getfile(self.__class__).rsplit(\"python/EXT_PF1_GEN_P2_LIBS/exml/\")[0] \\\n-                    +\"conf\"+os.sep +'EXT_PF1_GEN_P2_LIBS' +os.sep +'ExtSourceCatalogTemplate.xml'\n+                    +\"conf\"+os.sep +'EXT_PF1_GEN_P2_LIBS' +os.sep +'ExtCoaddCatalogTemplate.xml'\n         f = open(template, 'r')\n         self.xml_contents = f.read()\n         f.close()\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -27,10 +27,10 @@ class ExtCoaddSourceCatalogTExporter(HeaderExporter):\n     \n     def __init__(self, catalog_fits_file):\n         HeaderExporter.__init__(self)\n-        self.product_type = \"DpdExtCoaddCatalog\"\n+        self.product_type = \"DpdExtSourceCatalog\"\n         \n         template = inspect.getfile(self.__class__).rsplit(\"python/EXT_PF1_GEN_P2_LIBS/exml/\")[0] \\\n-                    +\"conf\"+os.sep +'EXT_PF1_GEN_P2_LIBS' +os.sep +'ExtCoaddCatalogTemplate.xml'\n+                    +\"conf\"+os.sep +'EXT_PF1_GEN_P2_LIBS' +os.sep +'ExtSourceCatalogTemplate.xml'\n         f = open(template, 'r')\n         self.xml_contents = f.read()\n         f.close()\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -27,10 +27,10 @@ class ExtCoaddSourceCatalogTExporter(HeaderExporter):\n     \n     def __init__(self, catalog_fits_file):\n         HeaderExporter.__init__(self)\n-        self.product_type = \"DpdExtCoaddCatalog\"\n+        self.product_type = \"DpdExtSourceCatalog\"\n         \n         template = inspect.getfile(self.__class__).rsplit(\"python/EXT_PF1_GEN_P2_LIBS/exml/\")[0] \\\n-                    +\"conf\"+os.sep +'EXT_PF1_GEN_P2_LIBS' +os.sep +'ExtCoaddCatalogTemplate.xml'\n+                    +\"conf\"+os.sep +'EXT_PF1_GEN_P2_LIBS' +os.sep +'ExtSourceCatalogTemplate.xml'\n         f = open(template, 'r')\n         self.xml_contents = f.read()\n         f.close()\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/exml/FileExporter.py": [
                        [
                            "@@ -380,7 +380,8 @@ class FileExporter(object):\n         Uses DM Bindings to create a DOM object from \n         the provided xml file\n         \"\"\"\n-        NS_PREFIX = \"http://euclid.esa.org/schema/\"\n+        #NS_PREFIX = \"http://euclid.esa.org/schema/\"\n+        NS_PREFIX = \"http://ecdm.euclid-ec.org/schema/\"\n         bindings_module = 'ST_DataModelBindings'\n         \n         def xml2module(xml_file):\n@@ -390,13 +391,16 @@ class FileExporter(object):\n             with open(xml_file, 'r') as f:\n                 doc = etree.parse(f)\n                 root = doc.getroot()\n+                print(\"root = \", root)\n                 module = root.nsmap[root.prefix]\n+                print(\"module from xml = \", module)\n                 if module.startswith(NS_PREFIX):\n                     module = module[len(NS_PREFIX):]\n                 module = module.replace('/', '.')\n                 module = module.replace('-', '_')\n                 module += '_stub'\n                 module = bindings_module+'.'+module\n+                print(\"module string = \", module)\n                 return module\n             #\n         #\n@@ -527,10 +531,10 @@ class FileExporter(object):\n         filled_vertices = \"\"\n         for point in points:\n             #print(\"template_vertex \", template_vertex)\n-            vt = self.set_xml_value(template_vertex, ('Vertex', 'Position', 'C1'), str(point[0]))\n+            vt = self.set_xml_value(template_vertex, ('Vertex', 'C1'), str(point[0]))\n             #print(\"template_vertex \", vt)\n             \n-            vt = self.set_xml_value(vt, ('Vertex', 'Position', 'C2'), str(point[1]))\n+            vt = self.set_xml_value(vt, ('Vertex', 'C2'), str(point[1]))\n             #print(\"template_vertex \", vt)\n             filled_vertices = filled_vertices +vt\n         #\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -380,8 +380,7 @@ class FileExporter(object):\n         Uses DM Bindings to create a DOM object from \n         the provided xml file\n         \"\"\"\n-        #NS_PREFIX = \"http://euclid.esa.org/schema/\"\n-        NS_PREFIX = \"http://ecdm.euclid-ec.org/schema/\"\n+        NS_PREFIX = \"http://euclid.esa.org/schema/\"\n         bindings_module = 'ST_DataModelBindings'\n         \n         def xml2module(xml_file):\n@@ -391,16 +390,13 @@ class FileExporter(object):\n             with open(xml_file, 'r') as f:\n                 doc = etree.parse(f)\n                 root = doc.getroot()\n-                print(\"root = \", root)\n                 module = root.nsmap[root.prefix]\n-                print(\"module from xml = \", module)\n                 if module.startswith(NS_PREFIX):\n                     module = module[len(NS_PREFIX):]\n                 module = module.replace('/', '.')\n                 module = module.replace('-', '_')\n                 module += '_stub'\n                 module = bindings_module+'.'+module\n-                print(\"module string = \", module)\n                 return module\n             #\n         #\n@@ -531,10 +527,10 @@ class FileExporter(object):\n         filled_vertices = \"\"\n         for point in points:\n             #print(\"template_vertex \", template_vertex)\n-            vt = self.set_xml_value(template_vertex, ('Vertex', 'C1'), str(point[0]))\n+            vt = self.set_xml_value(template_vertex, ('Vertex', 'Position', 'C1'), str(point[0]))\n             #print(\"template_vertex \", vt)\n             \n-            vt = self.set_xml_value(vt, ('Vertex', 'C2'), str(point[1]))\n+            vt = self.set_xml_value(vt, ('Vertex', 'Position', 'C2'), str(point[1]))\n             #print(\"template_vertex \", vt)\n             filled_vertices = filled_vertices +vt\n         #\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -380,8 +380,7 @@ class FileExporter(object):\n         Uses DM Bindings to create a DOM object from \n         the provided xml file\n         \"\"\"\n-        #NS_PREFIX = \"http://euclid.esa.org/schema/\"\n-        NS_PREFIX = \"http://ecdm.euclid-ec.org/schema/\"\n+        NS_PREFIX = \"http://euclid.esa.org/schema/\"\n         bindings_module = 'ST_DataModelBindings'\n         \n         def xml2module(xml_file):\n@@ -391,16 +390,13 @@ class FileExporter(object):\n             with open(xml_file, 'r') as f:\n                 doc = etree.parse(f)\n                 root = doc.getroot()\n-                print(\"root = \", root)\n                 module = root.nsmap[root.prefix]\n-                print(\"module from xml = \", module)\n                 if module.startswith(NS_PREFIX):\n                     module = module[len(NS_PREFIX):]\n                 module = module.replace('/', '.')\n                 module = module.replace('-', '_')\n                 module += '_stub'\n                 module = bindings_module+'.'+module\n-                print(\"module string = \", module)\n                 return module\n             #\n         #\n@@ -531,10 +527,10 @@ class FileExporter(object):\n         filled_vertices = \"\"\n         for point in points:\n             #print(\"template_vertex \", template_vertex)\n-            vt = self.set_xml_value(template_vertex, ('Vertex', 'C1'), str(point[0]))\n+            vt = self.set_xml_value(template_vertex, ('Vertex', 'Position', 'C1'), str(point[0]))\n             #print(\"template_vertex \", vt)\n             \n-            vt = self.set_xml_value(vt, ('Vertex', 'C2'), str(point[1]))\n+            vt = self.set_xml_value(vt, ('Vertex', 'Position', 'C2'), str(point[1]))\n             #print(\"template_vertex \", vt)\n             filled_vertices = filled_vertices +vt\n         #\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/exml/HeaderExporter.py": [
                        [
                            "@@ -24,11 +24,12 @@ class HeaderExporter(FileExporter):\n     \n     def __init__(self):\n         \"\"\"\n-        Populates this:\n+        Populates this for DM 9.1.5\n+        <Header>\n         <ProductId>ProductId0</ProductId>\n-        <ProductType>ProductType0</ProductType>\n+        <ProductType>DpdDqcDynamicFlags</ProductType>\n         <SoftwareName>SoftwareName0</SoftwareName>\n-        <SoftwareRelease>Softwa</SoftwareRelease>\n+        <SoftwareRelease>SoftwareRel</SoftwareRelease>\n         <ProdSDC>ProdSDC0</ProdSDC>\n         <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n@@ -39,6 +40,7 @@ class HeaderExporter(FileExporter):\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n         <Curator>Curator0</Curator>\n         <CreationDate>2006-05-04T18:13:51.0</CreationDate>\n+        </Header>\n         \"\"\"\n         FileExporter.__init__(self)\n         \n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -24,12 +24,11 @@ class HeaderExporter(FileExporter):\n     \n     def __init__(self):\n         \"\"\"\n-        Populates this for DM 9.1.5\n-        <Header>\n+        Populates this:\n         <ProductId>ProductId0</ProductId>\n-        <ProductType>DpdDqcDynamicFlags</ProductType>\n+        <ProductType>ProductType0</ProductType>\n         <SoftwareName>SoftwareName0</SoftwareName>\n-        <SoftwareRelease>SoftwareRel</SoftwareRelease>\n+        <SoftwareRelease>Softwa</SoftwareRelease>\n         <ProdSDC>ProdSDC0</ProdSDC>\n         <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n@@ -40,7 +39,6 @@ class HeaderExporter(FileExporter):\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n         <Curator>Curator0</Curator>\n         <CreationDate>2006-05-04T18:13:51.0</CreationDate>\n-        </Header>\n         \"\"\"\n         FileExporter.__init__(self)\n         \n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -24,12 +24,11 @@ class HeaderExporter(FileExporter):\n     \n     def __init__(self):\n         \"\"\"\n-        Populates this for DM 9.1.5\n-        <Header>\n+        Populates this:\n         <ProductId>ProductId0</ProductId>\n-        <ProductType>DpdDqcDynamicFlags</ProductType>\n+        <ProductType>ProductType0</ProductType>\n         <SoftwareName>SoftwareName0</SoftwareName>\n-        <SoftwareRelease>SoftwareRel</SoftwareRelease>\n+        <SoftwareRelease>Softwa</SoftwareRelease>\n         <ProdSDC>ProdSDC0</ProdSDC>\n         <DataSetRelease>DataSetRelease0</DataSetRelease>\n         <Purpose>DATA_RELEASE</Purpose>\n@@ -40,7 +39,6 @@ class HeaderExporter(FileExporter):\n         <ManualValidationStatus>VALID</ManualValidationStatus>\n         <Curator>Curator0</Curator>\n         <CreationDate>2006-05-04T18:13:51.0</CreationDate>\n-        </Header>\n         \"\"\"\n         FileExporter.__init__(self)\n         \n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/tests/python/TestExtSingleEpochFrameTExporter_test.py": [
                        [
                            "@@ -79,6 +79,7 @@ class TestExtSingleEpochFrameTExporter(unittest.TestCase):\n         path = self.se_fits_file.rsplit(os.sep, 1)[0]\n         original_file = self.se_fits_file.rsplit(os.sep, 1)[-1].split(\".fits\")[0]\n         xml_file = path +os.sep +self.dce.product_type +\"_\" +original_file +'.xml'\n+        print(\"xml file \", xml_file)\n         self.dce.saveProductMetadata(xml_file)\n         \n         DOM = self.dce.createFromDocument(xml_file)\n@@ -137,7 +138,7 @@ class TestExtSingleEpochFrameTExporter(unittest.TestCase):\n         self.header[\"PV1_4\"] = 0.002        \n         self.dce._set_wcs(self.header)\n         \n-        wcskeys = FitsServices.get_wcs_keys()\n+        wcskeys = FitsServices.get_wcs_tpv_keys()\n         wcskeys.remove(\"CTYPE1\") #only manually\n         wcskeys.remove(\"CTYPE2\")\n         \n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -79,7 +79,6 @@ class TestExtSingleEpochFrameTExporter(unittest.TestCase):\n         path = self.se_fits_file.rsplit(os.sep, 1)[0]\n         original_file = self.se_fits_file.rsplit(os.sep, 1)[-1].split(\".fits\")[0]\n         xml_file = path +os.sep +self.dce.product_type +\"_\" +original_file +'.xml'\n-        print(\"xml file \", xml_file)\n         self.dce.saveProductMetadata(xml_file)\n         \n         DOM = self.dce.createFromDocument(xml_file)\n@@ -138,7 +137,7 @@ class TestExtSingleEpochFrameTExporter(unittest.TestCase):\n         self.header[\"PV1_4\"] = 0.002        \n         self.dce._set_wcs(self.header)\n         \n-        wcskeys = FitsServices.get_wcs_tpv_keys()\n+        wcskeys = FitsServices.get_wcs_keys()\n         wcskeys.remove(\"CTYPE1\") #only manually\n         wcskeys.remove(\"CTYPE2\")\n         \n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -79,7 +79,6 @@ class TestExtSingleEpochFrameTExporter(unittest.TestCase):\n         path = self.se_fits_file.rsplit(os.sep, 1)[0]\n         original_file = self.se_fits_file.rsplit(os.sep, 1)[-1].split(\".fits\")[0]\n         xml_file = path +os.sep +self.dce.product_type +\"_\" +original_file +'.xml'\n-        print(\"xml file \", xml_file)\n         self.dce.saveProductMetadata(xml_file)\n         \n         DOM = self.dce.createFromDocument(xml_file)\n@@ -138,7 +137,7 @@ class TestExtSingleEpochFrameTExporter(unittest.TestCase):\n         self.header[\"PV1_4\"] = 0.002        \n         self.dce._set_wcs(self.header)\n         \n-        wcskeys = FitsServices.get_wcs_tpv_keys()\n+        wcskeys = FitsServices.get_wcs_keys()\n         wcskeys.remove(\"CTYPE1\") #only manually\n         wcskeys.remove(\"CTYPE2\")\n         \n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/tests/python/TestFitsServices_test.py": [
                        [
                            "@@ -72,8 +72,8 @@ class TestFitsServices(unittest.TestCase):\n         \n         d = FitsServices.fill_header_values_to_dict(h, {('tag', 'list'):self.key})\n         self.assertTrue(list(d.values())[0].strip() == self.value)\n-    def test_get_wcs_keys(self):\n-        wcs_keys = FitsServices.get_wcs_keys()\n+    def test_get_wcs_tpv_keys(self):\n+        wcs_keys = FitsServices.get_wcs_tpv_keys()\n         self.assertTrue(len(wcs_keys) == 32)\n         for i in range(11):\n             self.assertTrue(\"PV1_\" +str(i) in wcs_keys)\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -72,8 +72,8 @@ class TestFitsServices(unittest.TestCase):\n         \n         d = FitsServices.fill_header_values_to_dict(h, {('tag', 'list'):self.key})\n         self.assertTrue(list(d.values())[0].strip() == self.value)\n-    def test_get_wcs_tpv_keys(self):\n-        wcs_keys = FitsServices.get_wcs_tpv_keys()\n+    def test_get_wcs_keys(self):\n+        wcs_keys = FitsServices.get_wcs_keys()\n         self.assertTrue(len(wcs_keys) == 32)\n         for i in range(11):\n             self.assertTrue(\"PV1_\" +str(i) in wcs_keys)\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -72,8 +72,8 @@ class TestFitsServices(unittest.TestCase):\n         \n         d = FitsServices.fill_header_values_to_dict(h, {('tag', 'list'):self.key})\n         self.assertTrue(list(d.values())[0].strip() == self.value)\n-    def test_get_wcs_tpv_keys(self):\n-        wcs_keys = FitsServices.get_wcs_tpv_keys()\n+    def test_get_wcs_keys(self):\n+        wcs_keys = FitsServices.get_wcs_keys()\n         self.assertTrue(len(wcs_keys) == 32)\n         for i in range(11):\n             self.assertTrue(\"PV1_\" +str(i) in wcs_keys)\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/tests/python/TestProcessManager_test.py": [
                        [
                            "@@ -34,7 +34,11 @@ class TestProcessManager(unittest.TestCase):\n         t = time.time()\n         sleeptime = 5\n         self.pm.add_process_and_start(sleep, [sleeptime])\n-\n+        \n+        #returns immediately\n+        self.assertFalse(time.time() - t > sleeptime-1)\n+        \n+        self.pm.block_until_free_process()\n         self.assertTrue(time.time() - t > sleeptime-1)\n     #\n     def test_free(self):\n",
                            "triple number of open files for swarp to test influence on Panstarrs, HEAD to DM 9.1",
                            "Michael",
                            "2023-04-20T14:57:05.000+02:00",
                            "42e8138bad8b1321c3faee1b14b2239b3690850a"
                        ],
                        [
                            "@@ -34,11 +34,7 @@ class TestProcessManager(unittest.TestCase):\n         t = time.time()\n         sleeptime = 5\n         self.pm.add_process_and_start(sleep, [sleeptime])\n-        \n-        #returns immediately\n-        self.assertFalse(time.time() - t > sleeptime-1)\n-        \n-        self.pm.block_until_free_process()\n+\n         self.assertTrue(time.time() - t > sleeptime-1)\n     #\n     def test_free(self):\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:44:16.000+02:00",
                            "24ff822997eab439d4cc5e3f9045dd0ee8b19066"
                        ],
                        [
                            "@@ -34,11 +34,7 @@ class TestProcessManager(unittest.TestCase):\n         t = time.time()\n         sleeptime = 5\n         self.pm.add_process_and_start(sleep, [sleeptime])\n-        \n-        #returns immediately\n-        self.assertFalse(time.time() - t > sleeptime-1)\n-        \n-        self.pm.block_until_free_process()\n+\n         self.assertTrue(time.time() - t > sleeptime-1)\n     #\n     def test_free(self):\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/etc/cfhtls.swarp": [
                        [
                            "@@ -96,3 +96,6 @@ WRITE_FILEINFO\t\tN\t\t# Write information about each input\n VERBOSE_TYPE\t\tNORMAL\t\t# \"QUIET\",\"NORMAL\" or \"FULL\"\n \n NTHREADS\t\t1\t\t# 1 single thread\n+\n+NOPENFILES_MAX         6048     # Maximum number of files opened by SWarp\n+\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:34:27.000+02:00",
                            "48c424c070eaf7a23581e056f466814a31eaf48c"
                        ],
                        [
                            "@@ -81,9 +81,9 @@ BACK_FILTTHRESH\t\t0.0\t\t# Threshold above which the background-\n #------------------------------ Memory management -----------------------------\n  \n VMEM_DIR\t\t.\t\t# Directory path for swap files\n-VMEM_MAX\t\t2047\t\t# Maximum amount of virtual memory (MB)\n-MEM_MAX\t\t\t2500\t\t# Maximum amount of usable RAM (MB)\n-COMBINE_BUFSIZE\t\t2000\t\t# RAM dedicated to co-addition(MB)\n+VMEM_MAX\t\t6047\t\t# Maximum amount of virtual memory (MB)\n+MEM_MAX\t\t\t6500\t\t# Maximum amount of usable RAM (MB)\n+COMBINE_BUFSIZE\t\t6000\t\t# RAM dedicated to co-addition(MB)\n  \n #------------------------------ Miscellaneous ---------------------------------\n  \n",
                            "triple memory for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:06:53.000+02:00",
                            "95b22606964e6640368759d9608f24a195f8d2d4"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/etc/default.swarp": [
                        [
                            "@@ -96,3 +96,6 @@ WRITE_FILEINFO\t\tN\t\t# Write information about each input\n VERBOSE_TYPE\t\tNORMAL\t\t# \"QUIET\",\"NORMAL\" or \"FULL\"\n \n NTHREADS\t\t1\t\t# 1 single thread\n+\n+NOPENFILES_MAX         6048     # Maximum number of files opened by SWarp\n+\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:34:27.000+02:00",
                            "48c424c070eaf7a23581e056f466814a31eaf48c"
                        ],
                        [
                            "@@ -81,9 +81,9 @@ BACK_FILTTHRESH\t\t0.0\t\t# Threshold above which the background-\n #------------------------------ Memory management -----------------------------\n  \n VMEM_DIR\t\t.\t\t# Directory path for swap files\n-VMEM_MAX\t\t2047\t\t# Maximum amount of virtual memory (MB)\n-MEM_MAX\t\t\t2500\t\t# Maximum amount of usable RAM (MB)\n-COMBINE_BUFSIZE\t\t2000\t\t# RAM dedicated to co-addition(MB)\n+VMEM_MAX\t\t6047\t\t# Maximum amount of virtual memory (MB)\n+MEM_MAX\t\t\t6500\t\t# Maximum amount of usable RAM (MB)\n+COMBINE_BUFSIZE\t\t6000\t\t# RAM dedicated to co-addition(MB)\n  \n #------------------------------ Miscellaneous ---------------------------------\n  \n",
                            "triple memory for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:06:53.000+02:00",
                            "95b22606964e6640368759d9608f24a195f8d2d4"
                        ],
                        [
                            "@@ -81,9 +81,9 @@ BACK_FILTTHRESH\t\t0.0\t\t# Threshold above which the background-\n #------------------------------ Memory management -----------------------------\n  \n VMEM_DIR\t\t.\t\t# Directory path for swap files\n-VMEM_MAX\t\t2047\t\t# Maximum amount of virtual memory (MB)\n-MEM_MAX\t\t\t2500\t\t# Maximum amount of usable RAM (MB)\n-COMBINE_BUFSIZE\t\t2000\t\t# RAM dedicated to co-addition(MB)\n+VMEM_MAX\t\t6047\t# Maximum amount of virtual memory (MB)\n+MEM_MAX\t\t\t7500\t# Maximum amount of usable RAM (MB)\n+COMBINE_BUFSIZE\t6000\t# RAM dedicated to co-addition(MB)\n  \n #------------------------------ Miscellaneous ---------------------------------\n  \n@@ -96,3 +96,5 @@ WRITE_FILEINFO\t\tN\t\t# Write information about each input\n VERBOSE_TYPE\t\tNORMAL\t\t# \"QUIET\",\"NORMAL\" or \"FULL\"\n \n NTHREADS\t\t1\t\t# 1 single thread\n+NOPENFILES_MAX         2048            # Maximum number of files opened by SWarp\n+\n",
                            "more memory for swarp",
                            "Michael",
                            "2023-03-22T16:17:40.000+01:00",
                            "1a149f5041b9de0f14d7f52073cf6e1c827ba2f8"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/conf/etc/des.swarp": [
                        [
                            "@@ -96,3 +96,6 @@ WRITE_FILEINFO\t\tN\t\t# Write information about each input\n VERBOSE_TYPE\t\tNORMAL\t\t# \"QUIET\",\"NORMAL\" or \"FULL\"\n \n NTHREADS\t\t1\t\t# 1 single thread\n+\n+NOPENFILES_MAX         6048     # Maximum number of files opened by SWarp\n+\n",
                            "triple number of open files for swarp to test influence on Panstarrs",
                            "Michael",
                            "2023-04-20T14:34:27.000+02:00",
                            "48c424c070eaf7a23581e056f466814a31eaf48c"
                        ]
                    ],
                    "EXT_PF1_GEN_P2_LIBS/python/EXT_PF1_GEN_P2_LIBS/file/FitsServices.py": [
                        [
                            "@@ -330,7 +330,10 @@ def read_header(fitsfile, hdu):\n \n def _read_fitsio_header(fitsfile, hdu):\n     try:\n-        return fitsio.read_header(fitsfile, hdu)\n+        h = fitsio.read_header(fitsfile, hdu)\n+        if None in h:\n+            h.delete(None)\n+        return h\n     except Exception as e:\n         logging.info(e)\n     return None\n",
                            "Added a None key check to FitsServices fitsio header reading",
                            "Michael",
                            "2023-03-20T10:26:13.000+01:00",
                            "39c2e72f1814f8b160b36a8313633ce50afae619"
                        ],
                        [
                            "@@ -469,13 +469,13 @@ def write_keys(fitsfile, hdu, headerdict):\n #\n \"\"\"\n \n-def write_keys(fitsfile, hdu, headerdict):\n-    if not _write_astropy_keys(fitsfile, hdu, headerdict):\n-        _write_fitsio_header_bf(fitsfile, hdu, headerdict)\n+def write_keys(fitsfile, hdu, headerdict, header_keys_to_remove = []):\n+    if not _write_astropy_keys(fitsfile, hdu, headerdict, header_keys_to_remove):\n+        _write_fitsio_header_bf(fitsfile, hdu, headerdict, header_keys_to_remove)\n     #\n #\n \n-def _write_astropy_keys(fitsfile, hdu, headerdict, ignore_missing_end = True):\n+def _write_astropy_keys(fitsfile, hdu, headerdict, header_keys_to_remove = [], ignore_missing_end = True):\n     try:\n         hdul = astrofits.open(fitsfile, mode = 'update', ignore_missing_end = ignore_missing_end)\n         h = hdul[hdu].header\n@@ -485,6 +485,12 @@ def _write_astropy_keys(fitsfile, hdu, headerdict, ignore_missing_end = True):\n                 c = record['comment']\n             h.set(record['name'], value = record['value'], comment = c)\n         #\n+        \n+        for key in header_keys_to_remove:\n+            if key in h:\n+                del h[key]\n+            #\n+        #\n         hdul.flush()\n         hdul.close()\n     except Exception as e:\n@@ -514,7 +520,7 @@ def _write_fitsio_header_cards(fitsfile, hdu, headerdict):\n #\n \"\"\"\n \n-def _write_fitsio_header_bf(fitsfile, hdu, headerdict):\n+def _write_fitsio_header_bf(fitsfile, hdu, headerdict, header_keys_to_remove = []):\n     try:\n         fits = FITS(fitsfile)\n         ffile = dict()\n@@ -539,6 +545,12 @@ def _write_fitsio_header_bf(fitsfile, hdu, headerdict):\n             record = remaningkey\n             ffile[ename]['head'].add_record(remaningkey)\n         #\n+        \n+        for key in header_keys_to_remove:\n+            if key in ffile[ename]['head']:\n+                ffile[ename]['head'].delete(key)\n+            #\n+        #\n         ffile[ename]['head'].clean()\n         \n         #overwrite the old stuff\n@@ -655,7 +667,15 @@ def correct_compression_keywords(header):\n #\n \n \n-def get_wcs_keys():\n+def get_linear_wcs_keys():\n+    wcs_keys = ['CRPIX1','CRPIX2','CRVAL1','CRVAL2','CTYPE1','CTYPE2',\n+        'CD1_1','CD1_2','CD2_1','CD2_2']\n+        \n+    return wcs_keys\n+#\n+\n+\n+def get_wcs_tpv_keys():\n     wcs_keys = ['CRPIX1','CRPIX2','CRVAL1','CRVAL2','CTYPE1','CTYPE2',\n                    'CD1_1','CD1_2','CD2_1','CD2_2',\n                    'PV1_0','PV1_1','PV1_2','PV1_3','PV1_4','PV1_5','PV1_6','PV1_7','PV1_8','PV1_9','PV1_10',\n@@ -664,7 +684,7 @@ def get_wcs_keys():\n #\n \n \n-def read_wcs(fitsfile, hdu, wcs_keys = get_wcs_keys()):\n+def read_wcs(fitsfile, hdu, wcs_keys = get_wcs_tpv_keys()):\n     \"\"\"\n     returns a list of available header dict-keycards of the structure\n     {'name':keytupel[0], 'value':keytupel[1], 'comment':keytupel[2]}\n",
                            "count up version. Improved ProcessManager",
                            "Michael",
                            "2023-03-13T13:52:35.000+01:00",
                            "30b6fdedd75433b62e05d99c8b27c4d428d77d1d"
                        ]
                    ]
                },
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": [
                    {
                        "name": "0.78.0",
                        "created_at": "2023-03-06T10:10:48.000+01:00",
                        "author_name": "Michael"
                    },
                    {
                        "name": "0.9.0",
                        "created_at": "2023-07-03T17:42:16.000+02:00",
                        "author_name": "Michael"
                    }
                ]
            },
            "PF-EXT/ext_des_st1_ou": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/ext-data-quality-assessment": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/realexternaldata": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_IAL_Pipelines": {
                "start date": "2023-03-12T22:10:27",
                "end date": "2023-07-04T07:53:54",
                "start tag": "0.78.0",
                "end tag": "0.9.0",
                "count_files_modified": "23",
                "modifications_by_file": {
                    ".jenkinsFile": [
                        [
                            "@@ -1,3 +1,3 @@\n #!groovy\n-@Library('integration-library@release-9') _\n-pipelineElements(artifactId:\"EXT_IAL_Pipelines\", component:'eden.3.0')\n+@Library('integration-library@release-10') _\n+pipelineElements(name:\"EXT_IAL_Pipelines\", component:'eden.3.1')\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:11:32.000+02:00",
                            "ed936ffc36623065cbffd5f3e4bf837701a43fe4"
                        ]
                    ],
                    "CMakeLists.txt": [
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.82 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.9 USE ST_DataModel 9.2.0) # ST_PipelineChecker 1.2.2)\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.81 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.82 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n",
                            "more RAM and increase version to 0.82",
                            "Jelte de Jong",
                            "2023-05-30T23:10:33.000+02:00",
                            "785aa0c04bfc582020a2245149152ea0a02773c5"
                        ],
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.80 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.81 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.80 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.81 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n",
                            "add crossmatched catalogs as EXT_PF1_GEN internal files; set version to 0.81",
                            "Jelte de Jong",
                            "2023-04-06T17:02:51.000+02:00",
                            "d98cbb27bb4aec1b039bdc4bfb9975de00407420"
                        ],
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.79 USE ST_DataModel 9.0.3) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.80 USE ST_DataModel 9.1.5) # ST_PipelineChecker 1.2.2)\n",
                            "Update to DM 9.1.5",
                            "Jelte de Jong",
                            "2023-03-21T13:52:09.000+01:00",
                            "b5ba2ad6136a354f7959065aecd7f5f7a72ad769"
                        ],
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.80 USE ST_DataModel 9.0.3) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.79 USE ST_DataModel 9.0.3) # ST_PipelineChecker 1.2.2)\n",
                            "added tmpsize=200 to coadd and Panstarrs pipeline.",
                            "Michael",
                            "2023-03-17T12:01:07.000+01:00",
                            "aaf92c6a033f27fe3e74d34c71c2641eb67857a7"
                        ],
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.79 USE ST_DataModel 9.0.3) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.80 USE ST_DataModel 9.0.3) # ST_PipelineChecker 1.2.2)\n",
                            "link to coadd pipeline version 0.79",
                            "Michael",
                            "2023-03-13T17:57:26.000+01:00",
                            "24678ed993e15f87880655cb8ea285715ff48d57"
                        ],
                        [
                            "@@ -12,4 +12,4 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(EXT_IAL_Pipelines 0.78 USE ST_DataModel 9.0.3) # ST_PipelineChecker 1.2.2)\n+elements_project(EXT_IAL_Pipelines 0.79 USE ST_DataModel 9.0.3) # ST_PipelineChecker 1.2.2)\n",
                            "Set version to 0.79 and EXT_PF1_GEN version to 3.2",
                            "Jelte de Jong",
                            "2023-03-12T22:14:33.000+01:00",
                            "c96eb0c6fce3d86b53e8d5cb55e4779c282286b6"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_CFIS_Pipeline/PackageDef.py": [
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.9 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "updated coadd pipelines to call 0.81",
                            "Michael",
                            "2023-06-09T09:55:32.000+02:00",
                            "2b4e6a1fd71cf52d38f7002e1b2408d5684d57b2"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Jelte de Jong",
                            "2023-04-21T11:38:28.000+02:00",
                            "2e242f3559b885aa40367e5c2c8bc4481bdb8236"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "swapped coadd pipelines to DM 9.1.5 version 0.80",
                            "Michael",
                            "2023-04-19T09:43:12.000+02:00",
                            "8f653bd5ae2313e65005a44b3e4b2ce5cac372fa"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.78 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "link to coadd pipeline version 0.79",
                            "Michael",
                            "2023-03-13T17:57:26.000+01:00",
                            "24678ed993e15f87880655cb8ea285715ff48d57"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Debug_Pipeline/PackageDef.py": [
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.9 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "updated coadd pipelines to call 0.81",
                            "Michael",
                            "2023-06-09T09:55:32.000+02:00",
                            "2b4e6a1fd71cf52d38f7002e1b2408d5684d57b2"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=30.0, dynamic_max_ram=90.0, vms=30.0, dynamic_max_vms=90.0, walltime=120.0)\n                               )\n\\ No newline at end of file\n",
                            "resources again. Values are not per cpu but the actual resources.",
                            "Michael",
                            "2023-05-16T15:12:10.000+02:00",
                            "40e3914893ef1162df40743e5c2435d80774c624"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0)\n                               )\n\\ No newline at end of file\n",
                            "defined coadd debug tmp pipeline with 200 G tmp dir",
                            "Michael",
                            "2023-05-09T16:10:21.000+02:00",
                            "2fa7ff3b0109d08ca32b97c90d528ccad1306d65"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "added tmp to debug pipeline",
                            "Michael",
                            "2023-05-09T16:05:19.000+02:00",
                            "f96e3840f56b561b75b428e73baa8c59c72ac59f"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, ram=32.0, walltime=30.0)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0)\n                               )\n\\ No newline at end of file\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=18, ram=24.0, vms=64.0, walltime=60.0)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0)\n                               )\n\\ No newline at end of file\n",
                            "adapted resources to measured values and adding dynamic resource allocation",
                            "Michael",
                            "2023-05-03T12:17:53.000+02:00",
                            "7e4203384cceb5d0dd7707d9ffc493a471239b44"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=18, ram=24.0, vms=64.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=18, ram=24.0, vms=64.0, walltime=60.0)\n                               )\n\\ No newline at end of file\n",
                            "take out tmp dir for debug pipeline",
                            "Michael",
                            "2023-05-02T20:34:57.000+02:00",
                            "d904eb5dc425220a22e7246a21a3602d2c744fa8"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=18, ram=24.0, vms=64.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "added 18 cpu to coadd debug",
                            "Michael",
                            "2023-04-26T17:33:37.000+02:00",
                            "c98c53f517f8778c4a81d2dc320042bec922cbe3"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, ram=32.0, walltime=30.0)\n+                              resources=ComputingResources(cores=6, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Jelte de Jong",
                            "2023-04-21T11:38:28.000+02:00",
                            "2e242f3559b885aa40367e5c2c8bc4481bdb8236"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, ram=32.0, walltime=30.0)\n+                              resources=ComputingResources(cores=6, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "enhance the resources for the coadd debug pipeline so it can handle Panstarrs",
                            "Michael",
                            "2023-04-20T14:05:20.000+02:00",
                            "2179b4fea0fcde50cf3e82d2988c3a0e64235a49"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "swapped coadd pipelines to DM 9.1.5 version 0.80",
                            "Michael",
                            "2023-04-19T09:43:12.000+02:00",
                            "8f653bd5ae2313e65005a44b3e4b2ce5cac372fa"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.78 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "link to coadd pipeline version 0.79",
                            "Michael",
                            "2023-03-13T17:57:26.000+01:00",
                            "24678ed993e15f87880655cb8ea285715ff48d57"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Debug_Tmp_Pipeline/PackageDef.py": [
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.9 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "updated coadd pipelines to call 0.81",
                            "Michael",
                            "2023-06-09T09:55:32.000+02:00",
                            "2b4e6a1fd71cf52d38f7002e1b2408d5684d57b2"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=30.0, dynamic_max_ram=90.0, vms=30.0, dynamic_max_vms=90.0, walltime=120.0, tmpsize=300., dynamic_max_tmpsize=500.)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=60.0, dynamic_max_ram=90.0, vms=60.0, dynamic_max_vms=90.0, walltime=120.0, tmpsize=300., dynamic_max_tmpsize=500.)\n                               )\n\\ No newline at end of file\n",
                            "more ram",
                            "Michael",
                            "2023-05-22T16:37:50.000+02:00",
                            "adf4a8500a7bd6de00def0bf6a0485770d6596c8"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=12.0, walltime=120.0, tmpsize=30., dynamic_max_tmpsize=50.)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=30.0, dynamic_max_ram=90.0, vms=30.0, dynamic_max_vms=90.0, walltime=120.0, tmpsize=300., dynamic_max_tmpsize=500.)\n                               )\n\\ No newline at end of file\n",
                            "resources again. Values are not per cpu but the actual resources.",
                            "Michael",
                            "2023-05-16T15:12:10.000+02:00",
                            "40e3914893ef1162df40743e5c2435d80774c624"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=12.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=30., dynamic_max_tmpsize=50.)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=12.0, walltime=120.0, tmpsize=30., dynamic_max_tmpsize=50.)\n                               )\n\\ No newline at end of file\n",
                            "reduced max walltime",
                            "Michael",
                            "2023-05-15T15:42:53.000+02:00",
                            "71bed15bde1224de01469f22eb099d93b7df5b7f"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=12.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200., dynamic_max_tmpsize=450.)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=12.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=30., dynamic_max_tmpsize=50.)\n                               )\n\\ No newline at end of file\n",
                            "reduced max tmp",
                            "Michael",
                            "2023-05-15T14:01:20.000+02:00",
                            "b27a7b247e0d50b16bc114a42c106bcea96c048a"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200., dynamic_max_tmpsize=450.)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=12.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200., dynamic_max_tmpsize=450.)\n                               )\n\\ No newline at end of file\n",
                            "reduced max vms for the Tmp packagedef as the Pilot definitons requires <= 98 GB",
                            "Michael",
                            "2023-05-15T13:14:59.000+02:00",
                            "ca4f65b3c6434cc445d5d6cafcdb3fd7a6b168a3"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200., dynamic_max_tmpsize=450.)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=8, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200., dynamic_max_tmpsize=450.)\n                               )\n\\ No newline at end of file\n",
                            "reduced max cores for the Tmp packagedef as the Pilot definitons requires <= 8 cores",
                            "Michael",
                            "2023-05-15T11:21:11.000+02:00",
                            "5be95c1991f87be13f7bcda2a7e6c582d2d28f3d"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200., dynamic_max_tmpsize=450.)\n                               )\n\\ No newline at end of file\n",
                            "added dynamic tempsize, more walltime",
                            "Michael",
                            "2023-05-11T10:47:20.000+02:00",
                            "43d2b0d6967b275502e6ed5b6ab2b995af52747f"
                        ],
                        [
                            "@@ -0,0 +1,30 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n+\n+\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+                              inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n+                                      Input('mer_star_cat', content_type=\"listfile\"),\n+                                      Input('mer_galaxy_cat', content_type=\"listfile\"),\n+                                      Input('tileinfo'),\n+                                      Input('coadd_props_file', content_type=\"listfile\")],\n+                              outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n+                                       Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n+                                       Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=2.0, dynamic_max_ram=4.0, vms=4.0, dynamic_max_vms=16.0, walltime=60.0, tmpsize=200)\n+                              )\n\\ No newline at end of file\n",
                            "defined coadd debug tmp pipeline with 200 G tmp dir",
                            "Michael",
                            "2023-05-09T16:10:21.000+02:00",
                            "2fa7ff3b0109d08ca32b97c90d528ccad1306d65"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_NOS_Pipeline/PackageDef.py": [
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.9 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "updated coadd pipelines to call 0.81",
                            "Michael",
                            "2023-06-09T09:55:32.000+02:00",
                            "2b4e6a1fd71cf52d38f7002e1b2408d5684d57b2"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Jelte de Jong",
                            "2023-04-21T11:38:28.000+02:00",
                            "2e242f3559b885aa40367e5c2c8bc4481bdb8236"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "swapped coadd pipelines to DM 9.1.5 version 0.80",
                            "Michael",
                            "2023-04-19T09:43:12.000+02:00",
                            "8f653bd5ae2313e65005a44b3e4b2ce5cac372fa"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.78 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "link to coadd pipeline version 0.79",
                            "Michael",
                            "2023-03-13T17:57:26.000+01:00",
                            "24678ed993e15f87880655cb8ea285715ff48d57"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Panstarrs_Pipeline/PackageDef.py": [
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.9 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=60.0, dynamic_max_ram=90.0, vms=60.0, dynamic_max_vms=90.0, walltime=120.0, tmpsize=300., dynamic_max_tmpsize=500.)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=80.0, dynamic_max_ram=90.0, vms=80.0, dynamic_max_vms=90.0, walltime=120.0, tmpsize=900., dynamic_max_tmpsize=900.)\n                               )\n\\ No newline at end of file\n",
                            "1TB tempdir size for Panstarrs",
                            "Michael",
                            "2023-06-16T10:55:27.000+02:00",
                            "1dd6f9976d70d98e1ec5056020ae5aac295cc9f5"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "updated coadd pipelines to call 0.81",
                            "Michael",
                            "2023-06-09T10:13:47.000+02:00",
                            "585c57fff9297c0df03128d02d8e6938aa9f0df4"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=3, dynamic_max_cores=6, ram=2.0, dynamic_max_ram=4.0, vms=8.0, dynamic_max_vms=16.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200.0, dynamic_max_tmpsize=450.0)\n+                              resources=ComputingResources(cores=6, dynamic_max_cores=18, ram=60.0, dynamic_max_ram=90.0, vms=60.0, dynamic_max_vms=90.0, walltime=120.0, tmpsize=300., dynamic_max_tmpsize=500.)\n                               )\n\\ No newline at end of file\n",
                            "updated Panstarrs resources",
                            "Michael",
                            "2023-06-06T13:07:58.000+02:00",
                            "26710719a8e2f3109f4158a29f47fe444eca9a31"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=3, dynamic_max_cores=6, ram=2.0, dynamic_max_ram=4.0, vms=8.0, dynamic_max_vms=16.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=3, dynamic_max_cores=6, ram=2.0, dynamic_max_ram=4.0, vms=8.0, dynamic_max_vms=16.0, walltime=120.0, dynamic_max_walltime=240.0, tmpsize=200.0, dynamic_max_tmpsize=450.0)\n                               )\n\\ No newline at end of file\n",
                            "added dynamic tempsize, more walltime",
                            "Michael",
                            "2023-05-11T10:47:20.000+02:00",
                            "43d2b0d6967b275502e6ed5b6ab2b995af52747f"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=4, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=3, dynamic_max_cores=6, ram=2.0, dynamic_max_ram=4.0, vms=8.0, dynamic_max_vms=16.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=3, dynamic_max_cores=6, ram=2.0, dynamic_max_ram=4.0, vms=8.0, dynamic_max_vms=16.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "adapted resources to measured values and adding dynamic resource allocation",
                            "Michael",
                            "2023-05-03T12:17:53.000+02:00",
                            "7e4203384cceb5d0dd7707d9ffc493a471239b44"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=4, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=6, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Jelte de Jong",
                            "2023-04-21T11:38:28.000+02:00",
                            "2e242f3559b885aa40367e5c2c8bc4481bdb8236"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=4, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n+                              resources=ComputingResources(cores=6, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "enhance the resources for the coadd debug pipeline so it can handle Panstarrs",
                            "Michael",
                            "2023-04-20T14:05:20.000+02:00",
                            "2179b4fea0fcde50cf3e82d2988c3a0e64235a49"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "swapped coadd pipelines to DM 9.1.5 version 0.80",
                            "Michael",
                            "2023-04-19T09:43:12.000+02:00",
                            "8f653bd5ae2313e65005a44b3e4b2ce5cac372fa"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=4, ram=8.0, vms=32.0, walltime=60.0)\n+                              resources=ComputingResources(cores=4, ram=8.0, vms=32.0, walltime=60.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "added tmpsize=200 to coadd and Panstarrs pipeline.",
                            "Michael",
                            "2023-03-17T12:01:07.000+01:00",
                            "aaf92c6a033f27fe3e74d34c71c2641eb67857a7"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.78 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "link to coadd pipeline version 0.79",
                            "Michael",
                            "2023-03-13T17:57:26.000+01:00",
                            "24678ed993e15f87880655cb8ea285715ff48d57"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Pipeline/PackageDef.py": [
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.9 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.81 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "updated coadd pipelines to call 0.81",
                            "Michael",
                            "2023-06-09T09:55:32.000+02:00",
                            "2b4e6a1fd71cf52d38f7002e1b2408d5684d57b2"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Jelte de Jong",
                            "2023-04-21T11:38:28.000+02:00",
                            "2e242f3559b885aa40367e5c2c8bc4481bdb8236"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "swapped coadd pipelines to DM 9.1.5 version 0.80",
                            "Michael",
                            "2023-04-19T09:43:12.000+02:00",
                            "8f653bd5ae2313e65005a44b3e4b2ce5cac372fa"
                        ],
                        [
                            "@@ -26,5 +26,5 @@ coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Co\n                               outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n                                        Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n-                              resources=ComputingResources(cores=6, ram=32.0, walltime=30.0)\n+                              resources=ComputingResources(cores=6, ram=32.0, walltime=30.0, tmpsize=200)\n                               )\n\\ No newline at end of file\n",
                            "added tmpsize=200 to coadd and Panstarrs pipeline.",
                            "Michael",
                            "2023-03-17T12:01:07.000+01:00",
                            "aaf92c6a033f27fe3e74d34c71c2641eb67857a7"
                        ],
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.78 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.79 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "link to coadd pipeline version 0.79",
                            "Michael",
                            "2023-03-13T17:57:26.000+01:00",
                            "24678ed993e15f87880655cb8ea285715ff48d57"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_SUBARU_MULTIBAND_Pipeline/PackageDef.py": [
                        [
                            "@@ -17,7 +17,7 @@ the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n \n-coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.90 EXT_PF1_Coadd\",\n                               inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n                                       Input('mer_star_cat', content_type=\"listfile\"),\n                                       Input('mer_galaxy_cat', content_type=\"listfile\"),\n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ],
                        [
                            "@@ -0,0 +1,30 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n+\n+\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+                              inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n+                                      Input('mer_star_cat', content_type=\"listfile\"),\n+                                      Input('mer_galaxy_cat', content_type=\"listfile\"),\n+                                      Input('tileinfo'),\n+                                      Input('coadd_props_file', content_type=\"listfile\")],\n+                              outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n+                                       Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n+                                       Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n+                              resources=ComputingResources(cores=18, ram=24.0, vms=64.0, walltime=60.0, tmpsize=200)\n+                              )\n\\ No newline at end of file\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -0,0 +1,30 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+\n+from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n+\n+\n+coadd_stage2_pipeline = Executable(command=\"E-Run EXT_PF1_GEN_P2 0.80 EXT_PF1_Coadd\",\n+                              inputs=[Input(\"single_epoch_images\", content_type=\"listfile\"),\n+                                      Input('mer_star_cat', content_type=\"listfile\"),\n+                                      Input('mer_galaxy_cat', content_type=\"listfile\"),\n+                                      Input('tileinfo'),\n+                                      Input('coadd_props_file', content_type=\"listfile\")],\n+                              outputs=[Output(\"coadds\", mime_type=\"json\", content_type=\"listfile\"),\n+                                       Output(\"validationreport\", mime_type=\"json\", content_type=\"listfile\"),\n+                                       Output(\"coadd_catalogs\", mime_type=\"json\", content_type=\"listfile\")],\n+                              resources=ComputingResources(cores=18, ram=24.0, vms=64.0, walltime=60.0, tmpsize=200)\n+                              )\n\\ No newline at end of file\n",
                            "added 18 cpu SUBURU multiband",
                            "Michael",
                            "2023-04-26T17:18:30.000+02:00",
                            "3196a23075fb18a6b9e9a720bc21188050f2acb4"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/tests/python/EXT_PF1_GEN_Coadd_Pipeline_vs_Datamodel_test.py": [
                        [
                            "@@ -18,7 +18,7 @@\n \n import os\n import unittest\n-import ST_DataModelBindings.interfaces.sys.tsk_stub as tsk\n+#import ST_DataModelBindings.interfaces.sys.tsk_stub as tsk\n import ST_DM_DmUtils.DmUtils as dm_utils\n import inspect\n \n",
                            "Elements 6.2.1",
                            "Michael",
                            "2023-06-28T17:10:04.000+02:00",
                            "3e5a9fd3b3324d4c99d7df718055933a6931d0f7"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_PostValidate_Pipeline/PkgDef_EXT_PostValidateStackListFull.py": [
                        [
                            "@@ -30,7 +30,7 @@ ENV_VARIABLES = {\n }\n \n EXT_AstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -39,7 +39,7 @@ EXT_AstromPhotomValidation = Executable(\n     env_variables=ENV_VARIABLES)\n \n EXT_GaiaAstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_GaiaAstromPhotomValidation\",\n     inputs=[Input(\"stack_GaiaAstromPhotom\"),\n             Input(\"gaia_catalog\"),\n             Input(\"configuration_set\"),\n@@ -50,7 +50,7 @@ EXT_GaiaAstromPhotomValidation = Executable(\n \n \n EXT_PixelStatsValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PixelStatsValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_PixelStatsValidation\",\n     inputs=[Input(\"stack_PixelStats\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -59,7 +59,7 @@ EXT_PixelStatsValidation = Executable(\n     env_variables=ENV_VARIABLES)\n \n EXT_FwhmValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_FwhmValidation\",\n     inputs=[Input(\"stack_Fwhm\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\"),\n@@ -69,7 +69,7 @@ EXT_FwhmValidation = Executable(\n     env_variables=ENV_VARIABLES)\n \n EXT_PsfValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_PsfValidation\",\n     inputs=[Input(\"stack_Psf\"),\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n@@ -79,7 +79,7 @@ EXT_PsfValidation = Executable(\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.0),\n",
                            "postvalidation pipeline version updated to 1.7",
                            "Frederic Raison",
                            "2023-06-28T14:24:40.000+02:00",
                            "7be95cb14083fe8aeba6386c24f635ba625c4080"
                        ],
                        [
                            "@@ -25,13 +25,18 @@ from euclidwf.framework.taskdefs import CreateListFile\n from euclidwf.framework.taskdefs import (Executable, Input, Output,\n                                          ComputingResources)\n \n+ENV_VARIABLES = {\n+        \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n EXT_AstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"AstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_GaiaAstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n@@ -40,7 +45,8 @@ EXT_GaiaAstromPhotomValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"GaiaAstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_PixelStatsValidation = Executable(\n@@ -49,7 +55,8 @@ EXT_PixelStatsValidation = Executable(\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"PixelStats_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_FwhmValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n@@ -58,7 +65,8 @@ EXT_FwhmValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"Fwhm_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_PsfValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n@@ -66,14 +74,16 @@ EXT_PsfValidation = Executable(\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"Psf_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n \n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Michael",
                            "2023-05-09T16:13:50.000+02:00",
                            "388c4544592fec3c6ea0637984e5d0c38617c455"
                        ],
                        [
                            "@@ -25,13 +25,18 @@ from euclidwf.framework.taskdefs import CreateListFile\n from euclidwf.framework.taskdefs import (Executable, Input, Output,\n                                          ComputingResources)\n \n+ENV_VARIABLES = {\n+        \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n EXT_AstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"AstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_GaiaAstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n@@ -40,7 +45,8 @@ EXT_GaiaAstromPhotomValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"GaiaAstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_PixelStatsValidation = Executable(\n@@ -49,7 +55,8 @@ EXT_PixelStatsValidation = Executable(\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"PixelStats_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_FwhmValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n@@ -58,7 +65,8 @@ EXT_FwhmValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"Fwhm_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_PsfValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n@@ -66,14 +74,16 @@ EXT_PsfValidation = Executable(\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"Psf_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n \n",
                            "added ENV_VARIABLES to solve  python cache issue #22039",
                            "Frederic Raison",
                            "2023-05-08T18:29:22.000+02:00",
                            "b904d4e185b495685a1f418dee57ae550d57eb79"
                        ],
                        [
                            "@@ -26,7 +26,7 @@ from euclidwf.framework.taskdefs import (Executable, Input, Output,\n                                          ComputingResources)\n \n EXT_AstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_AstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -34,7 +34,7 @@ EXT_AstromPhotomValidation = Executable(\n     resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n \n EXT_GaiaAstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_GaiaAstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n     inputs=[Input(\"stack_GaiaAstromPhotom\"),\n             Input(\"gaia_catalog\"),\n             Input(\"configuration_set\"),\n@@ -44,7 +44,7 @@ EXT_GaiaAstromPhotomValidation = Executable(\n \n \n EXT_PixelStatsValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_PixelStatsValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PixelStatsValidation\",\n     inputs=[Input(\"stack_PixelStats\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -52,7 +52,7 @@ EXT_PixelStatsValidation = Executable(\n     resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n \n EXT_FwhmValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_FwhmValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n     inputs=[Input(\"stack_Fwhm\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\"),\n@@ -61,7 +61,7 @@ EXT_FwhmValidation = Executable(\n     resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n \n EXT_PsfValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_PsfValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n     inputs=[Input(\"stack_Psf\"),\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n@@ -70,7 +70,7 @@ EXT_PsfValidation = Executable(\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_MergeAnalysisResultsTask\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n",
                            "call version 1.6 of Postvalidation",
                            "Frederic Raison",
                            "2023-03-28T16:52:29.000+02:00",
                            "1865e7b3456e6cd69b1cadd38923e30b808f9c15"
                        ]
                    ],
                    "README.md": [
                        [
                            "@@ -21,3 +21,4 @@ For each of these the repository contains:\n \n ## HandyLinks\n * [Technical documentation portal for EXT-PF1-Stage2, EXT-PF2 and EXT-PF4](https://euclid.roe.ac.uk/projects/ousdca/wiki/Wiki#Technical-documentation-portal-for-EXT-PF1-Stage2-EXT-PF2-and-EXT-PF4)\n+* [StackedFrame Validation Pipeline code repo](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_PF1_GEN_PostValidation)\n",
                            "Adds link to SEF Validation Pipeline PPO creation Jupyter notebook.",
                            "Gijs Verdoes Kleijn",
                            "2023-05-31T18:28:40.000+00:00",
                            "46382810521574816e04079739b4258a342eaf37"
                        ],
                        [
                            "@@ -14,3 +14,10 @@ For each of these the repository contains:\n * PackageDefinition (filename like PkgDef*py, [example file](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines/-/blob/develop/EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Validate_Pipeline/PkgDef_EXT_PF1_GEN_Validate.py))= this python script defines per processing element (that together form the pipeline) the executable, the input and the output.\n * PipelineScript (filename PipelineScript*.py, [example file](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines/-/blob/develop/EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Pipeline/PipelineScript.py)) :   the script that executes the pipeline\n \n+## PPO creation without using COORS\n+* [SEF Validation Pipeline PPO creation Jupyter notebook](https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_SUM#B-Using-MakeExtS2P1Ppoipynb-to-create-PPOs)\n+* [StackedFrame Production Pipeline PPO creation python script](https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Python-module-for-PPO-creation)\n+* [StackedFrame Validation Pipeline manual PPO creation](https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_PostValidation_SUM#C-Creating-a-PPO-or-a-list-of-PPOs) \n+\n+## HandyLinks\n+* [Technical documentation portal for EXT-PF1-Stage2, EXT-PF2 and EXT-PF4](https://euclid.roe.ac.uk/projects/ousdca/wiki/Wiki#Technical-documentation-portal-for-EXT-PF1-Stage2-EXT-PF2-and-EXT-PF4)\n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Michael",
                            "2023-05-16T15:12:18.000+02:00",
                            "e1540bafdb4f22bb2b8aa38ec51461e0a6bbfc9e"
                        ],
                        [
                            "@@ -14,3 +14,10 @@ For each of these the repository contains:\n * PackageDefinition (filename like PkgDef*py, [example file](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines/-/blob/develop/EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Validate_Pipeline/PkgDef_EXT_PF1_GEN_Validate.py))= this python script defines per processing element (that together form the pipeline) the executable, the input and the output.\n * PipelineScript (filename PipelineScript*.py, [example file](https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines/-/blob/develop/EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Pipeline/PipelineScript.py)) :   the script that executes the pipeline\n \n+## PPO creation without using COORS\n+* [SEF Validation Pipeline PPO creation Jupyter notebook](https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_SUM#B-Using-MakeExtS2P1Ppoipynb-to-create-PPOs)\n+* [StackedFrame Production Pipeline PPO creation python script](https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_P2_User_Manual#Python-module-for-PPO-creation)\n+* [StackedFrame Validation Pipeline manual PPO creation](https://euclid.roe.ac.uk/projects/sgu/wiki/EXT_PF1_GEN_PostValidation_SUM#C-Creating-a-PPO-or-a-list-of-PPOs) \n+\n+## HandyLinks\n+* [Technical documentation portal for EXT-PF1-Stage2, EXT-PF2 and EXT-PF4](https://euclid.roe.ac.uk/projects/ousdca/wiki/Wiki#Technical-documentation-portal-for-EXT-PF1-Stage2-EXT-PF2-and-EXT-PF4)\n",
                            "Update README.md",
                            "Gijs Verdoes Kleijn",
                            "2023-05-16T05:25:24.000+00:00",
                            "ea4be54a22909d3bf531f9f544a8d020dcffdcaa"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Validate_Pipeline/PkgDef_EXT_PF1_GEN_Validate.py": [
                        [
                            "@@ -20,7 +20,7 @@ EXT_PF1_PrepareValidation = Executable(\n         Output('fullyenclosedframes', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         Output('valbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    resources=ComputingResources(cores=1, ram=3.0, walltime=1.0),\n     env_variables=ENV_VARIABLES\n     )\n \n@@ -121,7 +121,7 @@ EXT_PF1_AggregateValidation = Executable(\n         Output('validatedbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['subtile_results', 'bandpass', 'configset']),\n         Output('validationreport', lineage=['subtile_results', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    resources=ComputingResources(cores=1, ram=32.0, walltime=1.0),\n     env_variables=ENV_VARIABLES\n     )\n \n",
                            "more RAM and increase version to 0.82",
                            "Jelte de Jong",
                            "2023-05-30T23:10:33.000+02:00",
                            "785aa0c04bfc582020a2245149152ea0a02773c5"
                        ],
                        [
                            "@@ -1,6 +1,10 @@\n from euclidwf.framework.taskdefs import Executable, TYPE_LISTFILE, MIME_JSON, MIME_TXT\n from euclidwf.framework.taskdefs import Input, Output, ComputingResources, OneByOne\n \n+ENV_VARIABLES = {\n+    \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n version = '3.2'\n \n EXT_PF1_PrepareValidation = Executable(\n@@ -16,7 +20,8 @@ EXT_PF1_PrepareValidation = Executable(\n         Output('fullyenclosedframes', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         Output('valbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -30,7 +35,8 @@ EXT_PF1_ValidateFormat = Executable(\n         Output('formatlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -46,7 +52,8 @@ EXT_PF1_ValidateAstrom = Executable(\n         Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -64,7 +71,8 @@ EXT_PF1_ValidatePhotom = Executable(\n         Output('photomlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -79,7 +87,8 @@ EXT_PF1_ValidatePsf = Executable(\n         Output('psflog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -94,7 +103,8 @@ EXT_PF1_ValidateBandpass = Executable(\n     outputs=[\n         Output('bandpasslog', mime_type=MIME_TXT),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0) # TBC\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -111,6 +121,7 @@ EXT_PF1_AggregateValidation = Executable(\n         Output('validatedbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['subtile_results', 'bandpass', 'configset']),\n         Output('validationreport', lineage=['subtile_results', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Michael",
                            "2023-05-09T16:13:50.000+02:00",
                            "388c4544592fec3c6ea0637984e5d0c38617c455"
                        ],
                        [
                            "@@ -1,6 +1,10 @@\n from euclidwf.framework.taskdefs import Executable, TYPE_LISTFILE, MIME_JSON, MIME_TXT\n from euclidwf.framework.taskdefs import Input, Output, ComputingResources, OneByOne\n \n+ENV_VARIABLES = {\n+    \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n version = '3.2'\n \n EXT_PF1_PrepareValidation = Executable(\n@@ -16,7 +20,8 @@ EXT_PF1_PrepareValidation = Executable(\n         Output('fullyenclosedframes', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         Output('valbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -30,7 +35,8 @@ EXT_PF1_ValidateFormat = Executable(\n         Output('formatlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -46,7 +52,8 @@ EXT_PF1_ValidateAstrom = Executable(\n         Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -64,7 +71,8 @@ EXT_PF1_ValidatePhotom = Executable(\n         Output('photomlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -79,7 +87,8 @@ EXT_PF1_ValidatePsf = Executable(\n         Output('psflog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -94,7 +103,8 @@ EXT_PF1_ValidateBandpass = Executable(\n     outputs=[\n         Output('bandpasslog', mime_type=MIME_TXT),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0) # TBC\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -111,6 +121,7 @@ EXT_PF1_AggregateValidation = Executable(\n         Output('validatedbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['subtile_results', 'bandpass', 'configset']),\n         Output('validationreport', lineage=['subtile_results', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n",
                            "specify matplotlib config directory to solve issue #22039",
                            "Jelte de Jong",
                            "2023-05-09T13:35:15.000+02:00",
                            "adc59eb82bb8d4594317e3b0f031ba093684bc47"
                        ],
                        [
                            "@@ -43,6 +43,7 @@ EXT_PF1_ValidateAstrom = Executable(\n         ],\n     outputs=[\n         Output('astromlog', mime_type=MIME_TXT),\n+        Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n@@ -53,6 +54,7 @@ EXT_PF1_ValidatePhotom = Executable(\n     command='E-Run EXT_PF1_GEN %s EXT_PF1_ValidatePhotom' % version,\n     inputs=[\n         Input('subtile', content_type=TYPE_LISTFILE),\n+        Input('subtile_cat'),\n         Input('gaiacutout'),\n         Input('bandpass', content_type=TYPE_LISTFILE),\n         Input('configset', content_type=TYPE_LISTFILE),\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -43,6 +43,7 @@ EXT_PF1_ValidateAstrom = Executable(\n         ],\n     outputs=[\n         Output('astromlog', mime_type=MIME_TXT),\n+        Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n@@ -53,6 +54,7 @@ EXT_PF1_ValidatePhotom = Executable(\n     command='E-Run EXT_PF1_GEN %s EXT_PF1_ValidatePhotom' % version,\n     inputs=[\n         Input('subtile', content_type=TYPE_LISTFILE),\n+        Input('subtile_cat'),\n         Input('gaiacutout'),\n         Input('bandpass', content_type=TYPE_LISTFILE),\n         Input('configset', content_type=TYPE_LISTFILE),\n",
                            "add crossmatched catalogs as EXT_PF1_GEN internal files; set version to 0.81",
                            "Jelte de Jong",
                            "2023-04-06T17:02:51.000+02:00",
                            "d98cbb27bb4aec1b039bdc4bfb9975de00407420"
                        ],
                        [
                            "@@ -1,7 +1,7 @@\n from euclidwf.framework.taskdefs import Executable, TYPE_LISTFILE, MIME_JSON, MIME_TXT\n from euclidwf.framework.taskdefs import Input, Output, ComputingResources, OneByOne\n \n-version = '3.1'\n+version = '3.2'\n \n EXT_PF1_PrepareValidation = Executable(\n     command='E-Run EXT_PF1_GEN %s EXT_PF1_PrepareValidation' % version,\n",
                            "Set version to 0.79 and EXT_PF1_GEN version to 3.2",
                            "Jelte de Jong",
                            "2023-03-12T22:14:33.000+01:00",
                            "c96eb0c6fce3d86b53e8d5cb55e4779c282286b6"
                        ]
                    ],
                    "Makefile": [
                        [
                            "@@ -44,170 +44,19 @@\n #\n ################################################################################\n \n-# settings\n-CMAKE := cmake\n-CTEST := ctest\n-NINJA := $(shell which ninja-build 2> /dev/null)\n-ifeq ($(NINJA),)\n-  NINJA := $(shell which ninja 2> /dev/null)\n-endif\n-\n-# Looking for the Custom make library\n-\n-CUSTOM_MAKE_LIB := Custom.mk\n-\n-ifneq ($(wildcard $(CURDIR)/make/$(CUSTOM_MAKE_LIB)),)\n-  CUSTOM_MAKE_LIB_FILE := $(CURDIR)/make/$(CUSTOM_MAKE_LIB)\n-else\n-  ifneq ($(CMAKE_PREFIX_PATH),)\n-    PREFIX_LIST := $(subst :, ,$(CMAKE_PREFIX_PATH))\n-    CUSTOM_MAKE_LIB_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/share/Elements/make/$(CUSTOM_MAKE_LIB) $(dir)/../make/$(CUSTOM_MAKE_LIB)))\n-  endif\n-  CUSTOM_MAKE_LIB_LIST += /usr/share/Elements/make/$(CUSTOM_MAKE_LIB)\n-  CUSTOM_MAKE_LIB_FILE := $(firstword $(CUSTOM_MAKE_LIB_LIST))\n-endif\n-\n-# Looking for the ToolChain\n-\n-TOOLCHAIN_NAME := ElementsToolChain.cmake\n+ELEMENTS_MAKE_LIB := Elements.mk\n \n-ifneq ($(wildcard $(CURDIR)/cmake/$(TOOLCHAIN_NAME)),)\n-  TOOLCHAIN_FILE := $(CURDIR)/cmake/$(TOOLCHAIN_NAME)\n+ifneq ($(wildcard $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)),)\n+  ELEMENTS_MAKE_LIB_FILE := $(CURDIR)/make/$(ELEMENTS_MAKE_LIB)\n else\n   ifneq ($(CMAKE_PREFIX_PATH),)\n     PREFIX_LIST := $(subst :, ,$(CMAKE_PREFIX_PATH))\n-    TOOLCHAIN_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/lib*/cmake/ElementsProject/$(TOOLCHAIN_NAME) $(dir)/$(TOOLCHAIN_NAME)))\n-    TOOLCHAIN_FILE := $(firstword $(TOOLCHAIN_LIST))\n-  endif\n-endif\n-\n-override ALL_CMAKEFLAGS := -Wno-dev --no-warn-unused-cli\n-\n-ifneq ($(TOOLCHAIN_FILE),)\n-  # A toolchain has been found. Lets use it.\n-  override ALL_CMAKEFLAGS += -DCMAKE_TOOLCHAIN_FILE=$(TOOLCHAIN_FILE)\n-endif\n-\n-\n-BUILD_PREFIX_NAME := build\n-\n-override ALL_CMAKEFLAGS += -DUSE_LOCAL_INSTALLAREA=ON -DBUILD_PREFIX_NAME:STRING=$(BUILD_PREFIX_NAME)\n-override ALL_CMAKEFLAGS += -DUSE_VERSIONED_LIBRARIES=OFF\n-\n-ifndef BINARY_TAG\n-  ifdef CMAKECONFIG\n-    BINARY_TAG := ${CMAKECONFIG}\n-  else\n-    ifdef CMTCONFIG\n-      BINARY_TAG := ${CMTCONFIG}\n-    endif\n-  endif\n-endif\n-\n-ifdef BINARY_TAG\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME).$(BINARY_TAG)\n-else\n-  BUILD_SUBDIR := $(BUILD_PREFIX_NAME)\n-endif\n-BUILDDIR := $(CURDIR)/$(BUILD_SUBDIR)\n-\n-# build tool\n-\n-ifneq ($(USE_NINJA),)\n-  # enable Ninja\n-  override ALL_CMAKEFLAGS += -GNinja\n-  BUILD_CONF_FILE := build.ninja\n-  BUILDFLAGS := $(NINJAFLAGS)\n-  ifneq ($(VERBOSE),)\n-    BUILDFLAGS := -v $(BUILDFLAGS)\n+    ELEMENTS_MAKE_LIB_LIST := $(foreach dir,$(PREFIX_LIST),$(wildcard $(dir)/share/Elements/make/$(ELEMENTS_MAKE_LIB) $(dir)/../make/$(ELEMENTS_MAKE_LIB)))\n   endif\n-else\n-  BUILD_CONF_FILE := Makefile\n-endif\n-BUILD_CMD := $(CMAKE) --build $(BUILD_SUBDIR) --target\n-\n-\n-# Use environment variable for extra flags\n-\n-# Replace the \":\" from eclipse variable list to spaces  \n-ifneq ($(EXPAND_FLAGS),)\n-  CMAKEFLAGS := $(subst :-, -,$(CMAKEFLAGS))\n-endif\n-\n-ifneq ($(CMAKEFLAGS),)\n-  override ALL_CMAKEFLAGS += $(CMAKEFLAGS)\n+  ELEMENTS_MAKE_LIB_LIST += /usr/share/Elements/make/$(ELEMENTS_MAKE_LIB)\n+  ELEMENTS_MAKE_LIB_FILE := $(firstword $(ELEMENTS_MAKE_LIB_LIST))\n endif\n \n-# default target\n-all:\n-\n-# deep clean\n-purge:\n-\t$(RM) -r $(BUILDDIR) $(CURDIR)/InstallArea/$(BINARY_TAG)\n-\tfind $(CURDIR) \"(\" -name \"InstallArea\" -prune -o -name \"*.pyc\" -o -name \"*.pyo\" \")\" -a -type f -exec $(RM) -v \\{} \\;\n-\tfind $(CURDIR) -depth -type d -name \"__pycache__\" -exec $(RM) -rv \\{} \\;\n-\n-# Remove all the possible directories and the whole InstallArea as well\n-mrproper:\n-\t$(RM) -r $(CURDIR)/build $(CURDIR)/build.* $(CURDIR)/InstallArea\n-\tfind $(CURDIR) \"(\" -name \"*.pyc\" -o -name \"*.pyo\" \")\" -a -type f -exec $(RM) -v \\{} \\;\n-\tfind $(CURDIR) -depth -type d -name \"__pycache__\" -exec $(RM) -rv \\{} \\;\n-\n-# delegate any target to the build directory (except 'purge')\n-ifneq ($(MAKECMDGOALS),purge)\n-ifneq ($(MAKECMDGOALS),mrproper)\n-%: $(BUILDDIR)/$(BUILD_CONF_FILE) FORCE\n-\t+$(BUILD_CMD) $* -- $(BUILDFLAGS)\n-endif\n-endif\n-\n-# aliases\n-.PHONY: configure tests FORCE\n-ifneq ($(wildcard $(BUILDDIR)/$(BUILD_CONF_FILE)),)\n-configure: rebuild_cache\n-else\n-configure: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-endif\n-\t@ # do not delegate further\n-\n-\n-# This wrapping around the test target is used to ensure the generation of\n-# the XML output from ctest.\n-test: $(BUILDDIR)/$(BUILD_CONF_FILE)\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-\n-# This target ensures that the \"all\" target is called before\n-# running the tests (unlike the \"test\" default target of CMake)\n-tests: all\n-\t$(RM) -r $(BUILDDIR)/Testing $(BUILDDIR)/html\n-\t-cd $(BUILDDIR) && $(CTEST) -T test $(ARGS)\n-\t+$(BUILD_CMD) JUnitSummary\n-\n-ifeq ($(VERBOSE),)\n-# less verbose install\n-# (emulate the default CMake install target)\n-install: all\n-\tcd $(BUILDDIR) && $(CMAKE) -P cmake_install.cmake | grep -v \"^-- Up-to-date:\"\n-endif\n-\n-# import the library to look for a custom Makefile\n--include $(CUSTOM_MAKE_LIB_FILE)\n-\n-# ensure that the target are always passed to the CMake Makefile\n-FORCE: ;\n-\n-# Makefiles are used as implicit targets in make, but we should not consider\n-# them for delegation.\n-$(MAKEFILE_LIST): ;\n-\n-\n-# trigger CMake configuration\n-$(BUILDDIR)/$(BUILD_CONF_FILE): | $(BUILDDIR)\n-\tcd $(BUILDDIR) && $(CMAKE) $(ALL_CMAKEFLAGS) $(CURDIR)\n-\n-$(BUILDDIR):\n-\tmkdir -p $(BUILDDIR)\n+$(info Using the $(ELEMENTS_MAKE_LIB_FILE) make library)\n+include $(ELEMENTS_MAKE_LIB_FILE)\n \n",
                            "updated Makefile for elements 6.1.1",
                            "Michael",
                            "2023-05-12T11:21:45.000+02:00",
                            "a649a7a5de0892b98bb4b6d00a2f06c8aabd5cc5"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Debug_Tmp_Pipeline/PipelineDef.xml": [
                        [
                            "@@ -0,0 +1,98 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<tsk1:PipelineDef xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n+    xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n+    xmlns:tsk1=\"http://euclid.esa.org/schema/interfaces/sys/tsk\"\n+    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n+    <EdenVersion>Eden-2.1-dev</EdenVersion>\n+    <CommonDataModelVersion>8.0.5</CommonDataModelVersion>\n+    <Id>EXT_PF1_GEN_Coadd_v0.68</Id> <!-- ID processed by CODEEN Script with pattern from the Schema -->\n+    <PipelineVersion>0.2</PipelineVersion>\n+    <ArchiveProcessingFlag>PROCESSING_ONLY</ArchiveProcessingFlag> <!--This new flag is used by IAL/EAS to specify if this pipeline is a EAS pipeline (AF) or a PF pipeline (default is : PROCESSING_ONLY)   -->\n+    <PipelineContext>VALIDATION</PipelineContext>\n+    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-2.1/opt/euclid/EXT_IAL_Pipelines/0.69/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Coadd_Debug_Pipeline/PipelineScript.py</PipelineScriptPath>\n+    <InputDataSet>\n+        <KeyProductInputDataPlan>\n+            <InputPortName>tileinfo</InputPortName>\n+            <DataProductType>dpdMerTile</DataProductType>\n+            <InputQuerySpecPlan>(tileinfo.TileIndex=='UNKNOWN') \n+                            &amp; (tileinfo.Header.ManualValidationStatus != 'INVALID') \n+                            &amp; (tileinfo.Data.TileUseCase == 'UNKNOWN')</InputQuerySpecPlan>\n+            <InputQuerySpecPlan>tileinfo.Polygon INTERSECT(\"UNKNOWN\",\"UNKNOWN\") 'POLYGON(\"UNKNOWN\")'</InputQuerySpecPlan>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>1</Max>\n+            </Cardinality>\n+        </KeyProductInputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>single_epoch_images</InputPortName>\n+            <DataProductType>dpdExtSingleEpochFrame</DataProductType>\n+            <InputQuerySpecPlan>(single_epoch_images.Header.ManualValidationStatus != 'INVALID') &amp; (single_epoch_images.Data.Instrument.InstrumentName=='UNKNOWN')</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> <!-- fails if no DpdExtDesCalibratedFrame is returned  so no need to run from IAL -->\n+                <Min>1</Min>\n+                <Max>2147483647</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>mer_galaxy_cat</InputPortName>\n+            <DataProductType>dpdMerTrueUniverseGalaxyCatalog</DataProductType>\n+            <InputQuerySpecPlan>mer_galaxy_cat.Header.ManualValidationStatus != 'INVALID'</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> \n+                <Min>1</Min>\n+                <Max>100</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>mer_star_cat</InputPortName>\n+            <DataProductType>dpdMerTrueUniverseStarCatalog</DataProductType>\n+            <InputQuerySpecPlan>mer_star_cat.Header.ManualValidationStatus != 'INVALID'</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> \n+                <Min>1</Min>\n+                <Max>100</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <Dependencies>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.Polygon INTERSECT(0.01,100) single_epoch_images.Data.ImgSpatialFootprint.Polygon</Query><!--all calibrated frames must overlap the tile polygon -->\n+            </LinkedBy>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.Data.TileIndex == mer_star_cat.Data.TileIndex</Query><!--the MER catalog must overlap the tile polygon -->\n+            </LinkedBy>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.Data.TileIndex == mer_galaxy_cat.Data.TileIndex</Query><!--the MER catalog must overlap the tile polygon -->\n+            </LinkedBy>\n+        </Dependencies>\n+    </InputDataSet>\n+    <OutputDataSet>\n+        <OutputDataProduct>\n+            <OutputPortName>coadds</OutputPortName>\n+            <DataProductType>DpdExtStackedFrame</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+        <OutputDataProduct>\n+            <OutputPortName>validationreport</OutputPortName>\n+            <DataProductType>DpdExtValidationReport</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+        <OutputDataProduct>\n+            <OutputPortName>coadd_catalogs</OutputPortName>\n+            <DataProductType>DpdExtSourceCatalog</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+    </OutputDataSet> \n+</tsk1:PipelineDef>\n",
                            "defined coadd debug tmp pipeline with 200 G tmp dir",
                            "Michael",
                            "2023-05-09T16:10:21.000+02:00",
                            "2fa7ff3b0109d08ca32b97c90d528ccad1306d65"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Debug_Tmp_Pipeline/PipelineScript.py": [
                        [
                            "@@ -0,0 +1,24 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+from __future__ import absolute_import\n+from euclidwf.framework.workflow_dsl import pipeline\n+from PackageDef import coadd_stage2_pipeline\n+\n+@pipeline(outputs=('coadds', 'validationreport', 'coadd_catalogs'))\n+def coadd_stage2_single_step(single_epoch_images, mer_star_cat, mer_galaxy_cat, tileinfo, coadd_props_file):\n+    coadds, validationreport, coadd_catalogs = coadd_stage2_pipeline(single_epoch_images=single_epoch_images,\n+    mer_star_cat=mer_star_cat, mer_galaxy_cat=mer_galaxy_cat, tileinfo=tileinfo, coadd_props_file=coadd_props_file)\n+    return coadds, validationreport, coadd_catalogs\n",
                            "defined coadd debug tmp pipeline with 200 G tmp dir",
                            "Michael",
                            "2023-05-09T16:10:21.000+02:00",
                            "2fa7ff3b0109d08ca32b97c90d528ccad1306d65"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_Debug_Tmp_Pipeline/__init__.py": [
                        [
                            "@@ -0,0 +1,5 @@\n+\"\"\"\n+Package hierarchy support file.\n+\"\"\"\n+from pkgutil import extend_path\n+__path__ = extend_path(__path__, __name__)\n",
                            "defined coadd debug tmp pipeline with 200 G tmp dir",
                            "Michael",
                            "2023-05-09T16:10:21.000+02:00",
                            "2fa7ff3b0109d08ca32b97c90d528ccad1306d65"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_SUBARU_MULTIBAND_Pipeline/PipelineDef.xml": [
                        [
                            "@@ -0,0 +1,98 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<tsk1:PipelineDef xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n+    xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n+    xmlns:tsk1=\"http://euclid.esa.org/schema/interfaces/sys/tsk\"\n+    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n+    <EdenVersion>Eden-2.1-dev</EdenVersion>\n+    <CommonDataModelVersion>8.0.5</CommonDataModelVersion>\n+    <Id>EXT_PF1_GEN_Coadd_v0.68</Id> <!-- ID processed by CODEEN Script with pattern from the Schema -->\n+    <PipelineVersion>0.2</PipelineVersion>\n+    <ArchiveProcessingFlag>PROCESSING_ONLY</ArchiveProcessingFlag> <!--This new flag is used by IAL/EAS to specify if this pipeline is a EAS pipeline (AF) or a PF pipeline (default is : PROCESSING_ONLY)   -->\n+    <PipelineContext>VALIDATION</PipelineContext>\n+    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-2.1/opt/euclid/EXT_IAL_Pipelines/0.69/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Coadd_NOS_Pipeline/PipelineScript.py</PipelineScriptPath>\n+    <InputDataSet>\n+        <KeyProductInputDataPlan>\n+            <InputPortName>tileinfo</InputPortName>\n+            <DataProductType>dpdMerTile</DataProductType>\n+            <InputQuerySpecPlan>(tileinfo.Data.TileIndex==\"UNKNOWN\") \n+                            AND (tileinfo.Header.ManualValidationStatus.ManualValidationStatus != 'INVALID') \n+                            AND (tileinfo.Data.TileUseCase.MerTileUseCase == \"UNKNOWN\") </InputQuerySpecPlan>\n+            <InputQuerySpecPlan>tileinfo.OuterSpatialFoorprint.Polygon INTERSECT(\"UNKNOWN\",\"UNKNOWN\") 'POLYGON(\"UNKNOWN\")'</InputQuerySpecPlan>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>1</Max>\n+            </Cardinality>\n+        </KeyProductInputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>single_epoch_images</InputPortName>\n+            <DataProductType>dpdExtValidatedSingleEpochFrame</DataProductType>\n+            <InputQuerySpecPlan>(single_epoch_images.Header.ManualValidationStatusManualValidationStatus != 'INVALID') &amp; (single_epoch_images.Data.Instrument.InstrumentName==\"UNKNOWN\")</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> <!-- fails if no DpdExtDesCalibratedFrame is returned  so no need to run from IAL -->\n+                <Min>1</Min>\n+                <Max>2147483647</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>mer_galaxy_cat</InputPortName>\n+            <DataProductType>dpdMerTrueUniverseGalaxyCatalog</DataProductType>\n+            <InputQuerySpecPlan>mer_galaxy_cat.Header.ManualValidationStatus != 'INVALID'</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> \n+                <Min>1</Min>\n+                <Max>100</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>mer_star_cat</InputPortName>\n+            <DataProductType>dpdMerTrueUniverseStarCatalog</DataProductType>\n+            <InputQuerySpecPlan>mer_star_cat.Header.ManualValidationStatus != 'INVALID'</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> \n+                <Min>1</Min>\n+                <Max>100</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <Dependencies>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.OuterSpatialFoorprint.Polygon INTERSECT(0.01,100) single_epoch_images.Data.ImgSpatialFootprint.Polygon</Query><!--all calibrated frames must overlap the tile polygon -->\n+            </LinkedBy>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.Data.TileIndex == mer_star_cat.Data.TileIndex</Query><!--the MER catalog must overlap the tile polygon -->\n+            </LinkedBy>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.Data.TileIndex == mer_galaxy_cat.Data.TileIndex</Query><!--the MER catalog must overlap the tile polygon -->\n+            </LinkedBy>\n+        </Dependencies>\n+    </InputDataSet>\n+    <OutputDataSet>\n+        <OutputDataProduct>\n+            <OutputPortName>coadds</OutputPortName>\n+            <DataProductType>DpdExtStackedFrame</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+        <OutputDataProduct>\n+            <OutputPortName>validationreport</OutputPortName>\n+            <DataProductType>DpdExtValidationReport</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+        <OutputDataProduct>\n+            <OutputPortName>coadd_catalogs</OutputPortName>\n+            <DataProductType>DpdExtSourceCatalog</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+    </OutputDataSet> \n+</tsk1:PipelineDef>\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_SUBARU_MULTIBAND_Pipeline/PipelineDef.xml~": [
                        [
                            "@@ -0,0 +1,98 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<tsk1:PipelineDef xmlns:sys=\"http://euclid.esa.org/schema/sys\"\n+    xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\"\n+    xmlns:tsk1=\"http://euclid.esa.org/schema/interfaces/sys/tsk\"\n+    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n+    <EdenVersion>Eden-2.1-dev</EdenVersion>\n+    <CommonDataModelVersion>8.0.5</CommonDataModelVersion>\n+    <Id>EXT_PF1_GEN_Coadd_v0.64</Id> <!-- ID processed by CODEEN Script with pattern from the Schema -->\n+    <PipelineVersion>0.2</PipelineVersion>\n+    <ArchiveProcessingFlag>PROCESSING_ONLY</ArchiveProcessingFlag> <!--This new flag is used by IAL/EAS to specify if this pipeline is a EAS pipeline (AF) or a PF pipeline (default is : PROCESSING_ONLY)   -->\n+    <PipelineContext>VALIDATION</PipelineContext>\n+    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-2.1/opt/euclid/EXT_IAL_Pipelines/2.65/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Coadd_NOS_Pipeline/PipelineScript.py</PipelineScriptPath>\n+    <InputDataSet>\n+        <KeyProductInputDataPlan>\n+            <InputPortName>tileinfo</InputPortName>\n+            <DataProductType>dpdMerTile</DataProductType>\n+            <InputQuerySpecPlan>(tileinfo.Data.TileIndex==\"UNKNOWN\") \n+                            AND (tileinfo.Header.ManualValidationStatus.ManualValidationStatus != 'INVALID') \n+                            AND (tileinfo.Data.TileUseCase.MerTileUseCase == \"UNKNOWN\") </InputQuerySpecPlan>\n+            <InputQuerySpecPlan>tileinfo.OuterSpatialFoorprint.Polygon INTERSECT(\"UNKNOWN\",\"UNKNOWN\") 'POLYGON(\"UNKNOWN\")'</InputQuerySpecPlan>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>1</Max>\n+            </Cardinality>\n+        </KeyProductInputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>single_epoch_images</InputPortName>\n+            <DataProductType>dpdExtValidatedSingleEpochFrame</DataProductType>\n+            <InputQuerySpecPlan>(single_epoch_images.Header.ManualValidationStatusManualValidationStatus != 'INVALID') &amp; (single_epoch_images.Data.Instrument.InstrumentName==\"UNKNOWN\")</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> <!-- fails if no DpdExtDesCalibratedFrame is returned  so no need to run from IAL -->\n+                <Min>1</Min>\n+                <Max>2147483647</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>mer_galaxy_cat</InputPortName>\n+            <DataProductType>dpdMerTrueUniverseGalaxyCatalog</DataProductType>\n+            <InputQuerySpecPlan>mer_galaxy_cat.Header.ManualValidationStatus != 'INVALID'</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> \n+                <Min>1</Min>\n+                <Max>100</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <InputDataPlan>\n+            <InputPortName>mer_star_cat</InputPortName>\n+            <DataProductType>dpdMerTrueUniverseStarCatalog</DataProductType>\n+            <InputQuerySpecPlan>mer_star_cat.Header.ManualValidationStatus != 'INVALID'</InputQuerySpecPlan> <!-- COORS UI to select Date Beg and date End for selection -->\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality> \n+                <Min>1</Min>\n+                <Max>100</Max> \n+            </Cardinality>\n+        </InputDataPlan>\n+        <Dependencies>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.OuterSpatialFoorprint.Polygon INTERSECT(0.01,100) single_epoch_images.Data.ImgSpatialFootprint.Polygon</Query><!--all calibrated frames must overlap the tile polygon -->\n+            </LinkedBy>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.Data.TileIndex == mer_star_cat.Data.TileIndex</Query><!--the MER catalog must overlap the tile polygon -->\n+            </LinkedBy>\n+            <LinkedBy refPort=\"tileinfo\">\n+                <Query>tileinfo.Data.TileIndex == mer_galaxy_cat.Data.TileIndex</Query><!--the MER catalog must overlap the tile polygon -->\n+            </LinkedBy>\n+        </Dependencies>\n+    </InputDataSet>\n+    <OutputDataSet>\n+        <OutputDataProduct>\n+            <OutputPortName>coadds</OutputPortName>\n+            <DataProductType>dpdExtStackedFrame</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+        <OutputDataProduct>\n+            <OutputPortName>validationreport</OutputPortName>\n+            <DataProductType>dpdExtValidationReport</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+        <OutputDataProduct>\n+            <OutputPortName>coadd_catalogs</OutputPortName>\n+            <DataProductType>dpdExtSourceCatalog</DataProductType>\n+            <Cardinality>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n+                <Max>100</Max>\n+            </Cardinality>\n+        </OutputDataProduct>\n+    </OutputDataSet> \n+</tsk1:PipelineDef>\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_SUBARU_MULTIBAND_Pipeline/PipelineScript.py": [
                        [
                            "@@ -0,0 +1,24 @@\n+\"\"\"\n+Copyright (C) 2012-2020 Euclid Science Ground Segment\n+\n+This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\n+Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\n+any later version.\n+\n+This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\n+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+details.\n+\n+You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\n+the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+\n+\"\"\"\n+from __future__ import absolute_import\n+from euclidwf.framework.workflow_dsl import pipeline\n+from PackageDef import coadd_stage2_pipeline\n+\n+@pipeline(outputs=('coadds', 'validationreport', 'coadd_catalogs'))\n+def coadd_stage2_single_step(single_epoch_images, mer_star_cat, mer_galaxy_cat, tileinfo, coadd_props_file):\n+    coadds, validationreport, coadd_catalogs = coadd_stage2_pipeline(single_epoch_images=single_epoch_images,\n+    mer_star_cat=mer_star_cat, mer_galaxy_cat=mer_galaxy_cat, tileinfo=tileinfo, coadd_props_file=coadd_props_file)\n+    return coadds, validationreport, coadd_catalogs\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Coadd_SUBARU_MULTIBAND_Pipeline/__init__.py": [
                        [
                            "@@ -0,0 +1,5 @@\n+\"\"\"\n+Package hierarchy support file.\n+\"\"\"\n+from pkgutil import extend_path\n+__path__ = extend_path(__path__, __name__)\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipDef_EXT_PF1_GEN_Validate.xml": [
                        [
                            "@@ -1,12 +1,12 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <tsk1:PipelineDef xmlns:sys=\"http://euclid.esa.org/schema/sys\" xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\" xmlns:tsk1=\"http://euclid.esa.org/schema/interfaces/sys/tsk\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://euclid.esa.org/schema/interfaces/sys/tsk ../../../../../../ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/interfaces/sys/tsk/euc-test-tsk-interface.xsd\">\n     <EdenVersion>Eden-3.0-dev</EdenVersion>\n-    <CommonDataModelVersion>9.0.3</CommonDataModelVersion>\n+    <CommonDataModelVersion>9.1.5</CommonDataModelVersion>\n     <Id>PipDef_EXT_PF1_GEN_Validate_v3.2</Id>\n     <PipelineVersion>3.2</PipelineVersion>\n     <ArchiveProcessingFlag>PROCESSING_ONLY</ArchiveProcessingFlag>\n     <PipelineContext>TEST</PipelineContext>\n-    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-3.0/opt/euclid/EXT_IAL_Pipelines/0.79/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipScr_EXT_PF1_GEN_Validate.py</PipelineScriptPath>\n+    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-3.0/opt/euclid/EXT_IAL_Pipelines/0.81/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipScr_EXT_PF1_GEN_Validate.py</PipelineScriptPath>\n     <InputDataSet>\n         <KeyProductInputDataPlan>\n             <InputPortName>supertile</InputPortName>\n@@ -69,11 +69,17 @@\n         <InputDataPlan>\n             <InputPortName>configset</InputPortName>\n             <DataProductType>DpdExtConfigurationSet</DataProductType>\n-            <InputQuerySpecPlan>(configset.Data.Comment == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n+            <InputQuerySpecPlan>(configset.Data.Pipeline == \"EXT_PF1_GEN\") AND\n+(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n+            </InputQuerySpecPlan>\n+            <InputQuerySpecPlan>(configset.Data.Pipeline == \"EXT_PF1_GEN\") AND\n+(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND\n+(configset.Data.Comment == \"UNKNOWN\") AND\n+(configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n             </InputQuerySpecPlan>\n             <Cardinality>\n-                <Optionality>OPTIONAL</Optionality>\n-                <Min>0</Min>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n                 <Max>1</Max>\n             </Cardinality>\n         </InputDataPlan>\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -69,9 +69,11 @@\n         <InputDataPlan>\n             <InputPortName>configset</InputPortName>\n             <DataProductType>DpdExtConfigurationSet</DataProductType>\n-            <InputQuerySpecPlan>(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n+            <InputQuerySpecPlan>(configset.Data.Pipeline == \"EXT_PF1_GEN\") AND\n+(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n             </InputQuerySpecPlan>\n-            <InputQuerySpecPlan>(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND\n+            <InputQuerySpecPlan>(configset.Data.Pipeline == \"EXT_PF1_GEN\") AND\n+(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND\n (configset.Data.Comment == \"UNKNOWN\") AND\n (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n             </InputQuerySpecPlan>\n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Michael",
                            "2023-04-26T17:18:55.000+02:00",
                            "a9f39f47172282eca275dcd88f37cec7dd7131cf"
                        ],
                        [
                            "@@ -69,9 +69,11 @@\n         <InputDataPlan>\n             <InputPortName>configset</InputPortName>\n             <DataProductType>DpdExtConfigurationSet</DataProductType>\n-            <InputQuerySpecPlan>(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n+            <InputQuerySpecPlan>(configset.Data.Pipeline == \"EXT_PF1_GEN\") AND\n+(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n             </InputQuerySpecPlan>\n-            <InputQuerySpecPlan>(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND\n+            <InputQuerySpecPlan>(configset.Data.Pipeline == \"EXT_PF1_GEN\") AND\n+(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND\n (configset.Data.Comment == \"UNKNOWN\") AND\n (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n             </InputQuerySpecPlan>\n",
                            "use Pipeline attribute to select DpdExtConfigurationSet",
                            "Jelte de Jong",
                            "2023-04-21T11:37:44.000+02:00",
                            "674d561dbf5db8815a09bc6311d673b951823997"
                        ],
                        [
                            "@@ -1,12 +1,12 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <tsk1:PipelineDef xmlns:sys=\"http://euclid.esa.org/schema/sys\" xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\" xmlns:tsk1=\"http://euclid.esa.org/schema/interfaces/sys/tsk\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://euclid.esa.org/schema/interfaces/sys/tsk ../../../../../../ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/interfaces/sys/tsk/euc-test-tsk-interface.xsd\">\n     <EdenVersion>Eden-3.0-dev</EdenVersion>\n-    <CommonDataModelVersion>9.0.3</CommonDataModelVersion>\n+    <CommonDataModelVersion>9.1.5</CommonDataModelVersion>\n     <Id>PipDef_EXT_PF1_GEN_Validate_v3.2</Id>\n     <PipelineVersion>3.2</PipelineVersion>\n     <ArchiveProcessingFlag>PROCESSING_ONLY</ArchiveProcessingFlag>\n     <PipelineContext>TEST</PipelineContext>\n-    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-3.0/opt/euclid/EXT_IAL_Pipelines/0.79/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipScr_EXT_PF1_GEN_Validate.py</PipelineScriptPath>\n+    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-3.0/opt/euclid/EXT_IAL_Pipelines/0.81/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipScr_EXT_PF1_GEN_Validate.py</PipelineScriptPath>\n     <InputDataSet>\n         <KeyProductInputDataPlan>\n             <InputPortName>supertile</InputPortName>\n@@ -69,11 +69,15 @@\n         <InputDataPlan>\n             <InputPortName>configset</InputPortName>\n             <DataProductType>DpdExtConfigurationSet</DataProductType>\n-            <InputQuerySpecPlan>(configset.Data.Comment == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n+            <InputQuerySpecPlan>(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND (configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n+            </InputQuerySpecPlan>\n+            <InputQuerySpecPlan>(configset.Data.Survey == \"UNKNOWN\") AND (configset.Data.DataType == \"UNKNOWN\") AND\n+(configset.Data.Comment == \"UNKNOWN\") AND\n+(configset.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")\n             </InputQuerySpecPlan>\n             <Cardinality>\n-                <Optionality>OPTIONAL</Optionality>\n-                <Min>0</Min>\n+                <Optionality>MANDATORY</Optionality>\n+                <Min>1</Min>\n                 <Max>1</Max>\n             </Cardinality>\n         </InputDataPlan>\n",
                            "update EXT_PF1_GEN pipdef",
                            "Jelte de Jong",
                            "2023-04-18T17:11:38.000+02:00",
                            "583448600f994d8620d647e7085c723bee903483"
                        ],
                        [
                            "@@ -1,12 +1,12 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <tsk1:PipelineDef xmlns:sys=\"http://euclid.esa.org/schema/sys\" xmlns:dss=\"http://euclid.esa.org/schema/sys/dss\" xmlns:tsk1=\"http://euclid.esa.org/schema/interfaces/sys/tsk\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://euclid.esa.org/schema/interfaces/sys/tsk ../../../../../../ST_DataModel/ST_DM_Schema/auxdir/ST_DM_Schema/interfaces/sys/tsk/euc-test-tsk-interface.xsd\">\n     <EdenVersion>Eden-3.0-dev</EdenVersion>\n-    <CommonDataModelVersion>9.0.2</CommonDataModelVersion>\n-    <Id>PipDef_EXT_PF1_GEN_Validate_v3.1</Id>\n-    <PipelineVersion>3.1</PipelineVersion>\n+    <CommonDataModelVersion>9.0.3</CommonDataModelVersion>\n+    <Id>PipDef_EXT_PF1_GEN_Validate_v3.2</Id>\n+    <PipelineVersion>3.2</PipelineVersion>\n     <ArchiveProcessingFlag>PROCESSING_ONLY</ArchiveProcessingFlag>\n-    <PipelineContext>PipelinePostSC8</PipelineContext>\n-    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-3.0/opt/euclid/EXT_IAL_Pipelines/0.75/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipScr_EXT_PF1_GEN_Validate.py</PipelineScriptPath>\n+    <PipelineContext>TEST</PipelineContext>\n+    <PipelineScriptPath>/cvmfs/euclid-dev.in2p3.fr/CentOS7/EDEN-3.0/opt/euclid/EXT_IAL_Pipelines/0.79/InstallArea/x86_64-conda_cos6-gcc73-o2g/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipScr_EXT_PF1_GEN_Validate.py</PipelineScriptPath>\n     <InputDataSet>\n         <KeyProductInputDataPlan>\n             <InputPortName>supertile</InputPortName>\n",
                            "Set version to 0.79 and EXT_PF1_GEN version to 3.2",
                            "Jelte de Jong",
                            "2023-03-12T22:14:33.000+01:00",
                            "c96eb0c6fce3d86b53e8d5cb55e4779c282286b6"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Validate_Pipeline/PipScr_EXT_PF1_GEN_Validate.py": [
                        [
                            "@@ -20,7 +20,7 @@ def PE_EXT_ValidateParallel(subtile, bandpass, gaiacutout, configset): #, gaiaph\n         )\n     \n     # Validate astrometry\n-    astromlog, subtile_astrom = EXT_PF1_ValidateAstrom(\n+    astromlog, subtile_cat, subtile_astrom = EXT_PF1_ValidateAstrom(\n         subtile = subtile_format,\n         gaiacutout = gaiacutout,\n         configset = configset,\n@@ -29,6 +29,7 @@ def PE_EXT_ValidateParallel(subtile, bandpass, gaiacutout, configset): #, gaiaph\n     # Validate photometry\n     photomlog, subtile_photom = EXT_PF1_ValidatePhotom(\n         subtile = subtile_astrom,\n+        subtile_cat = subtile_cat,\n         gaiacutout = gaiacutout,\n         bandpass = bandpass,\n         configset = configset,\n@@ -51,7 +52,7 @@ def PE_EXT_ValidateParallel(subtile, bandpass, gaiacutout, configset): #, gaiaph\n         )\n \n     # Return validation logs and matched catalog        \n-    return formatlog, astromlog, photomlog, psflog, bandpasslog\n+    return formatlog, astromlog, photomlog, psflog, bandpasslog, subtile_cat\n \n @pipeline(outputs=('validatedsingleepochframes', 'validatedbandpass', 'validationreport'))\n def PE_EXT_Validate(supertile,\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ]
                    ]
                },
                "selected_modifications": {
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_PostValidate_Pipeline/PkgDef_EXT_PostValidateStackListFull.py": [
                        [
                            "@@ -30,7 +30,7 @@ ENV_VARIABLES = {\n }\n \n EXT_AstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -39,7 +39,7 @@ EXT_AstromPhotomValidation = Executable(\n     env_variables=ENV_VARIABLES)\n \n EXT_GaiaAstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_GaiaAstromPhotomValidation\",\n     inputs=[Input(\"stack_GaiaAstromPhotom\"),\n             Input(\"gaia_catalog\"),\n             Input(\"configuration_set\"),\n@@ -50,7 +50,7 @@ EXT_GaiaAstromPhotomValidation = Executable(\n \n \n EXT_PixelStatsValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PixelStatsValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_PixelStatsValidation\",\n     inputs=[Input(\"stack_PixelStats\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -59,7 +59,7 @@ EXT_PixelStatsValidation = Executable(\n     env_variables=ENV_VARIABLES)\n \n EXT_FwhmValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_FwhmValidation\",\n     inputs=[Input(\"stack_Fwhm\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\"),\n@@ -69,7 +69,7 @@ EXT_FwhmValidation = Executable(\n     env_variables=ENV_VARIABLES)\n \n EXT_PsfValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_PsfValidation\",\n     inputs=[Input(\"stack_Psf\"),\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n@@ -79,7 +79,7 @@ EXT_PsfValidation = Executable(\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.7 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.0),\n",
                            "postvalidation pipeline version updated to 1.7",
                            "Frederic Raison",
                            "2023-06-28T14:24:40.000+02:00",
                            "7be95cb14083fe8aeba6386c24f635ba625c4080"
                        ],
                        [
                            "@@ -25,13 +25,18 @@ from euclidwf.framework.taskdefs import CreateListFile\n from euclidwf.framework.taskdefs import (Executable, Input, Output,\n                                          ComputingResources)\n \n+ENV_VARIABLES = {\n+        \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n EXT_AstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"AstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_GaiaAstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n@@ -40,7 +45,8 @@ EXT_GaiaAstromPhotomValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"GaiaAstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_PixelStatsValidation = Executable(\n@@ -49,7 +55,8 @@ EXT_PixelStatsValidation = Executable(\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"PixelStats_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_FwhmValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n@@ -58,7 +65,8 @@ EXT_FwhmValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"Fwhm_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_PsfValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n@@ -66,14 +74,16 @@ EXT_PsfValidation = Executable(\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"Psf_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n \n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Michael",
                            "2023-05-09T16:13:50.000+02:00",
                            "388c4544592fec3c6ea0637984e5d0c38617c455"
                        ],
                        [
                            "@@ -25,13 +25,18 @@ from euclidwf.framework.taskdefs import CreateListFile\n from euclidwf.framework.taskdefs import (Executable, Input, Output,\n                                          ComputingResources)\n \n+ENV_VARIABLES = {\n+        \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n EXT_AstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"AstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_GaiaAstromPhotomValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n@@ -40,7 +45,8 @@ EXT_GaiaAstromPhotomValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"GaiaAstromPhotom_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_PixelStatsValidation = Executable(\n@@ -49,7 +55,8 @@ EXT_PixelStatsValidation = Executable(\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"PixelStats_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=14.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_FwhmValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n@@ -58,7 +65,8 @@ EXT_FwhmValidation = Executable(\n             Input(\"configuration_set\"),\n             Input(\"configuration_set2\")],\n     outputs=[Output(\"Fwhm_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n EXT_PsfValidation = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n@@ -66,14 +74,16 @@ EXT_PsfValidation = Executable(\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n     outputs=[Output(\"Psf_analysis_result_out\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n     command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES)\n \n \n \n",
                            "added ENV_VARIABLES to solve  python cache issue #22039",
                            "Frederic Raison",
                            "2023-05-08T18:29:22.000+02:00",
                            "b904d4e185b495685a1f418dee57ae550d57eb79"
                        ],
                        [
                            "@@ -26,7 +26,7 @@ from euclidwf.framework.taskdefs import (Executable, Input, Output,\n                                          ComputingResources)\n \n EXT_AstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_AstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_AstromPhotomValidation\",\n     inputs=[Input(\"stack_AstromPhotom\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -34,7 +34,7 @@ EXT_AstromPhotomValidation = Executable(\n     resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n \n EXT_GaiaAstromPhotomValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_GaiaAstromPhotomValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_GaiaAstromPhotomValidation\",\n     inputs=[Input(\"stack_GaiaAstromPhotom\"),\n             Input(\"gaia_catalog\"),\n             Input(\"configuration_set\"),\n@@ -44,7 +44,7 @@ EXT_GaiaAstromPhotomValidation = Executable(\n \n \n EXT_PixelStatsValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_PixelStatsValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PixelStatsValidation\",\n     inputs=[Input(\"stack_PixelStats\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\")],\n@@ -52,7 +52,7 @@ EXT_PixelStatsValidation = Executable(\n     resources=ComputingResources(cores=1, ram=14.0, walltime=1.0))\n \n EXT_FwhmValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_FwhmValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_FwhmValidation\",\n     inputs=[Input(\"stack_Fwhm\"),\n             Input(\"reference_catalog\"),\n             Input(\"configuration_set\"),\n@@ -61,7 +61,7 @@ EXT_FwhmValidation = Executable(\n     resources=ComputingResources(cores=1, ram=8.0, walltime=1.0))\n \n EXT_PsfValidation = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_PsfValidation\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_PsfValidation\",\n     inputs=[Input(\"stack_Psf\"),\n             Input(\"coadd_catalog\"),\n             Input(\"configuration_set\")],\n@@ -70,7 +70,7 @@ EXT_PsfValidation = Executable(\n \n \n EXT_MergeAnalysisResultsTask = Executable(\n-    command=\"E-Run EXT_PF1_GEN_PostValidation 1.5 EXT_MergeAnalysisResultsTask\",\n+    command=\"E-Run EXT_PF1_GEN_PostValidation 1.6 EXT_MergeAnalysisResultsTask\",\n     inputs=[Input(\"analysis_results\", content_type=\"listfile\")],\n     outputs=[Output(\"merged_analysis_result\", mime_type=\"xml\")],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n",
                            "call version 1.6 of Postvalidation",
                            "Frederic Raison",
                            "2023-03-28T16:52:29.000+02:00",
                            "1865e7b3456e6cd69b1cadd38923e30b808f9c15"
                        ]
                    ],
                    "EXT_PF1_GEN_Pipelines/auxdir/EXT_PF1_GEN_Validate_Pipeline/PkgDef_EXT_PF1_GEN_Validate.py": [
                        [
                            "@@ -20,7 +20,7 @@ EXT_PF1_PrepareValidation = Executable(\n         Output('fullyenclosedframes', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         Output('valbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    resources=ComputingResources(cores=1, ram=3.0, walltime=1.0),\n     env_variables=ENV_VARIABLES\n     )\n \n@@ -121,7 +121,7 @@ EXT_PF1_AggregateValidation = Executable(\n         Output('validatedbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['subtile_results', 'bandpass', 'configset']),\n         Output('validationreport', lineage=['subtile_results', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    resources=ComputingResources(cores=1, ram=32.0, walltime=1.0),\n     env_variables=ENV_VARIABLES\n     )\n \n",
                            "more RAM and increase version to 0.82",
                            "Jelte de Jong",
                            "2023-05-30T23:10:33.000+02:00",
                            "785aa0c04bfc582020a2245149152ea0a02773c5"
                        ],
                        [
                            "@@ -1,6 +1,10 @@\n from euclidwf.framework.taskdefs import Executable, TYPE_LISTFILE, MIME_JSON, MIME_TXT\n from euclidwf.framework.taskdefs import Input, Output, ComputingResources, OneByOne\n \n+ENV_VARIABLES = {\n+    \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n version = '3.2'\n \n EXT_PF1_PrepareValidation = Executable(\n@@ -16,7 +20,8 @@ EXT_PF1_PrepareValidation = Executable(\n         Output('fullyenclosedframes', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         Output('valbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -30,7 +35,8 @@ EXT_PF1_ValidateFormat = Executable(\n         Output('formatlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -46,7 +52,8 @@ EXT_PF1_ValidateAstrom = Executable(\n         Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -64,7 +71,8 @@ EXT_PF1_ValidatePhotom = Executable(\n         Output('photomlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -79,7 +87,8 @@ EXT_PF1_ValidatePsf = Executable(\n         Output('psflog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -94,7 +103,8 @@ EXT_PF1_ValidateBandpass = Executable(\n     outputs=[\n         Output('bandpasslog', mime_type=MIME_TXT),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0) # TBC\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -111,6 +121,7 @@ EXT_PF1_AggregateValidation = Executable(\n         Output('validatedbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['subtile_results', 'bandpass', 'configset']),\n         Output('validationreport', lineage=['subtile_results', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n",
                            "Merge branch 'develop' of gitlab.euclid-sgs.uk:PF-EXT/EXT_IAL_Pipelines into develop",
                            "Michael",
                            "2023-05-09T16:13:50.000+02:00",
                            "388c4544592fec3c6ea0637984e5d0c38617c455"
                        ],
                        [
                            "@@ -1,6 +1,10 @@\n from euclidwf.framework.taskdefs import Executable, TYPE_LISTFILE, MIME_JSON, MIME_TXT\n from euclidwf.framework.taskdefs import Input, Output, ComputingResources, OneByOne\n \n+ENV_VARIABLES = {\n+    \"MPLCONFIGDIR\": \"/dev/null\"\n+}\n+\n version = '3.2'\n \n EXT_PF1_PrepareValidation = Executable(\n@@ -16,7 +20,8 @@ EXT_PF1_PrepareValidation = Executable(\n         Output('fullyenclosedframes', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         Output('valbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['supertile', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -30,7 +35,8 @@ EXT_PF1_ValidateFormat = Executable(\n         Output('formatlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -46,7 +52,8 @@ EXT_PF1_ValidateAstrom = Executable(\n         Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -64,7 +71,8 @@ EXT_PF1_ValidatePhotom = Executable(\n         Output('photomlog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=1.5),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -79,7 +87,8 @@ EXT_PF1_ValidatePsf = Executable(\n         Output('psflog', mime_type=MIME_TXT),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n-    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0)\n+    resources=ComputingResources(cores=1, ram=4.0, walltime=6.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -94,7 +103,8 @@ EXT_PF1_ValidateBandpass = Executable(\n     outputs=[\n         Output('bandpasslog', mime_type=MIME_TXT),\n         ],\n-    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0) # TBC\n+    resources=ComputingResources(cores=1, ram=2.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n \n@@ -111,6 +121,7 @@ EXT_PF1_AggregateValidation = Executable(\n         Output('validatedbandpass', mime_type=MIME_JSON, content_type=TYPE_LISTFILE, lineage=['subtile_results', 'bandpass', 'configset']),\n         Output('validationreport', lineage=['subtile_results', 'bandpass', 'configset']),\n         ],\n-    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0)\n+    resources=ComputingResources(cores=1, ram=8.0, walltime=1.0),\n+    env_variables=ENV_VARIABLES\n     )\n \n",
                            "specify matplotlib config directory to solve issue #22039",
                            "Jelte de Jong",
                            "2023-05-09T13:35:15.000+02:00",
                            "adc59eb82bb8d4594317e3b0f031ba093684bc47"
                        ],
                        [
                            "@@ -43,6 +43,7 @@ EXT_PF1_ValidateAstrom = Executable(\n         ],\n     outputs=[\n         Output('astromlog', mime_type=MIME_TXT),\n+        Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n@@ -53,6 +54,7 @@ EXT_PF1_ValidatePhotom = Executable(\n     command='E-Run EXT_PF1_GEN %s EXT_PF1_ValidatePhotom' % version,\n     inputs=[\n         Input('subtile', content_type=TYPE_LISTFILE),\n+        Input('subtile_cat'),\n         Input('gaiacutout'),\n         Input('bandpass', content_type=TYPE_LISTFILE),\n         Input('configset', content_type=TYPE_LISTFILE),\n",
                            "Merge branch 'develop' of https://gitlab.euclid-sgs.uk/PF-EXT/EXT_IAL_Pipelines into develop",
                            "Frederic Raison",
                            "2023-05-08T18:29:54.000+02:00",
                            "2ade2385ad64dd1d1423c0ee4d5089efd5014a0e"
                        ],
                        [
                            "@@ -43,6 +43,7 @@ EXT_PF1_ValidateAstrom = Executable(\n         ],\n     outputs=[\n         Output('astromlog', mime_type=MIME_TXT),\n+        Output('subtile_cat', mime_type=\"fits\"),\n         Output('subtile_out', content_type=TYPE_LISTFILE), \n         ],\n     resources=ComputingResources(cores=1, ram=4.0, walltime=1.5)\n@@ -53,6 +54,7 @@ EXT_PF1_ValidatePhotom = Executable(\n     command='E-Run EXT_PF1_GEN %s EXT_PF1_ValidatePhotom' % version,\n     inputs=[\n         Input('subtile', content_type=TYPE_LISTFILE),\n+        Input('subtile_cat'),\n         Input('gaiacutout'),\n         Input('bandpass', content_type=TYPE_LISTFILE),\n         Input('configset', content_type=TYPE_LISTFILE),\n",
                            "add crossmatched catalogs as EXT_PF1_GEN internal files; set version to 0.81",
                            "Jelte de Jong",
                            "2023-04-06T17:02:51.000+02:00",
                            "d98cbb27bb4aec1b039bdc4bfb9975de00407420"
                        ],
                        [
                            "@@ -1,7 +1,7 @@\n from euclidwf.framework.taskdefs import Executable, TYPE_LISTFILE, MIME_JSON, MIME_TXT\n from euclidwf.framework.taskdefs import Input, Output, ComputingResources, OneByOne\n \n-version = '3.1'\n+version = '3.2'\n \n EXT_PF1_PrepareValidation = Executable(\n     command='E-Run EXT_PF1_GEN %s EXT_PF1_PrepareValidation' % version,\n",
                            "Set version to 0.79 and EXT_PF1_GEN version to 3.2",
                            "Jelte de Jong",
                            "2023-03-12T22:14:33.000+01:00",
                            "c96eb0c6fce3d86b53e8d5cb55e4779c282286b6"
                        ]
                    ]
                },
                "count_selected_modifications": "2",
                "tags_in_period": [
                    {
                        "name": "0.78.0",
                        "created_at": "2023-03-12T22:10:27.000+01:00",
                        "author_name": "Jelte de Jong"
                    },
                    {
                        "name": "0.9.0",
                        "created_at": "2023-07-04T07:53:54.000+02:00",
                        "author_name": "Michael"
                    }
                ]
            },
            "PF-EXT/EXT_DES_TestTools": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_DES_Validation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_LSST_Testing": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_DES": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_DES_Integration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_KiDS": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-EXT/EXT_PF1_GEN": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            }
        }
    }
}