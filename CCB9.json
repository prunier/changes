{
    "LE2": {
        "PF-VIS": {
            "PF-VIS/VIS_DiffractionSpikesFocus": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/vis_instrument_tools": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_DQReport": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "OU-VIS/VIS_PTC": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_ImageLib": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/vis_autoarray": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Common": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_CTI_Pipeline": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_AutoFit": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_LargeFlatCalibration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/vis_illumcalibration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/vis_darkcorrection": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/vis_darkcalibration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_IAL_Pipelines": {
                "start date": "2023-08-22T16:21:35",
                "end date": "2023-08-29T19:48:30",
                "start tag": "13.0.14",
                "end tag": "13.0.16",
                "count_files_modified": "8",
                "modifications_by_file": {
                    "VIS_IAL_Pipelines/auxdir/VIS_IAL_Pipelines/VIS_CTICalibration_Pipeline/PkgDef_VIS_CTICalibration.py": [
                        [
                            "@@ -58,7 +58,7 @@ calibrate_cti = Executable(\n         Output(\"cti_parallel_output\", mime_type=\"json\"),\n         Output(\"cti_serial_output\", mime_type=\"json\")\n     ],\n-    resources=ComputingResources(cores=1, ram=256.0, walltime=16.0),\n+    resources=ComputingResources(cores=16, ram=64.0, walltime=15.0),\n )\n \n VIS_cti_xml_out = Executable(\n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23537 CTI memory",
                            "Catherine Grenet",
                            "2023-08-23T17:49:41.000+02:00",
                            "261a390535fd546fba02e6f00babb91446b862b2"
                        ],
                        [
                            "@@ -18,8 +18,6 @@ import os\n \n PIPE_NAME = \"VIS_\" + os.path.splitext( os.path.basename( __file__))[0].split( \"VIS_\")[1]\n \n-TASKS_VER = \"13.0.14\"\n-\n VIS_cti_xml_in = Executable(\n     command=f\"E-Run VIS_Tasks {TASKS_VER} VIS_xml_in --pipeline_name={PIPE_NAME} \",\n     inputs=[\n@@ -60,7 +58,7 @@ calibrate_cti = Executable(\n         Output(\"cti_parallel_output\", mime_type=\"json\"),\n         Output(\"cti_serial_output\", mime_type=\"json\")\n     ],\n-    resources=ComputingResources(cores=16, ram=64.0, walltime=8.0),\n+    resources=ComputingResources(cores=16, ram=64.0, walltime=15.0),\n )\n \n VIS_cti_xml_out = Executable(\n",
                            "revert local-run changes",
                            "James Nightingale",
                            "2023-08-23T11:52:51.000+02:00",
                            "275f09500f076a91e706290b048fab9638c6350e"
                        ],
                        [
                            "@@ -60,7 +60,7 @@ calibrate_cti = Executable(\n         Output(\"cti_parallel_output\", mime_type=\"json\"),\n         Output(\"cti_serial_output\", mime_type=\"json\")\n     ],\n-    resources=ComputingResources(cores=8, ram=128.0, walltime=8.0),\n+    resources=ComputingResources(cores=16, ram=64.0, walltime=8.0),\n )\n \n VIS_cti_xml_out = Executable(\n",
                            "settled on VIS_CTICalibration using 16 cores",
                            "James Nightingale",
                            "2023-08-22T19:49:52.000+02:00",
                            "4b3b16b2ad2faf4297853b033c9790a4e73dd7ff"
                        ]
                    ],
                    "VIS_IAL_Pipelines/auxdir/VIS_IAL_Pipelines/VIS_CTIMasterCI_Pipeline/PkgDef_VIS_CTIMasterCI.py": [
                        [
                            "@@ -49,7 +49,7 @@ master_ci_estimate = Executable(\n         Input(\"quad_list_of_lists\")\n     ],\n     outputs=[Output(\"cti_master_ci\", mime_type=\"json\")],\n-    resources=ComputingResources(cores=1, ram=256.0, walltime=4.0),\n+    resources=ComputingResources(cores=1, ram=24.0, walltime=6.0),\n )\n \n quad2FPA = Executable(\n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23537 CTI memory",
                            "Catherine Grenet",
                            "2023-08-23T17:49:41.000+02:00",
                            "261a390535fd546fba02e6f00babb91446b862b2"
                        ],
                        [
                            "@@ -19,8 +19,6 @@ import os\n \n PIPE_NAME = \"VIS_\" + os.path.splitext( os.path.basename( __file__))[0].split( \"VIS_\")[1]\n \n-TASKS_VER = \"13.0.14\"\n-\n VIS_cti_xml_in = Executable(\n     command=f\"E-Run VIS_Tasks {TASKS_VER} VIS_xml_in --pipeline_name={PIPE_NAME} \",\n     inputs=[\n",
                            "revert local-run changes",
                            "James Nightingale",
                            "2023-08-23T11:52:51.000+02:00",
                            "275f09500f076a91e706290b048fab9638c6350e"
                        ],
                        [
                            "@@ -51,7 +51,7 @@ master_ci_estimate = Executable(\n         Input(\"quad_list_of_lists\")\n     ],\n     outputs=[Output(\"cti_master_ci\", mime_type=\"json\")],\n-    resources=ComputingResources(cores=1, ram=24.0, walltime=4.0),\n+    resources=ComputingResources(cores=1, ram=24.0, walltime=6.0),\n )\n \n quad2FPA = Executable(\n",
                            "settled on VIS_CTICalibration using 16 cores",
                            "James Nightingale",
                            "2023-08-22T19:49:52.000+02:00",
                            "4b3b16b2ad2faf4297853b033c9790a4e73dd7ff"
                        ]
                    ],
                    "doc/release_note/VIS_Software_Release_Note.md": [
                        [
                            "@@ -1,9 +1,9 @@\n |         |**Document Identification**                                                                          |\n |---------|-----------------------------------------------------------------------------------------------------|\n |Title|Euclid SGS VIS PF Software Release Note |\n-|PF Release| 13.0.13 |\n-|Date:|16/08/2023|\n-|Doc. Issue| 13.0.13 |\n+|PF Release| 13.0.14 |\n+|Date:|22/08/2023|\n+|Doc. Issue| 13.0.14 |\n |Reference:|EUCL-IAP-TN-8-019|\n |Custodian:|C. Grenet|\n \n@@ -22,6 +22,7 @@\n 5. [Patch releases](#patches)\n    * [5.1 Patch 13.0.12](#patch13.0.12)\n    * [5.2 Patch 13.0.13](#patch13.0.13)\n+   * [5.3 Patch 13.0.14](#patch13.0.14)\n 6. [Known issues](#known_issues)\n \n # 1. Purpose and Scope <a name=\"purpose_and_scope\"></a>\n@@ -58,7 +59,7 @@ The PF User Manual [RD5] describes the principal software parts of the system, i\n |------|------|\n | EDEN | `3.1` |\n | Data Model | `9.2.0` |\n-| PF IAL Pipelines conf. | [13.0.13](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_IAL_Pipelines/-/tree/13.0.13) |\n+| PF IAL Pipelines conf. | [13.0.14](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_IAL_Pipelines/-/tree/13.0.14) |\n \n \n ## 3.2 PF software products configuration <a name=\"products\"></a>\n@@ -66,7 +67,7 @@ The PF User Manual [RD5] describes the principal software parts of the system, i\n \n | Gitlab project    | Gitlab tag | SonarQube analysis\n | ----------- | ----------- | ----------- |\n-| [VIS_Tasks](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks) | [13.0.13](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks/-/tags/13.0.13)|[release-13.0](https://codeen-app.euclid-ec.org/sonar/dashboard?branch=release-13.0&id=VIS_Tasks)|\n+| [VIS_Tasks](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks) | [13.0.14](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks/-/tags/13.0.13)|[release-13.0](https://codeen-app.euclid-ec.org/sonar/dashboard?branch=release-13.0&id=VIS_Tasks)|\n | [VIS_PyLibrary](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_PyLibrary) | [3.20.2](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_PyLibrary/-/tags/3.20.2) |[release-3.20](https://codeen-app.euclid-ec.org/sonar/dashboard?branch=release-2.20&id=VIS_PyLibrary)|\n | [VIS_ImageTools](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_ImageTools) | [3.16.0](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_ImageTools/-/tags/3.16.0) | [release-3.16](https://codeen-app.euclid-ec.org/sonar/dashboard?branch=release-3.16&id=VIS_ImageTools)|\n  | [VIS_CTI](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_CTI) | [5.9.0](https://gitlab.euclid-sgs.uk/PF-VIS/VIS_CTI/-/tags/5.9.0) |[release-5.9](https://codeen-app.euclid-ec.org/sonar/dashboard?branch=release-5.9&id=VIS_CTI)|\n@@ -141,7 +142,7 @@ New features:\n | [#23227](https://euclid.roe.ac.uk/issues/23227) | BloomingCalibration: PTC values added to output product\n | None                                            | VIS_CTI: new version 5.9.0\n \n-# 5.1 Patch 13.0.13 <a name=\"patch13.0.13\"></a>\n+# 5.2 Patch 13.0.13 <a name=\"patch13.0.13\"></a>\n \n Solved issues:\n \n@@ -151,7 +152,22 @@ Solved issues:\n | [#23486](https://euclid.roe.ac.uk/issues/23486) | MasterBias pipeline creates incorrect DpdVisAnalysisResult that fails to be ingested\n | [#23494](https://euclid.roe.ac.uk/issues/23494) | VIS MasterDark crash even on long exposures\n \n+# 5.3 Patch 13.0.14 <a name=\"patch13.0.14\"></a>\n \n+Solved issues:\n+\n+| Issue                                           | Title       |\n+| ----------------------------------------------- | ----------- |\n+| [#23453](https://euclid.roe.ac.uk/issues/23453) | In the stacked images produced of the commissioning data, the astrometric solution is not correct |\n+| [#23511](https://euclid.roe.ac.uk/issues/23511) | [CALBLOCK-PV-027] TrapPumping processing crash\n+| [#23513](https://euclid.roe.ac.uk/issues/23513) | VIS_MasterDark_Pipeline: Data.ExposureTime missing in DpdVisMasterDarkFrame\n+| [#23522](https://euclid.roe.ac.uk/issues/23522) | VIS_BloomingCalibration_Pipeline: blooming model has inconsistent unit\n+\n+New features:\n+\n+| Issue                                           | Title       |\n+| ----------------------------------------------- | ----------- |\n+| [#23518](https://euclid.roe.ac.uk/issues/23518) | VIS_BloomingCalibration pipeline: use object masks\n \n # 6. Known issues <a name=\"known_issues\"></a>\n \n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23537 CTI memory",
                            "Catherine Grenet",
                            "2023-08-23T17:49:41.000+02:00",
                            "261a390535fd546fba02e6f00babb91446b862b2"
                        ]
                    ],
                    "CMakeLists.txt": [
                        [
                            "@@ -12,6 +12,6 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project( VIS_IAL_Pipelines 13.0.13\n+elements_project( VIS_IAL_Pipelines 13.0.15\n                   USE ST_PipelineChecker 1.5.0\n                   DESCRIPTION \"VIS Processing Function pipelines interface description\")\n",
                            "[FIX]Re-bump version to 13.0.15 (overwritten by previous merge, sigh)",
                            "Catherine Grenet",
                            "2023-08-23T17:35:12.000+02:00",
                            "05a478263b8a4415056a46a562c575c7c2c53fa4"
                        ],
                        [
                            "@@ -12,6 +12,6 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project( VIS_IAL_Pipelines 13.0.14\n+elements_project( VIS_IAL_Pipelines 13.0.13\n                   USE ST_PipelineChecker 1.5.0\n                   DESCRIPTION \"VIS Processing Function pipelines interface description\")\n",
                            "Merge branch 'memory_requirements' into 'release-13.0'",
                            "Catherine Grenet",
                            "2023-08-23T15:32:12.000+00:00",
                            "3a27e38453fb99ae43d78f66b6f673bb4c54ca63"
                        ],
                        [
                            "@@ -12,6 +12,6 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project( VIS_IAL_Pipelines 13.0.14\n+elements_project( VIS_IAL_Pipelines 13.0.13\n                   USE ST_PipelineChecker 1.5.0\n                   DESCRIPTION \"VIS Processing Function pipelines interface description\")\n",
                            "revert local-run changes",
                            "James Nightingale",
                            "2023-08-23T11:52:51.000+02:00",
                            "275f09500f076a91e706290b048fab9638c6350e"
                        ]
                    ],
                    "VIS_IAL_Pipelines/auxdir/VIS_IAL_Pipelines/VIS_BloomingCalibration_Pipeline/PipScript_VIS_BloomingCalibration.py": [
                        [
                            "@@ -28,7 +28,7 @@ def process_quad(config, MDB, exposure_in):\n \n @pipeline(outputs=('blooming_model_xml_out'))\n def VIS_BloomingCalibration( raw_frames_in, vis_config_in, MDB, flagmaps, gains_model, masterbias,\n-                             xtalk_model, nlcorr_model, saturation_model, ron_model, cti_model, bfe_model):\n+                             xtalk_model, nlcorr_model, saturation_model, ron_model, cti_model, bfe_model, object_masks):\n \n    rawexp_list, config = \\\n         PkgDef.VIS_blooming_xml_in( vis_config_in=   vis_config_in,\n@@ -42,7 +42,8 @@ def VIS_BloomingCalibration( raw_frames_in, vis_config_in, MDB, flagmaps, gains_\n                                     saturation_model=saturation_model,\n                                     ron_model=       ron_model,\n                                     cti_model=       cti_model,\n-                                    bfe_model=       bfe_model)\n+                                    bfe_model=       bfe_model,\n+                                    object_masks=    object_masks)\n \n    processed_quad_list, processed_quad_flg_list = process_quad( config=config,\n                                                                 MDB=MDB,\n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:51:41.000+02:00",
                            "2fa25b4a7d81e30ceeaa37073d1416688aead0ec"
                        ]
                    ],
                    "VIS_IAL_Pipelines/auxdir/VIS_IAL_Pipelines/VIS_BloomingCalibration_Pipeline/PkgDef_VIS_BloomingCalibration.py": [
                        [
                            "@@ -26,7 +26,8 @@ VIS_blooming_xml_in = Executable(command   = f\"E-Run VIS_Tasks {TASKS_VER} VIS_x\n                                           Input(\"nlcorr_model\",     content_type=\"listfile\"),\n                                           Input(\"ron_model\",        content_type=\"listfile\"),\n                                           Input(\"cti_model\",        content_type=\"listfile\"),\n-                                          Input(\"bfe_model\",        content_type=\"listfile\"),],\n+                                          Input(\"bfe_model\",        content_type=\"listfile\"),\n+                                          Input(\"object_masks\",   content_type=\"listfile\")],\n                              outputs   = [Output(\"rawexp_list\", mime_type=\"json\", content_type=\"listfile\"),\n                                           Output(\"piperun_config\", mime_type=\"cfg\")],\n                              resources = ComputingResources(cores = 1,\n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:51:41.000+02:00",
                            "2fa25b4a7d81e30ceeaa37073d1416688aead0ec"
                        ]
                    ],
                    "doc/user_manual/chap06_BloomingCalibration.md": [
                        [
                            "@@ -34,15 +34,16 @@ graph LR\n     DpdVisNonLinearityModel -.->|nlcorr_model<br>0..1| pipeline\n     DpdVisCTIModel -.->|cti_model<br>0..1| pipeline\n     DpdVisBFEModel -.->|bfe_model<br>0..1| pipeline\n+    DpdVisFlagMap -.->|object_masks<br>0..*| pipeline\n     pipeline -->|dpdcalib_blooming<br>1..*| DpdVisBloomingModel&nbsp\n ```\n \n **Ports description:**\n \n-* `DpdVisRawFrame`: raw flat field frames\n+* `raw_frames_in`: raw flat field frames\n * `vis_config_in`: VIS PF configuration file\n * `MDB`: Mission DataBase\n-* `DpdVisGainModel`: gain model from `VIS_GainCalibration` pipeline\n+* `gains_model`: gain model from `VIS_GainCalibration` pipeline\n * `ron_model`: readout noise model\n * `saturation_model`: saturation model, not used\n * `masterbias`: master bias frame from `VIS_MasterBias` pipeline\n@@ -51,6 +52,7 @@ graph LR\n * `nlcorr_model`: non-linearity correction model\n * `cti_model`: CTI model\n * `bfe_model`: bfe model\n+* `object_masks`: objet masks from VIS_FlagObjects pipeline\n \n \n \n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:51:41.000+02:00",
                            "2fa25b4a7d81e30ceeaa37073d1416688aead0ec"
                        ]
                    ],
                    "doc/user_manual/chap29_PVOperations.md": [
                        [
                            "@@ -21,7 +21,8 @@ Some pipelines will use on-ground models. These on-ground models are:\n - saturation_model: DpdVisBloomingModel ProductId=DpdVisBloomingModel_EUC_VIS_MDL-SC8_BLOOM_ELE_json\n - zero_point: DpdVisZeroPoint ProductId=DpdVisZeroPoint_2436_040223_json\n - ghost_model: DpdVisGhostModel ProductId=DpdVisGhostModel_ghost_model_JAN2022_json\n-- starmask_model: DpdVisStarMask ProductId= DpdVisStarMask_starmask_JAN2022_fits\n+- starmask_model: DpdVisStarMask ProductId=DpdVisStarMask_starmask_JAN2022_fits\n+- crosstalk model: DpdVisXTalkModel ProductId=DpdVisXTalkModel_EUC_VIS_MDL-XTK-VGCC-coeffs_20210915_json\n \n Hereafter these products are referenced as _on-ground_.\n \n@@ -31,6 +32,18 @@ VIS_FlagObjects pipeline is intended to run on Calbloks PV-022, PV-029, and PV-0\n \n The detailed description of VIS_FlagObjects is given only once for PV-022 variant PRNU2-10000ADU. For the other calblocks and variants we just mention the differences with this first run.\n \n+VIS_MasterFlat pipeline is intended to run on all variants of Calbloks PV-022. It produces a master flat for each pair of LED and fluence.\n+LED selection is done with CalibUnit.LedMask:\n+\n+- LED1: CalibUnit.LedMask=0x1\n+- LED2: CalibUnit.LedMask=0x2\n+- LED3: CalibUnit.LedMask=0x4\n+- LED4: CalibUnit.LedMask=0x8\n+\n+Fluence selection is done with the calblock variant, except for LED 5 and 6 (PV-022 variant LED56-PRNU1-PRNU2-BF).\n+\n+The detailed description of VIS_MasterFlat pipeline is given only once for PV-022 variant PRNU2-10000ADU. For other variants we just mention the differences with this first run.\n+\n ## PV-021\n \n ### Pipeline: VIS_MasterBias\n@@ -113,13 +126,6 @@ The detailed description of VIS_FlagObjects is given only once for PV-022 varian\n \n 4 instances of the pipelines will be run, one for each of LEDs 1, 2, 3, 4. The 4 instances can be run in parallel.\n \n- LED selection is done with CalibUnit.LedMask:\n-\n-- LED1: CalibUnit.LedMask=0x1\n-- LED2: CalibUnit.LedMask=0x2\n-- LED3: CalibUnit.LedMask=0x4\n-- LED4: CalibUnit.LedMask=0x8\n-\n **Inputs:**\n \n - raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=PRNU2-10000ADU & ImgType.FirstType=FLAT & CalibUnit.LedMask=_mask for LEDi_\n@@ -140,7 +146,7 @@ The detailed description of VIS_FlagObjects is given only once for PV-022 varian\n **Outputs:**\n \n - dpdcalib_data: master flat _LEDi_ 10kADU\n-- dpdcalib_flag: map of defects in photoresponse at 10 kADU, second part of CALPRODUCT VS-007 VIS Cosmetic Map\n+- dpdcalib_flag: map of defects in photoresponse at 10 kADU\n - dpdanalysis: quality assessment parameters\n - dpd_ledprofile: calibration unit profile\n \n@@ -153,9 +159,6 @@ Same as in PV-022 PRNU2-10000ADU except:\n **Inputs:**\n \n - raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-029 &  ObservationSequence.CalBlockVariant=PTC-SERIES-1 & ImgType.FirstType=SURVEY\n-- flagmaps:\n-  - map of defects in darkness, output of PV-021\n-  - map of defects in photoresponse for LED3, output of PV-022 PRNU2-10000ADU MasterFlat LED3\n \n ### Pipeline: VIS_BloomingCalibration\n \n@@ -175,6 +178,7 @@ Same as in PV-022 PRNU2-10000ADU except:\n - nlcorr_model: empty\n - cti_model: empty\n - bfe_model: empty\n+- object_masks: outputs of VIS_FlagObjects pipeline\n \n **Outputs:**\n \n@@ -379,9 +383,6 @@ Quality control plots are in the workdir in logs/VIS_astrom_calib.\n **Inputs:**\n \n - raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=LOW1-1000ADU & ImgType.FirstType=SURVEY\n-- flagmaps:\n-  - map of defects in darkness, output of PV-021\n-  - map of defects in photoresponse at 10 kADU, output of PV-022 - PRNU2-10000ADU\n \n **Outputs:**\n \n@@ -392,7 +393,7 @@ Quality control plots are in the workdir in logs/VIS_astrom_calib.\n \n **Inputs:**\n \n-- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=LOW1-1000ADU & ImgType.FirstType=FLAT & CalibUnit.LedMask=Ox4\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=LOW1-1000ADU & ImgType.FirstType=FLAT & CalibUnit.LedMask=0x4\n - vis_config_in:\n - MDB:\n - gains_model: on-ground\n@@ -413,8 +414,45 @@ Quality control plots are in the workdir in logs/VIS_astrom_calib.\n \n - gain_model_xml_out: gain model, part of CALPRODUCT VS-011\n \n+### Pipeline: VIS_MasterFlat\n+\n+4 instances of the pipelines will be run, one for each of LEDs 1, 2, 3, 4. The 4 instances can be run in parallel.\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=LOW1-1000ADU & ImgType.FirstType=FLAT & CalibUnit.LedMask=_mask for LEDi_\n+\n+**Outputs:**\n+\n+- dpdcalib_data: master flat _LEDi_ 1kADU\n+- dpdcalib_flag: map of defects in photoresponse at 1 kADU\n+- dpdanalysis: quality assessment parameters\n+- dpd_ledprofile: calibration unit profile\n+\n+\n ## PV-022 - LOW2-5000ADU\n \n+### Pipeline: VIS_FlagObjects\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=LOW2-5000ADU & ImgType.FirstType=SURVEY\n+\n+### Pipeline: VIS_MasterFlat\n+\n+4 instances of the pipelines will be run, one for each of LEDs 1, 2, 3, 4. The 4 instances can be run in parallel.\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=LOW2-5000ADU & ImgType.FirstType=FLAT & CalibUnit.LedMask=_mask for LEDi_\n+\n+**Outputs:**\n+\n+- dpdcalib_data: master flat _LEDi_ 5kADU\n+- dpdcalib_flag: map of defects in photoresponse at 5 kADU\n+- dpdanalysis: quality assessment parameters\n+- dpd_ledprofile: calibration unit profile\n+\n ## PV-022 - BF-40000ADU\n \n ### Pipeline: VIS_FlagObjects\n@@ -422,9 +460,6 @@ Quality control plots are in the workdir in logs/VIS_astrom_calib.\n **Inputs:**\n \n - raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=BF-40000ADU & ImgType.FirstType=SURVEY\n-- flagmaps:\n-  - map of defects in darkness, output of PV-021\n-  - map of defects in photoresponse at 10 kADU, output of PV-022 - PRNU2-10000ADU\n \n **Outputs:**\n \n@@ -454,6 +489,22 @@ Quality control plots are in the workdir in logs/VIS_astrom_calib.\n \n - bfecalib_xml_out: brighter-fatter model CALPRODUCT VS-004 VIS Brighter-fatter Effect\n \n+### Pipeline: VIS_MasterFlat\n+\n+4 instances of the pipelines will be run, one for each of LEDs 1, 2, 3, 4. The 4 instances can be run in parallel.\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=BF-40000ADU & ImgType.FirstType=FLAT & CalibUnit.LedMask=_mask for LEDi_\n+\n+**Outputs:**\n+\n+- dpdcalib_data: master flat _LEDi_ 40kADU\n+- dpdcalib_flag: map of defects in photoresponse at 40 kADU\n+- dpdanalysis: quality assessment parameters\n+- dpd_ledprofile: calibration unit profile\n+\n+\n ## PV-007 - PROPOSAL-08-M33\n ## PV-007 - PROPOSAL-11-A370\n ## PV-007 - PROPOSAL-11-N01\n@@ -655,11 +706,129 @@ This pipeline aims at producing the astrometric catalogue for NIR. This step can\n \n ## PV-029 PTC-SERIES-2\n \n+### Pipeline: VIS_FlagObjects\n+\n+Same as in PV-022 PRNU2-10000ADU except:\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-029 &  ObservationSequence.CalBlockVariant=PTC-SERIES-2 & ImgType.FirstType=SURVEY\n+\n+\n ## PV-031\n \n+### Pipeline: VIS_FlagObjects\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-031 & ImgType.FirstType=SURVEY\n+\n+### Pipeline: VIS_BloomingCalibration\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame (ObservationSequence.CalblockId=PV-029 | ObservationSequence.CalblockId=PV-031) & ImgType.FirstType=FLAT\n+- vis_config_in:\n+- MDB:\n+- gains_model: gains model CALPRODUCT VS-011\n+- ron_model: on-ground\n+- saturation_model: on-ground\n+- masterbias: CALPRODUCT VS-003\n+- flagmaps:\n+  - map of defects in darkness, output of PV-021\n+  - map of defects in photoresponse for LED3, output of PV-022 PRNU2-10000ADU MasterFlat LED3\n+- xtalk_model: CALPRODUCT VS-008\n+- nlcorr_model: empty\n+- cti_model: empty\n+- bfe_model: empty\n+- object_masks: outputs of VIS_FlagObjects pipeline\n+\n+**Outputs:**\n+\n+- dpdcalib_blooming:\n+  - VIS Photon Transfer Curve, part of CALPRODUCT VS-011\n+  - VIS Blooming threshold CALPRODUCT VS-020\n+\n+### Pipeline:  VIS_PTCNLCalibration\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-029 & ImgType.FirstType=FLAT\n+- vis_config_in:\n+- MDB:\n+- gains_model: gains model CALPRODUCT VS-011\n+- ron_model: on-ground\n+- saturation_model: CALPRODUCT VS-020\n+- masterbias:  CALPRODUCT VS-003\n+- flagmaps:\n+  - map of defects in darkness, output of PV-021\n+  - map of defects in photoresponse for LED3, output of PV-022 PRNU2-10000ADU MasterFlat LED3\n+- xtalk_model: CALPRODUCT VS-008\n+- nlcorr_model: empty\n+- cti_model: empty\n+- masterdark: empty\n+- bfe_model: empty\n+- object_masks: outputs of VIS_FlagObjects pipeline\n+\n+**Outputs:**\n+\n+- dpdcalib_ptc: ???\n+- dpdcalib_nl: non-linearity model, candidate CALPRODUCT VS-016 VIS Nonlinearity\n+\n+\n+## PV-004 GLINT-CHECK\n+\n+## PV-007 PROPOSAL-09\n+\n+## PV-005 CDFS-24\n+## PV-005 EDFS-24-47ROS\n+\n+## PV-001 VISIT-2\n+\n+## PV-005 EDFN-FULL-25\n+## PV-005 EDFN-3x3-25\n+\n+## PV-001 VISIT-3\n+\n+## PV-022 PRNU1-25000ADU\n+\n+### Pipeline: VIS_FlagObjects\n+\n+**Inputs:**\n+\n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=PRNU1-25000ADU & ImgType.FirstType=SURVEY\n \n+### Pipeline: VIS_MasterFlat\n+\n+4 instances of the pipelines will be run, one for each of LEDs 1, 2, 3, 4. The 4 instances can be run in parallel.\n+\n+**Inputs:**\n \n+- raw_frames_in: DpdVisRawFrame ObservationSequence.CalblockId=PV-022 & ObservationSequence.CalBlockVariant=PRNU1-25000ADU & ImgType.FirstType=FLAT & CalibUnit.LedMask=_mask for LEDi_\n+\n+**Outputs:**\n+\n+- dpdcalib_data: master flat _LEDi_ 25kADU\n+- dpdcalib_flag: map of defects in photoresponse at 25 kADU\n+- dpdanalysis: quality assessment parameters\n+- dpd_ledprofile: calibration unit profile\n+\n+### Pipeline: VIS_SmallScaleFlat\n+\n+This pipeline combines all the individual single-LED (1, 2, 3, 4) single-fluence (10k, 25k, 40k) MasterFlat frames available so far to produce one unique \"small-scale\" MasterFlat to use in all subsequent masterflat input ports.\n+\n+**Inputs:**\n+\n+- vis_config_in:\n+- MDB:\n+- prnulist_in: DpdVisMasterFlatFrame outputs of previous PV-022 VIS_MasterFlat pipelines (12 objects: 4 LEDs x 3 fluences)\n+- flagmaps_in: map of defects in photoresponse, dpdcalib_flag outputs of previous 12 PV-022 VIS_MasterFlat pipelines\n+\n+**Outputs:**\n \n+- smallscaleflat_out: DpdVisMasterFlatFrame containing the combined small-scale MasterFlat CALPRODUCT VS-022\n+- prnuflagmap_out: DpdVisFlagMap associated to smallscaleflat_out\n+- dqc_out: DpdVisAnalysisResults, data quality control product, containing the 'slope' and 'standard error' products of each extrapolated to 0-signal flat per LED\n \n \n \n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:51:41.000+02:00",
                            "2fa25b4a7d81e30ceeaa37073d1416688aead0ec"
                        ]
                    ]
                },
                "selected_modifications": {
                    "VIS_IAL_Pipelines/auxdir/VIS_IAL_Pipelines/VIS_CTICalibration_Pipeline/PkgDef_VIS_CTICalibration.py": [
                        [
                            "@@ -58,7 +58,7 @@ calibrate_cti = Executable(\n         Output(\"cti_parallel_output\", mime_type=\"json\"),\n         Output(\"cti_serial_output\", mime_type=\"json\")\n     ],\n-    resources=ComputingResources(cores=1, ram=256.0, walltime=16.0),\n+    resources=ComputingResources(cores=16, ram=64.0, walltime=15.0),\n )\n \n VIS_cti_xml_out = Executable(\n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23537 CTI memory",
                            "Catherine Grenet",
                            "2023-08-23T17:49:41.000+02:00",
                            "261a390535fd546fba02e6f00babb91446b862b2"
                        ],
                        [
                            "@@ -18,8 +18,6 @@ import os\n \n PIPE_NAME = \"VIS_\" + os.path.splitext( os.path.basename( __file__))[0].split( \"VIS_\")[1]\n \n-TASKS_VER = \"13.0.14\"\n-\n VIS_cti_xml_in = Executable(\n     command=f\"E-Run VIS_Tasks {TASKS_VER} VIS_xml_in --pipeline_name={PIPE_NAME} \",\n     inputs=[\n@@ -60,7 +58,7 @@ calibrate_cti = Executable(\n         Output(\"cti_parallel_output\", mime_type=\"json\"),\n         Output(\"cti_serial_output\", mime_type=\"json\")\n     ],\n-    resources=ComputingResources(cores=16, ram=64.0, walltime=8.0),\n+    resources=ComputingResources(cores=16, ram=64.0, walltime=15.0),\n )\n \n VIS_cti_xml_out = Executable(\n",
                            "revert local-run changes",
                            "James Nightingale",
                            "2023-08-23T11:52:51.000+02:00",
                            "275f09500f076a91e706290b048fab9638c6350e"
                        ],
                        [
                            "@@ -60,7 +60,7 @@ calibrate_cti = Executable(\n         Output(\"cti_parallel_output\", mime_type=\"json\"),\n         Output(\"cti_serial_output\", mime_type=\"json\")\n     ],\n-    resources=ComputingResources(cores=8, ram=128.0, walltime=8.0),\n+    resources=ComputingResources(cores=16, ram=64.0, walltime=8.0),\n )\n \n VIS_cti_xml_out = Executable(\n",
                            "settled on VIS_CTICalibration using 16 cores",
                            "James Nightingale",
                            "2023-08-22T19:49:52.000+02:00",
                            "4b3b16b2ad2faf4297853b033c9790a4e73dd7ff"
                        ]
                    ],
                    "VIS_IAL_Pipelines/auxdir/VIS_IAL_Pipelines/VIS_CTIMasterCI_Pipeline/PkgDef_VIS_CTIMasterCI.py": [
                        [
                            "@@ -49,7 +49,7 @@ master_ci_estimate = Executable(\n         Input(\"quad_list_of_lists\")\n     ],\n     outputs=[Output(\"cti_master_ci\", mime_type=\"json\")],\n-    resources=ComputingResources(cores=1, ram=256.0, walltime=4.0),\n+    resources=ComputingResources(cores=1, ram=24.0, walltime=6.0),\n )\n \n quad2FPA = Executable(\n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23537 CTI memory",
                            "Catherine Grenet",
                            "2023-08-23T17:49:41.000+02:00",
                            "261a390535fd546fba02e6f00babb91446b862b2"
                        ],
                        [
                            "@@ -19,8 +19,6 @@ import os\n \n PIPE_NAME = \"VIS_\" + os.path.splitext( os.path.basename( __file__))[0].split( \"VIS_\")[1]\n \n-TASKS_VER = \"13.0.14\"\n-\n VIS_cti_xml_in = Executable(\n     command=f\"E-Run VIS_Tasks {TASKS_VER} VIS_xml_in --pipeline_name={PIPE_NAME} \",\n     inputs=[\n",
                            "revert local-run changes",
                            "James Nightingale",
                            "2023-08-23T11:52:51.000+02:00",
                            "275f09500f076a91e706290b048fab9638c6350e"
                        ],
                        [
                            "@@ -51,7 +51,7 @@ master_ci_estimate = Executable(\n         Input(\"quad_list_of_lists\")\n     ],\n     outputs=[Output(\"cti_master_ci\", mime_type=\"json\")],\n-    resources=ComputingResources(cores=1, ram=24.0, walltime=4.0),\n+    resources=ComputingResources(cores=1, ram=24.0, walltime=6.0),\n )\n \n quad2FPA = Executable(\n",
                            "settled on VIS_CTICalibration using 16 cores",
                            "James Nightingale",
                            "2023-08-22T19:49:52.000+02:00",
                            "4b3b16b2ad2faf4297853b033c9790a4e73dd7ff"
                        ]
                    ],
                    "VIS_IAL_Pipelines/auxdir/VIS_IAL_Pipelines/VIS_BloomingCalibration_Pipeline/PkgDef_VIS_BloomingCalibration.py": [
                        [
                            "@@ -26,7 +26,8 @@ VIS_blooming_xml_in = Executable(command   = f\"E-Run VIS_Tasks {TASKS_VER} VIS_x\n                                           Input(\"nlcorr_model\",     content_type=\"listfile\"),\n                                           Input(\"ron_model\",        content_type=\"listfile\"),\n                                           Input(\"cti_model\",        content_type=\"listfile\"),\n-                                          Input(\"bfe_model\",        content_type=\"listfile\"),],\n+                                          Input(\"bfe_model\",        content_type=\"listfile\"),\n+                                          Input(\"object_masks\",   content_type=\"listfile\")],\n                              outputs   = [Output(\"rawexp_list\", mime_type=\"json\", content_type=\"listfile\"),\n                                           Output(\"piperun_config\", mime_type=\"cfg\")],\n                              resources = ComputingResources(cores = 1,\n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:51:41.000+02:00",
                            "2fa25b4a7d81e30ceeaa37073d1416688aead0ec"
                        ]
                    ]
                },
                "count_selected_modifications": "3",
                "tags_in_period": [
                    {
                        "name": "13.0.14",
                        "created_at": "2023-08-22T16:21:35.000+02:00",
                        "author_name": "Catherine Grenet"
                    },
                    {
                        "name": "13.0.15",
                        "created_at": "2023-08-24T17:35:46.000+02:00",
                        "author_name": "Catherine Grenet"
                    },
                    {
                        "name": "13.0.16",
                        "created_at": "2023-08-29T19:48:30.000+02:00",
                        "author_name": "Catherine Grenet"
                    }
                ]
            },
            "PF-VIS/VIS_Transients": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_XTalk": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_BiasCorrection": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_BiasCalibration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_PRNUCalibration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_PRNUCorrection": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Helper_scripts": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_PSF": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Validation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Cosmics": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_CTI": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_NonLinCorrection": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Pipelines": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Tasks": {
                "start date": "2023-08-22T13:27:01",
                "end date": "2023-08-29T14:03:14",
                "start tag": "13.0.14",
                "end tag": "13.0.16",
                "count_files_modified": "51",
                "modifications_by_file": {
                    "HISTORY.txt": [
                        [
                            "@@ -1,6 +1,12 @@\n VIS_Tasks changelog\n -------------------\n \n+* 25 Aug 2023 - mottet@iap.fr\n+** VIS_flag_ghosts_lib.py: update ghost flagging quality control plot fitting intervals for GAIA and real data\n+\n+* 24 Aug 2023 - mottet@iap.fr\n+** VIS_gather_ccd.py: add new configuration option 'GatherCCD' which allows to switch between per-CCD (when False, default) and per-quadrant (when True) VIS_ProcessField\n+\n * 23 Aug 2023 - mottet@iap.fr\n ** VIS_gather_quadrants.py: first draft version allowing to run VIS_AstrometryCalibration per quadrant\n \n",
                            "VIS_flag_ghosts_lib.py: update ghost flagging quality control plot fitting intervals for GAIA and real data",
                            "Sylvain Mottet",
                            "2023-08-25T16:03:02.000+02:00",
                            "873ba08a42cf5e06ca3c3de60f175a20122b5049"
                        ],
                        [
                            "@@ -1,6 +1,45 @@\n VIS_Tasks changelog\n -------------------\n \n+* 23 Aug 2023 - mottet@iap.fr\n+** VIS_gather_quadrants.py: first draft version allowing to run VIS_AstrometryCalibration per quadrant\n+\n+* 20 Aug 2023 - matt.wander@open.ac.uk\n+** VIS_TrapPumping: fix various bugs found in first PV pipeline runs (#23511)\n+*** VIS_TrapPumping_Analysis: handle where dipole masked in too many frames, handle where only one trap is found\n+*** VIS_TrapPumping_IO: raise error if input files not valid, handle results with only bad model fits\n+\n+* 17 Aug 2023 - herent@iap.fr\n+** VIS_PSF/auxdir/VIS_PSF/default.psfex: PSF_SIZE is now set to 21,21 to match the vignet size\n+** VIS_SourceExtraction/python/VIS_SourceExtraction/VIS_extract_sources.py: vignet size is now an odd number (21x21 pixels)\n+\n+* 17 Aug 2023 - mottet@iap.fr\n+** FromToXML.py: fix #23266: stacking output product XML is empty when Stack_Photometry=False and stack PSF file is None\n+\n+* 17 Aug 2023 - mottet@iap.fr\n+** VIS_gather_ccd.py: disable NoiseChisel background, which takes time but isn't used anywhere for now\n+** VIS_PRNU/VIS_CombinePRNU.py: remove file which superseded by VIS_PRNU/VIS_SmallScaleFlat/VIS_CombinePRNU.py\n+\n+* 15 Aug 2023 - mottet@iap.fr\n+** FromToXML.py: fix #23486, InputRawFrameReference default value doesn't exist in EUCLID, don't use it\n+\n+* 11 Aug 2023 - mottet@iap.fr\n+** VIS_science_xml_out.py: fix #23096, compress EUC_VIS_DET-SWL*.fits separately after data product creation\n+\n+* 10 Aug 2023 - mottet@iap.fr\n+** VIS_SmallScaleFlat: add DQC ouptut, should now accept any combination of single-LED/single-fluence master flats\n+\n+* 04 Aug 2023 - herent@iap.fr\n+** VIS_Photometry/python/VIS_Photometry/VIS_Calibrate_Photometry.py: fix photometry issue #23277 for PDC dataset\n+\n+* 31 Jul 2023 - mottet@iap.fr\n+** VIS_process_quad.py: fix #23240, add config_filename to NL correction PE call\n+\n+* 29 Jul 2023 - catherine.grenet@iap.fr\n+Feature ##23191 Astrometric calibration (use-fpa-model): adapt magnitude range selection to exposure duration\n+*** Modules.py: Wcsfit() new argument exptime\n+*** VIS_process_astro_field.py: pass EXPTIME to Modules.Wcsfit()\n+\n * 28 Jul 2023 - mottet@iap.fr\n ** VIS_xml_in.py, VIS_ghosts_calibration: fix #23163, now use DpdVisGhostModel to store the ghost model.\n \n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ],
                        [
                            "@@ -1,6 +1,9 @@\n VIS_Tasks changelog\n -------------------\n \n+* 23 Aug 2023 - mottet@iap.fr\n+** VIS_gather_quadrants.py: first draft version allowing to run VIS_AstrometryCalibration per quadrant\n+\n * 20 Aug 2023 - matt.wander@open.ac.uk\n ** VIS_TrapPumping: fix various bugs found in first PV pipeline runs (#23511)\n *** VIS_TrapPumping_Analysis: handle where dipole masked in too many frames, handle where only one trap is found\n",
                            "VIS_gather_quadrants.py: first draft version allowing to run VIS_AstrometryCalibration per quadrant",
                            "Sylvain Mottet",
                            "2023-08-23T10:37:59.000+02:00",
                            "a9a0acc55312d966ab1f24f12df89b724c31b1d0"
                        ],
                        [
                            "@@ -1,6 +1,10 @@\n VIS_Tasks changelog\n -------------------\n \n+* 20 Aug 2023 - matt.wander@open.ac.uk\n+** VIS_TrapPumping: fix various bugs found in first PV pipeline runs (#23511)\n+*** VIS_TrapPumping_Analysis: handle where dipole masked in too many frames, handle where only one trap is found\n+*** VIS_TrapPumping_IO: raise error if input files not valid, handle results with only bad model fits\n \n * 15 Aug 2023 - mottet@iap.fr\n ** FromToXML.py: fix #23486, InputRawFrameReference default value doesn't exist in EUCLID, don't use it\n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:48:12.000+02:00",
                            "6f4bb5791ddb1a33888248a6fb0cbd43c4b1bd55"
                        ],
                        [
                            "@@ -1,6 +1,10 @@\n VIS_Tasks changelog\n -------------------\n \n+* 20 Aug 2023 - matt.wander@open.ac.uk\n+** VIS_TrapPumping: fix various bugs found in first PV pipeline runs (#23511)\n+*** VIS_TrapPumping_Analysis: handle where dipole masked in too many frames, handle where only one trap is found\n+*** VIS_TrapPumping_IO: raise error if input files not valid, handle results with only bad model fits\n \n * 15 Aug 2023 - mottet@iap.fr\n ** FromToXML.py: fix #23486, InputRawFrameReference default value doesn't exist in EUCLID, don't use it\n",
                            "Merge branch 'release-13.0' of https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks into release-13.0",
                            "James Nightingale",
                            "2023-08-22T19:47:57.000+02:00",
                            "3887ab35499dbc803602db855483bfdea07cf240"
                        ],
                        [
                            "@@ -1,6 +1,10 @@\n VIS_Tasks changelog\n -------------------\n \n+* 20 Aug 2023 - matt.wander@open.ac.uk\n+** VIS_TrapPumping: fix various bugs found in first PV pipeline runs (#23511)\n+*** VIS_TrapPumping_Analysis: handle where dipole masked in too many frames, handle where only one trap is found\n+*** VIS_TrapPumping_IO: raise error if input files not valid, handle results with only bad model fits\n \n * 15 Aug 2023 - mottet@iap.fr\n ** FromToXML.py: fix #23486, InputRawFrameReference default value doesn't exist in EUCLID, don't use it\n",
                            "Merge branch 'release-13.0' into 'feature_#23227_BloomingCalib'",
                            "Thomas Flanet",
                            "2023-08-22T15:48:48.000+00:00",
                            "2d156ac3b1fd63e45e002c217d42c01a4c694040"
                        ]
                    ],
                    "VIS_Ghosts/python/VIS_Ghosts/VIS_flag_ghosts_lib.py": [
                        [
                            "@@ -438,13 +438,13 @@ class FlagGhostsConfig():\n     # fraction of flagged ghost pixels in mask below which ghost is ignored\n     self.MASKED_RATIO_THRESHOLD = 0.2\n     # magnitude interval for star brightness fitting\n-    self.SFITMIN = 19\n-    self.SFITMAX = 25\n+    self.SFITMIN = 18\n+    self.SFITMAX = 21\n     # magnitude interval for ghost brightness fitting\n-    self.GFITMIN = 9\n-    self.GFITMAX = 14\n+    self.GFITMIN = 10\n+    self.GFITMAX = 12\n     # magnitude for which the ghost to star brightness ratio is computed\n-    self.GHOST_RATIO_MAG = 12\n+    self.GHOST_RATIO_MAG = 11\n \n     if self.QC_THUMBS:\n       # thumb images of ghost areas\n",
                            "VIS_flag_ghosts_lib.py: update ghost flagging quality control plot fitting intervals for GAIA and real data",
                            "Sylvain Mottet",
                            "2023-08-25T16:03:02.000+02:00",
                            "873ba08a42cf5e06ca3c3de60f175a20122b5049"
                        ],
                        [
                            "@@ -491,7 +491,7 @@ class FlagGhostsConfig():\n # OBSID           = {self.OBSID}\n # DITHER          = {self.DITHER}\n # EXPNUM          = {self.EXPNUM}\n-# OBSID DITHER EXPNUM CCDID STAR_ID STAR_MAG STAR_RA STAR_DEC STAR_X STAR_Y STAR_ADU GHOST_X GHOST_Y GHOST_ADU GHOST_PIX_NUM MASKED_RATIO CALIB_CX CALIB_CY CALIB_RATIO\n+# OBSID DITHER EXPNUM EXTID STAR_ID STAR_MAG STAR_RA STAR_DEC STAR_X STAR_Y STAR_ADU GHOST_X GHOST_Y GHOST_ADU GHOST_PIX_NUM MASKED_RATIO CALIB_CX CALIB_CY CALIB_RATIO\n \"\"\"\n \n \n@@ -605,7 +605,12 @@ class FlagGhostsConfig():\n     \"\"\"\n \n     # get info from extension header\n-    CCDID = sci_header['CCDID'].strip()\n+    if sci_header[\"EXTNAME\"][-1] in \"EFGH\":\n+      # MEF extensions are quadrants\n+      EXTID = sci_header[\"EXTNAME\"]\n+    else:\n+      # MEF extensions are CCD\n+      EXTID = sci_header[\"CCDID\"]\n     self.get_ccd_star_positions( sci_header)\n     SATPIXEL = int( sci_header['SATURATE'])\n     ccdny, ccdnx = sci_data.shape\n@@ -664,7 +669,7 @@ class FlagGhostsConfig():\n \n       # log star brightest pixel for unflagged faint stars not logged in ghost flagging\n       if self.star_mag[istar] > self.GHOSTS_MAGLIMIT:\n-        self.ghost_cat += GC_encode_line( self.OBSID, self.DITHER, self.EXPNUM, f\"CCD_{CCDID}\",\n+        self.ghost_cat += GC_encode_line( self.OBSID, self.DITHER, self.EXPNUM, f\"EXT_{EXTID}\",\n                                           istar, self.star_mag[istar],\n                                           self.star_ra[istar], self.star_dec[istar],\n                                           star_x, star_y, star_adu,\n@@ -675,7 +680,7 @@ class FlagGhostsConfig():\n       flagmap.set_mask( \"STARSIGNAL\", mask, star_y - mask_radius, star_x - mask_radius)\n \n     logger.info( f\"flagged {flagged_stars} stars\")\n-    self.n_stars[CCDID] = flagged_stars\n+    self.n_stars[EXTID] = flagged_stars\n     self.star_pixels += np.count_nonzero( np.bitwise_and( flagmap.flags, VIS_FLAGS[\"STARSIGNAL\"]))\n \n     return flagmap.flags\n@@ -693,8 +698,13 @@ class FlagGhostsConfig():\n     # shortcuts for ease of writing and reading\n     ccdny, ccdnx = sci_data.shape\n     assert sci_data.shape == (ccdny, ccdnx)\n-    CCDID = sci_header['CCDID'].strip()\n-    ccd_row, ccd_col = re.findall( \"[1-6]\", CCDID)\n+    if sci_header[\"EXTNAME\"][-1] in \"EFGH\":\n+      # MEF extensions are quadrants\n+      EXTID = sci_header[\"EXTNAME\"]\n+    else:\n+      # MEF extensions are CCD\n+      EXTID = sci_header[\"CCDID\"]\n+    ccd_row, ccd_col = re.findall( \"[1-6]\", EXTID)\n     nghosts = 0\n \n     # currently only processing first star image reflection (first ghost order)\n@@ -714,8 +724,8 @@ class FlagGhostsConfig():\n     if bkg_data is None:\n       bkg_data = make_constant_background( sci_data, flg_data)\n \n-    self.avg_bkg[CCDID] = float( bkg_data.mean())\n-    logger.info( f\"background average value: {self.avg_bkg[CCDID]:.2f} ADU\")\n+    self.avg_bkg[EXTID] = float( bkg_data.mean())\n+    logger.info( f\"background average value: {self.avg_bkg[EXTID]:.2f} ADU\")\n \n \n # filter star catalogue to only keep stars that can produce a ghost on the CCD\n@@ -750,6 +760,7 @@ class FlagGhostsConfig():\n # TODO 2\n # * remove alltogether when charge injection line filling is implemented in VIS_Tasks\n \n+    \"\"\"\n     # check that injection lines are actually where we think they are\n     assert np.all( flagmap.flags[2065,:]      & VIS_FLAGS[\"CHARINJ\"] == 0)\n     assert np.all( flagmap.flags[2066:2070,:] & VIS_FLAGS[\"CHARINJ\"] != 0)\n@@ -761,11 +772,12 @@ class FlagGhostsConfig():\n     clean_sci[2067,:] = clean_sci[2065,:]\n     clean_sci[2068,:] = clean_sci[2070,:]\n     clean_sci[2069,:] = clean_sci[2070,:]\n+    \"\"\"\n \n     # denoise science image with median filter\n     clean_sci = scipy.ndimage.median_filter( clean_sci, 9, mode=\"nearest\")\n     if self.DEBUG_FILES_PREFIX is not None:\n-      pyfits.PrimaryHDU( clean_sci).writeto( f\"dbg_clean_sci_obs{self.OBSID}_dith{self.DITHER}_CCD{sci_header['CCDID']}_denoised_science.fits\", clobber=True)\n+      pyfits.PrimaryHDU( clean_sci).writeto( f\"dbg_clean_sci_obs{self.OBSID}_dith{self.DITHER}_{EXTID}_denoised_science.fits\", clobber=True)\n \n #-------------------------------------------------------------------------------\n # actually flag ghosts\n@@ -814,7 +826,7 @@ class FlagGhostsConfig():\n \n       # compute ghost centroid offset from star for calibration pipeline\n       # use a higher signal threshold to escape background noise and have a clearer mask\n-      calib_mask = np.logical_and( self.gdisk == 1, clean_sci_cut >= 5 * np.sqrt(self.avg_bkg[CCDID]))\n+      calib_mask = np.logical_and( self.gdisk == 1, clean_sci_cut >= 5 * np.sqrt(self.avg_bkg[EXTID]))\n       calib_cy, calib_cx, npts = mask_centroid( calib_mask)\n       if npts > 0:\n         calib_cx -= self.GHOST_RADIUS\n@@ -852,7 +864,7 @@ class FlagGhostsConfig():\n         masked_pix_ratio = 0\n \n       # trace ghost catalogue line in job log\n-      gcat_line = GC_encode_line( self.OBSID, self.DITHER, self.EXPNUM, f\"CCD_{CCDID}\",\n+      gcat_line = GC_encode_line( self.OBSID, self.DITHER, self.EXPNUM, f\"EXT_{EXTID}\",\n                                   istar, self.star_mag[istar],\n                                   self.star_ra[istar], self.star_dec[istar],\n                                   star_x, star_y, star_adu,\n@@ -870,7 +882,7 @@ class FlagGhostsConfig():\n         thumb_bkg_img = extract_rectangle( bkg_data, self.gdisk, disk_x, disk_y)\n         thumb_sci_img -= thumb_bkg_img\n         zmin, zmax = ZScaleInterval().get_limits( thumb_sci_img.ravel())\n-        key = f\"{CCDID}_{istar}_{self.star_mag[istar]:.2f}_{ghost_adu:.2f}_{calib_ratio:.3f}_{zmin:.1f}_{zmax:.1f}\"\n+        key = f\"{EXTID}_{istar}_{self.star_mag[istar]:.2f}_{ghost_adu:.2f}_{calib_ratio:.3f}_{zmin:.1f}_{zmax:.1f}\"\n         self.thumb_sience[key]    = PlottingTools.ndarray_to_image( thumb_sci_img, zmin=zmin, zmax=zmax, saveto=\"HTML\")\n         self.thumb_cleaned[key]   = PlottingTools.ndarray_to_image( clean_sci_cut, zmin=zmin, zmax=zmax, saveto=\"HTML\")\n         self.thumb_gmask[key]     = PlottingTools.ndarray_to_image( ghost_mask, color_scale=\"bw\", saveto=\"HTML\")\n@@ -878,7 +890,7 @@ class FlagGhostsConfig():\n \n \n     logger.info( f\"flagged {nghosts} ghost{'s' if nghosts > 1 else ''}\")\n-    self.n_ghosts[CCDID] = nghosts\n+    self.n_ghosts[EXTID] = nghosts\n     self.ghost_pixels += np.count_nonzero( np.bitwise_and( flagmap.flags, VIS_FLAGS[\"GHOST\"]))\n     self.total_pixels += ccdnx * ccdny - np.count_nonzero( np.bitwise_and( flagmap.flags, VIS_FLAGS[\"CHARINJ\"]))\n     FlagMap.set_invalid_flag( flagmap.flags)\n",
                            "VIS_process_photo_exp: update CCD specific code to be compatible with quadrants",
                            "Sylvain Mottet",
                            "2023-08-24T16:42:36.000+02:00",
                            "f50698df683a181bf4c8ddbcc9d4c764a2731c9c"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_gather_ccd.py": [
                        [
                            "@@ -46,6 +46,8 @@ import VIS_PyLibrary_Common.common_definitions as visdef\n from   VIS_Tasks_Common import FromToXML\n import VIS_Tasks_Common.VIS_diagnostics as visdiags\n \n+from VIS_Tasks_M import VIS_gather_quadrants\n+\n # initialise global logger\n import ElementsKernel.Logging\n logger = ElementsKernel.Logging.getLogger( __name__)\n@@ -132,24 +134,20 @@ def mainMethod(args):\n     pipe_tools.log_task_environment( logger)\n     logger.info( '#')\n \n-    logdir          = os.path.join( args.workdir, args.logdir)\n-    config_filename = os.path.join( args.workdir, args.config)\n-    input_quads     = os.path.join( args.workdir, args.input_quads)\n-    exposure        = os.path.join( args.workdir, args.exposure)\n-\n-    # Check that configuration file exists\n-    if ( os.path.isfile(config_filename) == False ):\n-        logger.error(\"%s does not exist or is not readable\", config_filename)\n-        exit (1)\n-    # Instantiate the configuration file parser\n-    config = RawConfigParser()\n-    # Make it case-sensitive for parameter names\n-    config.optionxform = str\n-    # Open the configuration file\n-    config.read (config_filename)\n-\n-    # Set up logger\n-#    logger.setLevel('DEBUG')\n+    logdir      = os.path.join( args.workdir, args.logdir)\n+    input_quads = os.path.join( args.workdir, args.input_quads)\n+    exposure    = os.path.join( args.workdir, args.exposure)\n+\n+    # read config file\n+    config = FromToXML.read_config( args.config)\n+\n+    # check if pipeline must glue quadrants in CCDs\n+    # if not, execute alternative VIS_gather_quadrants.py\n+    gather_ccd = config.getboolean( 'General', 'GatherCCD', fallback=True)\n+    if not gather_ccd:\n+      logger.info( \"configuration file GatherCCD is False, switching to VIS_gather_quadrants.py\")\n+      VIS_gather_quadrants.mainMethod( args)\n+      return\n \n     # Read parameters from config file\n     vis_ccd_row      = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDRow')) # 6\n",
                            "VIS_gather_ccd: add new configuration option 'GatherCCD' which allows to switch between per-CCD (when False, default) and per-quadrant (when True) VIS_ProcessField",
                            "Sylvain Mottet",
                            "2023-08-24T16:43:15.000+02:00",
                            "9e9cc4429bf90c14fb6fc27e621977450c24053e"
                        ],
                        [
                            "@@ -330,12 +330,14 @@ def mainMethod(args):\n       # and adds the CCD to the FPA plot image\n       fpa_plot.plot_ext( ccd_science.data, ccd_science.header)\n       # and produce a background FITS using NoiseChisel\n-      try:\n-        # ignore exceptions, this is just test code for now\n-        bkg, det = VIS_Gnuastro.NoiseChisel_array_to_array( ccd_science.data)\n-        fits.append( ncbkg_filename, bkg, header=ccd_science.header, verify=False)\n-      except:\n-        logger.info( \"CAUGHT AN EXCEPTION IN VIS_Gnuastro.NoiseChisel_array_to_array\")\n+\n+# SM 17 aug 23: disabled until useful\n+#      try:\n+#        # ignore exceptions, this is just test code for now\n+#        bkg, det = VIS_Gnuastro.NoiseChisel_array_to_array( ccd_science.data)\n+#        fits.append( ncbkg_filename, bkg, header=ccd_science.header, verify=False)\n+#      except:\n+#        logger.info( \"CAUGHT AN EXCEPTION IN VIS_Gnuastro.NoiseChisel_array_to_array\")\n \n \n     # remove temporary files\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_gather_quadrants.py": [
                        [
                            "@@ -65,24 +65,12 @@ def mainMethod(args):\n     pipe_tools.log_task_environment( logger)\n     logger.info( '#')\n \n-    logdir          = os.path.join( args.workdir, args.logdir)\n-    config_filename = os.path.join( args.workdir, args.config)\n-    input_quads     = os.path.join( args.workdir, args.input_quads)\n-    exposure        = os.path.join( args.workdir, args.exposure)\n-\n-    # Check that configuration file exists\n-    if ( os.path.isfile(config_filename) == False ):\n-        logger.error(\"%s does not exist or is not readable\", config_filename)\n-        exit (1)\n-    # Instantiate the configuration file parser\n-    config = RawConfigParser()\n-    # Make it case-sensitive for parameter names\n-    config.optionxform = str\n-    # Open the configuration file\n-    config.read (config_filename)\n-\n-    # Set up logger\n-#    logger.setLevel('DEBUG')\n+    logdir      = os.path.join( args.workdir, args.logdir)\n+    input_quads = os.path.join( args.workdir, args.input_quads)\n+    exposure    = os.path.join( args.workdir, args.exposure)\n+\n+    # read config file\n+    config = FromToXML.read_config( args.config)\n \n     # Read parameters from config file\n     vis_ccd_row      = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDRow')) # 6\n@@ -224,8 +212,7 @@ def mainMethod(args):\n \n       # free mem\n       if master_flat_filename:\n-        for extname in quad_extnames:\n-          flat_hdul[extname].data = None\n+        flat_hdul[extname].data = None\n \n     # close master flat\n     if master_flat_filename:\n@@ -299,7 +286,6 @@ def mainMethod(args):\n         flagmap_name = exposure_prefix + \".flags.fits\" + quad_ext\n         os.remove( flagmap_name)\n \n-\n     # save output exposure list\n     FromToXML.write_json( os.path.basename( exposure_name), exposure)\n \n",
                            "VIS_gather_ccd: add new configuration option 'GatherCCD' which allows to switch between per-CCD (when False, default) and per-quadrant (when True) VIS_ProcessField",
                            "Sylvain Mottet",
                            "2023-08-24T16:43:15.000+02:00",
                            "9e9cc4429bf90c14fb6fc27e621977450c24053e"
                        ],
                        [
                            "@@ -0,0 +1,332 @@\n+\"\"\"\n+@file: python/Tasks/VIS_gather_quadrants.py\n+@author: mottet@iap\n+@date: 2023-08-23\n+\"\"\"\n+\n+\n+# Standard library imports\n+import argparse\n+import os\n+import sys\n+from configparser import RawConfigParser\n+import shutil\n+from glob import glob\n+\n+# Euclid specific imports\n+from astropy.io import fits\n+import numpy as np\n+import re\n+\n+# Local imports\n+from VIS_ImageTools_M.VIS_LE1_headers import get_LE1_header\n+from VIS_ImageTools_M import FlagMap\n+from VIS_ImageTools_M.FlagMap import VIS_FLAGS\n+from VIS_ImageTools_M import VIS_plot_FPA\n+from VIS_ImageTools_Gnuastro import VIS_Gnuastro\n+\n+from   VIS_PyLibrary_Common import pe_run_information\n+from   VIS_PyLibrary_Common import pipe_tools\n+import VIS_PyLibrary_Common.common_definitions as visdef\n+\n+from   VIS_Tasks_Common import FromToXML\n+import VIS_Tasks_Common.VIS_diagnostics as visdiags\n+\n+# initialise global logger\n+import logging\n+logger = logging.getLogger( __name__)\n+\n+\n+\n+#-------------------------------------------------------------------------------\n+\n+def defineSpecificProgramOptions():\n+\n+    # Inputs\n+    parser = argparse.ArgumentParser()\n+\n+    parser.add_argument('--workdir',  help='absolute path to the workdir')\n+    parser.add_argument('--logdir',   help='path to the logdir, relative to the workdir')\n+    parser.add_argument('--config',   help='configuration file path')\n+    parser.add_argument('--input_quads')\n+    # Outputs\n+    parser.add_argument('--exposure', help='MEF fits exposure file')\n+\n+    return parser\n+\n+\n+#-------------------------------------------------------------------------------\n+\n+def mainMethod(args):\n+\n+    logger.info( '#')\n+    logger.info( '# Entering %s mainMethod()' % __name__)\n+    logger.info( '# command line: ' + pipe_tools.get_erun_commandline())\n+    pipe_tools.log_task_environment( logger)\n+    logger.info( '#')\n+\n+    logdir          = os.path.join( args.workdir, args.logdir)\n+    config_filename = os.path.join( args.workdir, args.config)\n+    input_quads     = os.path.join( args.workdir, args.input_quads)\n+    exposure        = os.path.join( args.workdir, args.exposure)\n+\n+    # Check that configuration file exists\n+    if ( os.path.isfile(config_filename) == False ):\n+        logger.error(\"%s does not exist or is not readable\", config_filename)\n+        exit (1)\n+    # Instantiate the configuration file parser\n+    config = RawConfigParser()\n+    # Make it case-sensitive for parameter names\n+    config.optionxform = str\n+    # Open the configuration file\n+    config.read (config_filename)\n+\n+    # Set up logger\n+#    logger.setLevel('DEBUG')\n+\n+    # Read parameters from config file\n+    vis_ccd_row      = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDRow')) # 6\n+    vis_ccd_col      = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDColumn')) # 6\n+    charge_injection = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDChargeInjection')) # 4\n+    rm_tmpfiles  = config.getboolean( 'General', 'RemoveTempFiles', fallback=False)\n+    interpol_CIL = config.getboolean( 'General', 'InterpolCIL', fallback=False)\n+    treshold_OOLCosmicRay = config.getfloat( 'Cosmics', 'treshold_OOLCosmicRay', fallback=2.2)\n+    logger.info(\"Number of charge injection lines: %d\", charge_injection)\n+\n+    # initialise PERunInformation dictionary for Data Quality Control\n+    peri = dict()\n+\n+    # Load exposure list\n+    expquad_list = FromToXML.load_list_from_file( input_quads)\n+    assert len( expquad_list) > 0, \"ERROR: quadrant list is empty\"\n+    assert len( expquad_list) % 4 == 0, f\"ERROR: quadrant list length must be a multiple of 4 ({len( expquad_list)})\"\n+    # prefix file names with workdir\n+    for i in range( len( expquad_list)):\n+      expquad_list[i] = os.path.join( args.workdir, expquad_list[i])\n+\n+    # get master flat filename from config.cfg\n+    master_flat_filename = config.get( \"CalibDataProducts\", \"master_flat\", fallback=None)\n+    if master_flat_filename:\n+      master_flat_filename = os.path.join( args.workdir, master_flat_filename)\n+\n+    # buid output MEF file names\n+    # expquad_list[0] is something like: \"EUC_VIS_EXP_031280_01.fits_sa_xt_bs_ge_nl_cti_por_ffd.001\"\n+    exposure_name      = expquad_list[0][:-4]\n+    exposure_prefix    = exposure_name.split('.fits')[0]\n+    flagmap_filename   = exposure_prefix + \".flags.fits\"\n+    weightmap_filename = exposure_prefix + \".weight.fits\"\n+    ncbkg_filename     = exposure_prefix + \".noisechisel_bkg.fits\"\n+\n+    logger.info( \"output CCD science filename: \" + exposure_name)\n+    logger.info( \"output CCD flags filename:   \" + flagmap_filename)\n+    logger.info( \"output CCD weights filename: \" + weightmap_filename)\n+    logger.info( \"output NoiseChisel background filename: \" + ncbkg_filename)\n+    logger.info( \"\")\n+\n+    # scan workdir for input quadrant FITS files\n+    fitsfile_list = glob( exposure_name + \".[01][0-9][0-9]\")\n+    # build a list of quadrants for present CCDs only,\n+    # missing quadrants in a CCD are set to None\n+    quads_to_process = list()\n+    for iext in range( 36):\n+      ccd_quads = list()\n+      ccd_nquads = 0\n+      for iquad in range( 4):\n+        quad_ext = f\".{iext*4+iquad+1:03d}\"\n+        if exposure_name + quad_ext in fitsfile_list:\n+          ccd_quads.append( exposure_name + quad_ext)\n+          ccd_nquads += 1\n+        else:\n+          ccd_quads.append( None)\n+      if ccd_nquads > 0:\n+        quads_to_process.extend( ccd_quads)\n+    assert len( quads_to_process) % 4 == 0\n+    N_EXT = len( quads_to_process)\n+\n+    # get output MEFs primary header\n+    prim_hdr = fits.getheader( expquad_list[0], ext=0)\n+    prim_hdr[\"N_CCD\"] = N_EXT\n+\n+    # create flag, weight and background MEFs\n+    fits.HDUList( [fits.PrimaryHDU( header=prim_hdr)]).writeto( flagmap_filename,   overwrite=True)\n+    fits.HDUList( [fits.PrimaryHDU()]                ).writeto( weightmap_filename, overwrite=True)\n+    fits.HDUList( [fits.PrimaryHDU( header=prim_hdr)]).writeto( ncbkg_filename,     overwrite=True)\n+\n+    # open input master flat MEF, used to produce weight FITS\n+    if master_flat_filename:\n+      flat_hdul = fits.open( master_flat_filename, memmap=False)\n+\n+    # variable to store the number of cosmics in Flag\n+    numberCosmics = 0\n+    # variable to count the total number of pixels (excluding those in transmission error)\n+    totalPixels = 0\n+    # scan all FLAG fits\n+    for iext in range( N_EXT):\n+      \"\"\"\n+      # read existing quadrants and build missing ones\n+      quad_extnames, quad_hdulist = get_quadrant_list( quads_to_process[iext*4:iext*4+4], isFlag=True)\n+      # stick them together in a flagmap CCD\n+      ext_flagmap = quadrants2CCD_hdu( quad_hdulist, charge_injection, flag_CIL=True)\n+      FlagMap.set_invalid_flag( ext_flagmap.data)\n+      \"\"\"\n+\n+      fname = quads_to_process[iext]\n+      quad_ext = os.path.splitext( fname)[1]\n+      fname = fname.split( '.fits')[0] + \".flags.fits\" + quad_ext\n+\n+      # and append the CCD flagmap header and data to the flagmap output MEF\n+      with fits.open( fname) as quadflag_hdul:\n+        ext_flagmap = quadflag_hdul[1].copy()\n+      fits.append( flagmap_filename, ext_flagmap.data, header=ext_flagmap.header, verify=False)\n+      extname = ext_flagmap.header[\"EXTNAME\"]\n+\n+      # counting the number of cosmic rays\n+      numberCosmics +=np.count_nonzero(ext_flagmap.data & VIS_FLAGS[\"COSMIC\"])\n+      # counting the total number of pixel excluding the pixels being in transmission error\n+      totalPixels+=np.count_nonzero(ext_flagmap.data != VIS_FLAGS[\"TXERROR\"])\n+\n+      ## save FLAGS PERunInformation for DQC\n+      totpix = np.count_nonzero( np.bitwise_and( ext_flagmap.data, VIS_FLAGS[\"TXERROR\"]) == 0)\n+      peri[f\"{extname}.sat_pixel_ratio\"]    = np.count_nonzero( np.bitwise_and( ext_flagmap.data, VIS_FLAGS[\"SAT\"])) / totpix\n+      peri[f\"{extname}.cosmic_pixel_ratio\"] = np.count_nonzero( np.bitwise_and( ext_flagmap.data, VIS_FLAGS[\"COSMIC\"])) / totpix\n+\n+      \"\"\"\n+      ## process WEIGHTS\n+      if master_flat_filename:\n+        # read the 4 flat quadrant HDUs using science extension names\n+        quad_hdulist = list()\n+        for extname in quad_extnames:\n+          flat_dat = flat_hdul[extname].data\n+          flat_hdr = flat_hdul[extname].header\n+          quad_hdulist.append( fits.ImageHDU( flat_dat, flat_hdr))\n+      else:\n+        # no master flat, reuse flagmap header and set initial weightmap data to 1.0\n+        for i in range( len( quad_hdulist)):\n+          quad_hdulist[i].data[:] = 1.0\n+\"\"\"\n+\n+      # stick them together in a CCD flat\n+#      ccd_flat = quadrants2CCD_hdu( quad_hdulist, charge_injection)\n+      if master_flat_filename:\n+        ccd_flat = flat_hdul[extname]\n+      else:\n+        ccd_flat = ext_flagmap.copy()\n+        ccd_flat.data[:] = 1.0\n+      # weightmap is flat with pixels set to 0 for all INVALID flagged pixels\n+      FlagMap.set_invalid_pixels_to_zero( ccd_flat.data, ext_flagmap.data)\n+      # create the Image header\n+      w_header = fits.ImageHDU().header\n+      w_header['EXTNAME'] = extname\n+#      ccdrow, ccdcol = re.findall( \"[1-6]\", quad_extnames[0])\n+#      w_header['CCDID'] = ccdrow + ccdcol\n+      # and append the CCD weightmap header and data to the wieightmap output MEF\n+      fits.append( weightmap_filename, ccd_flat.data, header=w_header, verify=False)\n+\n+      # free mem\n+      if master_flat_filename:\n+        for extname in quad_extnames:\n+          flat_hdul[extname].data = None\n+\n+    # close master flat\n+    if master_flat_filename:\n+      flat_hdul.close()\n+\n+    # create science MEF\n+\n+    # Count of cosmics\n+    # calculate the ratio between the total number of cosmics and the total number of pixels (percentage)\n+    ratioCosmics = 0\n+    if totalPixels != 0:\n+        ratioCosmics = (numberCosmics/totalPixels)*100\n+\n+    # calculate the cosmicRayPixelDensity . ie : the density of cosmic by unity of time\n+    unit_time = prim_hdr[\"EXPTIME\"]\n+    cosmicRayPixelDensity = 0\n+    if unit_time != 0:\n+        cosmicRayPixelDensity = ratioCosmics/unit_time\n+\n+    # calculate the OOLCosmicRay . ie : True if RatioCosmics > treshold_OOLCosmicRay, False otherwise\n+    OOLCosmicRay = False\n+    if ratioCosmics > treshold_OOLCosmicRay:\n+        OOLCosmicRay = True\n+\n+    # UPDATE 'prim_hdr' to add flag stats in it\n+    prim_hdr[\"CRPIXCNT\"] = numberCosmics\n+    logger.debug('CRPIXCNT written into header')\n+    prim_hdr.append(('CRPIXFRA', ratioCosmics, '%'))\n+    logger.debug('CRPIXFRA written into header, and percentage written as a comment')\n+    prim_hdr[\"CRPIXDEN\"] = cosmicRayPixelDensity\n+    logger.debug('CRPIXDEN written into header')\n+    prim_hdr[\"CRPIXOOL\"] = OOLCosmicRay\n+    logger.debug('CRPIXOOL written into header')\n+\n+    fits.HDUList( [fits.PrimaryHDU( header=prim_hdr)]).writeto( exposure_name, overwrite=True)\n+\n+    # create output FPA_Plot object\n+    fpa_plot = VIS_plot_FPA.FPA_plot()\n+\n+    # scan all science fits\n+    for iext in range( N_EXT):\n+      \"\"\"\n+      # read existing quadrants and build missing ones\n+      quad_extnames, quad_hdulist = get_quadrant_list( quads_to_process[iext*4:iext*4+4])\n+      # stick them together in a science CCD\n+      ccd_science = quadrants2CCD_hdu( quad_hdulist, charge_injection, interpol_CIL=interpol_CIL)\n+      \"\"\"\n+      fname = quads_to_process[iext]\n+      with fits.open( fname) as quadsci_hdul:\n+        ccd_science = quadsci_hdul[1].copy()\n+      # and append the CCD science header and data to the science output MEF\n+      fits.append( exposure_name, ccd_science.data, header=ccd_science.header, verify=False)\n+      # and adds the CCD to the FPA plot image\n+#      fpa_plot.plot_ext( ccd_science.data, ccd_science.header)\n+      # and produce a background FITS using NoiseChisel\n+\n+# SM 17 aug 23: disabled until useful\n+#      try:\n+#        # ignore exceptions, this is just test code for now\n+#        bkg, det = VIS_Gnuastro.NoiseChisel_array_to_array( ccd_science.data)\n+#        fits.append( ncbkg_filename, bkg, header=ccd_science.header, verify=False)\n+#      except:\n+#        logger.info( \"CAUGHT AN EXCEPTION IN VIS_Gnuastro.NoiseChisel_array_to_array\")\n+\n+\n+    # remove temporary files\n+    if rm_tmpfiles:\n+      for iext in range( N_EXT):\n+        os.remove( expquad_list[iext])\n+        quad_ext = os.path.splitext( expquad_list[iext])[1]\n+        flagmap_name = exposure_prefix + \".flags.fits\" + quad_ext\n+        os.remove( flagmap_name)\n+\n+\n+    # save output exposure list\n+    FromToXML.write_json( os.path.basename( exposure_name), exposure)\n+\n+    # save FPA plot jpeg image\n+    try: # failsafe mode\n+      expid = visdef.get_expid_from_filename( exposure_prefix)\n+      img_filename = visdiags.diagdir_filename( args.logdir,\n+                                                visdiags.calframe_preview_filename( expid))\n+      fpa_plot.close( title=args.workdir + \"\\n\" + os.path.basename( exposure_name),\n+                      img_filename=img_filename,\n+                      allfpa=True)\n+      #visdiags.link_diags_to_data( args.logdir, img_filename)\n+    except Exception as e:\n+      logger.info( f\"!!! caught exception '{e}' in VIS_plot_FPA.py, skipping...\")\n+    del fpa_plot\n+\n+    # save PERunInformation dictionary to JSON for Data Quality Control\n+    peri_file_prefix = pipe_tools.get_vis_expid( prim_hdr)\n+    pe_name = \"VIS_satpixels_and_CR\"\n+    logger.info( f\"writing PERunInformation to {peri_file_prefix}_{pe_name}\")\n+    pe_run_information.save_peri( peri,\n+                                  peri_file_prefix,\n+                                  pe_name,\n+                                  obs_id  = prim_hdr[\"OBS_ID\"],\n+                                  dithobs = prim_hdr[\"DITHOBS\"],\n+                                  expnum  = prim_hdr[\"EXPNUM\"])\n+\n+    logger.info('#')\n+    logger.info('# Exiting %s mainMethod()' % __name__)\n+    logger.info('#')\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ],
                        [
                            "@@ -0,0 +1,332 @@\n+\"\"\"\n+@file: python/Tasks/VIS_gather_quadrants.py\n+@author: mottet@iap\n+@date: 2023-08-23\n+\"\"\"\n+\n+\n+# Standard library imports\n+import argparse\n+import os\n+import sys\n+from configparser import RawConfigParser\n+import shutil\n+from glob import glob\n+\n+# Euclid specific imports\n+from astropy.io import fits\n+import numpy as np\n+import re\n+\n+# Local imports\n+from VIS_ImageTools_M.VIS_LE1_headers import get_LE1_header\n+from VIS_ImageTools_M import FlagMap\n+from VIS_ImageTools_M.FlagMap import VIS_FLAGS\n+from VIS_ImageTools_M import VIS_plot_FPA\n+from VIS_ImageTools_Gnuastro import VIS_Gnuastro\n+\n+from   VIS_PyLibrary_Common import pe_run_information\n+from   VIS_PyLibrary_Common import pipe_tools\n+import VIS_PyLibrary_Common.common_definitions as visdef\n+\n+from   VIS_Tasks_Common import FromToXML\n+import VIS_Tasks_Common.VIS_diagnostics as visdiags\n+\n+# initialise global logger\n+import logging\n+logger = logging.getLogger( __name__)\n+\n+\n+\n+#-------------------------------------------------------------------------------\n+\n+def defineSpecificProgramOptions():\n+\n+    # Inputs\n+    parser = argparse.ArgumentParser()\n+\n+    parser.add_argument('--workdir',  help='absolute path to the workdir')\n+    parser.add_argument('--logdir',   help='path to the logdir, relative to the workdir')\n+    parser.add_argument('--config',   help='configuration file path')\n+    parser.add_argument('--input_quads')\n+    # Outputs\n+    parser.add_argument('--exposure', help='MEF fits exposure file')\n+\n+    return parser\n+\n+\n+#-------------------------------------------------------------------------------\n+\n+def mainMethod(args):\n+\n+    logger.info( '#')\n+    logger.info( '# Entering %s mainMethod()' % __name__)\n+    logger.info( '# command line: ' + pipe_tools.get_erun_commandline())\n+    pipe_tools.log_task_environment( logger)\n+    logger.info( '#')\n+\n+    logdir          = os.path.join( args.workdir, args.logdir)\n+    config_filename = os.path.join( args.workdir, args.config)\n+    input_quads     = os.path.join( args.workdir, args.input_quads)\n+    exposure        = os.path.join( args.workdir, args.exposure)\n+\n+    # Check that configuration file exists\n+    if ( os.path.isfile(config_filename) == False ):\n+        logger.error(\"%s does not exist or is not readable\", config_filename)\n+        exit (1)\n+    # Instantiate the configuration file parser\n+    config = RawConfigParser()\n+    # Make it case-sensitive for parameter names\n+    config.optionxform = str\n+    # Open the configuration file\n+    config.read (config_filename)\n+\n+    # Set up logger\n+#    logger.setLevel('DEBUG')\n+\n+    # Read parameters from config file\n+    vis_ccd_row      = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDRow')) # 6\n+    vis_ccd_col      = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDColumn')) # 6\n+    charge_injection = int( config.getfloat( 'MDB', 'SpaceSegment.Instrument.VIS.VISCCDChargeInjection')) # 4\n+    rm_tmpfiles  = config.getboolean( 'General', 'RemoveTempFiles', fallback=False)\n+    interpol_CIL = config.getboolean( 'General', 'InterpolCIL', fallback=False)\n+    treshold_OOLCosmicRay = config.getfloat( 'Cosmics', 'treshold_OOLCosmicRay', fallback=2.2)\n+    logger.info(\"Number of charge injection lines: %d\", charge_injection)\n+\n+    # initialise PERunInformation dictionary for Data Quality Control\n+    peri = dict()\n+\n+    # Load exposure list\n+    expquad_list = FromToXML.load_list_from_file( input_quads)\n+    assert len( expquad_list) > 0, \"ERROR: quadrant list is empty\"\n+    assert len( expquad_list) % 4 == 0, f\"ERROR: quadrant list length must be a multiple of 4 ({len( expquad_list)})\"\n+    # prefix file names with workdir\n+    for i in range( len( expquad_list)):\n+      expquad_list[i] = os.path.join( args.workdir, expquad_list[i])\n+\n+    # get master flat filename from config.cfg\n+    master_flat_filename = config.get( \"CalibDataProducts\", \"master_flat\", fallback=None)\n+    if master_flat_filename:\n+      master_flat_filename = os.path.join( args.workdir, master_flat_filename)\n+\n+    # buid output MEF file names\n+    # expquad_list[0] is something like: \"EUC_VIS_EXP_031280_01.fits_sa_xt_bs_ge_nl_cti_por_ffd.001\"\n+    exposure_name      = expquad_list[0][:-4]\n+    exposure_prefix    = exposure_name.split('.fits')[0]\n+    flagmap_filename   = exposure_prefix + \".flags.fits\"\n+    weightmap_filename = exposure_prefix + \".weight.fits\"\n+    ncbkg_filename     = exposure_prefix + \".noisechisel_bkg.fits\"\n+\n+    logger.info( \"output CCD science filename: \" + exposure_name)\n+    logger.info( \"output CCD flags filename:   \" + flagmap_filename)\n+    logger.info( \"output CCD weights filename: \" + weightmap_filename)\n+    logger.info( \"output NoiseChisel background filename: \" + ncbkg_filename)\n+    logger.info( \"\")\n+\n+    # scan workdir for input quadrant FITS files\n+    fitsfile_list = glob( exposure_name + \".[01][0-9][0-9]\")\n+    # build a list of quadrants for present CCDs only,\n+    # missing quadrants in a CCD are set to None\n+    quads_to_process = list()\n+    for iext in range( 36):\n+      ccd_quads = list()\n+      ccd_nquads = 0\n+      for iquad in range( 4):\n+        quad_ext = f\".{iext*4+iquad+1:03d}\"\n+        if exposure_name + quad_ext in fitsfile_list:\n+          ccd_quads.append( exposure_name + quad_ext)\n+          ccd_nquads += 1\n+        else:\n+          ccd_quads.append( None)\n+      if ccd_nquads > 0:\n+        quads_to_process.extend( ccd_quads)\n+    assert len( quads_to_process) % 4 == 0\n+    N_EXT = len( quads_to_process)\n+\n+    # get output MEFs primary header\n+    prim_hdr = fits.getheader( expquad_list[0], ext=0)\n+    prim_hdr[\"N_CCD\"] = N_EXT\n+\n+    # create flag, weight and background MEFs\n+    fits.HDUList( [fits.PrimaryHDU( header=prim_hdr)]).writeto( flagmap_filename,   overwrite=True)\n+    fits.HDUList( [fits.PrimaryHDU()]                ).writeto( weightmap_filename, overwrite=True)\n+    fits.HDUList( [fits.PrimaryHDU( header=prim_hdr)]).writeto( ncbkg_filename,     overwrite=True)\n+\n+    # open input master flat MEF, used to produce weight FITS\n+    if master_flat_filename:\n+      flat_hdul = fits.open( master_flat_filename, memmap=False)\n+\n+    # variable to store the number of cosmics in Flag\n+    numberCosmics = 0\n+    # variable to count the total number of pixels (excluding those in transmission error)\n+    totalPixels = 0\n+    # scan all FLAG fits\n+    for iext in range( N_EXT):\n+      \"\"\"\n+      # read existing quadrants and build missing ones\n+      quad_extnames, quad_hdulist = get_quadrant_list( quads_to_process[iext*4:iext*4+4], isFlag=True)\n+      # stick them together in a flagmap CCD\n+      ext_flagmap = quadrants2CCD_hdu( quad_hdulist, charge_injection, flag_CIL=True)\n+      FlagMap.set_invalid_flag( ext_flagmap.data)\n+      \"\"\"\n+\n+      fname = quads_to_process[iext]\n+      quad_ext = os.path.splitext( fname)[1]\n+      fname = fname.split( '.fits')[0] + \".flags.fits\" + quad_ext\n+\n+      # and append the CCD flagmap header and data to the flagmap output MEF\n+      with fits.open( fname) as quadflag_hdul:\n+        ext_flagmap = quadflag_hdul[1].copy()\n+      fits.append( flagmap_filename, ext_flagmap.data, header=ext_flagmap.header, verify=False)\n+      extname = ext_flagmap.header[\"EXTNAME\"]\n+\n+      # counting the number of cosmic rays\n+      numberCosmics +=np.count_nonzero(ext_flagmap.data & VIS_FLAGS[\"COSMIC\"])\n+      # counting the total number of pixel excluding the pixels being in transmission error\n+      totalPixels+=np.count_nonzero(ext_flagmap.data != VIS_FLAGS[\"TXERROR\"])\n+\n+      ## save FLAGS PERunInformation for DQC\n+      totpix = np.count_nonzero( np.bitwise_and( ext_flagmap.data, VIS_FLAGS[\"TXERROR\"]) == 0)\n+      peri[f\"{extname}.sat_pixel_ratio\"]    = np.count_nonzero( np.bitwise_and( ext_flagmap.data, VIS_FLAGS[\"SAT\"])) / totpix\n+      peri[f\"{extname}.cosmic_pixel_ratio\"] = np.count_nonzero( np.bitwise_and( ext_flagmap.data, VIS_FLAGS[\"COSMIC\"])) / totpix\n+\n+      \"\"\"\n+      ## process WEIGHTS\n+      if master_flat_filename:\n+        # read the 4 flat quadrant HDUs using science extension names\n+        quad_hdulist = list()\n+        for extname in quad_extnames:\n+          flat_dat = flat_hdul[extname].data\n+          flat_hdr = flat_hdul[extname].header\n+          quad_hdulist.append( fits.ImageHDU( flat_dat, flat_hdr))\n+      else:\n+        # no master flat, reuse flagmap header and set initial weightmap data to 1.0\n+        for i in range( len( quad_hdulist)):\n+          quad_hdulist[i].data[:] = 1.0\n+\"\"\"\n+\n+      # stick them together in a CCD flat\n+#      ccd_flat = quadrants2CCD_hdu( quad_hdulist, charge_injection)\n+      if master_flat_filename:\n+        ccd_flat = flat_hdul[extname]\n+      else:\n+        ccd_flat = ext_flagmap.copy()\n+        ccd_flat.data[:] = 1.0\n+      # weightmap is flat with pixels set to 0 for all INVALID flagged pixels\n+      FlagMap.set_invalid_pixels_to_zero( ccd_flat.data, ext_flagmap.data)\n+      # create the Image header\n+      w_header = fits.ImageHDU().header\n+      w_header['EXTNAME'] = extname\n+#      ccdrow, ccdcol = re.findall( \"[1-6]\", quad_extnames[0])\n+#      w_header['CCDID'] = ccdrow + ccdcol\n+      # and append the CCD weightmap header and data to the wieightmap output MEF\n+      fits.append( weightmap_filename, ccd_flat.data, header=w_header, verify=False)\n+\n+      # free mem\n+      if master_flat_filename:\n+        for extname in quad_extnames:\n+          flat_hdul[extname].data = None\n+\n+    # close master flat\n+    if master_flat_filename:\n+      flat_hdul.close()\n+\n+    # create science MEF\n+\n+    # Count of cosmics\n+    # calculate the ratio between the total number of cosmics and the total number of pixels (percentage)\n+    ratioCosmics = 0\n+    if totalPixels != 0:\n+        ratioCosmics = (numberCosmics/totalPixels)*100\n+\n+    # calculate the cosmicRayPixelDensity . ie : the density of cosmic by unity of time\n+    unit_time = prim_hdr[\"EXPTIME\"]\n+    cosmicRayPixelDensity = 0\n+    if unit_time != 0:\n+        cosmicRayPixelDensity = ratioCosmics/unit_time\n+\n+    # calculate the OOLCosmicRay . ie : True if RatioCosmics > treshold_OOLCosmicRay, False otherwise\n+    OOLCosmicRay = False\n+    if ratioCosmics > treshold_OOLCosmicRay:\n+        OOLCosmicRay = True\n+\n+    # UPDATE 'prim_hdr' to add flag stats in it\n+    prim_hdr[\"CRPIXCNT\"] = numberCosmics\n+    logger.debug('CRPIXCNT written into header')\n+    prim_hdr.append(('CRPIXFRA', ratioCosmics, '%'))\n+    logger.debug('CRPIXFRA written into header, and percentage written as a comment')\n+    prim_hdr[\"CRPIXDEN\"] = cosmicRayPixelDensity\n+    logger.debug('CRPIXDEN written into header')\n+    prim_hdr[\"CRPIXOOL\"] = OOLCosmicRay\n+    logger.debug('CRPIXOOL written into header')\n+\n+    fits.HDUList( [fits.PrimaryHDU( header=prim_hdr)]).writeto( exposure_name, overwrite=True)\n+\n+    # create output FPA_Plot object\n+    fpa_plot = VIS_plot_FPA.FPA_plot()\n+\n+    # scan all science fits\n+    for iext in range( N_EXT):\n+      \"\"\"\n+      # read existing quadrants and build missing ones\n+      quad_extnames, quad_hdulist = get_quadrant_list( quads_to_process[iext*4:iext*4+4])\n+      # stick them together in a science CCD\n+      ccd_science = quadrants2CCD_hdu( quad_hdulist, charge_injection, interpol_CIL=interpol_CIL)\n+      \"\"\"\n+      fname = quads_to_process[iext]\n+      with fits.open( fname) as quadsci_hdul:\n+        ccd_science = quadsci_hdul[1].copy()\n+      # and append the CCD science header and data to the science output MEF\n+      fits.append( exposure_name, ccd_science.data, header=ccd_science.header, verify=False)\n+      # and adds the CCD to the FPA plot image\n+#      fpa_plot.plot_ext( ccd_science.data, ccd_science.header)\n+      # and produce a background FITS using NoiseChisel\n+\n+# SM 17 aug 23: disabled until useful\n+#      try:\n+#        # ignore exceptions, this is just test code for now\n+#        bkg, det = VIS_Gnuastro.NoiseChisel_array_to_array( ccd_science.data)\n+#        fits.append( ncbkg_filename, bkg, header=ccd_science.header, verify=False)\n+#      except:\n+#        logger.info( \"CAUGHT AN EXCEPTION IN VIS_Gnuastro.NoiseChisel_array_to_array\")\n+\n+\n+    # remove temporary files\n+    if rm_tmpfiles:\n+      for iext in range( N_EXT):\n+        os.remove( expquad_list[iext])\n+        quad_ext = os.path.splitext( expquad_list[iext])[1]\n+        flagmap_name = exposure_prefix + \".flags.fits\" + quad_ext\n+        os.remove( flagmap_name)\n+\n+\n+    # save output exposure list\n+    FromToXML.write_json( os.path.basename( exposure_name), exposure)\n+\n+    # save FPA plot jpeg image\n+    try: # failsafe mode\n+      expid = visdef.get_expid_from_filename( exposure_prefix)\n+      img_filename = visdiags.diagdir_filename( args.logdir,\n+                                                visdiags.calframe_preview_filename( expid))\n+      fpa_plot.close( title=args.workdir + \"\\n\" + os.path.basename( exposure_name),\n+                      img_filename=img_filename,\n+                      allfpa=True)\n+      #visdiags.link_diags_to_data( args.logdir, img_filename)\n+    except Exception as e:\n+      logger.info( f\"!!! caught exception '{e}' in VIS_plot_FPA.py, skipping...\")\n+    del fpa_plot\n+\n+    # save PERunInformation dictionary to JSON for Data Quality Control\n+    peri_file_prefix = pipe_tools.get_vis_expid( prim_hdr)\n+    pe_name = \"VIS_satpixels_and_CR\"\n+    logger.info( f\"writing PERunInformation to {peri_file_prefix}_{pe_name}\")\n+    pe_run_information.save_peri( peri,\n+                                  peri_file_prefix,\n+                                  pe_name,\n+                                  obs_id  = prim_hdr[\"OBS_ID\"],\n+                                  dithobs = prim_hdr[\"DITHOBS\"],\n+                                  expnum  = prim_hdr[\"EXPNUM\"])\n+\n+    logger.info('#')\n+    logger.info('# Exiting %s mainMethod()' % __name__)\n+    logger.info('#')\n",
                            "VIS_gather_quadrants.py: first draft version allowing to run VIS_AstrometryCalibration per quadrant",
                            "Sylvain Mottet",
                            "2023-08-23T10:37:59.000+02:00",
                            "a9a0acc55312d966ab1f24f12df89b724c31b1d0"
                        ]
                    ],
                    "VIS_Ghosts/python/VIS_Ghosts/VIS_flag_ghosts.py": [
                        [
                            "@@ -91,7 +91,10 @@ def mainMethod( args):\n   logger.info( \"#\")\n \n   # write Quality Control HTML report, ghost catalogue text file and XML, and log statistics\n-  fg.write_quality_control()\n+  try:\n+    fg.write_quality_control()\n+  except Exception as e:\n+    logger.error( f\"!!! caught exception '{e}' in write_quality_control(), ignoring....\")\n   fg.write_PERunInformation()\n   fg.write_ghost_catalogue()\n   fg.write_ghost_catalogue_dp_xml()\n",
                            "VIS_process_photo_exp: update CCD specific code to be compatible with quadrants",
                            "Sylvain Mottet",
                            "2023-08-24T16:42:36.000+02:00",
                            "f50698df683a181bf4c8ddbcc9d4c764a2731c9c"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_process_photo_exp.py": [
                        [
                            "@@ -109,10 +109,8 @@ def mainMethod(args):\n         flag_count = VIS_plot_flag_counts.FlagCount()\n         for extension in range(1, len(wgt_hdul)):\n           wgt_hdr = wgt_hdul[extension].read_header()\n+          extname = wgt_hdr[\"EXTNAME\"]\n           wgt_dat = wgt_hdul[extension].read()\n-          # CAVEAT: CCDID format in weightmap is <row><col>\n-          ccd_rowcol = str(wgt_hdr[\"CCDID\"])\n-          extname = \"CCDID \" + ccd_rowcol[0] + \"-\" + ccd_rowcol[1]\n           flagmap = flg_hdul[extname].read()\n           flag_count.add_extension( flagmap)\n           FlagMap.set_invalid_pixels_to_zero( wgt_dat, flagmap)\n",
                            "VIS_process_photo_exp: update CCD specific code to be compatible with quadrants",
                            "Sylvain Mottet",
                            "2023-08-24T16:42:36.000+02:00",
                            "f50698df683a181bf4c8ddbcc9d4c764a2731c9c"
                        ]
                    ],
                    "CMakeLists.txt": [
                        [
                            "@@ -10,9 +10,9 @@ find_package(ElementsProject)\n \n elements_project( VIS_Tasks 13.1\n                   USE Elements 6.2.1\n-                      VIS_PyLibrary 3.20.1\n+                      VIS_PyLibrary 3.20.3\n                       VIS_ImageTools 3.17\n-                      VIS_CTI 5.8.1\n+                      VIS_CTI 5.9.1\n                       VIS_Instrument_Tools 0.7.0\n                       EL_Utils 1.4.0\n                       ST_DataModelTools 9.2.0\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ],
                        [
                            "@@ -10,9 +10,9 @@ find_package(ElementsProject)\n \n elements_project( VIS_Tasks 13.1\n                   USE Elements 6.2.1\n-                      VIS_PyLibrary 3.20.2\n+                      VIS_PyLibrary 3.20.3\n                       VIS_ImageTools 3.17\n-                      VIS_CTI 5.9.0\n+                      VIS_CTI 5.9.1\n                       VIS_Instrument_Tools 0.7.0\n                       EL_Utils 1.4.0\n                       ST_DataModelTools 9.2.0\n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23537 CTI memory, [ENH]#23507 common PTC library",
                            "Catherine Grenet",
                            "2023-08-23T17:17:57.000+02:00",
                            "0ed4e3afda1ba97017107eb8fec41d9c67155c12"
                        ],
                        [
                            "@@ -8,9 +8,9 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.8)\n #===============================================================================\n \n-elements_project( VIS_Tasks 13.0.13\n+elements_project( VIS_Tasks 13.0.15\n                   USE Elements 6.2.1\n-                      VIS_PyLibrary 3.20.2\n+                      VIS_PyLibrary 3.20.3\n                       VIS_ImageTools 3.16.0\n                       VIS_CTI 5.9.1\n                       VIS_Instrument_Tools 0.7.0\n",
                            "Merge branch 'release-13.0' into 'memory_requirements'",
                            "Catherine Grenet",
                            "2023-08-23T14:30:37.000+00:00",
                            "0af6dc7121fa66eca1be3e96eb9c76ed9be3a526"
                        ],
                        [
                            "@@ -10,7 +10,7 @@ find_package(ElementsProject)\n \n elements_project( VIS_Tasks 13.0.15\n                   USE Elements 6.2.1\n-                      VIS_PyLibrary 3.20.2\n+                      VIS_PyLibrary 3.20.3\n                       VIS_ImageTools 3.16.0\n                       VIS_CTI 5.9.1\n                       VIS_Instrument_Tools 0.7.0\n",
                            "[UPD]Update VIS_PyLibrary dependency to 3.20.3",
                            "Catherine Grenet",
                            "2023-08-23T16:27:22.000+02:00",
                            "6a1ba42d4720ec8d7fb6fcdd35ec83dcea0f1f8f"
                        ],
                        [
                            "@@ -8,7 +8,7 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.8)\n #===============================================================================\n \n-elements_project( VIS_Tasks 13.0.14\n+elements_project( VIS_Tasks 13.0.15\n                   USE Elements 6.2.1\n                       VIS_PyLibrary 3.20.2\n                       VIS_ImageTools 3.16.0\n",
                            "Bump version to 13.0.15",
                            "Catherine Grenet",
                            "2023-08-23T16:25:23.000+02:00",
                            "33112421bab8aa6bfb56a9e9a542066dc0893b3c"
                        ],
                        [
                            "@@ -8,7 +8,7 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.8)\n #===============================================================================\n \n-elements_project( VIS_Tasks 13.0.14\n+elements_project( VIS_Tasks 13.0.13\n                   USE Elements 6.2.1\n                       VIS_PyLibrary 3.20.2\n                       VIS_ImageTools 3.16.0\n",
                            "revert VIS_Tasks to 13.0.13",
                            "James Nightingale",
                            "2023-08-23T11:51:21.000+02:00",
                            "ebef3ea9872ec61b02069916dc577bc059c42a43"
                        ],
                        [
                            "@@ -8,7 +8,7 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.8)\n #===============================================================================\n \n-elements_project( VIS_Tasks 13.0.13\n+elements_project( VIS_Tasks 13.0.14\n                   USE Elements 6.2.1\n                       VIS_PyLibrary 3.20.2\n                       VIS_ImageTools 3.16.0\n",
                            "revert Tasks to 13.0.14 for one more local test run",
                            "James Nightingale",
                            "2023-08-22T19:48:43.000+02:00",
                            "c0f148cd181c3cbeca7a15ccd27e660f2eec4da3"
                        ],
                        [
                            "@@ -8,7 +8,7 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.8)\n #===============================================================================\n \n-elements_project( VIS_Tasks 13.0.14\n+elements_project( VIS_Tasks 13.0.13\n                   USE Elements 6.2.1\n                       VIS_PyLibrary 3.20.2\n                       VIS_ImageTools 3.16.0\n",
                            "update CMake",
                            "James Nightingale",
                            "2023-08-22T19:47:44.000+02:00",
                            "ce140b77efd4b42f8163f534acac5f6eabf7af8a"
                        ]
                    ],
                    "VIS_Blooming/python/VIS_Blooming/VIS_blooming_calib.py": [
                        [
                            "@@ -35,6 +35,9 @@ import tarfile\n \n from configparser import RawConfigParser\n \n+# pdf containing the png files\n+from matplotlib.backends.backend_pdf import PdfPages\n+\n import numpy as np\n from astropy.io import fits\n \n@@ -120,25 +123,27 @@ def mainMethod(args):\n     for item_flg in heap_flg_list:\n       if list_flg_filename[-1] != item_flg[:-4]:\n         list_flg_filename.append(item_flg[:-4])\n-\n-    # threshold representing the max value acceptable for a mean of flagmap\n-    # the more the value is high, the more the flagmap contains data, and the less the corresponding quadrant has\n-    max_flat_invalid_pixels = config.getint( 'BloomingCalib', 'max_flat_invalid_pixels', fallback=97490)\n-\n-    # 144 values, one per quadrant\n+    \n+    # 144 values, one per quadrant, representing the blooming threshold in electrons\n     blooming_threshold_per_quadrant = np.empty(144)\n-    blooming_threshold_per_quadrant[:] = 65535\n+    blooming_threshold_per_quadrant[:] = 192000.0\n \n     blooming_json_dict = dict()\n     blooming_json_dict['model_name'] = \"VIS_blooming_calib\"\n     blooming_json_dict['date'] = time.ctime()\n     blooming_json_dict['source'] = \"\"\n     blooming_json_dict['author'] = __file__\n-    blooming_json_dict['units'] = \"e-/ADU\"   \n-\n-\n+    blooming_json_dict['units'] = \"e-\"   \n \n+    # contains the 144 json files that contain the PTC data\n+    #ptc_json_list = list()\n+    ptc_json_dict_list = list()\n     blooming_model_file = \"\"\n+    \n+    # pdf file to store all the images\n+    output_PTC_graphs_filename = os.path.join( workdir+\"/data/\", 'PTC_graphs.pdf' )\n+    pdf = PdfPages(output_PTC_graphs_filename)\n+\n     for ext in range(0, nquad):\n        logger.info (\"==>Processing quadrant %s\", ext)\n \n@@ -146,38 +151,19 @@ def mainMethod(args):\n        # so adding the extension number to the individual filename is the pile\n        detrended_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_data_filename]\n        flag_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_flg_filename]\n-\n-       # if the flagmap associated with a quadrant contains too much data,\n-       # it means that the quadrant has been too much flagged, possibly because it is too much saturated\n-       # so we remove the quadrant from the list (and remove the corresponding quadrant on the other list as well)\n-       for i in  range(len(flag_quadrant_list)-1, -1, -1):\n-           # open up fits filename\n-           hdu_flag = fits.open(flag_quadrant_list[i], memmap=False)\n-           # get the data\n-           hdu_data = hdu_flag[1].data\n-           mean_flag_quadrant = np.mean(hdu_data)\n-           # removing the quadrant from the list\n-           if mean_flag_quadrant > max_flat_invalid_pixels:\n-               logger.info(\"From detrended_quadrant_list, removing %s\", detrended_quadrant_list[i])\n-               detrended_quadrant_list.remove(detrended_quadrant_list[i])\n-               flag_quadrant_list.remove(flag_quadrant_list[i])\n-           hdu_flag.close()\n-\n-\n+       \n        # we call the pairing_flats function to get two lists\n        list1, list2 = pairing_flats(detrended_quadrant_list, config, False)\n        list1_flg, list2_flg = pairing_flats(flag_quadrant_list, config, True)\n \n        logger.info(list1)\n        logger.info(list2)\n-       image3D_flag_1 = fpe.file_layer_entry_point(list1, \"\", in_flagmap=list1_flg)\n-       image3D_flag_2 = fpe.file_layer_entry_point(list2, \"\", in_flagmap=list2_flg)\n+       image3D_flag_1, quadname = fpe.file_layer_entry_point(list1, \"\", in_flagmap=list1_flg)\n+       image3D_flag_2, _ = fpe.file_layer_entry_point(list2, \"\", in_flagmap=list2_flg)\n \n-       signal, variance = ptc_compute.wharper(image3D_flag_1,\n+       signal, variance, var_err = ptc_compute.wharper(image3D_flag_1,\n                                               image3D_flag_2,\n-                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300),\n-                                              ext,\n-                                              workdir)\n+                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300))\n \n        del image3D_flag_1\n        del image3D_flag_2\n@@ -199,60 +185,40 @@ def mainMethod(args):\n            if signal[i] < PTC_fit_xlimit:\n                signal_truncated.append(signal[i])\n                variance_truncated.append(variance[i])\n-       # straight line to show the non-linearity effect when comparing to PTC_fittee\n-       PTC_droite_fittee = np.polyfit(signal_truncated, variance_truncated, 1)\n-\n-       #######################\n-\n-       # computing the distance to the straight line (polynome of deg 1), to get rid off outliners\n-       list_distance_to_line = []\n-       for i in range(len(signal_truncated)):\n-           list_distance_to_line.append(math.sqrt(np.power(([np.polyval(PTC_droite_fittee, signal[i])] - variance[i]), 2)))\n-\n-       # computing the std of list_distance_to_line\n-       std_points_from_line = np.std(list_distance_to_line)\n-\n-       # get rid of outliers with sigma clipping\n-       signal_without_outliers = []\n-       variance_without_outliers = []\n-       sigma_lower = 2\n-       sigma_upper = 2\n-       for i in range(len(signal_truncated)):\n-           if ((list_distance_to_line[i] > np.mean(list_distance_to_line) - sigma_lower*np.std(list_distance_to_line)) and\n-              (list_distance_to_line[i] < np.mean(list_distance_to_line) + sigma_upper*np.std(list_distance_to_line))):\n-               signal_without_outliers.append(signal[i])\n-               variance_without_outliers.append(variance[i])\n-\n+       # straight line to show the non-linearity effect when comparing to PTC_fit\n+       straight_PTC_fit = np.polyfit(signal_truncated, variance_truncated, 1)\n \n        #######################\n \n-       # fit of the actual PTC\n-       PTC_fittee = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       # fit of the truncated PTC\n+       PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n \n        #######################\n \n        # computing the distance of the points to the curved fitted PTC\n        threshold_distance = 0\n-       for i in range(len(signal_without_outliers)):\n-           threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fittee, signal[i])\n+       for i in range(len(signal)):\n+           threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n-           distance_PTC = math.sqrt(np.power(([np.polyval(PTC_fittee, signal_without_outliers[i])] - variance_without_outliers[i]), 2))\n-           if distance_PTC > threshold_distance and signal_without_outliers[i] > PTC_fit_xlimit:\n-               logger.info(\"Blooming threshold found at : %s\", signal_without_outliers[i])\n-               blooming_threshold_per_quadrant[ext] = signal_without_outliers[i]\n+           # the distance is not an abs value, because we only search for a positive distance (ie :when the real data is smaller than the data of the ptc curve)\n+           # because the blooming effect is caracterized by a \"fall\" of the ptc data\n+           distance_PTC = [np.polyval(PTC_fit, signal[i])] - variance[i]\n+           if distance_PTC > threshold_distance and signal[i] > PTC_fit_xlimit:\n+               logger.info(\"Blooming threshold found at : %s\", signal[i])\n+               blooming_threshold_per_quadrant[ext] = signal[i] * config.getfloat( \"GainPerQuad\", quadname)\n                break\n \n        #######################\n \n        # calcul of the gain : get the slope of the straight line\n-       res = linregress(signal_without_outliers, variance_without_outliers)\n-       gain = 1/linregress(signal_without_outliers, variance_without_outliers)[0]\n+       res = linregress(signal, variance)\n+       gain = 1/linregress(signal, variance)[0]\n \n        logger.info(f\"GAIN IS : {gain}\")\n \n        #######################\n \n-       # print PTC_fittee\n+       # print PTC_fit\n        subdir = workdir+\"/Truncated_PTCs_graph_fit/\"\n        if not os.path.exists(subdir):\n            os.mkdir(subdir)\n@@ -261,52 +227,65 @@ def mainMethod(args):\n        plt.figure(ext)\n \n        fig, ax_scr = plt.subplots()\n-       x_series = np.linspace(signal_without_outliers[0],55000,len(signal_without_outliers))\n+       x_series = np.linspace(signal[0],55000,len(signal))\n \n        lin = res.slope*x_series+res.intercept\n-       diff_nl = ([np.polyval(PTC_fittee, i) for i in x_series] - lin) / lin * 100\n+       diff_nl = ([np.polyval(PTC_fit, i) for i in x_series] - lin) / lin * 100\n        #logger.info(diff_nl)\n        #logger.info(lin)\n-       #logger.info([np.polyval(PTC_fittee, i) for i in x_series])\n+       #logger.info([np.polyval(PTC_fit, i) for i in x_series])\n \n        #ax_scr.scatter(signal, variance, s = 10)\n-       ax_scr.scatter(signal_without_outliers, variance_without_outliers, s = 10)\n+       ax_scr.scatter(signal, variance, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n-       ax_scr.plot(x_series, [np.polyval(PTC_fittee, i) for i in x_series], 'g', label = 'Truncated PTC fitted')\n-       ax_scr.set_xlabel(\"signal\")\n-       ax_scr.set_ylabel(\"variance\")\n+       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fit')\n+       ax_scr.set_xlabel(\"signal (adu)\")\n+       ax_scr.set_ylabel(\"variance (adu)\")\n \n        plt.grid(axis='both')\n \n \n        ax_vic = ax_scr.twinx()\n        ax_vic.plot(x_series, diff_nl, 'y', label = 'diff PTC - lin')\n-       ax_vic.set_ylabel( \"error between PTC and lin\", color='y')\n-\n+       ax_vic.set_ylabel( \"error between PTC and lin (percent)\", color='y')\n+       ax_scr.legend(bbox_to_anchor=(0.2, 1.15))\n+       #ax_vic.legend()\n+       # on the graph, we want the value in adu\n+       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), \n+                   color = 'b', \n+                   label =f'blooming thr = {round(blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname))} adu')\n+\n+       #plt.legend(bbox_to_anchor=(1.15, 1.15))\n+       plt.legend()\n \n-       plt.axvline(x = blooming_threshold_per_quadrant[ext], color = 'b')\n+       plt.title(\"PTC_quadrant_\"+quadname)\n+       plt.savefig(subdir + \"PTC_quadrant_\"+quadname+\".png\")\n+       logger.info(\"saving of PTC_quadrant_\"+quadname+\".png fit\")\n \n-       plt.legend()\n-       ax_scr.legend()\n+       pdf.savefig(fig)\n \n-       plt.title(\"PTC_quadrant_\"+str(ext))\n-       plt.savefig(subdir + \"PTC_quadrant_\"+str(ext)+\".png\")\n-       logger.info(\"saving of PTC_quadrant_\"+str(ext)+\".png fitted\")\n \n        #############################################################################################\n        # creation of the calibration product containing blooming threshold for all the quadrants\n        #############################################################################################\n-\n-       hdu = fits.open(detrended_quadrant_list[0], memmap=False)\n-       quadname = hdu[1].header[\"EXTNAME\"]\n-       hdu.close()\n-\n+       \n        # log of results\n-       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]}\")\n+       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]} in electrons\")\n        \n        #### update json file with blooming threshold estimation ####\n        blooming_json_dict[quadname] = blooming_threshold_per_quadrant[ext]\n+       \n+       # creation of a json file to be put in the data product\n+       ptc_json_dict = dict()\n+\n+       ptc_json_dict[\"Quadrant name\"] = quadname\n+       ptc_json_dict[\"units\"] = \"adu\"\n+       ptc_json_dict[\"signal\"] = \"(variance, variance error)\"\n+       for elmt_signal, elmt_var, elmt_var_err in zip(signal, variance, var_err):\n+         ptc_json_dict[elmt_signal] = (elmt_var, elmt_var_err)\n+\n+       ptc_json_dict_list.append(ptc_json_dict)\n \n     # Write end data product\n \n@@ -324,16 +303,22 @@ def mainMethod(args):\n \n     data_product = FromToXML.create_DpdVisBloomingModel( os.path.basename( blooming_model_file))\n \n-    # tar of the 144 png images into one single file, that will be added to the calibration product into Data.\n-    output_PTC_graphs_filename = \"PTC_graphs.tar\"\n-\n-    with tarfile.open(output_PTC_graphs_filename, \"w:gz\") as tar:\n-      for filename in os.listdir(subdir):\n-        tar.add(os.path.join(subdir, filename), arcname=filename)\n \n+    # Closing pdf\n+    pdf.close()\n     # adding to the data_product, inside Data.QualityParameterStorage, the file output_PTC_graphs_filename\n     FromToXML.add_quality_parameter_file(data_product, output_PTC_graphs_filename)\n \n+    DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n+    \n+    ptc_file = workdir+\"/data/list-PTCs.json\"\n+    with open(ptc_file, 'w') as fp:\n+       for elmt in ptc_json_dict_list:\n+        fp.write(json.dumps(elmt, indent=2, default=np.ndarray.tolist))  \n+\n+\n+    FromToXML.add_quality_parameter_file(data_product, ptc_file)\n+\n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n \n     logger.info(blooming_threshold_per_quadrant)\n@@ -341,4 +326,4 @@ def mainMethod(args):\n     logger.info( \"#\")\n     logger.info( \"# Exiting %s mainMethod()\", __name__)\n     logger.info( \"#\")\n-\n+    \n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ],
                        [
                            "@@ -123,11 +123,7 @@ def mainMethod(args):\n     for item_flg in heap_flg_list:\n       if list_flg_filename[-1] != item_flg[:-4]:\n         list_flg_filename.append(item_flg[:-4])\n-\n-    # threshold representing the max value acceptable for a mean of flagmap\n-    # the more the value is high, the more the flagmap contains data, and the less the corresponding quadrant has\n-    max_flat_invalid_pixels = config.getint( 'BloomingCalib', 'max_flat_invalid_pixels', fallback=97490)\n-\n+    \n     # 144 values, one per quadrant, representing the blooming threshold in electrons\n     blooming_threshold_per_quadrant = np.empty(144)\n     blooming_threshold_per_quadrant[:] = 192000.0\n@@ -165,11 +161,9 @@ def mainMethod(args):\n        image3D_flag_1, quadname = fpe.file_layer_entry_point(list1, \"\", in_flagmap=list1_flg)\n        image3D_flag_2, _ = fpe.file_layer_entry_point(list2, \"\", in_flagmap=list2_flg)\n \n-       signal, variance = ptc_compute.wharper(image3D_flag_1,\n+       signal, variance, var_err = ptc_compute.wharper(image3D_flag_1,\n                                               image3D_flag_2,\n-                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300),\n-                                              ext,\n-                                              workdir)\n+                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300))\n \n        del image3D_flag_1\n        del image3D_flag_2\n@@ -196,54 +190,29 @@ def mainMethod(args):\n \n        #######################\n \n-       # computing the distance to the straight line (polynome of deg 1), to get rid off outliners\n-       list_distance_to_line = []\n-       for i in range(len(signal_truncated)):\n-           list_distance_to_line.append(math.sqrt(np.power(([np.polyval(straight_PTC_fit, signal[i])] - variance[i]), 2)))\n-\n-       # computing the std of list_distance_to_line\n-       std_points_from_line = np.std(list_distance_to_line)\n-\n-       # get rid of outliers with sigma clipping\n-       signal_without_outliers = []\n-       variance_without_outliers = []\n-       sigma_lower = 2\n-       sigma_upper = 2\n-       for i in range(len(signal_truncated)):\n-           if ((list_distance_to_line[i] > np.mean(list_distance_to_line) - sigma_lower*np.std(list_distance_to_line)) and\n-              (list_distance_to_line[i] < np.mean(list_distance_to_line) + sigma_upper*np.std(list_distance_to_line))):\n-               signal_without_outliers.append(signal[i])\n-               variance_without_outliers.append(variance[i])\n-       \n-\n-       #######################\n-\n-       # fit of the actual PTC\n-       #PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n        # fit of the truncated PTC\n        PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n \n-\n        #######################\n \n        # computing the distance of the points to the curved fitted PTC\n        threshold_distance = 0\n-       for i in range(len(signal_without_outliers)):\n+       for i in range(len(signal)):\n            threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n            # the distance is not an abs value, because we only search for a positive distance (ie :when the real data is smaller than the data of the ptc curve)\n            # because the blooming effect is caracterized by a \"fall\" of the ptc data\n-           distance_PTC = [np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]\n-           if distance_PTC > threshold_distance and signal_without_outliers[i] > PTC_fit_xlimit:\n-               logger.info(\"Blooming threshold found at : %s\", signal_without_outliers[i])\n-               blooming_threshold_per_quadrant[ext] = signal_without_outliers[i] * config.getfloat( \"GainPerQuad\", quadname)\n+           distance_PTC = [np.polyval(PTC_fit, signal[i])] - variance[i]\n+           if distance_PTC > threshold_distance and signal[i] > PTC_fit_xlimit:\n+               logger.info(\"Blooming threshold found at : %s\", signal[i])\n+               blooming_threshold_per_quadrant[ext] = signal[i] * config.getfloat( \"GainPerQuad\", quadname)\n                break\n \n        #######################\n \n        # calcul of the gain : get the slope of the straight line\n-       res = linregress(signal_without_outliers, variance_without_outliers)\n-       gain = 1/linregress(signal_without_outliers, variance_without_outliers)[0]\n+       res = linregress(signal, variance)\n+       gain = 1/linregress(signal, variance)[0]\n \n        logger.info(f\"GAIN IS : {gain}\")\n \n@@ -258,7 +227,7 @@ def mainMethod(args):\n        plt.figure(ext)\n \n        fig, ax_scr = plt.subplots()\n-       x_series = np.linspace(signal_without_outliers[0],55000,len(signal_without_outliers))\n+       x_series = np.linspace(signal[0],55000,len(signal))\n \n        lin = res.slope*x_series+res.intercept\n        diff_nl = ([np.polyval(PTC_fit, i) for i in x_series] - lin) / lin * 100\n@@ -267,7 +236,7 @@ def mainMethod(args):\n        #logger.info([np.polyval(PTC_fit, i) for i in x_series])\n \n        #ax_scr.scatter(signal, variance, s = 10)\n-       ax_scr.scatter(signal_without_outliers, variance_without_outliers, s = 10)\n+       ax_scr.scatter(signal, variance, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n        ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fit')\n@@ -309,11 +278,12 @@ def mainMethod(args):\n        \n        # creation of a json file to be put in the data product\n        ptc_json_dict = dict()\n+\n        ptc_json_dict[\"Quadrant name\"] = quadname\n        ptc_json_dict[\"units\"] = \"adu\"\n-       ptc_json_dict[\"signal\"] = \"variance\"\n-       for elmt_signal, elmt_var in zip(signal_without_outliers, variance_without_outliers):\n-         ptc_json_dict[elmt_signal] = elmt_var\n+       ptc_json_dict[\"signal\"] = \"(variance, variance error)\"\n+       for elmt_signal, elmt_var, elmt_var_err in zip(signal, variance, var_err):\n+         ptc_json_dict[elmt_signal] = (elmt_var, elmt_var_err)\n \n        ptc_json_dict_list.append(ptc_json_dict)\n \n",
                            "Merge branch 'release-13.0' into develop: [ENH]#23620 Blooming use common PTC library",
                            "Catherine Grenet",
                            "2023-08-24T09:24:50.000+02:00",
                            "26bc1f542f45b7b9b63f436f45e51496dd0364f7"
                        ],
                        [
                            "@@ -123,11 +123,7 @@ def mainMethod(args):\n     for item_flg in heap_flg_list:\n       if list_flg_filename[-1] != item_flg[:-4]:\n         list_flg_filename.append(item_flg[:-4])\n-\n-    # threshold representing the max value acceptable for a mean of flagmap\n-    # the more the value is high, the more the flagmap contains data, and the less the corresponding quadrant has\n-    max_flat_invalid_pixels = config.getint( 'BloomingCalib', 'max_flat_invalid_pixels', fallback=97490)\n-\n+    \n     # 144 values, one per quadrant, representing the blooming threshold in electrons\n     blooming_threshold_per_quadrant = np.empty(144)\n     blooming_threshold_per_quadrant[:] = 192000.0\n@@ -165,11 +161,9 @@ def mainMethod(args):\n        image3D_flag_1, quadname = fpe.file_layer_entry_point(list1, \"\", in_flagmap=list1_flg)\n        image3D_flag_2, _ = fpe.file_layer_entry_point(list2, \"\", in_flagmap=list2_flg)\n \n-       signal, variance = ptc_compute.wharper(image3D_flag_1,\n+       signal, variance, var_err = ptc_compute.wharper(image3D_flag_1,\n                                               image3D_flag_2,\n-                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300),\n-                                              ext,\n-                                              workdir)\n+                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300))\n \n        del image3D_flag_1\n        del image3D_flag_2\n@@ -196,54 +190,29 @@ def mainMethod(args):\n \n        #######################\n \n-       # computing the distance to the straight line (polynome of deg 1), to get rid off outliners\n-       list_distance_to_line = []\n-       for i in range(len(signal_truncated)):\n-           list_distance_to_line.append(math.sqrt(np.power(([np.polyval(straight_PTC_fit, signal[i])] - variance[i]), 2)))\n-\n-       # computing the std of list_distance_to_line\n-       std_points_from_line = np.std(list_distance_to_line)\n-\n-       # get rid of outliers with sigma clipping\n-       signal_without_outliers = []\n-       variance_without_outliers = []\n-       sigma_lower = 2\n-       sigma_upper = 2\n-       for i in range(len(signal_truncated)):\n-           if ((list_distance_to_line[i] > np.mean(list_distance_to_line) - sigma_lower*np.std(list_distance_to_line)) and\n-              (list_distance_to_line[i] < np.mean(list_distance_to_line) + sigma_upper*np.std(list_distance_to_line))):\n-               signal_without_outliers.append(signal[i])\n-               variance_without_outliers.append(variance[i])\n-       \n-\n-       #######################\n-\n-       # fit of the actual PTC\n-       #PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n        # fit of the truncated PTC\n        PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n \n-\n        #######################\n \n        # computing the distance of the points to the curved fitted PTC\n        threshold_distance = 0\n-       for i in range(len(signal_without_outliers)):\n+       for i in range(len(signal)):\n            threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n            # the distance is not an abs value, because we only search for a positive distance (ie :when the real data is smaller than the data of the ptc curve)\n            # because the blooming effect is caracterized by a \"fall\" of the ptc data\n-           distance_PTC = [np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]\n-           if distance_PTC > threshold_distance and signal_without_outliers[i] > PTC_fit_xlimit:\n-               logger.info(\"Blooming threshold found at : %s\", signal_without_outliers[i])\n-               blooming_threshold_per_quadrant[ext] = signal_without_outliers[i] * config.getfloat( \"GainPerQuad\", quadname)\n+           distance_PTC = [np.polyval(PTC_fit, signal[i])] - variance[i]\n+           if distance_PTC > threshold_distance and signal[i] > PTC_fit_xlimit:\n+               logger.info(\"Blooming threshold found at : %s\", signal[i])\n+               blooming_threshold_per_quadrant[ext] = signal[i] * config.getfloat( \"GainPerQuad\", quadname)\n                break\n \n        #######################\n \n        # calcul of the gain : get the slope of the straight line\n-       res = linregress(signal_without_outliers, variance_without_outliers)\n-       gain = 1/linregress(signal_without_outliers, variance_without_outliers)[0]\n+       res = linregress(signal, variance)\n+       gain = 1/linregress(signal, variance)[0]\n \n        logger.info(f\"GAIN IS : {gain}\")\n \n@@ -258,7 +227,7 @@ def mainMethod(args):\n        plt.figure(ext)\n \n        fig, ax_scr = plt.subplots()\n-       x_series = np.linspace(signal_without_outliers[0],55000,len(signal_without_outliers))\n+       x_series = np.linspace(signal[0],55000,len(signal))\n \n        lin = res.slope*x_series+res.intercept\n        diff_nl = ([np.polyval(PTC_fit, i) for i in x_series] - lin) / lin * 100\n@@ -267,7 +236,7 @@ def mainMethod(args):\n        #logger.info([np.polyval(PTC_fit, i) for i in x_series])\n \n        #ax_scr.scatter(signal, variance, s = 10)\n-       ax_scr.scatter(signal_without_outliers, variance_without_outliers, s = 10)\n+       ax_scr.scatter(signal, variance, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n        ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fit')\n@@ -309,11 +278,12 @@ def mainMethod(args):\n        \n        # creation of a json file to be put in the data product\n        ptc_json_dict = dict()\n+\n        ptc_json_dict[\"Quadrant name\"] = quadname\n        ptc_json_dict[\"units\"] = \"adu\"\n-       ptc_json_dict[\"signal\"] = \"variance\"\n-       for elmt_signal, elmt_var in zip(signal_without_outliers, variance_without_outliers):\n-         ptc_json_dict[elmt_signal] = elmt_var\n+       ptc_json_dict[\"signal\"] = \"(variance, variance error)\"\n+       for elmt_signal, elmt_var, elmt_var_err in zip(signal, variance, var_err):\n+         ptc_json_dict[elmt_signal] = (elmt_var, elmt_var_err)\n \n        ptc_json_dict_list.append(ptc_json_dict)\n \n",
                            "Merge branch 'feature_#23620_CommonPTC_Library_Blooming' into 'release-13.0'",
                            "Catherine Grenet",
                            "2023-08-24T07:19:17.000+00:00",
                            "3698966ec5bed603a2e4bc187c72063cd738c982"
                        ],
                        [
                            "@@ -35,6 +35,9 @@ import tarfile\n \n from configparser import RawConfigParser\n \n+# pdf containing the png files\n+from matplotlib.backends.backend_pdf import PdfPages\n+\n import numpy as np\n from astropy.io import fits\n \n@@ -133,9 +136,14 @@ def mainMethod(args):\n     blooming_json_dict['units'] = \"e-\"   \n \n     # contains the 144 json files that contain the PTC data\n-    ptc_json_list = list()\n-\n+    #ptc_json_list = list()\n+    ptc_json_dict_list = list()\n     blooming_model_file = \"\"\n+    \n+    # pdf file to store all the images\n+    output_PTC_graphs_filename = os.path.join( workdir+\"/data/\", 'PTC_graphs.pdf' )\n+    pdf = PdfPages(output_PTC_graphs_filename)\n+\n     for ext in range(0, nquad):\n        logger.info (\"==>Processing quadrant %s\", ext)\n \n@@ -182,7 +190,7 @@ def mainMethod(args):\n \n        #######################\n \n-       # fit of the actual PTC\n+       # fit of the truncated PTC\n        PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n \n        #######################\n@@ -192,7 +200,9 @@ def mainMethod(args):\n        for i in range(len(signal)):\n            threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n-           distance_PTC = math.sqrt(np.power(([np.polyval(PTC_fit, signal[i])] - variance[i]), 2))\n+           # the distance is not an abs value, because we only search for a positive distance (ie :when the real data is smaller than the data of the ptc curve)\n+           # because the blooming effect is caracterized by a \"fall\" of the ptc data\n+           distance_PTC = [np.polyval(PTC_fit, signal[i])] - variance[i]\n            if distance_PTC > threshold_distance and signal[i] > PTC_fit_xlimit:\n                logger.info(\"Blooming threshold found at : %s\", signal[i])\n                blooming_threshold_per_quadrant[ext] = signal[i] * config.getfloat( \"GainPerQuad\", quadname)\n@@ -229,26 +239,32 @@ def mainMethod(args):\n        ax_scr.scatter(signal, variance, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n-       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fitted')\n-       ax_scr.set_xlabel(\"signal\")\n-       ax_scr.set_ylabel(\"variance\")\n+       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fit')\n+       ax_scr.set_xlabel(\"signal (adu)\")\n+       ax_scr.set_ylabel(\"variance (adu)\")\n \n        plt.grid(axis='both')\n \n \n        ax_vic = ax_scr.twinx()\n        ax_vic.plot(x_series, diff_nl, 'y', label = 'diff PTC - lin')\n-       ax_vic.set_ylabel( \"error between PTC and lin\", color='y')\n-\n+       ax_vic.set_ylabel( \"error between PTC and lin (percent)\", color='y')\n+       ax_scr.legend(bbox_to_anchor=(0.2, 1.15))\n+       #ax_vic.legend()\n        # on the graph, we want the value in adu\n-       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), color = 'b')\n+       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), \n+                   color = 'b', \n+                   label =f'blooming thr = {round(blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname))} adu')\n \n+       #plt.legend(bbox_to_anchor=(1.15, 1.15))\n        plt.legend()\n-       ax_scr.legend()\n \n-       plt.title(\"PTC_quadrant_\"+str(ext))\n-       plt.savefig(subdir + \"PTC_quadrant_\"+str(ext)+\".png\")\n-       logger.info(\"saving of PTC_quadrant_\"+str(ext)+\".png fitted\")\n+       plt.title(\"PTC_quadrant_\"+quadname)\n+       plt.savefig(subdir + \"PTC_quadrant_\"+quadname+\".png\")\n+       logger.info(\"saving of PTC_quadrant_\"+quadname+\".png fit\")\n+\n+       pdf.savefig(fig)\n+\n \n        #############################################################################################\n        # creation of the calibration product containing blooming threshold for all the quadrants\n@@ -262,19 +278,14 @@ def mainMethod(args):\n        \n        # creation of a json file to be put in the data product\n        ptc_json_dict = dict()\n+\n+       ptc_json_dict[\"Quadrant name\"] = quadname\n+       ptc_json_dict[\"units\"] = \"adu\"\n+       ptc_json_dict[\"signal\"] = \"(variance, variance error)\"\n        for elmt_signal, elmt_var, elmt_var_err in zip(signal, variance, var_err):\n          ptc_json_dict[elmt_signal] = (elmt_var, elmt_var_err)\n \n-       ptc_json_file = FileNameProvider().get_allowed_filename( processing_function = 'VIS',\n-                                                                    type_name           = f\"PTC-QUADRANT-{ext}\",\n-                                                                    instance_id         = '',\n-                                                                    extension           = '.json')\n-       logger.info (\"==> Creating blooming calibration data product %s\", ptc_json_file)\n-       ptc_json_file = os.path.join( args.workdir, \"data\", ptc_json_file)\n-       with open(ptc_json_file, 'w') as fp:\n-         fp.write(json.dumps(ptc_json_dict, indent=2, default=np.ndarray.tolist))\n-\n-       ptc_json_list.append( os.path.basename(ptc_json_file))\n+       ptc_json_dict_list.append(ptc_json_dict)\n \n     # Write end data product\n \n@@ -292,26 +303,20 @@ def mainMethod(args):\n \n     data_product = FromToXML.create_DpdVisBloomingModel( os.path.basename( blooming_model_file))\n \n-    # tar of the 144 png images into one single file, that will be added to the calibration product into Data.\n-    output_PTC_graphs_filename = workdir+\"/data/PTC_graphs.tar\"\n-\n-    with tarfile.open(output_PTC_graphs_filename, \"w:gz\") as tar:\n-      for filename in os.listdir(subdir):\n-        #logger.info(f\"filename is : {filename}\")\n-        tar.add(os.path.join(subdir, filename), arcname=filename)\n \n+    # Closing pdf\n+    pdf.close()\n     # adding to the data_product, inside Data.QualityParameterStorage, the file output_PTC_graphs_filename\n     FromToXML.add_quality_parameter_file(data_product, output_PTC_graphs_filename)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n+    \n+    ptc_file = workdir+\"/data/list-PTCs.json\"\n+    with open(ptc_file, 'w') as fp:\n+       for elmt in ptc_json_dict_list:\n+        fp.write(json.dumps(elmt, indent=2, default=np.ndarray.tolist))  \n+\n \n-    # we add all the ptcs files into a single tar file, to be accessible at the eas\n-    ptc_file = workdir+\"/data/list-PTCs.tar\"\n-    with tarfile.open(ptc_file, \"w:gz\") as tar:\n-      for filename in ptc_json_list:\n-        logger.info(f\"in ptc_json_list, filename is : {filename}\")\n-        tar.add(os.path.join(workdir + \"/data/\", filename), arcname=filename)\n-   \n     FromToXML.add_quality_parameter_file(data_product, ptc_file)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n",
                            "Merge branch 'release-13.0' into 'feature_#23620_CommonPTC_Library_Blooming'",
                            "Thomas Flanet",
                            "2023-08-23T17:28:19.000+00:00",
                            "b662bd82e10d33df3f00405b1fe91761a847533f"
                        ],
                        [
                            "@@ -35,6 +35,9 @@ import tarfile\n \n from configparser import RawConfigParser\n \n+# pdf containing the png files\n+from matplotlib.backends.backend_pdf import PdfPages\n+\n import numpy as np\n from astropy.io import fits\n \n@@ -137,9 +140,14 @@ def mainMethod(args):\n     blooming_json_dict['units'] = \"e-\"   \n \n     # contains the 144 json files that contain the PTC data\n-    ptc_json_list = list()\n-\n+    #ptc_json_list = list()\n+    ptc_json_dict_list = list()\n     blooming_model_file = \"\"\n+    \n+    # pdf file to store all the images\n+    output_PTC_graphs_filename = os.path.join( workdir+\"/data/\", 'PTC_graphs.pdf' )\n+    pdf = PdfPages(output_PTC_graphs_filename)\n+\n     for ext in range(0, nquad):\n        logger.info (\"==>Processing quadrant %s\", ext)\n \n@@ -206,12 +214,15 @@ def mainMethod(args):\n               (list_distance_to_line[i] < np.mean(list_distance_to_line) + sigma_upper*np.std(list_distance_to_line))):\n                signal_without_outliers.append(signal[i])\n                variance_without_outliers.append(variance[i])\n-\n+       \n \n        #######################\n \n        # fit of the actual PTC\n-       PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       #PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       # fit of the truncated PTC\n+       PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n+\n \n        #######################\n \n@@ -220,7 +231,9 @@ def mainMethod(args):\n        for i in range(len(signal_without_outliers)):\n            threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n-           distance_PTC = math.sqrt(np.power(([np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]), 2))\n+           # the distance is not an abs value, because we only search for a positive distance (ie :when the real data is smaller than the data of the ptc curve)\n+           # because the blooming effect is caracterized by a \"fall\" of the ptc data\n+           distance_PTC = [np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]\n            if distance_PTC > threshold_distance and signal_without_outliers[i] > PTC_fit_xlimit:\n                logger.info(\"Blooming threshold found at : %s\", signal_without_outliers[i])\n                blooming_threshold_per_quadrant[ext] = signal_without_outliers[i] * config.getfloat( \"GainPerQuad\", quadname)\n@@ -257,26 +270,32 @@ def mainMethod(args):\n        ax_scr.scatter(signal_without_outliers, variance_without_outliers, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n-       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fitted')\n-       ax_scr.set_xlabel(\"signal\")\n-       ax_scr.set_ylabel(\"variance\")\n+       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fit')\n+       ax_scr.set_xlabel(\"signal (adu)\")\n+       ax_scr.set_ylabel(\"variance (adu)\")\n \n        plt.grid(axis='both')\n \n \n        ax_vic = ax_scr.twinx()\n        ax_vic.plot(x_series, diff_nl, 'y', label = 'diff PTC - lin')\n-       ax_vic.set_ylabel( \"error between PTC and lin\", color='y')\n-\n+       ax_vic.set_ylabel( \"error between PTC and lin (percent)\", color='y')\n+       ax_scr.legend(bbox_to_anchor=(0.2, 1.15))\n+       #ax_vic.legend()\n        # on the graph, we want the value in adu\n-       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), color = 'b')\n+       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), \n+                   color = 'b', \n+                   label =f'blooming thr = {round(blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname))} adu')\n \n+       #plt.legend(bbox_to_anchor=(1.15, 1.15))\n        plt.legend()\n-       ax_scr.legend()\n \n-       plt.title(\"PTC_quadrant_\"+str(ext))\n-       plt.savefig(subdir + \"PTC_quadrant_\"+str(ext)+\".png\")\n-       logger.info(\"saving of PTC_quadrant_\"+str(ext)+\".png fitted\")\n+       plt.title(\"PTC_quadrant_\"+quadname)\n+       plt.savefig(subdir + \"PTC_quadrant_\"+quadname+\".png\")\n+       logger.info(\"saving of PTC_quadrant_\"+quadname+\".png fit\")\n+\n+       pdf.savefig(fig)\n+\n \n        #############################################################################################\n        # creation of the calibration product containing blooming threshold for all the quadrants\n@@ -290,19 +309,13 @@ def mainMethod(args):\n        \n        # creation of a json file to be put in the data product\n        ptc_json_dict = dict()\n+       ptc_json_dict[\"Quadrant name\"] = quadname\n+       ptc_json_dict[\"units\"] = \"adu\"\n+       ptc_json_dict[\"signal\"] = \"variance\"\n        for elmt_signal, elmt_var in zip(signal_without_outliers, variance_without_outliers):\n          ptc_json_dict[elmt_signal] = elmt_var\n \n-       ptc_json_file = FileNameProvider().get_allowed_filename( processing_function = 'VIS',\n-                                                                    type_name           = f\"PTC-QUADRANT-{ext}\",\n-                                                                    instance_id         = '',\n-                                                                    extension           = '.json')\n-       logger.info (\"==> Creating blooming calibration data product %s\", ptc_json_file)\n-       ptc_json_file = os.path.join( args.workdir, \"data\", ptc_json_file)\n-       with open(ptc_json_file, 'w') as fp:\n-         fp.write(json.dumps(ptc_json_dict, indent=2, default=np.ndarray.tolist))\n-\n-       ptc_json_list.append( os.path.basename(ptc_json_file))\n+       ptc_json_dict_list.append(ptc_json_dict)\n \n     # Write end data product\n \n@@ -320,26 +333,20 @@ def mainMethod(args):\n \n     data_product = FromToXML.create_DpdVisBloomingModel( os.path.basename( blooming_model_file))\n \n-    # tar of the 144 png images into one single file, that will be added to the calibration product into Data.\n-    output_PTC_graphs_filename = workdir+\"/data/PTC_graphs.tar\"\n-\n-    with tarfile.open(output_PTC_graphs_filename, \"w:gz\") as tar:\n-      for filename in os.listdir(subdir):\n-        #logger.info(f\"filename is : {filename}\")\n-        tar.add(os.path.join(subdir, filename), arcname=filename)\n \n+    # Closing pdf\n+    pdf.close()\n     # adding to the data_product, inside Data.QualityParameterStorage, the file output_PTC_graphs_filename\n     FromToXML.add_quality_parameter_file(data_product, output_PTC_graphs_filename)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n+    \n+    ptc_file = workdir+\"/data/list-PTCs.json\"\n+    with open(ptc_file, 'w') as fp:\n+       for elmt in ptc_json_dict_list:\n+        fp.write(json.dumps(elmt, indent=2, default=np.ndarray.tolist))  \n+\n \n-    # we add all the ptcs files into a single tar file, to be accessible at the eas\n-    ptc_file = workdir+\"/data/list-PTCs.tar\"\n-    with tarfile.open(ptc_file, \"w:gz\") as tar:\n-      for filename in ptc_json_list:\n-        logger.info(f\"in ptc_json_list, filename is : {filename}\")\n-        tar.add(os.path.join(workdir + \"/data/\", filename), arcname=filename)\n-   \n     FromToXML.add_quality_parameter_file(data_product, ptc_file)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n",
                            "Merge branch 'release-13.0' into develop: [ENH]#23227 BloomingCalibration Group outputs and identify the quadrants",
                            "Catherine Grenet",
                            "2023-08-23T18:53:33.000+02:00",
                            "03cd16a1a000a945f4990fcf18712fd71ec15ecd"
                        ],
                        [
                            "@@ -35,6 +35,9 @@ import tarfile\n \n from configparser import RawConfigParser\n \n+# pdf containing the png files\n+from matplotlib.backends.backend_pdf import PdfPages\n+\n import numpy as np\n from astropy.io import fits\n \n@@ -137,9 +140,14 @@ def mainMethod(args):\n     blooming_json_dict['units'] = \"e-\"   \n \n     # contains the 144 json files that contain the PTC data\n-    ptc_json_list = list()\n-\n+    #ptc_json_list = list()\n+    ptc_json_dict_list = list()\n     blooming_model_file = \"\"\n+    \n+    # pdf file to store all the images\n+    output_PTC_graphs_filename = os.path.join( workdir+\"/data/\", 'PTC_graphs.pdf' )\n+    pdf = PdfPages(output_PTC_graphs_filename)\n+\n     for ext in range(0, nquad):\n        logger.info (\"==>Processing quadrant %s\", ext)\n \n@@ -206,12 +214,15 @@ def mainMethod(args):\n               (list_distance_to_line[i] < np.mean(list_distance_to_line) + sigma_upper*np.std(list_distance_to_line))):\n                signal_without_outliers.append(signal[i])\n                variance_without_outliers.append(variance[i])\n-\n+       \n \n        #######################\n \n        # fit of the actual PTC\n-       PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       #PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       # fit of the truncated PTC\n+       PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n+\n \n        #######################\n \n@@ -220,7 +231,9 @@ def mainMethod(args):\n        for i in range(len(signal_without_outliers)):\n            threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n-           distance_PTC = math.sqrt(np.power(([np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]), 2))\n+           # the distance is not an abs value, because we only search for a positive distance (ie :when the real data is smaller than the data of the ptc curve)\n+           # because the blooming effect is caracterized by a \"fall\" of the ptc data\n+           distance_PTC = [np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]\n            if distance_PTC > threshold_distance and signal_without_outliers[i] > PTC_fit_xlimit:\n                logger.info(\"Blooming threshold found at : %s\", signal_without_outliers[i])\n                blooming_threshold_per_quadrant[ext] = signal_without_outliers[i] * config.getfloat( \"GainPerQuad\", quadname)\n@@ -257,26 +270,32 @@ def mainMethod(args):\n        ax_scr.scatter(signal_without_outliers, variance_without_outliers, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n-       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fitted')\n-       ax_scr.set_xlabel(\"signal\")\n-       ax_scr.set_ylabel(\"variance\")\n+       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fit')\n+       ax_scr.set_xlabel(\"signal (adu)\")\n+       ax_scr.set_ylabel(\"variance (adu)\")\n \n        plt.grid(axis='both')\n \n \n        ax_vic = ax_scr.twinx()\n        ax_vic.plot(x_series, diff_nl, 'y', label = 'diff PTC - lin')\n-       ax_vic.set_ylabel( \"error between PTC and lin\", color='y')\n-\n+       ax_vic.set_ylabel( \"error between PTC and lin (percent)\", color='y')\n+       ax_scr.legend(bbox_to_anchor=(0.2, 1.15))\n+       #ax_vic.legend()\n        # on the graph, we want the value in adu\n-       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), color = 'b')\n+       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), \n+                   color = 'b', \n+                   label =f'blooming thr = {round(blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname))} adu')\n \n+       #plt.legend(bbox_to_anchor=(1.15, 1.15))\n        plt.legend()\n-       ax_scr.legend()\n \n-       plt.title(\"PTC_quadrant_\"+str(ext))\n-       plt.savefig(subdir + \"PTC_quadrant_\"+str(ext)+\".png\")\n-       logger.info(\"saving of PTC_quadrant_\"+str(ext)+\".png fitted\")\n+       plt.title(\"PTC_quadrant_\"+quadname)\n+       plt.savefig(subdir + \"PTC_quadrant_\"+quadname+\".png\")\n+       logger.info(\"saving of PTC_quadrant_\"+quadname+\".png fit\")\n+\n+       pdf.savefig(fig)\n+\n \n        #############################################################################################\n        # creation of the calibration product containing blooming threshold for all the quadrants\n@@ -290,19 +309,13 @@ def mainMethod(args):\n        \n        # creation of a json file to be put in the data product\n        ptc_json_dict = dict()\n+       ptc_json_dict[\"Quadrant name\"] = quadname\n+       ptc_json_dict[\"units\"] = \"adu\"\n+       ptc_json_dict[\"signal\"] = \"variance\"\n        for elmt_signal, elmt_var in zip(signal_without_outliers, variance_without_outliers):\n          ptc_json_dict[elmt_signal] = elmt_var\n \n-       ptc_json_file = FileNameProvider().get_allowed_filename( processing_function = 'VIS',\n-                                                                    type_name           = f\"PTC-QUADRANT-{ext}\",\n-                                                                    instance_id         = '',\n-                                                                    extension           = '.json')\n-       logger.info (\"==> Creating blooming calibration data product %s\", ptc_json_file)\n-       ptc_json_file = os.path.join( args.workdir, \"data\", ptc_json_file)\n-       with open(ptc_json_file, 'w') as fp:\n-         fp.write(json.dumps(ptc_json_dict, indent=2, default=np.ndarray.tolist))\n-\n-       ptc_json_list.append( os.path.basename(ptc_json_file))\n+       ptc_json_dict_list.append(ptc_json_dict)\n \n     # Write end data product\n \n@@ -320,26 +333,20 @@ def mainMethod(args):\n \n     data_product = FromToXML.create_DpdVisBloomingModel( os.path.basename( blooming_model_file))\n \n-    # tar of the 144 png images into one single file, that will be added to the calibration product into Data.\n-    output_PTC_graphs_filename = workdir+\"/data/PTC_graphs.tar\"\n-\n-    with tarfile.open(output_PTC_graphs_filename, \"w:gz\") as tar:\n-      for filename in os.listdir(subdir):\n-        #logger.info(f\"filename is : {filename}\")\n-        tar.add(os.path.join(subdir, filename), arcname=filename)\n \n+    # Closing pdf\n+    pdf.close()\n     # adding to the data_product, inside Data.QualityParameterStorage, the file output_PTC_graphs_filename\n     FromToXML.add_quality_parameter_file(data_product, output_PTC_graphs_filename)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n+    \n+    ptc_file = workdir+\"/data/list-PTCs.json\"\n+    with open(ptc_file, 'w') as fp:\n+       for elmt in ptc_json_dict_list:\n+        fp.write(json.dumps(elmt, indent=2, default=np.ndarray.tolist))  \n+\n \n-    # we add all the ptcs files into a single tar file, to be accessible at the eas\n-    ptc_file = workdir+\"/data/list-PTCs.tar\"\n-    with tarfile.open(ptc_file, \"w:gz\") as tar:\n-      for filename in ptc_json_list:\n-        logger.info(f\"in ptc_json_list, filename is : {filename}\")\n-        tar.add(os.path.join(workdir + \"/data/\", filename), arcname=filename)\n-   \n     FromToXML.add_quality_parameter_file(data_product, ptc_file)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n",
                            "Merge branch 'feature_#23227_BloomingCalib' into 'release-13.0'",
                            "Catherine Grenet",
                            "2023-08-23T16:47:59.000+00:00",
                            "9c47bdf97d8cf87c71876aa06cd1b1b61211cd97"
                        ],
                        [
                            "@@ -120,11 +120,7 @@ def mainMethod(args):\n     for item_flg in heap_flg_list:\n       if list_flg_filename[-1] != item_flg[:-4]:\n         list_flg_filename.append(item_flg[:-4])\n-\n-    # threshold representing the max value acceptable for a mean of flagmap\n-    # the more the value is high, the more the flagmap contains data, and the less the corresponding quadrant has\n-    max_flat_invalid_pixels = config.getint( 'BloomingCalib', 'max_flat_invalid_pixels', fallback=97490)\n-\n+    \n     # 144 values, one per quadrant, representing the blooming threshold in electrons\n     blooming_threshold_per_quadrant = np.empty(144)\n     blooming_threshold_per_quadrant[:] = 192000.0\n@@ -157,11 +153,9 @@ def mainMethod(args):\n        image3D_flag_1, quadname = fpe.file_layer_entry_point(list1, \"\", in_flagmap=list1_flg)\n        image3D_flag_2, _ = fpe.file_layer_entry_point(list2, \"\", in_flagmap=list2_flg)\n \n-       signal, variance = ptc_compute.wharper(image3D_flag_1,\n+       signal, variance, var_err = ptc_compute.wharper(image3D_flag_1,\n                                               image3D_flag_2,\n-                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300),\n-                                              ext,\n-                                              workdir)\n+                                              config.getint( 'GainCalib', 'tile_size_pixels', fallback=300))\n \n        del image3D_flag_1\n        del image3D_flag_2\n@@ -186,51 +180,29 @@ def mainMethod(args):\n        # straight line to show the non-linearity effect when comparing to PTC_fit\n        straight_PTC_fit = np.polyfit(signal_truncated, variance_truncated, 1)\n \n-       #######################\n-\n-       # computing the distance to the straight line (polynome of deg 1), to get rid off outliners\n-       list_distance_to_line = []\n-       for i in range(len(signal_truncated)):\n-           list_distance_to_line.append(math.sqrt(np.power(([np.polyval(straight_PTC_fit, signal[i])] - variance[i]), 2)))\n-\n-       # computing the std of list_distance_to_line\n-       std_points_from_line = np.std(list_distance_to_line)\n-\n-       # get rid of outliers with sigma clipping\n-       signal_without_outliers = []\n-       variance_without_outliers = []\n-       sigma_lower = 2\n-       sigma_upper = 2\n-       for i in range(len(signal_truncated)):\n-           if ((list_distance_to_line[i] > np.mean(list_distance_to_line) - sigma_lower*np.std(list_distance_to_line)) and\n-              (list_distance_to_line[i] < np.mean(list_distance_to_line) + sigma_upper*np.std(list_distance_to_line))):\n-               signal_without_outliers.append(signal[i])\n-               variance_without_outliers.append(variance[i])\n-\n-\n        #######################\n \n        # fit of the actual PTC\n-       PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n \n        #######################\n \n        # computing the distance of the points to the curved fitted PTC\n        threshold_distance = 0\n-       for i in range(len(signal_without_outliers)):\n+       for i in range(len(signal)):\n            threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n-           distance_PTC = math.sqrt(np.power(([np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]), 2))\n-           if distance_PTC > threshold_distance and signal_without_outliers[i] > PTC_fit_xlimit:\n-               logger.info(\"Blooming threshold found at : %s\", signal_without_outliers[i])\n-               blooming_threshold_per_quadrant[ext] = signal_without_outliers[i] * config.getfloat( \"GainPerQuad\", quadname)\n+           distance_PTC = math.sqrt(np.power(([np.polyval(PTC_fit, signal[i])] - variance[i]), 2))\n+           if distance_PTC > threshold_distance and signal[i] > PTC_fit_xlimit:\n+               logger.info(\"Blooming threshold found at : %s\", signal[i])\n+               blooming_threshold_per_quadrant[ext] = signal[i] * config.getfloat( \"GainPerQuad\", quadname)\n                break\n \n        #######################\n \n        # calcul of the gain : get the slope of the straight line\n-       res = linregress(signal_without_outliers, variance_without_outliers)\n-       gain = 1/linregress(signal_without_outliers, variance_without_outliers)[0]\n+       res = linregress(signal, variance)\n+       gain = 1/linregress(signal, variance)[0]\n \n        logger.info(f\"GAIN IS : {gain}\")\n \n@@ -245,7 +217,7 @@ def mainMethod(args):\n        plt.figure(ext)\n \n        fig, ax_scr = plt.subplots()\n-       x_series = np.linspace(signal_without_outliers[0],55000,len(signal_without_outliers))\n+       x_series = np.linspace(signal[0],55000,len(signal))\n \n        lin = res.slope*x_series+res.intercept\n        diff_nl = ([np.polyval(PTC_fit, i) for i in x_series] - lin) / lin * 100\n@@ -254,7 +226,7 @@ def mainMethod(args):\n        #logger.info([np.polyval(PTC_fit, i) for i in x_series])\n \n        #ax_scr.scatter(signal, variance, s = 10)\n-       ax_scr.scatter(signal_without_outliers, variance_without_outliers, s = 10)\n+       ax_scr.scatter(signal, variance, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n        ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fitted')\n@@ -290,8 +262,8 @@ def mainMethod(args):\n        \n        # creation of a json file to be put in the data product\n        ptc_json_dict = dict()\n-       for elmt_signal, elmt_var in zip(signal_without_outliers, variance_without_outliers):\n-         ptc_json_dict[elmt_signal] = elmt_var\n+       for elmt_signal, elmt_var, elmt_var_err in zip(signal, variance, var_err):\n+         ptc_json_dict[elmt_signal] = (elmt_var, elmt_var_err)\n \n        ptc_json_file = FileNameProvider().get_allowed_filename( processing_function = 'VIS',\n                                                                     type_name           = f\"PTC-QUADRANT-{ext}\",\n",
                            "VIS_Blooming : adaptation to the common ptc library",
                            "Thomas Flanet",
                            "2023-08-23T18:02:39.000+02:00",
                            "b70a7e74a3adba4a0401a97b25529a6093855508"
                        ],
                        [
                            "@@ -147,24 +147,7 @@ def mainMethod(args):\n        # so adding the extension number to the individual filename is the pile\n        detrended_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_data_filename]\n        flag_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_flg_filename]\n-\n-       # if the flagmap associated with a quadrant contains too much data,\n-       # it means that the quadrant has been too much flagged, possibly because it is too much saturated\n-       # so we remove the quadrant from the list (and remove the corresponding quadrant on the other list as well)\n-       for i in  range(len(flag_quadrant_list)-1, -1, -1):\n-           # open up fits filename\n-           hdu_flag = fits.open(flag_quadrant_list[i], memmap=False)\n-           # get the data\n-           hdu_data = hdu_flag[1].data\n-           mean_flag_quadrant = np.mean(hdu_data)\n-           # removing the quadrant from the list\n-           if mean_flag_quadrant > max_flat_invalid_pixels:\n-               logger.info(\"From detrended_quadrant_list, removing %s\", detrended_quadrant_list[i])\n-               detrended_quadrant_list.remove(detrended_quadrant_list[i])\n-               flag_quadrant_list.remove(flag_quadrant_list[i])\n-           hdu_flag.close()\n-\n-\n+       \n        # we call the pairing_flats function to get two lists\n        list1, list2 = pairing_flats(detrended_quadrant_list, config, False)\n        list1_flg, list2_flg = pairing_flats(flag_quadrant_list, config, True)\n@@ -285,8 +268,8 @@ def mainMethod(args):\n        ax_vic.plot(x_series, diff_nl, 'y', label = 'diff PTC - lin')\n        ax_vic.set_ylabel( \"error between PTC and lin\", color='y')\n \n-\n-       plt.axvline(x = blooming_threshold_per_quadrant[ext], color = 'b')\n+       # on the graph, we want the value in adu\n+       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), color = 'b')\n \n        plt.legend()\n        ax_scr.legend()\n@@ -300,7 +283,7 @@ def mainMethod(args):\n        #############################################################################################\n        \n        # log of results\n-       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]}\")\n+       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]} in electrons\")\n        \n        #### update json file with blooming threshold estimation ####\n        blooming_json_dict[quadname] = blooming_threshold_per_quadrant[ext]\n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:48:12.000+02:00",
                            "6f4bb5791ddb1a33888248a6fb0cbd43c4b1bd55"
                        ],
                        [
                            "@@ -147,24 +147,7 @@ def mainMethod(args):\n        # so adding the extension number to the individual filename is the pile\n        detrended_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_data_filename]\n        flag_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_flg_filename]\n-\n-       # if the flagmap associated with a quadrant contains too much data,\n-       # it means that the quadrant has been too much flagged, possibly because it is too much saturated\n-       # so we remove the quadrant from the list (and remove the corresponding quadrant on the other list as well)\n-       for i in  range(len(flag_quadrant_list)-1, -1, -1):\n-           # open up fits filename\n-           hdu_flag = fits.open(flag_quadrant_list[i], memmap=False)\n-           # get the data\n-           hdu_data = hdu_flag[1].data\n-           mean_flag_quadrant = np.mean(hdu_data)\n-           # removing the quadrant from the list\n-           if mean_flag_quadrant > max_flat_invalid_pixels:\n-               logger.info(\"From detrended_quadrant_list, removing %s\", detrended_quadrant_list[i])\n-               detrended_quadrant_list.remove(detrended_quadrant_list[i])\n-               flag_quadrant_list.remove(flag_quadrant_list[i])\n-           hdu_flag.close()\n-\n-\n+       \n        # we call the pairing_flats function to get two lists\n        list1, list2 = pairing_flats(detrended_quadrant_list, config, False)\n        list1_flg, list2_flg = pairing_flats(flag_quadrant_list, config, True)\n@@ -285,8 +268,8 @@ def mainMethod(args):\n        ax_vic.plot(x_series, diff_nl, 'y', label = 'diff PTC - lin')\n        ax_vic.set_ylabel( \"error between PTC and lin\", color='y')\n \n-\n-       plt.axvline(x = blooming_threshold_per_quadrant[ext], color = 'b')\n+       # on the graph, we want the value in adu\n+       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), color = 'b')\n \n        plt.legend()\n        ax_scr.legend()\n@@ -300,7 +283,7 @@ def mainMethod(args):\n        #############################################################################################\n        \n        # log of results\n-       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]}\")\n+       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]} in electrons\")\n        \n        #### update json file with blooming threshold estimation ####\n        blooming_json_dict[quadname] = blooming_threshold_per_quadrant[ext]\n",
                            "Merge branch 'release-13.0' of https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks into release-13.0",
                            "James Nightingale",
                            "2023-08-22T19:47:57.000+02:00",
                            "3887ab35499dbc803602db855483bfdea07cf240"
                        ],
                        [
                            "@@ -35,6 +35,9 @@ import tarfile\n \n from configparser import RawConfigParser\n \n+# pdf containing the png files\n+from matplotlib.backends.backend_pdf import PdfPages\n+\n import numpy as np\n from astropy.io import fits\n \n@@ -137,9 +140,14 @@ def mainMethod(args):\n     blooming_json_dict['units'] = \"e-\"   \n \n     # contains the 144 json files that contain the PTC data\n-    ptc_json_list = list()\n-\n+    #ptc_json_list = list()\n+    ptc_json_dict_list = list()\n     blooming_model_file = \"\"\n+    \n+    # pdf file to store all the images\n+    output_PTC_graphs_filename = os.path.join( workdir+\"/data/\", 'PTC_graphs.pdf' )\n+    pdf = PdfPages(output_PTC_graphs_filename)\n+\n     for ext in range(0, nquad):\n        logger.info (\"==>Processing quadrant %s\", ext)\n \n@@ -147,24 +155,7 @@ def mainMethod(args):\n        # so adding the extension number to the individual filename is the pile\n        detrended_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_data_filename]\n        flag_quadrant_list = [filename + \".{:03}\".format(ext + 1) for filename in list_flg_filename]\n-\n-       # if the flagmap associated with a quadrant contains too much data,\n-       # it means that the quadrant has been too much flagged, possibly because it is too much saturated\n-       # so we remove the quadrant from the list (and remove the corresponding quadrant on the other list as well)\n-       for i in  range(len(flag_quadrant_list)-1, -1, -1):\n-           # open up fits filename\n-           hdu_flag = fits.open(flag_quadrant_list[i], memmap=False)\n-           # get the data\n-           hdu_data = hdu_flag[1].data\n-           mean_flag_quadrant = np.mean(hdu_data)\n-           # removing the quadrant from the list\n-           if mean_flag_quadrant > max_flat_invalid_pixels:\n-               logger.info(\"From detrended_quadrant_list, removing %s\", detrended_quadrant_list[i])\n-               detrended_quadrant_list.remove(detrended_quadrant_list[i])\n-               flag_quadrant_list.remove(flag_quadrant_list[i])\n-           hdu_flag.close()\n-\n-\n+       \n        # we call the pairing_flats function to get two lists\n        list1, list2 = pairing_flats(detrended_quadrant_list, config, False)\n        list1_flg, list2_flg = pairing_flats(flag_quadrant_list, config, True)\n@@ -223,12 +214,15 @@ def mainMethod(args):\n               (list_distance_to_line[i] < np.mean(list_distance_to_line) + sigma_upper*np.std(list_distance_to_line))):\n                signal_without_outliers.append(signal[i])\n                variance_without_outliers.append(variance[i])\n-\n+       \n \n        #######################\n \n        # fit of the actual PTC\n-       PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       #PTC_fit = np.polyfit(signal_without_outliers, variance_without_outliers, 3)\n+       # fit of the truncated PTC\n+       PTC_fit = np.polyfit(signal_truncated, variance_truncated, 3)\n+\n \n        #######################\n \n@@ -237,7 +231,9 @@ def mainMethod(args):\n        for i in range(len(signal_without_outliers)):\n            threshold_distance = PTC_blooming_threshold*np.polyval(PTC_fit, signal[i])\n            #logger.info(threshold_distance)\n-           distance_PTC = math.sqrt(np.power(([np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]), 2))\n+           # the distance is not an abs value, because we only search for a positive distance (ie :when the real data is smaller than the data of the ptc curve)\n+           # because the blooming effect is caracterized by a \"fall\" of the ptc data\n+           distance_PTC = [np.polyval(PTC_fit, signal_without_outliers[i])] - variance_without_outliers[i]\n            if distance_PTC > threshold_distance and signal_without_outliers[i] > PTC_fit_xlimit:\n                logger.info(\"Blooming threshold found at : %s\", signal_without_outliers[i])\n                blooming_threshold_per_quadrant[ext] = signal_without_outliers[i] * config.getfloat( \"GainPerQuad\", quadname)\n@@ -274,52 +270,52 @@ def mainMethod(args):\n        ax_scr.scatter(signal_without_outliers, variance_without_outliers, s = 10)\n \n        ax_scr.plot(x_series, lin, 'r', label='y={:.2f}x+{:.2f}'.format(res.slope,res.intercept))\n-       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fitted')\n-       ax_scr.set_xlabel(\"signal\")\n-       ax_scr.set_ylabel(\"variance\")\n+       ax_scr.plot(x_series, [np.polyval(PTC_fit, i) for i in x_series], 'g', label = 'Truncated PTC fit')\n+       ax_scr.set_xlabel(\"signal (adu)\")\n+       ax_scr.set_ylabel(\"variance (adu)\")\n \n        plt.grid(axis='both')\n \n \n        ax_vic = ax_scr.twinx()\n        ax_vic.plot(x_series, diff_nl, 'y', label = 'diff PTC - lin')\n-       ax_vic.set_ylabel( \"error between PTC and lin\", color='y')\n+       ax_vic.set_ylabel( \"error between PTC and lin (percent)\", color='y')\n+       ax_scr.legend(bbox_to_anchor=(0.2, 1.15))\n+       #ax_vic.legend()\n+       # on the graph, we want the value in adu\n+       plt.axvline(x = blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname), \n+                   color = 'b', \n+                   label =f'blooming thr = {round(blooming_threshold_per_quadrant[ext] / config.getfloat( \"GainPerQuad\", quadname))} adu')\n+\n+       #plt.legend(bbox_to_anchor=(1.15, 1.15))\n+       plt.legend()\n \n+       plt.title(\"PTC_quadrant_\"+quadname)\n+       plt.savefig(subdir + \"PTC_quadrant_\"+quadname+\".png\")\n+       logger.info(\"saving of PTC_quadrant_\"+quadname+\".png fit\")\n \n-       plt.axvline(x = blooming_threshold_per_quadrant[ext], color = 'b')\n+       pdf.savefig(fig)\n \n-       plt.legend()\n-       ax_scr.legend()\n-\n-       plt.title(\"PTC_quadrant_\"+str(ext))\n-       plt.savefig(subdir + \"PTC_quadrant_\"+str(ext)+\".png\")\n-       logger.info(\"saving of PTC_quadrant_\"+str(ext)+\".png fitted\")\n \n        #############################################################################################\n        # creation of the calibration product containing blooming threshold for all the quadrants\n        #############################################################################################\n        \n        # log of results\n-       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]}\")\n+       logger.info(f\"blooming threshold for quadrant {quadname} is : {blooming_threshold_per_quadrant[ext]} in electrons\")\n        \n        #### update json file with blooming threshold estimation ####\n        blooming_json_dict[quadname] = blooming_threshold_per_quadrant[ext]\n        \n        # creation of a json file to be put in the data product\n        ptc_json_dict = dict()\n+       ptc_json_dict[\"Quadrant name\"] = quadname\n+       ptc_json_dict[\"units\"] = \"adu\"\n+       ptc_json_dict[\"signal\"] = \"variance\"\n        for elmt_signal, elmt_var in zip(signal_without_outliers, variance_without_outliers):\n          ptc_json_dict[elmt_signal] = elmt_var\n \n-       ptc_json_file = FileNameProvider().get_allowed_filename( processing_function = 'VIS',\n-                                                                    type_name           = f\"PTC-QUADRANT-{ext}\",\n-                                                                    instance_id         = '',\n-                                                                    extension           = '.json')\n-       logger.info (\"==> Creating blooming calibration data product %s\", ptc_json_file)\n-       ptc_json_file = os.path.join( args.workdir, \"data\", ptc_json_file)\n-       with open(ptc_json_file, 'w') as fp:\n-         fp.write(json.dumps(ptc_json_dict, indent=2, default=np.ndarray.tolist))\n-\n-       ptc_json_list.append( os.path.basename(ptc_json_file))\n+       ptc_json_dict_list.append(ptc_json_dict)\n \n     # Write end data product\n \n@@ -337,26 +333,20 @@ def mainMethod(args):\n \n     data_product = FromToXML.create_DpdVisBloomingModel( os.path.basename( blooming_model_file))\n \n-    # tar of the 144 png images into one single file, that will be added to the calibration product into Data.\n-    output_PTC_graphs_filename = workdir+\"/data/PTC_graphs.tar\"\n-\n-    with tarfile.open(output_PTC_graphs_filename, \"w:gz\") as tar:\n-      for filename in os.listdir(subdir):\n-        #logger.info(f\"filename is : {filename}\")\n-        tar.add(os.path.join(subdir, filename), arcname=filename)\n \n+    # Closing pdf\n+    pdf.close()\n     # adding to the data_product, inside Data.QualityParameterStorage, the file output_PTC_graphs_filename\n     FromToXML.add_quality_parameter_file(data_product, output_PTC_graphs_filename)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n+    \n+    ptc_file = workdir+\"/data/list-PTCs.json\"\n+    with open(ptc_file, 'w') as fp:\n+       for elmt in ptc_json_dict_list:\n+        fp.write(json.dumps(elmt, indent=2, default=np.ndarray.tolist))  \n+\n \n-    # we add all the ptcs files into a single tar file, to be accessible at the eas\n-    ptc_file = workdir+\"/data/list-PTCs.tar\"\n-    with tarfile.open(ptc_file, \"w:gz\") as tar:\n-      for filename in ptc_json_list:\n-        logger.info(f\"in ptc_json_list, filename is : {filename}\")\n-        tar.add(os.path.join(workdir + \"/data/\", filename), arcname=filename)\n-   \n     FromToXML.add_quality_parameter_file(data_product, ptc_file)\n \n     DmUtils.save_product_metadata( data_product, dpdcalib_blooming)\n",
                            "VIS_Blooming_calib.py : outputs grouped in single file and updated with quadrant name",
                            "Thomas Flanet",
                            "2023-08-22T17:41:44.000+02:00",
                            "c70c9fa18bc77e29fa77604c3f6bc0671b2086dc"
                        ]
                    ],
                    "VIS_Blooming/python/VIS_Blooming/file_entry_point.py": [
                        [
                            "@@ -17,6 +17,7 @@\n \n import astropy.io.fits as pf\n import numpy as np\n+import numpy.ma as ma\n \n import ElementsKernel.Logging as elog\n \n@@ -62,7 +63,6 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n     if not fv.input_fits_files_valid(input_files):\n         log.error(\"file_layer_entry_point(): input files failed validity tests\")\n     else:\n-        log.info(\"file_layer_entry_point(): Images on which to compute the blooming are: \" + str(input_files))\n         if (quadrant_id is None) or (quadrant_id == \"\"):\n             quadrant_id = 1 #use the first image extension\n         elif quadrant_id.isdigit():\n@@ -76,32 +76,11 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n         image_stack = None\n         if in_flagmap is None:\n           flagmap = None\n-          flag_prescan_size = None\n-          flag_serial_overscan_size = None\n-          flag_parallel_overscan_size = None\n-          flag_ccd_id = None\n-          flag_quad_id = None\n         else:\n           flagmap = fio.load_multi_same_ext(in_flagmap, quad_name, apply_gain=False)\n           #use int32 (signed) because that is supported by the FITS standard\n           flagmap = np.array(flagmap, dtype=np.int32, copy=True, order=\"C\")\n-          handle = pf.open(in_flagmap[0], memmap=False)\n-          flag_prescan_size = handle[quadrant_id].header[\"PRESCANX\"]\n-          flag_serial_overscan_size = handle[quadrant_id].header[\"OVRSCANX\"]\n-          flag_parallel_overscan_size = handle[quadrant_id].header[\"OVRSCANY\"]\n-          flag_ccd_id = handle[quadrant_id].header[\"CCDID\"]\n-          flag_quad_id = handle[quadrant_id].header[\"QUADID\"]\n-          handle.close()\n-        \n-        image_stack = algo.flagging_stack(image_stack32, \n-                                          flagmap, \n-                                          input_files, \n-                                          quad_name, \n-                                          interm_dir, \n-                                          flag_ccd_id, \n-                                          flag_quad_id, \n-                                          flag_prescan_size,\n-                                          flag_serial_overscan_size, \n-                                          flag_parallel_overscan_size)\n+\n+    image_stack = ma.array(image_stack32, mask=flagmap, copy=False)    \n     log.info(\"file_layer_entry_point(): end\")\n-    return image_stack\n+    return image_stack, quad_name\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ],
                        [
                            "@@ -63,7 +63,6 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n     if not fv.input_fits_files_valid(input_files):\n         log.error(\"file_layer_entry_point(): input files failed validity tests\")\n     else:\n-        log.info(\"file_layer_entry_point(): Images on which to compute the blooming are: \" + str(input_files))\n         if (quadrant_id is None) or (quadrant_id == \"\"):\n             quadrant_id = 1 #use the first image extension\n         elif quadrant_id.isdigit():\n@@ -77,34 +76,11 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n         image_stack = None\n         if in_flagmap is None:\n           flagmap = None\n-          flag_prescan_size = None\n-          flag_serial_overscan_size = None\n-          flag_parallel_overscan_size = None\n-          flag_ccd_id = None\n-          flag_quad_id = None\n         else:\n           flagmap = fio.load_multi_same_ext(in_flagmap, quad_name, apply_gain=False)\n           #use int32 (signed) because that is supported by the FITS standard\n           flagmap = np.array(flagmap, dtype=np.int32, copy=True, order=\"C\")\n-          handle = pf.open(in_flagmap[0], memmap=False)\n-          flag_prescan_size = handle[quadrant_id].header[\"PRESCANX\"]\n-          flag_serial_overscan_size = handle[quadrant_id].header[\"OVRSCANX\"]\n-          flag_parallel_overscan_size = handle[quadrant_id].header[\"OVRSCANY\"]\n-          flag_ccd_id = handle[quadrant_id].header[\"CCDID\"]\n-          flag_quad_id = handle[quadrant_id].header[\"QUADID\"]\n-          handle.close()\n-        \n-        '''image_stack = algo.flagging_stack(image_stack32, \n-                                          flagmap, \n-                                          input_files, \n-                                          quad_name, \n-                                          interm_dir, \n-                                          flag_ccd_id, \n-                                          flag_quad_id, \n-                                          flag_prescan_size,\n-                                          flag_serial_overscan_size, \n-                                          flag_parallel_overscan_size)\n-        '''\n+\n     image_stack = ma.array(image_stack32, mask=flagmap, copy=False)    \n     log.info(\"file_layer_entry_point(): end\")\n     return image_stack, quad_name\n",
                            "Merge branch 'release-13.0' into 'feature_#23620_CommonPTC_Library_Blooming'",
                            "Thomas Flanet",
                            "2023-08-23T17:28:19.000+00:00",
                            "b662bd82e10d33df3f00405b1fe91761a847533f"
                        ],
                        [
                            "@@ -63,7 +63,6 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n     if not fv.input_fits_files_valid(input_files):\n         log.error(\"file_layer_entry_point(): input files failed validity tests\")\n     else:\n-        log.info(\"file_layer_entry_point(): Images on which to compute the blooming are: \" + str(input_files))\n         if (quadrant_id is None) or (quadrant_id == \"\"):\n             quadrant_id = 1 #use the first image extension\n         elif quadrant_id.isdigit():\n@@ -77,34 +76,11 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n         image_stack = None\n         if in_flagmap is None:\n           flagmap = None\n-          flag_prescan_size = None\n-          flag_serial_overscan_size = None\n-          flag_parallel_overscan_size = None\n-          flag_ccd_id = None\n-          flag_quad_id = None\n         else:\n           flagmap = fio.load_multi_same_ext(in_flagmap, quad_name, apply_gain=False)\n           #use int32 (signed) because that is supported by the FITS standard\n           flagmap = np.array(flagmap, dtype=np.int32, copy=True, order=\"C\")\n-          handle = pf.open(in_flagmap[0], memmap=False)\n-          flag_prescan_size = handle[quadrant_id].header[\"PRESCANX\"]\n-          flag_serial_overscan_size = handle[quadrant_id].header[\"OVRSCANX\"]\n-          flag_parallel_overscan_size = handle[quadrant_id].header[\"OVRSCANY\"]\n-          flag_ccd_id = handle[quadrant_id].header[\"CCDID\"]\n-          flag_quad_id = handle[quadrant_id].header[\"QUADID\"]\n-          handle.close()\n-        \n-        '''image_stack = algo.flagging_stack(image_stack32, \n-                                          flagmap, \n-                                          input_files, \n-                                          quad_name, \n-                                          interm_dir, \n-                                          flag_ccd_id, \n-                                          flag_quad_id, \n-                                          flag_prescan_size,\n-                                          flag_serial_overscan_size, \n-                                          flag_parallel_overscan_size)\n-        '''\n+\n     image_stack = ma.array(image_stack32, mask=flagmap, copy=False)    \n     log.info(\"file_layer_entry_point(): end\")\n     return image_stack, quad_name\n",
                            "Merge branch 'release-13.0' into develop: [ENH]#23227 BloomingCalibration Group outputs and identify the quadrants",
                            "Catherine Grenet",
                            "2023-08-23T18:53:33.000+02:00",
                            "03cd16a1a000a945f4990fcf18712fd71ec15ecd"
                        ],
                        [
                            "@@ -63,7 +63,6 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n     if not fv.input_fits_files_valid(input_files):\n         log.error(\"file_layer_entry_point(): input files failed validity tests\")\n     else:\n-        log.info(\"file_layer_entry_point(): Images on which to compute the blooming are: \" + str(input_files))\n         if (quadrant_id is None) or (quadrant_id == \"\"):\n             quadrant_id = 1 #use the first image extension\n         elif quadrant_id.isdigit():\n@@ -77,34 +76,11 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n         image_stack = None\n         if in_flagmap is None:\n           flagmap = None\n-          flag_prescan_size = None\n-          flag_serial_overscan_size = None\n-          flag_parallel_overscan_size = None\n-          flag_ccd_id = None\n-          flag_quad_id = None\n         else:\n           flagmap = fio.load_multi_same_ext(in_flagmap, quad_name, apply_gain=False)\n           #use int32 (signed) because that is supported by the FITS standard\n           flagmap = np.array(flagmap, dtype=np.int32, copy=True, order=\"C\")\n-          handle = pf.open(in_flagmap[0], memmap=False)\n-          flag_prescan_size = handle[quadrant_id].header[\"PRESCANX\"]\n-          flag_serial_overscan_size = handle[quadrant_id].header[\"OVRSCANX\"]\n-          flag_parallel_overscan_size = handle[quadrant_id].header[\"OVRSCANY\"]\n-          flag_ccd_id = handle[quadrant_id].header[\"CCDID\"]\n-          flag_quad_id = handle[quadrant_id].header[\"QUADID\"]\n-          handle.close()\n-        \n-        '''image_stack = algo.flagging_stack(image_stack32, \n-                                          flagmap, \n-                                          input_files, \n-                                          quad_name, \n-                                          interm_dir, \n-                                          flag_ccd_id, \n-                                          flag_quad_id, \n-                                          flag_prescan_size,\n-                                          flag_serial_overscan_size, \n-                                          flag_parallel_overscan_size)\n-        '''\n+\n     image_stack = ma.array(image_stack32, mask=flagmap, copy=False)    \n     log.info(\"file_layer_entry_point(): end\")\n     return image_stack, quad_name\n",
                            "Merge branch 'feature_#23227_BloomingCalib' into 'release-13.0'",
                            "Catherine Grenet",
                            "2023-08-23T16:47:59.000+00:00",
                            "9c47bdf97d8cf87c71876aa06cd1b1b61211cd97"
                        ],
                        [
                            "@@ -63,7 +63,6 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n     if not fv.input_fits_files_valid(input_files):\n         log.error(\"file_layer_entry_point(): input files failed validity tests\")\n     else:\n-        log.info(\"file_layer_entry_point(): Images on which to compute the blooming are: \" + str(input_files))\n         if (quadrant_id is None) or (quadrant_id == \"\"):\n             quadrant_id = 1 #use the first image extension\n         elif quadrant_id.isdigit():\n@@ -77,34 +76,11 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n         image_stack = None\n         if in_flagmap is None:\n           flagmap = None\n-          flag_prescan_size = None\n-          flag_serial_overscan_size = None\n-          flag_parallel_overscan_size = None\n-          flag_ccd_id = None\n-          flag_quad_id = None\n         else:\n           flagmap = fio.load_multi_same_ext(in_flagmap, quad_name, apply_gain=False)\n           #use int32 (signed) because that is supported by the FITS standard\n           flagmap = np.array(flagmap, dtype=np.int32, copy=True, order=\"C\")\n-          handle = pf.open(in_flagmap[0], memmap=False)\n-          flag_prescan_size = handle[quadrant_id].header[\"PRESCANX\"]\n-          flag_serial_overscan_size = handle[quadrant_id].header[\"OVRSCANX\"]\n-          flag_parallel_overscan_size = handle[quadrant_id].header[\"OVRSCANY\"]\n-          flag_ccd_id = handle[quadrant_id].header[\"CCDID\"]\n-          flag_quad_id = handle[quadrant_id].header[\"QUADID\"]\n-          handle.close()\n-        \n-        '''image_stack = algo.flagging_stack(image_stack32, \n-                                          flagmap, \n-                                          input_files, \n-                                          quad_name, \n-                                          interm_dir, \n-                                          flag_ccd_id, \n-                                          flag_quad_id, \n-                                          flag_prescan_size,\n-                                          flag_serial_overscan_size, \n-                                          flag_parallel_overscan_size)\n-        '''\n+\n     image_stack = ma.array(image_stack32, mask=flagmap, copy=False)    \n     log.info(\"file_layer_entry_point(): end\")\n     return image_stack, quad_name\n",
                            "VIS_Blooming_calib.py : outputs grouped in single file and updated with quadrant name",
                            "Thomas Flanet",
                            "2023-08-22T17:41:44.000+02:00",
                            "c70c9fa18bc77e29fa77604c3f6bc0671b2086dc"
                        ]
                    ],
                    "VIS_BrighterFatter/python/VIS_BrighterFatter/VIS_BrighterFatter_calib.py": [
                        [
                            "@@ -251,8 +251,7 @@ def mainMethod(args):\n     dpdcalib_bfe = os.path.abspath( os.path.join( args.workdir, dpdcalib_bfe))\n     logger.info( \"** Writing Bfe model data product XML to \" + dpdcalib_bfe)\n     logger.info(dpdcalib_bfe)\n-    data_product = FromToXML.create_DpdVisFileContainer( datafile_list = [os.path.basename(bfe_model_file)],\n-                                                         vis_product=\"bfe_model\")\n+    data_product = FromToXML.create_DpdVisBFEModel( os.path.basename( bfe_model_file))\n     DmUtils.save_product_metadata( data_product, dpdcalib_bfe)       \n      \n     \n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_LargeFlat/python/VIS_LargeFlat/VIS_LargeFlat_Modelling/FPA_VIS_Geometry.py": [
                        [
                            "@@ -176,8 +176,6 @@ class FPA_VISGeometry():\n                       self.sizeDetX(), self.sizeGapsX()[3],  \\\n                       self.sizeDetX(), self.sizeGapsX()[4],  \\\n                       self.sizeDetX())) / self.sizeFPX()\n-        #c = np.cumsum(c) * 2 - 1\n-        #c[0] = -1.;  c[-1] = 1.\n         c = np.cumsum(c) * 3 - 1.5\n         c[0] = -1.5;  c[-1] = 1.5\n         return c\n@@ -191,9 +189,6 @@ class FPA_VISGeometry():\n                       self.sizeDetY(), self.sizeGapsY()[3],  \\\n                       self.sizeDetY(), self.sizeGapsY()[4],  \\\n                       self.sizeDetY())) / self.sizeFPY()\n-        #c = np.cumsum(c) * 2 - 1\n-        #c[0] = -1.;  c[-1] = 1.\n-        #c2 = np.cumsum(c) * 2 - 1.5\n         c = np.cumsum(c) * 3 - 1.5\n         c[0] = -1.5;  c[-1] = 1.5\n         return c\n@@ -231,7 +226,7 @@ class FPA_VISGeometry():\n         \"\"\"Return True if the chip of a detector is rotated by 180 degrees in the FPA.\"\"\"\n         if self.enableDetRotation:\n             (idetx, idety) = self.idetseq2xy(idet)\n-            return (idety >= 5)\n+            return (idetx >= 4)\n         return False\n     \n     \n@@ -268,9 +263,7 @@ class FPA_VISGeometry():\n         x = copy.copy(_x)\n         y = copy.copy(_y)\n \n-        #print(np.count_nonzero(y >= 4136))\n         y = np.where(y >= 4136, 4130, y)\n-        #print(np.count_nonzero(y >= 4136))\n \n \n         assert np.all(0. <= x);  assert np.all(x <= self.detector_pixel_shortdim)\n@@ -310,10 +303,6 @@ class FPA_VISGeometry():\n             y = np.array((y,))\n         if np.isscalar(idet):\n             idet = np.repeat(idet, x.size)\n-        #assert np.all(np.abs(x) <= 1)\n-        #assert np.all(np.abs(y) <= 1)\n-        \n-        \n         \n         assert np.all(np.abs(x) <= 1.5)\n         assert np.all(np.abs(y) <= 1.5)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_LargeFlat/python/VIS_LargeFlat/VIS_LargeFlat_Modelling/Fit2D.py": [
                        [
                            "@@ -4,13 +4,6 @@ from scipy.interpolate import CloughTocher2DInterpolator, interp1d, NearestNDInt\n # 2D profile fitting (non-parametric)\n class Fit2D_NP:\n \n-    #def coords(self):\n-    #    if self.atedge:\n-    #        return np.linspace(-1.5, 1.5, self.npar)\n-    #    else:\n-    #        g = np.linspace(-1.5, 1.5, self.npar+1)\n-    #        return (g[0:-1] + g[1:]) / 2.\n-        \n     def coords_x(self):\n         g = np.linspace(-1.5, 1.5, self.npar_x+1)\n         return (g[0:-1] + g[1:]) / 2.\n@@ -23,35 +16,6 @@ class Fit2D_NP:\n     Fit2D initialisation class\n     WALKTHRU_3-1\n     \"\"\"\n-    #def __init__(self, npar, meas_x, meas_y,logger,atedge=False):\n-    #    self.npar = npar\n-    #    self.logger=logger\n-    #    assert meas_x.size == meas_y.size\n-    #    assert np.max(np.abs(meas_x)) < 1.5\n-    #    assert np.max(np.abs(meas_y)) < 1.5\n-    #    self.meas_x = meas_x.copy()\n-    #    self.meas_y = meas_y.copy()\n-    #    self.param_val = np.zeros((npar, npar))\n-    #    self.param_unc = np.zeros((npar, npar))\n-    #    self.atedge = atedge\n-    #    x = self.coords()\n-    #    y = self.coords()\n-    #    step = x[1] - x[0]\n-    #    self.indexmap = []\n-    #    for ix in range(0, npar):\n-    #        for iy in range(0, npar):\n-    #            max_dist = step / 2 ##original from \n-                \n-    #            j = np.where((np.abs(self.meas_x - x[ix]) < max_dist)  &\n-    #                         (np.abs(self.meas_y - y[iy]) < max_dist))\n-    #            \"OU-NIR tends to throw an exception and quit. So NIR expects data points. Not reaching this in selfcal\"\n-    #            if len(j[0]) < 2:\n-    #                self.logger.warn(\"Not enough data in the square region around \" + str(x[iy]) + \", \" + str(y[iy]) + \" (size: \" + str(step) + \")\")\n-    #                print(\"Not enough data in the square region around \" + str(x[iy]) + \", \" + str(y[iy]) + \" (size: \" + str(step) + \")\")\n-    #            else : \n-    #                self.indexmap.append(j[0])\n-    #    print(\"End __init__ of fit2d\")\n-        \n     def __init__(self, npar_x, npar_y, meas_x, meas_y, logger):\n         self.npar_x = npar_x\n         self.npar_y = npar_y\n@@ -84,23 +48,6 @@ class Fit2D_NP:\n                     raise ValueError(\"Not enough data in the square region around \" + str(x[iy]) + \", \" + str(y[iy]) + \" (size: \" + str(step_x) + \")\")\n                 self.indexmap.append(j[0])\n         \n-\n-    #def fit(self, meas, uncert):\n-    #    print(\"Start fit2d.fit\")\n-    #    assert meas.ndim == 1\n-    #    assert meas.size == self.meas_x.size\n-    #    i = 0\n-    #    for ix in range(0, self.npar):\n-    #        for iy in range(0, self.npar):\n-    #            if(i < len(self.indexmap)):\n-    #                j = self.indexmap[i]\n-    #                i += 1\n-    #                if len(j) > 0:\n-    #                    self.param_val[ix, iy] = np.mean(meas[j])\n-    #                    self.param_unc[ix, iy] = np.mean(uncert[j]) / np.sqrt(len(j))\n-    #                    \n-    #    print(\"End fit of fit2d.fit\")\n-        \n     def fit(self, meas, uncert):\n         assert meas.ndim == 1\n         assert meas.size == self.meas_x.size\n@@ -114,24 +61,6 @@ class Fit2D_NP:\n                     self.param_unc[ix, iy] = np.mean(uncert[j]) / np.sqrt(len(j))        \n \n \n-    #def extrapolate(self, x, y):\n-    #    print(\"Start fit of extrapolate\")\n-\n-    #    assert np.isscalar(x)\n-    #    assert np.isscalar(y)\n-    #    assert ((np.abs(x) < 1.5) or (np.abs(y) < 1.5))\n-    #    if np.abs(x) < 1.5:\n-    #        mx = np.repeat(x, self.npar)\n-    #        my = self.coords()\n-    #        dd = self.predict_at(mx, my)\n-    #        return float(interp1d(my, dd, fill_value='extrapolate')(y))\n-    #    if np.abs(y) < 1.5:\n-    #        mx = self.coords()\n-    #        my = np.repeat(y, self.npar)\n-    #        dd = self.predict_at(mx, my)\n-    #        return float(interp1d(mx, dd, fill_value='extrapolate')(x))\n-    #    raise ValueError('Unexpected value for inputs (x=' + str(x) + ', y=' + str(y) + ').')\n-    \n     def extrapolate(self, x, y):\n         assert np.isscalar(x)\n         assert np.isscalar(y)\n@@ -151,26 +80,6 @@ class Fit2D_NP:\n         raise ValueError('Unexpected value for inputs (x=' + str(x) + ', y=' + str(y) + ').')\n     \n \n-    #def predict_at(self, x, y, uncert=False):\n-    #    if uncert:\n-    #        z = self.param_unc.transpose()\n-    #    else:\n-    #        z = self.param_val.transpose()\n-\n-    #    if self.atedge:\n-    #        x_grid, y_grid, z_grid = self.coords(), self.coords(), z\n-    #    else:\n-    #        x_grid, y_grid, z_grid = self.extended_grid(self.coords(), self.coords(), z)\n-\n-    #    (mx, my) = np.meshgrid(x_grid, y_grid)\n-    #    mx = mx.reshape(mx.size)\n-    #    my = my.reshape(my.size)\n-    #    points = np.vstack([mx, my]).transpose()\n-    #    itp = CloughTocher2DInterpolator(points, z_grid.reshape(z_grid.size))\n-    #    return itp(x, y)\n-    \n-    \n-    \n     # Predict model at coordinates x, y.  If uncert=true returns the\n     # model uncertainties at x,y.\n     def predict_at(self, x, y, uncert=False):\n@@ -184,8 +93,6 @@ class Fit2D_NP:\n         # coords_x/y() are always WITHIN, and not AT, the boundaries.\n         # Here we prepare a grid extending to the boundaries (-1.5:1.5)\n         # and extrapolate the \"z\" values on the added locations.\n-        #x_grid = np.hstack((-1., self.coords_x(), 1.))\n-        #y_grid = np.hstack((-1., self.coords_y(), 1.))\n         \n         x_grid = np.hstack((-1.5, self.coords_x(), 1.5))\n         y_grid = np.hstack((-1.5, self.coords_y(), 1.5))\n@@ -196,27 +103,19 @@ class Fit2D_NP:\n         # Extrapolate on the boundaries\n         for i in range(0, self.npar_y):\n             itp = interp1d(self.coords_x(), z[:, i], fill_value='extrapolate')\n-            #z_grid[ 0, i+1] = float(itp(-1.))\n-            #z_grid[-1, i+1] = float(itp( 1.))\n             z_grid[ 0, i+1] = float(itp(-1.5))\n             z_grid[-1, i+1] = float(itp( 1.5))\n             \n         for i in range(0, self.npar_x):\n             itp = interp1d(self.coords_y(), z[i, :], fill_value='extrapolate')\n-            #z_grid[i+1 , 0] = float(itp(-1.))\n-            #z_grid[i+1, -1] = float(itp( 1.))\n             z_grid[i+1 , 0] = float(itp(-1.5))\n             z_grid[i+1, -1] = float(itp( 1.5))\n \n         itp = interp1d(self.coords_y(), z_grid[ 0, 1:self.npar_y+1], fill_value='extrapolate')\n-        #z_grid[ 0,  0] = float(itp(-1.))\n-        #z_grid[ 0, -1] = float(itp( 1.))\n         z_grid[ 0,  0] = float(itp(-1.5))\n         z_grid[ 0, -1] = float(itp( 1.5))\n         \n         itp = interp1d(self.coords_y(), z_grid[-1, 1:self.npar_y+1], fill_value='extrapolate')\n-        #z_grid[-1,  0] = float(itp(-1.))\n-        #z_grid[-1, -1] = float(itp( 1.))\n         z_grid[-1,  0] = float(itp(-1.5))\n         z_grid[-1, -1] = float(itp( 1.5))\n \n@@ -243,28 +142,3 @@ class Fit2D_NP:\n \n     def addGlobalOffset(self, offset):\n         self.param_val[:] += offset\n-\n-\n-    # Extrapolate to the whole detector (i.e. in the range -1.5:1.5)\n-    #def extended_grid(self, x_inner, y_inner, z_inner):\n-    #    print(\"Start extended_grid\")\n-    #    x_outer = np.hstack((-1.5, x_inner, 1.5))\n-    #    y_outer = np.hstack((-1.5, y_inner, 1.5))\n-    #    z_outer = np.zeros((self.npar+2, self.npar+2))\n-    #    for i in range(0, self.npar):\n-    #        z_outer[1:self.npar+1, i+1] = z_inner[0:self.npar, i]\n-\n-    #    for i in range(0, self.npar):\n-    #        z_outer[i+1,  0] = float(interp1d(y_inner, z_inner[i, :], fill_value='extrapolate')(-1.5))\n-    #        z_outer[i+1, -1] = float(interp1d(y_inner, z_inner[i, :], fill_value='extrapolate')( 1.5))\n-\n-    #        z_outer[ 0, i+1] = float(interp1d(x_inner, z_inner[:, i], fill_value='extrapolate')(-1.5))\n-    #        z_outer[-1, i+1] = float(interp1d(x_inner, z_inner[:, i], fill_value='extrapolate')( 1.5))\n-\n-    #    z_outer[ 0,  0] = float(interp1d(x_inner, z_outer[1:self.npar+1,  0], fill_value='extrapolate')(-1.5))\n-    #    z_outer[-1,  0] = float(interp1d(x_inner, z_outer[1:self.npar+1,  0], fill_value='extrapolate')( 1.5))\n-\n-    #    z_outer[ 0, -1] = float(interp1d(x_inner, z_outer[1:self.npar+1, -1], fill_value='extrapolate')(-1.5))\n-    #    z_outer[-1, -1] = float(interp1d(x_inner, z_outer[1:self.npar+1, -1], fill_value='extrapolate')( 1.5))\n-    #    print(\"End extended_grid\")\n-    #    return (x_outer, y_outer, z_outer)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_LargeFlat/python/VIS_LargeFlat/VIS_LargeFlat_Modelling/SelfCalib_VIS.py": [
                        [
                            "@@ -8,9 +8,7 @@ import astropy.stats\n import tarfile\n \n import scipy.linalg\n-\n-\n-\n+from math import floor\n import astropy.coordinates\n from astropy.io import fits\n from astropy.table import Table\n@@ -107,52 +105,36 @@ def readFiles(filelist, ccdid_list, vis_fpa_geom, logger, outdir=None):\n             #    if hdul[ihdu].header['CCDID'] == vis_fpa_geom.idetseq2string(idet):\n             #        break                  \n             #assert hdul[ihdu].header['CCDID'] == vis_fpa_geom.idetseq2string(idet)\n+\n+            nextid_test = floor((idet-1)/6+1)\n+            mextid_test = (idet-1)%6+1\n+            extid_test = mextid_test + 6*(nextid_test-1)\n+            extid_test_new = nextid_test +6*(mextid_test-1)\n+            #extid_test_new = a +6*(b-1)\n             \n-            catTable_read = data['extid'] == idet  # Makes a boolean selection mask (numpy array)\n+            logger.info(\"idet: \" + str(idet) + \"ext_test: \" + str(extid_test) + \" ext_test_new: \" + str(extid_test_new) + \"  m: \" + str(mextid_test) + \" n: \" + str(nextid_test))\n+            \n+            #catTable_read = data['extid'] == idet  # Makes a boolean selection mask (numpy array)\n+            catTable_read = data['extid'] == extid_test_new # Makes a boolean selection mask (numpy array)\n             catTable = data[catTable_read]  # Makes a new astropy Table\n \n-            #****We don't need to look for ccd extension in the file. Always just extension 1.\n-            #data = Table.read(filelist[ifile], hdu=ihdu)\n-            #c = astropy.table.Column(name='ifile', data=np.repeat(ifile, len(data))); data.add_column(c)\n-            #c = astropy.table.Column(name='idet' , data=np.repeat(idet , len(data))); data.add_column(c)\n-            \n             c = astropy.table.Column(name='ifile', data=np.repeat(ifile, len(catTable))); catTable.add_column(c)\n             c = astropy.table.Column(name='idet' , data=np.repeat(idet , len(catTable))); catTable.add_column(c)\n             \n-            #****Rename of column for new wcsfit\n-            #(x, y) = vis_fpa_geom.pixel2focalplane(data['idet'], data['X_IMAGE'], data['Y_IMAGE'])\n-            #(x, y) = vis_fpa_geom.pixel2focalplane(data['idet'], data['XWIN_IMAGE'], data['YWIN_IMAGE'])\n-            \n             (x, y) = vis_fpa_geom.pixel2focalplane(catTable['idet'], catTable['XWIN_IMAGE'], catTable['YWIN_IMAGE'])            \n \n-            #c = astropy.table.Column(name='X_fp' , data=x); data.add_column(c)\n-            #c = astropy.table.Column(name='Y_fp' , data=y); data.add_column(c)\n             c = astropy.table.Column(name='X_fp' , data=x); catTable.add_column(c)\n             c = astropy.table.Column(name='Y_fp' , data=y); catTable.add_column(c)\n \n-\n-            #****Rename of column for new wcsfit\n-            #(x, y) = vis_fpa_geom.pixel2det(idet, data['X_IMAGE'], data['Y_IMAGE'])\n-            #(x, y) = vis_fpa_geom.pixel2det(idet, data['XWIN_IMAGE'], data['YWIN_IMAGE'])\n-            #logger.info(\"idetA: \" + str(idet) + \" \" + str(np.count_nonzero(catTable['YWIN_IMAGE'] >= 4136)))\n             (x, y) = vis_fpa_geom.pixel2det(idet, catTable['XWIN_IMAGE'], catTable['YWIN_IMAGE'])\n-            #c = astropy.table.Column(name='X_det' , data=x); data.add_column(c)\n-            #c = astropy.table.Column(name='Y_det' , data=y); data.add_column(c)\n-            #logger.info(\"idetB: \" + str(idet) + \" \" + str(np.count_nonzero(y >= 1.5)))\n+\n             c = astropy.table.Column(name='X_det' , data=x); catTable.add_column(c)\n             c = astropy.table.Column(name='Y_det' , data=y); catTable.add_column(c)\n \n-            \n-            #data['MAG_APER'][:] = np.add(data['MAG_APER'][:], 20).tolist()\n-            #print(\"readfiles ihdu: \" + str(ihdu))\n-\n-            #data.meta.clear()  # Avoid warnings when merging meta\n             catTable.meta.clear()\n             if len(obs) == 0:\n-                #obs = data.copy()\n                 obs = catTable.copy()\n             else:\n-                #obs = astropy.table.vstack([obs, data])\n                 obs = astropy.table.vstack([obs, catTable])\n \n     logger.info(\"Initial number of constraints : \" + str(len(obs)))\n@@ -177,11 +159,8 @@ def readFiles(filelist, ccdid_list, vis_fpa_geom, logger, outdir=None):\n     # Discard pairs whose distance is > distance threshold\n     dist_threshold = 0.3   # arcsec\n     #****WCFit change in column name but should it call findMatching twice? Rechck\n-    #(i1, i2) = findMatchingObs(obs['OBJECT_ID'])\n     (i1, i2) = findMatchingObs(obs['starid'])\n     #****WCSFit change in column name.\n-    #c1 = astropy.coordinates.SkyCoord(obs['ALPHA_J2000'][i1], obs['DELTA_J2000'][i1], unit=\"deg\")\n-    #c2 = astropy.coordinates.SkyCoord(obs['ALPHA_J2000'][i2], obs['DELTA_J2000'][i2], unit=\"deg\")\n     c1 = astropy.coordinates.SkyCoord(obs['RA'][i1], obs['DEC'][i1], unit=\"deg\")\n     c2 = astropy.coordinates.SkyCoord(obs['RA'][i2], obs['DEC'][i2], unit=\"deg\")\n \n@@ -233,7 +212,6 @@ class SelfCalibData:\n         # Prepare catalog of individual stars\n         catalog = Table()\n         #****\n-        #catalog['OBJECT_ID'] = np.unique(obs['OBJECT_ID'])\n         catalog['starid'] = np.unique(obs['starid'])\n         catalog['nobs'] = np.repeat(     0, len(catalog))\n         catalog['mag']  = np.repeat(np.NaN, len(catalog))\n@@ -248,7 +226,6 @@ class SelfCalibData:\n         for i in range(0, len(catalog)):\n             \n             #****\n-            #j = np.where(obs['OBJECT_ID'] == catalog['OBJECT_ID'][i])\n             j = np.where(obs['starid'] == catalog['starid'][i])\n             index_star2obs.append(j[0])\n             catalog['nobs'][i] = len(j[0])\n@@ -263,14 +240,12 @@ class SelfCalibData:\n             #****\n             #ss = np.sum(1 / obs['MAGERR_APER'][j])\n             ss = np.sum(1 / obs['MAGERR_AUTO'][j])\n-            #logger.info(\"in toopa i: \" + str(i))\n             for k in j[0]:\n                 rr.append(k)\n                 cc.append(i)\n                 #****\n                 #dd.append(1. / obs['MAGERR_APER'][k] / ss)\n                 dd.append(1. / obs['MAGERR_AUTO'][k] / ss)\n-            #logger.info(\"in loopb i: \" + str(i))\n \n         logger.info(\"calculate coomatrix\")\n         rr = np.asarray(rr)\n@@ -316,7 +291,6 @@ class SelfCalib:\n         mm = self\n \n         self.logger.info(\"In Prepare. Do FindMatchObject\")\n-        #(i1, i2) = findMatchingObs(obs['OBJECT_ID'])\n         (i1, i2) = findMatchingObs(obs['starid'])\n         self.logger.info(\"Done with FindMatchingObs in Prepare. Get Unique\")\n         i = np.unique([i1, i2])\n@@ -333,7 +307,6 @@ class SelfCalib:\n             i = mm.data.index_det2obs[idet]  \n             x = mm.data.obs['X_det'][i]\n             y = mm.data.obs['Y_det'][i]\n-            #print(\"Prepare idet: \" + str(idet) + \" ncoeff: \" + str(mm.ncoeff) + \"  i: \" + str(i) + \" x: \" + str(x) + \" y: \" + str(y) )\n             par_flat.append(Fit2D_NP(mm.ncoeff,mm.ncoeff, x, y,self.logger))\n \n         if hasattr(self, 'par_flat'):\n@@ -367,10 +340,6 @@ class SelfCalib:\n         \"\"\"TODO: Check about removing this loop.  Not sure why it is here except for debug to make sure it\n         has all the files.\n         \"\"\"\n-        #with open(os.path.join(outdir, \"filelist.txt\"), \"w\") as text_file:\n-        #    for ifile in range(0, len(filelist)):\n-        #        print(ifile, \"   \", filelist[ifile], file=text_file)\n-                \n         logger.info(\"Read files and prepare:\")\n         (obs, filter,exptime) = readFiles(filelist, ccdid_list, self.vis_fpa_geom,logger,outdir=outdir)\n         logger.info(\"Done with Read Files. Now Prepare\")\n@@ -493,9 +462,6 @@ class SelfCalib:\n         for i in range(0, gx.size):\n             mat[i, dets[i, 0]-1] =  1.5 ##Should it be 1.5\n             mat[i, dets[i, 1]-1] = -1.5\n-            #self.logger.info(\"KEVIN_MAT_NEW i: \" + str(i) + \" MAT: \" + str(mat[i]))\n-            #mat[i, dets[i, 0]-1] = -1 ##OUNIR Should it be 1.5. OUVIS shuld be 1.5\n-            #mat[i, dets[i, 1]-1] = -1 \n \n         mat[gx.size, :] = 1.5                  # add constraint for the average\n \n@@ -503,13 +469,11 @@ class SelfCalib:\n \n         #res = np.linalg.lstsq(mat, data, rcond=-1.)\n         res = np.linalg.lstsq(mat, data)\n-        #mm.par_detqe += res[0]\n         mm.par_detqe += res_scipy[0]\n \n         for idet in range(0, 36):  \n             val_scipy = -1. * res_scipy[0][idet]\n             mm.par_flat[idet].addGlobalOffset(val_scipy)\n-            #mm.par_flat[idet].addGlobalOffset(-1. * res[0][idet])\n \n         return out\n \n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_LargeFlat/python/VIS_LargeFlat/VIS_LargeFlat_Modelling/Utils.py": [
                        [
                            "@@ -6,7 +6,6 @@ import VIS_LargeFlat.VIS_LargeFlat_Modelling.FPA_VIS_Geometry as vis_geom\n \n def coordGapsX(vis_fpa_geom,n=1):\n     print(\"Start coordgapsx\")\n-    #print(\"Size detX: \" + str(vis_fpa_geom.sizeDetX()))\n     out = np.array(())\n     c = 0.\n     for i in range(0, 6):\n@@ -20,19 +19,13 @@ def coordGapsX(vis_fpa_geom,n=1):\n \n \n def coordGapsY(vis_fpa_geom,n=1):\n-    #print(\"Size detY: \" + str(vis_fpa_geom.sizeDetY()))\n-\n     out = np.array(())\n     c = 0.\n     for i in range(0, 6):\n         out = np.append(out, c + vis_fpa_geom.sizeDetY() * np.linspace(1, n, n) / (n+1));  c += vis_fpa_geom.sizeDetY();\n-        #if i <= 2:\n         if i <= 3:\n             out = np.append(out, c + vis_fpa_geom.sizeGapsY()[i] / 3);                     c += vis_fpa_geom.sizeGapsY()[i];\n-            #out = np.append(out, c + vis_fpa_geom.sizeGapsY()[i] / 2);                     c += vis_fpa_geom.sizeGapsY()[i];\n-    #out = np.asarray(out) / vis_fpa_geom.sizeFPY() * 2 - 1\n     out = np.asarray(out) / vis_fpa_geom.sizeFPY() * 3 - 1.5\n-    #out = np.asarray(out) / vis_fpa_geom.sizeFPY() * 2 - 1.5\n     return out\n \n \n@@ -43,7 +36,6 @@ def coordGaps(vis_fpa_geom,n=1):\n     y = coordGapsY(vis_fpa_geom,n)\n     outX = []\n     outY = []\n-    #print(\"CoordGaps LenX: \" + str(len(x)) + \" LenY: \" + str(len(y)))\n     for ix in range(0, len(x)):\n         for iy in range(0, len(y)):\n             # Discard points fallig on a detector\n@@ -56,29 +48,11 @@ def coordGaps(vis_fpa_geom,n=1):\n             (idet2, _x, _y) = vis_fpa_geom.focalplane2det(-1.5, y[iy])\n             if (idet1 == 0)  and  (idet2 == 0):\n                 continue\n-\n-            #print(\"GAPS idet: \" + str(idet) + \" idet1: \" + str(idet1) + \" idet2: \" + str(idet2) + \" xix: \" + str(x[ix]) + \" yiy: \" + str(y[iy]) + \" _x:\" + str(_x) + \" _y: \" + str(_y))\n             outX.append(x[ix])\n             outY.append(y[iy])\n     print(\"End coordGaps\")\n     return (np.asarray(outX), np.asarray(outY))\n \n-\n-#def allcombinations(x, y):\n-    #print(\"Start allcombinations\")\n-#    (x, y) = np.meshgrid(x, y)\n-#    x = x.reshape(x.size)\n-#    y = y.reshape(y.size)\n-    #print(\"End allcombinations\")\n-#    return (x, y)\n-\n-#def cartesianProduct(x, y):\n-#    (x, y) = np.meshgrid(x, y)\n-#    x = x.reshape(x.size)\n-#    y = y.reshape(y.size)\n-#    return (x, y)\n-\n-\n def cartesianProduct(x, y):\n     (x, y) = np.meshgrid(x, y, indexing='ij')\n     x = x.reshape(x.size)\n@@ -88,7 +62,6 @@ def cartesianProduct(x, y):\n def closestDetectors(vis_fpa_geom,x, y):\n     \n     c = np.array((-1.25,-0.75, -0.25, 0.25, 0.75,1.25))\n-    #(cy, cx) = allcombinations(c.copy(), c.copy())  # NOTE: this is correct: first y, then x.\n \n     (cx, cy) = cartesianProduct(c.copy(), c.copy())\n     idet = np.argsort(np.sqrt( (cx-x)**2  +  (cy-y)**2 ))[0:2] + 1\n@@ -100,37 +73,5 @@ def closestDetectors(vis_fpa_geom,x, y):\n         return idet[[1,0]]\n     return idet\n \n-\"\"\"\n-for loop in range(0, 2):\n-    fig = plt.figure()\n-    (x, y) = coordGaps(5); plt.scatter(x, y);\n-    for i in range(0, x.size):\n-        idet = closestDetectors(x[i], y[i])[loop]\n-        (idetx, idety) = idetseq2xy(idet)\n-        px = -0.75 + (idetx-1) * 0.5\n-        py = -0.75 + (idety-1) * 0.5\n-        plt.plot([x[i], px], [y[i], py])\n-plt.show()\n-\"\"\"\n-\n-\"\"\"\n-# Each point in the gaps (excluding points at the intersection of the\n-# gaps) has exactly two closest detectors.  Given x,y coordinate of a\n-# point, the closestDetectors() function returns the detector IDs of\n-# such detectors.  The following shows its usage in a plot:\n-for loop in range(0, 2):  # Select either the first or second closest det.\n-    fig = plt.figure()\n-    (x, y) = coordGaps(5);\n-    plt.scatter(x, y);\n-    for i in range(0, x.size):\n-        idet = closestDetectors(x[i], y[i])[loop]\n-        (idetx, idety) = idetseq2xy(idet)\n-        px = -0.75 + (idetx-1) * 0.5  # X coordinate of closest detector\n-        py = -0.75 + (idety-1) * 0.5  # Y coordinate of closest detector\n-        plt.plot([x[i], px], [y[i], py])\n-plt.show()\n-\"\"\"\n-\n-\n def gaussian(x, mu, sig):\n     return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))) / np.sqrt(2 * np.pi) / sig\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_LargeFlat/tests/python/EUC_MDB_2023-07-11_SpaceSegment.Instrument.VIS.xml": [
                        [
                            "@@ -0,0 +1,179 @@\n+<?xml version=\"1.0\" ?><ns0:DpdMdbDataBase xmlns:ns0=\"http://ecdm.euclid-ec.org/schema/dpd/mdb\">\n+ <Header>\n+  <ProductId>EUC_MDB_MISSIONCONFIGURATION-DEV_2023-07-11T15:00:00.00Z_00</ProductId>\n+  <ProductType>DpdMdbDataBase</ProductType>\n+  <SoftwareName>ST_MDB_Tools</SoftwareName>\n+  <SoftwareRelease>1.0</SoftwareRelease>\n+  <ProdSDC>UNKNOWN</ProdSDC>\n+  <DataSetRelease>NoDR</DataSetRelease>\n+  <Purpose>TEST</Purpose>\n+  <PlanId>NoPlan</PlanId>\n+  <PPOId>NoPPP</PPOId>\n+  <PipelineDefinitionId>NoPipelineDef</PipelineDefinitionId>\n+  <PpoStatus>COMPLETED</PpoStatus>\n+  <ManualValidationStatus>UNKNOWN</ManualValidationStatus>\n+  <Curator>SGS Euclid System Team</Curator>\n+  <CreationDate>2023-07-11T13:03:57.813000Z</CreationDate>\n+ </Header>\n+ <Data>\n+  <EuclidMissionParameterSet>\n+\n+\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISAveragePixelSizemicron\">\n+    <Description>This is the average physical size of the detector pixels on the focal plane.</Description>\n+    <Source>Input by J.A. As required. Not validated. For test in the MDB.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.1</Release>\n+    <Value>\n+     <Double unit=\"um\">\n+      <DoubleValue>12.0</DoubleValue>\n+     </Double>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISCCDChargeInjection\">\n+    <Description>This is the size of the charge injection structure that splits the CCD in the vertical direction.</Description>\n+    <Source>Input from J. Denniston EUCL-MSS-ICD-6-002 issue1.1. As Required</Source>\n+    <Expression>None</Expression>\n+    <Release>0.1</Release>\n+    <Value>\n+     <Long unit=\"pixel\">\n+      <LongValue>4</LongValue>\n+     </Long>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISCCDColumn\">\n+    <Description>This is the number of CCD on a column of the VIS focal plane assembly.</Description>\n+    <Source>Input by J.A. As required. Not validated. For test in the MDB.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.1</Release>\n+    <Value>\n+     <Long unit=\"NA\">\n+      <LongValue>6</LongValue>\n+     </Long>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISCCDGapLongDimensionNominalImage\">\n+    <Description>This is the nominal size of the gaps between 2 CCDs active area, in the VIS channel telescope image focal plane, in the longest dimension                of the VIS focal plane. The longest dimension of the VIS focal plane is projected on the sky along the Ysc axis. It corresponds to the                X_OPT_VIS axis. In order to get its size on the sky, the VIS channel telescope plate scale parameter must be used.</Description>\n+    <Source>Input by J.A. As required. Not validated, for test in the MDB. EUCL-MSS-RD-6-001 issue 1.3.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.2</Release>\n+    <Value>\n+     <Double unit=\"mm\">\n+      <DoubleValue>7.528</DoubleValue>\n+     </Double>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISCCDGapShortDimensionNominalImage\">\n+    <Description>This is the nominal size of the gaps between 2 CCDs active area, in the VIS channel telescope image focal plane, in the shortest                dimension of the VIS focal plane. The shortest dimension of the VIS focal plane is projected on the sky along the Xsc axis. It corresponds                to the Y_OPT_VIS axis. In order to get its size on the sky, the VIS channel telescope plate scale parameter must be used.</Description>\n+    <Source>Input by J.A. As required. Not validated, for test in the MDB. EUCL-MSS-RD-6-001 issue 1.3.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.2</Release>\n+    <Value>\n+     <Double unit=\"mm\">\n+      <DoubleValue>1.468</DoubleValue>\n+     </Double>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISCCDNumber\">\n+    <Description>This is the total number of CCD integrated into the VIS focal plane assembly.</Description>\n+    <Source>Input by J.A. As required. Not validated. For test in the MDB.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.1</Release>\n+    <Value>\n+     <Long unit=\"NA\">\n+      <LongValue>36</LongValue>\n+     </Long>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISCCDQuadrantList\">\n+    <Description>This is the set of identifiers of the Quadrants of the VIS detector. The detailed description of the quadrant configuration of the CCD                is in the document referenced in the source.</Description>\n+    <Source>Input from J. Denniston EUCL-MSS-ICD-6-002 issue1.1. As Required</Source>\n+    <Expression>None</Expression>\n+    <Release>0.1</Release>\n+    <Value>\n+     <Text>E F G H</Text>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISCCDRow\">\n+    <Description>This is the number of CCD on a row of the VIS focal plane assembly.</Description>\n+    <Source>Input by J.A. As required. Not validated. For test in the MDB.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.1</Release>\n+    <Value>\n+     <Long unit=\"NA\">\n+      <LongValue>6</LongValue>\n+     </Long>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISDetectorActivePixelLongDimensionFormat\">\n+    <Description>This is the minimum number of active pixels in the longest dimension of the VIS focal plane. Due to the injection line inserted in the                long direction of the CCD format, the active array is split in 2 equal active arrays of 2066 pixels.</Description>\n+    <Source>Input by J.A. As required. Not validated, for test in the MDB. EUCL-EST-PS-7-001.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.2</Release>\n+    <Value>\n+     <Long unit=\"pixel\">\n+      <LongValue>4132</LongValue>\n+     </Long>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISDetectorActivePixelShortDimensionFormat\">\n+    <Description>This is the minimum number of active pixels in the shortest dimension of the            VIS focal plane.</Description>\n+    <Source>Input by J.A. As required. Not validated, for test in the MDB.            EUCL-EST-PS-7-001.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.3</Release>\n+    <Value>\n+     <Long unit=\"pixel\">\n+      <LongValue>4096</LongValue>\n+     </Long>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISDetectorPixelLongDimensionFormat\">\n+    <Description>This is the minimum number of pixels in the longest dimension of the VIS focal plane. Due to the injection line inserted in the long                direction of the CCD format, the active array is split in 2 equal active arrays, with 4 inactive pixels in the middle for injection line.</Description>\n+    <Source>Input by J.A. As required. Not validated, for test in the MDB. EUCL-EST-PS-7-001.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.2</Release>\n+    <Value>\n+     <Long unit=\"pixel\">\n+      <LongValue>4136</LongValue>\n+     </Long>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISFocalPlaneAssemblyLongDimensionMaxImage\">\n+    <Description>This is the maximum size of the Focal Plane Assembly of the VIS, in the longest dimension of the VIS focal plane. The longest dimension                of the VIS focal plane is projected on the sky along the Ysc axis. It corresponds to the X_OPT_VIS axis. In order to get its size on the sky,                the VIS channel telescope plate scale parameter must be used.</Description>\n+    <Source>Input by J.A. As required. Not validated, for test in the MDB. EUCL-MSS-RD-6-001 issue 1.3.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.2</Release>\n+    <Value>\n+     <Double unit=\"mm\">\n+      <DoubleValue>336.932</DoubleValue>\n+     </Double>\n+    </Value>\n+   </Parameter>\n+\n+   <Parameter title=\"SpaceSegment.Instrument.VIS.VISFocalPlaneAssemblyShortDimensionMaxImage\">\n+    <Description>This is the minimum size of the Focal Plane Assembly of the VIS, in the shortest dimension of the VIS focal plane.                The shortest dimension of the VIS focal plane is projected on the sky along the Xsc axis. It corresponds to the Y_OPT_VIS axis.                In order to get its size on the sky, the VIS channel telescope plate scale parameter must be used.</Description>\n+    <Source>Input by J.A. As required. Not validated, for test in the MDB. EUCL-MSS-RD-6-001 issue 1.3.</Source>\n+    <Expression>None</Expression>\n+    <Release>0.2</Release>\n+    <Value>\n+     <Double unit=\"mm\">\n+      <DoubleValue>303.127</DoubleValue>\n+     </Double>\n+    </Value>\n+   </Parameter>\n+\n+  </EuclidMissionParameterSet>\n+ </Data>\n+</ns0:DpdMdbDataBase>\n\\ No newline at end of file\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_LargeFlat/tests/python/FPA_VIS_Geometry_test.py": [
                        [
                            "@@ -0,0 +1,108 @@\n+\"\"\"\n+@file VIS_Tasks/VIS_LargeFlat/tests/python/FPA_VIS_Geometry_test.py\n+@date 2023-07-25\n+@author mottet@iap.fr\n+\n+@description: VIS_LargeFlat/VIS_LargeFlat_Modelling/FPA_VIS_Geometry.py test script\n+\n+run with:\n+$ cd VIS_Tasks\n+$ make purge all install\n+$ ./build.*/run pytest --capture=no VIS_LargeFlat/tests/python/FPA_VIS_Geometry_test.py\n+\n+\"\"\"\n+\n+\n+################################################################################\n+\n+import pytest\n+import os\n+import re\n+import numpy as np\n+\n+import logging\n+logger = logging.getLogger( __name__)\n+\n+from ST_DM_MDBTools.Mdb import Mdb\n+\n+from VIS_LargeFlat.VIS_LargeFlat_Modelling import FPA_VIS_Geometry\n+from VIS_ImageTools_M import FPA_quadrant\n+\n+\n+#-------------------------------------------------------------------------------\n+\n+def ccd_row_col( ccdid):\n+  ccdrow, ccdcol = re.findall( \"[1-6]\", ccdid)\n+  return int( ccdrow), int( ccdcol)\n+\n+\n+#-------------------------------------------------------------------------------\n+\n+def is_close( x, y, err):\n+  return np.abs( x-y) < err\n+\n+\n+#-------------------------------------------------------------------------------\n+\n+def test_FPA_VIS_Geometry():\n+\n+  # clean log\n+  print()\n+\n+  script_dir = os.path.dirname( os.path.realpath( __file__))\n+\n+  mdb = Mdb( os.path.join( script_dir, \"EUC_MDB_2023-07-11_SpaceSegment.Instrument.VIS.xml\"))\n+\n+  vis_fpa_geom = FPA_VIS_Geometry.FPA_VISGeometry( mdb, logger)\n+\n+  ccd_list =      ['1-1','2-1','3-1','4-1','5-1','6-1']\n+  ccd_list.extend(['1-2','2-2','3-2','4-2','5-2','6-2'])\n+  ccd_list.extend(['1-3','2-3','3-3','4-3','5-3','6-3'])\n+  ccd_list.extend(['1-4','2-4','3-4','4-4','5-4','6-4'])\n+  ccd_list.extend(['1-5','2-5','3-5','4-5','5-5','6-5'])\n+  ccd_list.extend(['1-6','2-6','3-6','4-6','5-6','6-6'])\n+\n+  NCCD = len( ccd_list)\n+  assert NCCD == 36\n+\n+# test idetseq2string(), idetseq2xy() and idetxy2seq()\n+  for idet in range( 1, NCCD+1):\n+    ccd_id = ccd_list[idet-1]\n+    logger.debug( ccd_id)\n+    assert vis_fpa_geom.idetseq2string( idet) == ccd_id\n+    det_col, det_row = vis_fpa_geom.idetseq2xy( idet)\n+    idet_out = vis_fpa_geom.idetxy2seq( det_col, det_row)\n+    logger.info( f\"idetseq2xy( {ccd_id}): ccd_row={det_row}, ccd_col={det_col}\")\n+    assert idet_out == idet\n+    ccd_row, ccd_col = ccd_row_col( ccd_id)\n+    assert (det_col == ccd_col) and (det_row == ccd_row)\n+\n+# fpa_x/y are normalised FPA coordinates from -1.5 to 1.5\n+# det_x/y are normalised CCD coordinates from -1.5 to 1.5\n+# pix_x/y are CCD pixel coordinates, from (0, 0) to (4096, 4136)\n+\n+# test det2pixel(), pixel2det()\n+  maxerr = 0\n+  for det_x, det_y in ((-1.5, -1.5), (-1.5, +1.5), (+1.5, -1.5), (+1.5, +1.5), (0, 0)):\n+    for idet in (1, 6, 31, 36):\n+      ccd_id = vis_fpa_geom.idetseq2string( idet)\n+      pix_x, pix_y = vis_fpa_geom.det2pixel( idet, det_x, det_y)\n+      logger.info( f\"vis_fpa_geom.det2pixel( {ccd_id}, det_x={det_x}, det_y={det_y}): pix_x={pix_x}, pix_y={pix_y}\")\n+      det_x_out, det_y_out = vis_fpa_geom.pixel2det( idet, pix_x, pix_y)\n+      maxerr = max( np.abs( det_x - det_x_out), maxerr)\n+      maxerr = max( np.abs( det_y - det_y_out), maxerr)\n+  maxerr_pix = maxerr / 3.0 * 4100\n+  logger.info( f\"det2pixel() - pixel2det() max error = {maxerr_pix} pixels\")\n+\n+# test det2focalplane(), focalplane2det()\n+  for idet in range( 1, NCCD+1):\n+    for det_x, det_y in ((-1.5, -1.5), (+1.5, +1.5), (0, 0)):\n+      ccd_id = vis_fpa_geom.idetseq2string( idet)\n+      fpa_x, fpa_y = vis_fpa_geom.det2focalplane( idet, det_x, det_y)\n+      logger.info( f\"vis_fpa_geom.det2focalplane( {idet}, {det_x}, {det_y}): fpa_x={fpa_x}, fpa_y={fpa_y}\")\n+      idet_out, det_x_out, det_y_out = vis_fpa_geom.focalplane2det( fpa_x, fpa_y)\n+      assert idet == idet_out\n+      assert det_x == det_x_out\n+      assert det_y == det_y_out\n+\n+  return\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_NonLinearity/tests/python/VIS_remove_non_lin_flat_test.py": [
                        [
                            "@@ -36,7 +36,12 @@ logger = logging.getLogger( __name__)\n \n \n def test_non_lin_flat_removal():\n+  # to get the log messages, execute with:\n+  # build.*/run pytest --capture=no VIS_NonLinearity_Flat/tests/python/VIS_remove_non_lin_flat_test.py\n+\n   # from https://euclid.roe.ac.uk/issues/22517#note-28\n+  # first value in the pair is the linear signal in ADU,\n+  # the second one is the correcponding non-linear signal in ADU\n   NLMODEL = \"\"\"{\n \"1-1.E\": [\n     [\n@@ -101,18 +106,29 @@ def test_non_lin_flat_removal():\n     ]\n   ]\n }\"\"\"\n+  logger.info( \"\")\n   # correct nonlinearity in a random array\n   non_lin_flat_cfg = { \"linear_up_to_adu\": 300 }\n   model_points = np.array( json.loads( NLMODEL)[\"1-1.E\"])\n   nonlin_image = np.random.rand( 1000, 1000) * 65535.0 - 100.0\n   linear_image = VIS_NonLin_Removal.remove_non_lin_flat( nonlin_image, model_points, non_lin_flat_cfg)\n+  pixels_above_1000 = linear_image > 1000\n+\n+  # check that correction goes in the right way\n+  # (with model above, linear values above 1000 ADU are lower than non-linear values)\n+  logger.info( f\"above 1000 ADU correction mean: {np.mean( linear_image[pixels_above_1000] - nonlin_image[pixels_above_1000]):.2f} ADU\")\n+  assert np.all( linear_image[pixels_above_1000] < nonlin_image[pixels_above_1000])\n+\n   # invert nl correction model\n   inv_model = model_points[:,::-1]\n+\n   # convert linear_image back to non linear image\n   nonlin_image2 = VIS_NonLin_Removal.remove_non_lin_flat( linear_image, inv_model, non_lin_flat_cfg)\n   diff = nonlin_image2 - nonlin_image\n+\n+  # and check various stats\n   logger.info( f\"max difference={np.abs( diff).max()}\")\n   logger.info( f\"abs difference sum={np.abs( diff).sum()}\")\n-  logger.info( f\"different pixels count={np.count_nonzero( diff)}\")\n+  logger.info( f\"different pixels count={np.count_nonzero( diff):,}\")\n   assert np.abs( diff).sum() < 1\n   return\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PRNU/python/VIS_PRNU/VIS_CombinePRNU.py": [
                        [
                            "@@ -1,222 +0,0 @@\n-#\n-# Copyright (C) 2012-2020 Euclid Science Ground Segment\n-#\n-# This library is free software; you can redistribute it and/or modify it under\n-# the terms of the GNU Lesser General Public License as published by the Free\n-# Software Foundation; either version 3.0 of the License, or (at your option)\n-# any later version.\n-#\n-# This library is distributed in the hope that it will be useful, but WITHOUT\n-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-# FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n-# details.\n-#\n-# You should have received a copy of the GNU Lesser General Public License\n-# along with this library; if not, write to the Free Software Foundation, Inc.,\n-# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n-#\n-\n-\n-\"\"\"\n-:file: python/VIS_PRNU/VIS_CombinePRNU.py\n-\n-:date: 05 Jul. 22\n-:author: mottet@iap.fr\n-\n-\"\"\"\n-\n-import argparse\n-import os\n-from astropy.io import fits\n-import json\n-import logging\n-logger = logging.getLogger( __name__)\n-\n-from ST_DM_DmUtils import DmUtils\n-from ST_DM_FilenameProvider.FilenameProvider import FileNameProvider\n-from VIS_Tasks_Common import FromToXML\n-\n-\n-################################################################################\n-# global definitions\n-\n-LED_IDS = (\"LED1\", \"LED2\", \"LED3\", \"LED4\", \"LED5\", \"LED6\")\n-\n-\n-################################################################################\n-\n-def example_flat_model():\n-  \"\"\"Create a flat linear combination model:\n-model = {\n-  \"LED1\": 0.16666666666666667,\n-  \"LED2\": 0.16666666666666667,\n-  \"LED3\": 0.16666666666666667,\n-  \"LED4\": 0.16666666666666667,\n-  \"LED5\": 0.16666666666666667,\n-  \"LED6\": 0.16666666666666667\n-}\n-\"\"\"\n-\n-  model = dict()\n-  for led_id in LED_IDS:\n-    model[led_id] = 1.0 / len( LED_IDS)\n-  return model\n-\n-\n-################################################################################\n-\n-def write_combine_model( combination_model, filename):\n-  \"\"\"Write the 'combination_model' to 'filename' JSON file\"\"\"\n-\n-  with open( filename, \"w\") as f:\n-    json.dump( combination_model, f, indent=2)\n-  return\n-\n-\n-################################################################################\n-\n-def read_combine_model( filename):\n-  \"\"\"Read a PRNU combination model from 'filename' JSON file\n-  and test it contains exactly one element per LED\"\"\"\n-\n-  with open( filename, \"r\") as f:\n-    model = json.load( f)\n-  for led_id in LED_IDS:\n-    assert led_id in model\n-  return model\n-\n-\n-################################################################################\n-\n-def get_PRNU_LED( prnu_filename, prim_hdr):\n-  \"\"\"Return the number of the first LED which status is 'On' in 'prnu_filename' primary header, eg.:\n-CULEDDUR = 0.2 / If CU on: pulse duration in (s)\n-LED1STA  = Off / If CU on: LED 1 status: On or Off\n-LED1CUR  = 0   / If LED1 on: current (mA)\n-LED2STA  = On  / If CU on: LED 2 status: On or Off\n-LED2CUR  = 19  / If LED2 on: current (mA)\n-LED3STA  = Off / If CU on: LED 3 status: On or Off\n-LED3CUR  = 0   / If LED3 on: current (mA)\n-LED4STA  = Off / If CU on: LED 4 status: On or Off\n-LED4CUR  = 0   / If LED4 on: current (mA)\n-LED5STA  = Off / If CU on: LED 5 status: On or Off\n-LED5CUR  = 0   / If LED5 on: current (mA)\n-LED6STA  = Off / If CU on: LED 6 status: On or Off\n-LED6CUR  = 0   / If LED6 on: current (mA)\n-\"\"\"\n-\n-  for led_id in LED_IDS:\n-    card = f\"{led_id}STA\"\n-    if (card in prim_hdr) and (prim_hdr[card].strip().lower() == \"on\"):\n-      return led_id\n-  raise Exception( f\"No active LED found in input monochromatic PRNU {prnu_filename}\")\n-  return\n-\n-\n-################################################################################\n-\n-def combine_PRNU( prnu_filelist, combine_model, prnu_file_out):\n-  \"\"\"Linearly combine all quadrants in PRNU files in 'prnu_filelist'\n-  with coefficients in 'combine_model' and write the result in 'prnu_file_out' FITS file\n-\"\"\"\n-\n-  # get LED identifiant of each input monochromatic PRNU\n-  # and create a primary header storing the combined PRNU file names and coefficients\n-  prnu_dict = dict()\n-  prim_hdr = dict()\n-  for prnu_filename in prnu_filelist:\n-    hdul_in = fits.open( prnu_filename, memmap=False)\n-    led_id = get_PRNU_LED( prnu_filename, hdul_in[0].header)\n-    prnu_dict[led_id] = hdul_in\n-    prim_hdr[led_id + \"FILE\"] = os.path.basename( prnu_filename)\n-    prim_hdr[led_id + \"COEF\"] = combine_model[led_id]\n-\n-  assert set( prnu_dict.keys()) == set( LED_IDS), \\\n-    f\"ERROR: there must be exactly one input monochromatic PRNU for each LED ({len( LED_IDS)})\"\n-\n-  # create the output combined PRNU FITS file primary HDU\n-  logger.info( \"create output combined PRNU file: \" + prnu_file_out)\n-  fits.HDUList( [fits.PrimaryHDU( header=prim_hdr)]).writeto( prnu_file_out, overwrite=True)\n-\n-  # combine each quadrant and append it to ouput FITS file\n-  hdul0 = prnu_dict[LED_IDS[0]]\n-  for hdu in hdul0[1:]:\n-    combined_quad = np.zeros_like( hdu.data)\n-    extname = hdu.header[\"EXTNAME\"]\n-    logger.info( \"combine \" + extname)\n-    for led_id in LED_IDS:\n-      combined_quad += combine_model[led_id] * prnu_dict[led_id][extname].data\n-      prnu_dict[led_id][extname].data = None\n-    fits.append( prnu_file_out, fits.ImageHDU( data=combined_quad, name=extname), verify=False)\n-  return\n-\n-\n-################################################################################\n-\n-def defineSpecificProgramOptions():\n-\n-  ap = argparse.ArgumentParser( description=\"Monochromatic PRNU combination pipeline\")\n-\n-  # https://euclid.roe.ac.uk/projects/codeen-users/wiki/Pipeline_Interfaces#Arguments\n-  # \"The workdir and logdir parameters are mandatory - any task to be run as part of a pipeline needs to support these parameters\"\n-  ap.add_argument( \"--workdir\",        required=True, help=\"absolute path to the working directory\")\n-  ap.add_argument( \"--logdir\",         required=True, help=\"path to the log directory, relative to 'workdir'\")\n-  # parameters\n-  ap.add_argument( \"--MDB\",            required=True, help=\"MDB XML file path relative to 'workdir'\")\n-  # input\n-  ap.add_argument( \"--monoprnu_list\",  required=True, help=\"list of input monochromatic PRNU to combine\")\n-  # output\n-  ap.add_argument( \"--masterflat_out\", required=True, help=\"output model XML file path, relative to workdir\")\n-  return ap\n-\n-\n-################################################################################\n-\n-def mainMethod( args):\n-\n-  logger.info( \"#\")\n-  logger.info( \"# Entering %s mainMethod()\" % __name__)\n-  logger.info( \"# command line: \" + pipe_tools.get_erun_commandline())\n-  pipe_tools.log_task_environment( logger)\n-  logger.info( \"#\")\n-\n-  data_dir = os.path.join( args.workdir, \"data\")\n-\n-  # get input monochromatic PRNU files list\n-  prnu_filelist = list()\n-  prnu_xml_list = FromToXML.load_list_from_file( os.path.join( args.workdir, args.monoprnu_list))\n-  for xml_file in prnu_xml_list:\n-    prnu = FromToXML.get_xml_data_file_list( os.path.join( workdir, xml_file))[0]\n-    prnu_filelist.append( os.path.join( data_dir, prnu))\n-  assert len( prnu_filelist) == len( LED_IDS), \\\n-    f\"ERROR: there must be one input monochromatic PRNU file per LED ({len( LED_IDS)})\"\n-\n-  # get PRNU combination model from MDB\n-  if not os.path.isabs( args.MDB):\n-    args.MDB = os.path.join( args.workdir, args.MDB)\n-  mdb = ST_DM_MDBTools.Mdb.Mdb( args.MDB)\n-  combine_model_key = \"SpaceSegment.Instrument.VIS.PRNUCombineModel\"\n-  if combine_model_key in mdb:\n-    combine_model = read_combine_model( mdb.get_value( combine_model_key))\n-  else:\n-    # dev-only, to be removed when actual combination model put in MDB\n-    logger.error( f\"!!! '{combine_model_key}' not found in MDB, using flat model instead\")\n-    combine_model = example_flat_model()\n-\n-  # produce the output combined PRNU file\n-  combined_PRNU = FileNameProvider().get_allowed_filename( processing_function=\"VIS\",\n-                                                           type_name=\"MSR-FLA-\",\n-                                                           instance_id=\"\",\n-                                                           extension=\".fits\")\n-  combined_PRNU = os.path.join( data_dir, combined_PRNU)\n-  logger.info( \"writing output combined PRNU FITS file to \" + combined_PRNU)\n-  combine_PRNU( prnu_filelist, combine_model, combined_PRNU)\n-\n-  # create and write output XML file encapsulating combined PRNU FITS file\n-  xml_out = FromToXML.master_flat_dp( combined_PRNU)\n-  logger.info( \"writing output XML file to \" + args.masterflat_out)\n-  DmUtils.save_product_metadata( xml_out, args.masterflat_out)\n-\n-  logger.info( \"#\")\n-  logger.info( \"# Exiting %s mainMethod()\" % __name__)\n-  logger.info( \"#\")\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PRNU/python/VIS_PRNU/VIS_SmallScaleFlat/VIS_CombinePRNU.py": [
                        [
                            "@@ -77,7 +77,7 @@ def combine_flagmaps( flagin_list, flagmap_out):\n   hdul0 = fits.open( flagin_list[0], memmap=False)\n \n   # create output flagmap\n-  logger.info( \"writing output flagmap \" + flagmap_out)\n+  logger.info( \"creating output flagmap \" + flagmap_out)\n   fits.writeto( flagmap_out, data=None, header=hdul0[0].header, overwrite=True)\n \n   # scan and combine flagmaps extensions\n@@ -102,18 +102,23 @@ def combine_PRNU( prnu_filelist, combine_model, timestamp):\n   # get LED identifiant of each input monochromatic PRNU\n   # and create a primary header storing the combined PRNU file names and coefficients\n   prnu_dict = dict()\n-  prim_hdr = dict()\n+  fluences = set()\n+  prim_hdr = None\n   for prnu_filename in prnu_filelist:\n     hdul_in = fits.open( prnu_filename, memmap=False)\n+    if prim_hdr is None:\n+      prim_hdr = hdul_in[0].header\n     led_id, fluence = VIS_SmallScaleFlat_xml_in.get_PRNU_metadata( prnu_filename)\n-    logger.info( f\"{prnu_filename}, LED{led_id}\")\n+    logger.info( f\"{prnu_filename}: LEDID={led_id}, FLUENCE={fluence}\")\n     prnu_dict[led_id] = hdul_in\n+    fluences.add( fluence)\n     prim_hdr[led_id + \"FILE\"] = os.path.basename( prnu_filename)\n     prim_hdr[led_id + \"COEF\"] = combine_model[\"LED\"+led_id]\n \n   led_ids = sorted( list( set( prnu_dict.keys())))\n   assert len( led_ids) == len( prnu_dict.keys()), \\\n     f\"ERROR: there must be only one input zero-signal PRNU per LED\"\n+  prim_hdr[\"LEDID\"] = ''.join( led_ids)\n \n   norm = 0\n   for led_id in led_ids:\n@@ -121,8 +126,13 @@ def combine_PRNU( prnu_filelist, combine_model, timestamp):\n   logger.info( f\"model norm for leds {led_ids}: {norm}\")\n \n   # create the output combined PRNU FITS file primary HDU\n-  small_scale_flat_out = os.path.join( DATADIR, f\"EUC_VIS_MSR-FLA_LED{''.join( led_ids)}-000ADU__{timestamp}.fits\")\n-  logger.info( \"create output combined PRNU file: \" + small_scale_flat_out)\n+  led_id = ''.join( led_ids)\n+  if len( fluences) == 1:\n+    fluence = VIS_SmallScaleFlat_xml_in.fluence_to_str( list( fluences)[0])\n+  else:\n+    fluence = \"000\"\n+  small_scale_flat_out = os.path.join( DATADIR, f\"EUC_VIS_MSR-FLA_LED{led_id}-{fluence}ADU__{timestamp}.fits\")\n+  logger.info( \"creating output combined PRNU \" + small_scale_flat_out)\n   prim_hdu = fits.HDUList( [fits.PrimaryHDU( header=fits.Header( prim_hdr))])\n   prim_hdu.writeto( small_scale_flat_out, overwrite=True)\n \n@@ -159,7 +169,7 @@ def defineSpecificProgramOptions():\n   # outputs\n   ap.add_argument( \"--smallscaleflat_out\", help=\"output SmallScaleFlat data product XML file path\")\n   ap.add_argument( \"--prnuflagmap_out\",    help=\"output flagmap data product XML file path\")\n-  ap.add_argument( \"--dqc_data_out\",       help=\"output data quality control data product XML file path\")\n+  ap.add_argument( \"--dqc_out\",       help=\"output data quality control data product XML file path\")\n   return ap\n \n \n@@ -190,7 +200,7 @@ def mainMethod( args):\n   xml_list = FromToXML.load_list_from_file( args.flagmaps_in)\n   for xml in xml_list:\n     flagmaps_in.append( os.path.join( DATADIR, FromToXML.get_xml_data_file_list( xml)[0]))\n-  flagmap_out = small_scale_flat_out.replace(\"EUC_VIS_MSR-FLA_LED\", \"EUC_VIS_MSR-FLA-FLAGMAP_LED\")\n+  flagmap_out = small_scale_flat_out.replace( \"EUC_VIS_MSR-FLA_LED\", \"EUC_VIS_MSR-FLA-FLAGMAP_LED\")\n   combine_flagmaps( flagmaps_in, flagmap_out)\n \n   # create SmallScaleFlat data product\n@@ -203,6 +213,13 @@ def mainMethod( args):\n \n   # create data quality control data product\n   # encapsulating extrapolated master flats and standard errors and slopes\n+  dp_dqc_out = FromToXML.create_analysis_dp( result_passed=True,\n+                                             aux_file=None,\n+                                             raw_frame_id=None,\n+                                             analysis_id=None,\n+                                             obs_seq_foranalysis=None,\n+                                             analysis_data=all_flatfiles)\n+  DmUtils.save_product_metadata( dp_dqc_out, args.dqc_out)\n \n   logger.info( \"#\")\n   logger.info( \"# Exiting %s mainMethod()\" % __name__)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PRNU/python/VIS_PRNU/VIS_SmallScaleFlat/VIS_ExtrapolatePRNU.py": [
                        [
                            "@@ -38,6 +38,7 @@ logger = logging.getLogger( __name__)\n from astropy.io import fits\n \n from VIS_Tasks_Common import FromToXML\n+from VIS_PyLibrary_Common.common_definitions import DATADIR\n from VIS_PyLibrary_Common import pipe_tools\n from VIS_PRNU.VIS_SmallScaleFlat import VIS_SmallScaleFlat_xml_in\n \n@@ -91,15 +92,16 @@ def extrapolate_prnu( ledid, prnulist_in, fluences, out_fitsname, err_fitsname,\n         b = (n*Sxy - Sx*Sy) / nSxx_SxSx\n         # intercept\n         a = Sy/n - b * Sx/n\n-        # standard errors\n-        # https://en.wikipedia.org/wiki/Standard_error\n-        Se2 = (n*Syy - Sy*Sy - b*b*nSxx_SxSx) / (n * (n-2))\n-        Sb2 = n*Se2 / nSxx_SxSx\n-        Sa2 = Sb2 * Sxx / n\n         # save outputs\n         data_out[row, col]  = a\n-        error_out[row, col] = np.sqrt( Sa2)\n         slope_out[row, col] = b\n+        # standard errors\n+        # https://en.wikipedia.org/wiki/Standard_error\n+        if n>2:\n+          Se2 = (n*Syy - Sy*Sy - b*b*nSxx_SxSx) / (n * (n-2))\n+          Sb2 = n*Se2 / nSxx_SxSx\n+          Sa2 = Sb2 * Sxx / n\n+          error_out[row, col] = np.sqrt( Sa2)\n     hdu.data = None\n     fits.append( out_fitsname,   data_out,  header=hdu.header, verify=False)\n     fits.append( err_fitsname,   error_out, header=hdu.header, verify=False)\n@@ -150,10 +152,19 @@ def mainMethod( args):\n     fluences.append( fluence)\n \n   timestamp = FromToXML.microsec_timestamp()\n-  out_fitsname   = f\"EUC_VIS_MSR-FLA_LED{ledid}-000ADU__{timestamp}.fits\"\n-  err_fitsname   = f\"EUC_VIS_MSR-FLA_LED{ledid}-000ADU-stderr__{timestamp}.fits\"\n-  slope_fitsname = f\"EUC_VIS_MSR-FLA_LED{ledid}-000ADU-slope__{timestamp}.fits\"\n-  extrapolate_prnu( ledid, prnu_list, fluences, out_fitsname, err_fitsname, slope_fitsname)\n+\n+  if len( fluences) > 1:\n+    out_fitsname   = os.path.join( DATADIR, f\"EUC_VIS_MSR-FLA_LED{ledid}-000ADU__{timestamp}.fits\")\n+    err_fitsname   = os.path.join( DATADIR, f\"EUC_VIS_MSR-FLA_LED{ledid}-000ADU-stderr__{timestamp}.fits\")\n+    slope_fitsname = os.path.join( DATADIR, f\"EUC_VIS_MSR-FLA_LED{ledid}-000ADU-slope__{timestamp}.fits\")\n+    extrapolate_prnu( ledid, prnu_list, fluences, out_fitsname, err_fitsname, slope_fitsname)\n+  else:\n+    # only one fluence, can't extrapolate masterflat, just use it as is\n+    fluence        = VIS_SmallScaleFlat_xml_in.fluence_to_str( fluences[0])\n+    out_fitsname   = os.path.join( DATADIR, f\"EUC_VIS_MSR-FLA_LED{ledid}-{fluence}ADU__{timestamp}.fits\")\n+    err_fitsname   = None\n+    slope_fitsname = None\n+    FromToXML.failsafe_symlink( prnu_list[0], out_fitsname)\n \n   # write to output\n   FromToXML.write_json( (out_fitsname, err_fitsname, slope_fitsname), args.prnulist_out)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PRNU/python/VIS_PRNU/VIS_SmallScaleFlat/VIS_SmallScaleFlat_xml_in.py": [
                        [
                            "@@ -62,6 +62,16 @@ def get_PRNU_metadata( prnu_filename):\n   return str( prim_hdr[\"LEDID\"]), float( prim_hdr[\"FLUENCE\"])\n \n \n+################################################################################\n+\n+def fluence_to_str( fluence):\n+  \"\"\"convert fluence from float to string using the kilo prefix when needed\"\"\"\n+  if fluence < 1000:\n+    return f\"{int( fluence)}\"\n+  else:\n+    return f\"{int( fluence/1000)}K\"\n+\n+\n ################################################################################\n \n def defineSpecificProgramOptions():\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PSF/auxdir/VIS_PSF/default.psfex": [
                        [
                            "@@ -14,7 +14,7 @@ NEWBASIS_NUMBER 8               # Number of new basis vectors\n PSF_SAMPLING    1.0             # Sampling step in pixel units (0.0 = auto)\n PSF_PIXELSIZE   1.0             # Effective pixel size in pixel step units\n PSF_ACCURACY    0.015           # Accuracy to expect from PSF \"pixel\" values\n-PSF_SIZE        20,20           # Image size of the PSF model\n+PSF_SIZE        21,21           # Image size of the PSF model\n PSF_DGEOCORRECT N               # Use diff. geom. maps (if provided) Y/N?\n PSF_RECENTER    N               # Allow recentering of PSF-candidates Y/N ?\n MEF_TYPE        INDEPENDENT     # INDEPENDENT or COMMON\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PTC_NL/python/VIS_PTC_NL/VIS_PTC_NL_calibration.py": [
                        [
                            "@@ -58,7 +58,7 @@ import ElementsKernel.Logging\n logger = ElementsKernel.Logging.getLogger( __name__)\n \n \n-def compute_ptc(flat_1, flat_2, quad_name, n_tile=10, bf_corr_typ=1, kernelsize=12, fltn=0):\n+def compute_ptc(flat_1, flat_2, quad_name, n_tile=10, bf_corr_typ=1, fltn=0, kernelsize=12, sigclip=5):\n     '''\n     @brief\n     flat_1 and flat_2 are two flats of a single pair (already processed)\n@@ -77,57 +77,51 @@ def compute_ptc(flat_1, flat_2, quad_name, n_tile=10, bf_corr_typ=1, kernelsize=\n     tab_diff = np.array([])\n \n     if (bf_corr_typ == 0):\n-        tab_var, tab_mean, tab_err_var  = Compute_mean_var_correctedfromBFE(flat_1, flat_2, n_tile, 0, mm=1, fltn=fltn)\n-    elif (bf_corr_typ == 1):\n-         # we correct from brighter-fatter with covariance'\n-         tab_var, tab_mean, tab_err_var = Compute_mean_var_correctedfromBFE(flat_1, flat_2, n_tile, kernelsize, mm=1, fltn=fltn)\n+        tab_var, tab_mean, tab_err_var = Compute_mean_var_correctedfromBFE(flat_1-flat_2,(flat_1+flat_2)/2.,np.ones(np.shape(flat_1)), n_tile, fltn=fltn, sigclip=sigclip)\n+    #elif (bf_corr_typ == 1):\n+    #     # we correct from brighter-fatter with covariance'\n+    #     tab_var, tab_mean, tab_err_var = Compute_mean_var_correctedfromBFE(flat_1, flat_2, n_tile, kernelsize, mm=1, fltn=fltn)\n     else:\n          # we first rebin the quadrants\n-         new_flat_1, new_flat_2         = rebin3(flat_1, flat_2, kernelsize)\n-         tab_var, tab_mean, tab_err_var = Compute_mean_var_correctedfromBFE(new_flat_1, new_flat_2, n_tile, 0, mm=1, fltn=fltn)\n+\n+         new_flat_1, new_flat_2, wg     = rebin3(flat_1-flat_2, (flat_2+flat_1)/2., kernelsize, sigclip=sigclip)\n+         tab_var, tab_mean, tab_err_var = Compute_mean_var_correctedfromBFE(new_flat_1, new_flat_2, wg, n_tile, fltn=fltn,sigclip=sigclip)\n          tab_var                        = tab_var*kernelsize*kernelsize\n          tab_err_var                    = tab_err_var*(kernelsize*kernelsize)**2\n-\n     return tab_var, tab_mean, np.sqrt(tab_err_var)\n \n \n-def Compute_mean_var_correctedfromBFE(image_1, image_2, n_tile, kernelsize, mm=1, fltn=0):\n+def Compute_mean_var_correctedfromBFE(image_s1, image_s2, wg, n_tile, fltn=0, sigclip=5):\n     '''\n     @brief\n-    image_1 and image_2 are two offset-subtracted flats.\n-    kernelsize: if >0, BFE is corrected from the \"covariance method\"\n-    if fltn=1, the flats are flattened, and pixels are sorted by values and divided in a certain number of bins. Variance / mean are computed on\n+    image_1 and image_2 are respectively the difference and the mean of  two offset-subtracted flats.\n+    if fltn=1, the flats are flattened, and pixels are sorted by values and divided in a certain number of bins.\n     @return\n     '''\n-    DD = (image_1-image_2)\n-    MM = (image_1+image_2)/2.\n-\n-    if (kernelsize > 0):\n-        image_s1 = image_1[kernelsize:-kernelsize,kernelsize:-kernelsize]\n-        image_s2 = image_2[kernelsize:-kernelsize,kernelsize:-kernelsize]\n-    else:\n-        image_s1 = image_1\n-        image_s2 = image_2\n-\n-    if (fltn == 0):\n-        MM_s = (image_s1+image_s2)/2.\n-        DD_s = (image_s1-image_s2)\n-        # ww = np.where((np.abs(DD_s)<MM_s) * (np.isfinite(DD_s)))\n-    else:\n-        ### flatten the image and sort the pixels by their values\n-        MM_s = ((image_s1+image_s2).flatten()/2.)\n-        DD_s = ((image_s1-image_s2).flatten())\n-        idi = np.argsort(MM_s)\n-        # ww = np.where((np.abs(DD_s[idi])<MM_s[idi]) * (np.isfinite(DD_s[idi])))[0]\n-        # idi = idi[ww]\n+    DD_s = image_s1\n+    MM_s = image_s2\n+\n+    if (fltn == 1):\n+        MM_s = image_s2.flatten()\n+        DD_s = image_s1.flatten()\n+        wg   = wg.flatten()\n+        # keep only the non-masked pixels\n+        ww = np.where((ma.getmask(DD_s) == False)*(ma.getmask(MM_s) == False))[0]\n+        idi = np.argsort(MM_s[ww])\n+        idi = ww[idi]\n+        # additional sigma-clipping to be sure to remove the outliers\n+        mymas = astropy.stats.sigma_clip(DD_s[idi],sigma = sigclip)\n+        mask_ = ma.getmask(mymas)\n+        ww = np.where((mask_ == False))[0]\n+        idi = idi[ww]\n \n     n1 = np.shape(image_s1)[0]\n     n2 = np.shape(image_s2)[1]\n \n     if (fltn == 0):\n         #n_tile is the size in pixel of the tile\n-        n_tile_1 = n_tile # np.int(n1/n_tile)\n-        n_tile_2 = n_tile #np.int(n2/n_tile)\n+        n_tile_1 = n_tile\n+        n_tile_2 = n_tile\n         fluxmean_all = np.zeros(n_tile_1*n_tile_2)\n         variance_all = np.zeros(n_tile_1*n_tile_2)\n         err_var_all  = np.zeros(n_tile_1*n_tile_2)\n@@ -135,46 +129,27 @@ def Compute_mean_var_correctedfromBFE(image_1, image_2, n_tile, kernelsize, mm=1\n         n_pix_2 = np.int(n2/n_tile)\n         for k in range(n_tile_1):\n             for j in range(n_tile_2):\n+                ic = k*n_tile_2 +j\n                 MM_tmp = MM_s[k*n_pix_1:(k+1)*n_pix_1,j*n_pix_2:(j+1)*n_pix_2]\n                 DD_tmp = DD_s[k*n_pix_1:(k+1)*n_pix_1,j*n_pix_2:(j+1)*n_pix_2]\n-\n-                # ww = np.where((np.abs(DD_tmp)<MM_tmp)*(np.isfinite(DD_tmp)))\n-                # n_pt = len(MM[ww])\n-                DD_tmp = astropy.stats.sigma_clip(DD_tmp,sigma = 5)\n-                mask_ = ma.getmask(DD_tmp)\n+                wg_tmp = wg[k*n_pix_1:(k+1)*n_pix_1,j*n_pix_2:(j+1)*n_pix_2]\n+                mymas = astropy.stats.sigma_clip(DD_tmp,sigma = sigclip)\n+                mask_ = ma.getmask(mymas)\n                 w = np.where(mask_ == False)\n                 MM_tmp = MM_tmp[w]\n+                DD_tmp = DD_tmp[w]\n+                wg_tmp = wg_tmp[w]\n                 n_pt = len(MM_tmp.flatten())\n-                ic = k*n_tile_2 + j\n-\n-                if (mm==1):\n-                    fluxmean_all[ic] = np.mean(MM_tmp)\n-                else:\n-                    fluxmean_all[ic] = np.median(MM_tmp)\n-                if (kernelsize == 0):\n-                    variance_all[ic] = np.std((DD_tmp))**2\n-                    mu_4 = np.mean((DD_tmp - np.mean(DD_tmp))**4)\n-                    # err_var_all[k] = mu_4/n_pt - variance_all[k]**2*(n_pt-3)/(n_pt*(n_pt-1))\n-                    # variance_all[k] /= 2 ## because uncertainties on the two flats of the pair add up in quadrature. We don't want to count it twice\n-                    # err_var_all[k] /= 4 ## if we divide variance by 2, we also divide its std by 2, and therefore its variance by 4\n+                if ((n_pt >1 ) * (np.sum(wg_tmp)>0)):\n+                    fluxmean_all[ic] = np.ma.sum(wg_tmp*MM_tmp)/np.sum(wg_tmp)\n+                    dd_mean = np.ma.sum(wg_tmp*DD_tmp)/np.sum(wg_tmp)\n+                    variance_all[ic] = np.ma.sum(wg_tmp*(DD_tmp-dd_mean)**2)/np.sum(wg_tmp)\n+                    mu_4 = np.ma.sum(wg_tmp*(DD_tmp - dd_mean)**4)/np.sum(wg_tmp)\n                     err_var_all[ic] = mu_4/n_pt - variance_all[ic]**2*(n_pt-3)/(n_pt*(n_pt-1))\n                     variance_all[ic] /= 2 ## because uncertainties on the two flats of the pair add up in quadrature. We don't want to count it twice\n                     err_var_all[ic] /= 4 ## if we divide variance by 2, we also divide its std by 2, and therefore its variance by 4\n-                else:\n-                    med = np.median(MM_tmp)\n-                    for m in range(-kernelsize,kernelsize+1):\n-                        for n in range(-kernelsize,kernelsize+1):\n-                            newids1 = kernelsize + w[0]+ m #kernelsize+newids//n2+m  ##3\n-                            newids2 = kernelsize + w[1]+ n #kernelsize+newids%n2+n\n-                            # variance_all[k] = variance_all[k] + np.sum(DD_tmp*DD[newids1,newids2])/(len(ww[0])-1) - np.sum(DD_tmp)*np.sum(DD_tmp)/(len(ww)-1)**2\n-                            # mu_4 = np.mean((DD_tmp - np.mean(DD_tmp))**4)\n-                            # err_var_all[k] = mu_4/n_pt - variance_all[k]**4*(n_pt-3)/(n_pt*(n_pt-1))\n-                            # err_var_all[k] /= 4 ### check this factor 4\n-                            # ic = k*n_tile_2 + j\n-                            variance_all[ic] = variance_all[ic] + np.sum(DD_tmp*DD[newids1,newids2])/(len(w[0])-1) - np.sum(DD_tmp)*np.sum(DD_tmp)/(len(w)-1)**2\n-                            mu_4 = np.mean((DD_tmp - np.mean(DD_tmp))**4)\n-                            err_var_all[ic] = mu_4/n_pt - variance_all[ic]**4*(n_pt-3)/(n_pt*(n_pt-1))\n-                            err_var_all[ic] /= 4 ### check this factor 4\n+                    variance_all[ic] *= np.ma.mean(wg_tmp)\n+                    err_var_all[ic] *= np.ma.mean(wg_tmp)**2\n     else:\n         fluxmean_all = np.zeros(n_tile)\n         variance_all = np.zeros(n_tile)\n@@ -183,40 +158,25 @@ def Compute_mean_var_correctedfromBFE(image_1, image_2, n_tile, kernelsize, mm=1\n \n         for k in range(n_tile):\n             newids = idi[k*nnn:(k+1)*nnn] #ids of the pixels which will contribute to this bin\n-            DD_tmp = astropy.stats.sigma_clip(DD_s[newids],sigma = 5)\n-            mask_ = ma.getmask(DD_tmp)\n-            ww = np.where(mask_ == False)[0]\n-            newids = newids[ww]\n-\n+            # DD_tmp = astropy.stats.sigma_clip(DD_s[newids],sigma = 5)\n+            # mask_ = ma.getmask(DD_tmp)\n+            # ww = np.where(mask_ == False)[0]\n+            # newids = newids[ww]\n             n_pt = len(newids)\n-            if (mm==1):\n-                fluxmean_all[k] = np.mean(MM_s[newids])\n-            else:\n-                fluxmean_all[k] = np.median(MM_s[newids])\n-\n-            if (kernelsize == 0):\n-                variance_all[k] = np.std((DD_s[newids]))**2 #/2.\n-                mu_4 = np.mean((DD_s[newids] - np.mean(DD_s[newids]) )**4)\n-\n+            if ((n_pt > 1) * (np.sum(wg[newids])>0)):\n+                fluxmean_all[k] = np.ma.sum(MM_s[newids]*wg[newids])/np.sum(wg[newids])\n+                dd_mean = np.ma.sum(wg[newids]*DD_s[newids])/np.sum(wg[newids])\n+                variance_all[k] = np.ma.sum(wg[newids]*(DD_s[newids]-dd_mean)**2)/np.sum(wg[newids])\n+                mu_4 = np.ma.sum(wg[newids]*(DD_s[newids] - dd_mean)**4)/np.sum(wg[newids])\n                 err_var_all[k] =  (mu_4/n_pt - variance_all[k]**2*(n_pt-3)/(n_pt*(n_pt-1)))\n                 variance_all[k] /= 2 ## because uncertainties on the two flats of the pair add up in quadrature. We don't want to count it twice\n                 err_var_all[k] /= 4 ## if we divide variance by 2, we also divide its std by 2, and therefore its variance by 4\n-            else:\n-                med = np.median(MM_s[newids])\n-                ttt = np.zeros([len(newids),kernelsize*2+1,kernelsize*2+1])\n-\n-                for m in range(-kernelsize,kernelsize+1):\n-                    for n in range(-kernelsize,kernelsize+1):\n-                        newids1 = kernelsize+newids//n2+m  ##3\n-                        newids2 = kernelsize+newids%n2+n\n-                        variance_all[k] = variance_all[k]+np.sum(DD_s[newids]*DD[newids1,newids2])/(len(newids)-1) - np.sum(DD_s[newids])*np.sum(DD_s[newids])/(len(newids)-1)**2\n-                        mu_4 = np.mean((DD_s[newids] - np.mean(DD_s[newids]))**4)\n-                        err_var_all[k] = (mu_4/n_pt - variance_all[k]**2*(n_pt-3)/(n_pt*(n_pt-1)))\n-                        err_var_all[k] /= 4 ### check this factor 4\n-\n-    return variance_all, fluxmean_all, err_var_all\n-\n-def rebin3(quad1, quad2, kernelsize):\n+                variance_all[k] *= np.ma.mean(wg[newids])\n+                err_var_all[k] *= np.ma.mean(wg[newids])**2\n+            ########\n+    return variance_all[(fluxmean_all>0)], fluxmean_all[fluxmean_all>0], err_var_all[fluxmean_all>0]\n+\n+def rebin3(quad1, quad2, kernelsize, sigclip=5):\n     '''\n     @brief\n     rebin two Masked ndarray data in 2D into two smaller Masked ndarray of the same rank, whose dimensions\n@@ -227,18 +187,23 @@ def rebin3(quad1, quad2, kernelsize):\n     the two rebinned Masked ndarray data\n     '''\n     shape = quad1.shape\n-    narrays = (asarray(shape)%kernelsize)+1 #.astype(np.int)-1\n+    narrays = (asarray(shape)%kernelsize)+1\n     n_pix = asarray(shape)//kernelsize\n \n     nq1 = ma.zeros([n_pix[0],n_pix[1],narrays[1]*narrays[0]])\n     nq2 = ma.zeros([n_pix[0],n_pix[1],narrays[1]*narrays[0]])\n+    wg  = ma.zeros([n_pix[0],n_pix[1],narrays[1]*narrays[0]])\n+\n     for np1 in range(narrays[0]):\n             for np2 in range(narrays[1]):\n                 a = quad1[np1:,np2:][:n_pix[0]*kernelsize,:n_pix[1]*kernelsize]\n+                a =  astropy.stats.sigma_clip(a,sigma = sigclip)\n                 nn = np.ma.reshape(a,[n_pix[0],kernelsize,n_pix[1],kernelsize])\n-                nq1[:,:,np1*narrays[1]+np2] = np.mean(nn,axis=(1,3))\n+                nq1[:,:,np1*narrays[1]+np2] = np.ma.mean(nn,axis=(1,3))\n                 a = quad2[np1:,np2:][:n_pix[0]*kernelsize,:n_pix[1]*kernelsize]\n+                a =  astropy.stats.sigma_clip(a,sigma = sigclip)\n                 nn = np.ma.reshape(a,[n_pix[0],kernelsize,n_pix[1],kernelsize])\n-                nq2[:,:,np1*narrays[1]+np2] = np.mean(nn,axis=(1,3))\n+                nq2[:,:,np1*narrays[1]+np2] = np.ma.mean(nn,axis=(1,3))\n+                wg[:,:,np1*narrays[1]+np2] = (np.ma.count(nn,axis=(1,3)))/(kernelsize*kernelsize)\n \n-    return nq1[:,:,0], nq2[:,:,0] #np.mean(nq1, axis=2), np.mean(nq2, axis=2)\n+    return nq1[:,:,0], nq2[:,:,0], wg[:,:,0]\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PTC_NL/python/VIS_PTC_NL/VIS_mcmc_emcee.py": [
                        [
                            "@@ -208,12 +208,12 @@ def Make_Corner_Plot(out_chain, thetaref, rangetab, typ=\"mean\", burn_ = 100, log\n             else:\n                 plt.scatter(np.median(out_chain[:,n]),np.median(out_chain[:,n]),color=\"black\",marker='+')\n \n-            plt.scatter(thetaref[n],thetaref[k],color=\"black\",marker='+',s=200)\n+            # plt.scatter(thetaref[n],thetaref[k],color=\"black\",marker='+',s=200)\n             axes=plt.gca()\n             plt.text(0.05,0.08,f'measured: p{n+1}={np.round(np.mean(out_chain[:,n])*1000.)/1000.}; p{k+1}={np.round(np.mean(out_chain[:,k])*1000.)/1000.} ',\n                      transform=axes.transAxes,horizontalalignment='left',verticalalignment='bottom',fontsize=14,color=\"firebrick\")\n-            plt.text(0.05,0.02,f'model: p{n+1}={np.round(thetaref[n]*1000.)/1000.}; p{k+1}={np.round(thetaref[k]*1000.)/1000.} ',\n-                     transform=axes.transAxes,horizontalalignment='left',verticalalignment='bottom',fontsize=14,color=\"black\")\n+            # plt.text(0.05,0.02,f'model: p{n+1}={np.round(thetaref[n]*1000.)/1000.}; p{k+1}={np.round(thetaref[k]*1000.)/1000.} ',\n+                    #  transform=axes.transAxes,horizontalalignment='left',verticalalignment='bottom',fontsize=14,color=\"black\")\n             axes.tick_params(axis='both', which='major', labelsize=14)\n             axes.tick_params(axis='both', which='minor', labelsize=14)\n \n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_PTC_NL/python/VIS_PTC_NL/file_entry_point.py": [
                        [
                            "@@ -106,8 +106,8 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n         handle.close()\n         print(f\"flagmap handler closed...\")\n \n-        print(f\"applying algo.flagging_stack...\")\n-        flagging_stack_start = timer()\n+        # print(f\"applying algo.flagging_stack...\")\n+        # flagging_stack_start = timer()\n         # image_stack = algo.flagging_stack(image_stack32,\n         #                                   flagmap,\n         #                                   input_files,\n@@ -118,8 +118,8 @@ def file_layer_entry_point(input_files, interm_dir, quadrant_id=None, in_flagmap\n         #                                   flag_prescan_size,\n         #                                   flag_serial_overscan_size,\n         #                                   flag_parallel_overscan_size)\n-        flagging_stack_end = timer()\n-        print(f\"flagging_stack processed in {flagging_stack_end-flagging_stack_start} seconds...\")\n+        # flagging_stack_end = timer()\n+        # print(f\"flagging_stack processed in {flagging_stack_end-flagging_stack_start} seconds...\")\n     image_stack = ma.array(image_stack32, mask=flagmap, copy=False)\n \n     log.info(\"file_layer_entry_point(): end\")\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Photometry/python/VIS_Photometry/VIS_Calibrate_Photometry.py": [
                        [
                            "@@ -151,6 +151,9 @@ def get_zeropoint(catref_file, catmes_file, exp_time, logdir):\n              smp.add_ext( hdu)\n              ### Filtering VIS sources\n              mask = (catmes_data['SNR_WIN'] > 100) & (np.abs(catmes_data['SPREAD_MODEL']) < 0.005) & (catmes_data['ELLIPTICITY'] < 0.05) & (catmes_data['FLAGS'] == 0)\n+             if mask.sum() == 0:\n+               ## No need to continue in the loop\n+               continue\n              ### Fill in the dqc table\n              dqc_data = catmes_data[mask]\n              aper_cor_dqc_row = dict()\n@@ -190,6 +193,9 @@ def get_zeropoint(catref_file, catmes_file, exp_time, logdir):\n              ######\n \n              magref_matched  = magref_assoc[ dist_arcsec_abs < tol ]\n+             if len(magref_matched) < 3:\n+               # Not enough matched sources\n+               continue\n              ra_vis_matched  = ra_vis[ dist_arcsec_abs < tol ]\n              dec_vis_matched = dec_vis[ dist_arcsec_abs < tol ]\n              x_vis_matched   = x_vis[ dist_arcsec_abs < tol ]\n@@ -216,6 +222,9 @@ def get_zeropoint(catref_file, catmes_file, exp_time, logdir):\n \n         hdulist.close()\n \n+    if len(zero_point_dqc) == 0:\n+      return(-1)\n+\n     # write spread_model plot\n     try: # failsafe mode\n       expid = visdef.get_expid_from_filename( catmes_file)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_SourceExtraction/python/VIS_SourceExtraction/VIS_extract_sources.py": [
                        [
                            "@@ -104,7 +104,7 @@ def mainMethod(args):\n            'X_IMAGE','Y_IMAGE','XWIN_IMAGE','YWIN_IMAGE','ERRAWIN_IMAGE',\\\n            'ERRBWIN_IMAGE','ERRTHETAWIN_IMAGE','FLUX_APER(1)','FLUXERR_APER(1)',\\\n            'FLUX_APER(2)','FLUXERR_APER(2)','FLUX_APER(3)','FLUXERR_APER(3)',\\\n-           'BACKGROUND','ELONGATION','SNR_WIN','VIGNET(20,20)','FLAGS','FLAGS_WIN']\n+           'BACKGROUND','ELONGATION','SNR_WIN','VIGNET(21,21)','FLAGS','FLAGS_WIN']\n   if inflagmap is not None:\n     param += ['IMAFLAGS_ISO','NIMAFLAGS_ISO']\n \n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Stacking/python/VIS_Stacking/VIS_stacking.py": [
                        [
                            "@@ -367,7 +367,9 @@ def subtract( infile1, infile2, outfile, flagfile, extnum=-1, maxmem=MAXMEM):\n \n ################################################################################\n \n-def stack_xml_out( workdir, exposure_list, stack_file_prefix, dpdstacked_frame, dpdstacked_catalogue, ZIP_OUTPUTS=False):\n+def stack_xml_out( workdir, exposure_list, stack_file_prefix,\n+                   dpdstacked_frame, dpdstacked_catalogue, calblock_id,\n+                   ZIP_OUTPUTS=False):\n   \"\"\"Produce the stacking task output products\n   \"\"\"\n \n@@ -483,6 +485,7 @@ def stack_xml_out( workdir, exposure_list, stack_file_prefix, dpdstacked_frame,\n       stk_dp = FromToXML.stack_dp( os.path.join( datadir, euc_stk_file),\n                                    survey_type,\n                                    obs_id,\n+                                   calblock_id,\n                                    euc_psf_file,\n                                    euc_bkg_file,\n                                    euc_wgt_file)\n@@ -495,7 +498,7 @@ def stack_xml_out( workdir, exposure_list, stack_file_prefix, dpdstacked_frame,\n #        FromToXML.create_qualityparams_fromlist(stk_dp,pe_runinfo_consolidate_list)\n \n       # Creating xml file\n-      xml_filename = os.path.join( DATADIR, \"DpdVisStackedFrame.xml\")\n+      xml_filename = \"DpdVisStackedFrame.xml\"\n       DmUtils.save_product_metadata( stk_dp, os.path.join( workdir, xml_filename))\n       DpdVisStackedFrame_list.append( xml_filename)\n \n@@ -517,7 +520,7 @@ def stack_xml_out( workdir, exposure_list, stack_file_prefix, dpdstacked_frame,\n       stack_catalog_dp = FromToXML.stack_cat_dp( os.path.join( datadir, euc_cat_file),\n                                                  spatial_footprint,\n                                                  obs_id)\n-      xml_filename = os.path.join( DATADIR, \"DpdVisStackedFrameCatalog.xml\")\n+      xml_filename = \"DpdVisStackedFrameCatalog.xml\"\n       DmUtils.save_product_metadata( stack_catalog_dp, os.path.join( workdir, xml_filename))\n       DpdVisStackedCatalogue_list.append( xml_filename)\n \n@@ -593,6 +596,7 @@ def mainMethod(args):\n     input_files = sorted( FromToXML.read_json( args.exposure_list))\n     exposure_list = list()\n     wgt_files_list = list()\n+    calblock_id = \"\"\n     for input_file in input_files:\n       file_ext = os.path.splitext( input_file)[1].lower()\n       if file_ext == \".xml\":\n@@ -612,6 +616,10 @@ def mainMethod(args):\n         exposure_list.append( sci_datafile)\n         wgt_datafile = zip_tools.unzip( os.path.join( DATADIR, wgt_datafile))\n         wgt_files_list.append( wgt_datafile)\n+        # read calblock_id from input DpdVisCalibratedFrame XML file\n+        if calblock_id == \"\":\n+          dp = DmUtils.read_product_metadata( input_file)\n+          calblock_id = dp.Data.ObservationSequence.CalblockId\n       else:\n         # working as stacking task in another pipeline\n         # args.exposure_list contains \"process_photo_exp.reduce/tuplelist.json\"\n@@ -627,6 +635,15 @@ def mainMethod(args):\n         exp_json_list  = FromToXML.read_json( args.exposure_list)\n         exposure_list  = [FromToXML.read_json( exp_json) for exp_json in input_files]\n         wgt_files_list = None\n+        # read calblock_id from metadata stored in config.cfg by VIS_xml_in.py\n+        # and take the first one, supposing calblocks are not mixed in VIS pipelines...\n+        if calblock_id == \"\":\n+          # find first config file section containing input raw frame metadata\n+          for section in config.sections():\n+            if section.startswith( visdef.EUCVISEXP_PREFIX) and \\\n+               \"dp.Data.ObservationSequence.CalblockId\" in config[section]:\n+              calblock_id = config[section][\"dp.Data.ObservationSequence.CalblockId\"]\n+              break\n \n     logger.info( f\"flatten args.exposure_list={exposure_list}\")\n \n@@ -688,6 +705,7 @@ def mainMethod(args):\n                    stack_out_prefix,\n                    args.dpdstacked_frame,\n                    args.dpdstacked_catalogue,\n+                   calblock_id,\n                    ZIP_OUTPUTS)\n \n     # save (again) pipeline profiling, possibly replacing the one created by VIS_science_xml_out which should have completed first\n@@ -705,7 +723,13 @@ def mainMethod(args):\n   else:\n     logger.warning( \"Stacking disabled in config.cfg, produce empty outputs\")\n     # write empty output product XML list\n-    stack_xml_out( workdir, list(), stack_out_prefix, args.dpdstacked_frame, args.dpdstacked_catalogue, ZIP_OUTPUTS=False)\n+    stack_xml_out( workdir,\n+                   list(),\n+                   stack_out_prefix,\n+                   args.dpdstacked_frame,\n+                   args.dpdstacked_catalogue,\n+                   calblock_id=\"None\",\n+                   ZIP_OUTPUTS=False)\n \n   # and we're done\n   logger.info( \"#\")\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_Common/python/VIS_Tasks_Common/FromToXML.py": [
                        [
                            "@@ -66,6 +66,8 @@ from ST_DataModelBindings.dpd.vis import vismasterdarkframe_stub\n from ST_DataModelBindings.dpd.vis import vismasterflatframe_stub\n from ST_DataModelBindings.dpd.vis import visstackedframe_stub\n from ST_DataModelBindings.dpd.vis import visnonlinearitymodel_stub\n+from ST_DataModelBindings.dpd.vis import visctimodel_stub\n+from ST_DataModelBindings.dpd.vis import viscticalibrationresults_stub\n from ST_DataModelBindings.dpd.vis import vispsfmodel_stub\n from ST_DataModelBindings.dpd.vis import visdistortionmodel_stub\n from ST_DataModelBindings.dpd.vis import vislargeflatcalibration_stub\n@@ -75,6 +77,7 @@ from ST_DataModelBindings.dpd.vis import ppovalidation_stub\n from ST_DataModelBindings.dpd.vis import validationresult_stub\n from ST_DataModelBindings.dpd.vis import visfilecontainer_stub\n from ST_DataModelBindings.dpd.vis import visbloomingmodel_stub\n+from ST_DataModelBindings.dpd.vis import visbfemodel_stub\n from ST_DataModelBindings.dpd.vis import vistrappumpingmodel_stub\n from ST_DataModelBindings.dpd.vis import visgainmodel_stub\n from ST_DataModelBindings.dpd.vis import viszeropoint_stub\n@@ -619,7 +622,8 @@ def create_visFileContainerDataContainer( file_name,\n     # Fill it with the given values\n     data_container.FileName   = file_name\n     data_container.filestatus = file_status\n-    data_container.FileType   = file_type\n+# SM 7 aug. 2023: deprecated, or maybe never existed?\n+#    data_container.FileType   = file_type\n     return data_container\n \n \n@@ -802,6 +806,7 @@ def master_dark_dp(fits_file):\n \n     # Open the FITS file\n     fits = fitsio.FITS(fits_file, 'r')\n+    pri_header = fits[0].read_header()\n     nhdu = fits[-1].get_extnum()\n     header = fits[1]. read_header()\n \n@@ -814,6 +819,8 @@ def master_dark_dp(fits_file):\n     data.DataLength  = header[\"NAXIS1\"] * header[\"NAXIS2\"]\n     data.ValidityRange = create_validity_range()\n     data.Filter        = vis_filter()\n+    data.ExposureTime = exposure_time(pri_header[\"EXPTIME\"], \"s\")\n+\n     # DataStorage\n     data.DataStorage = DmUtils.create_fits_storage(\n                            vis_stub.visMasterDarkStorageFitsFile,\n@@ -893,6 +900,9 @@ def master_flat_dp(fits_file):\n     data.ValidityRange = create_validity_range()\n     data.Filter        = vis_filter()\n \n+    #Do not do. see comment 23513\n+    #data.ExposureTime = exposure_time(prim_hdr[\"EXPTIME\"], \"s\")\n+\n     calib_unit = vis_stub.visLedForFlat()\n     # fail if keyword absent\n     calib_unit.LedId   = str( prim_hdr[\"LEDID\"])\n@@ -913,7 +923,7 @@ def master_flat_dp(fits_file):\n # Single exposure catalog data product\n #-------------------------------------\n \n-def single_exp_cat_dp(fits_catalog, spatial_footprint, observation_sequence, obs_date_time, raw_frame_id):\n+def single_exp_cat_dp(fits_catalog, spatial_footprint, observation_sequence, obs_date_time, raw_frame_id=None):\n     \"\"\"Creates a VisCalibratedFrameCatalog from a FITS catalog\n \n     Parameters\n@@ -953,9 +963,10 @@ def single_exp_cat_dp(fits_catalog, spatial_footprint, observation_sequence, obs\n     data.ObservationSequence= observation_sequence\n     data.ObservationDateTime = obs_date_time\n     data.Instrument = base_instrument()\n-    data.InputRawFrameReference = sys_stub.dataProductReference()\n-    data.InputRawFrameReference.ProductID = raw_frame_id\n-    data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n+    if raw_frame_id is not None:\n+      data.InputRawFrameReference = sys_stub.dataProductReference()\n+      data.InputRawFrameReference.ProductID = raw_frame_id\n+      data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n \n     # DataStorage\n     data_storage = DmUtils.create_fits_storage(vis_stub.visCalibratedCatalogFitsFile,\n@@ -972,7 +983,7 @@ def single_exp_cat_dp(fits_catalog, spatial_footprint, observation_sequence, obs\n # Stack catalog data product\n #---------------------------\n \n-def stack_cat_dp(fits_catalog, spatial_footprint, observation_id, raw_frame_id=UNKNOWN_STRING):\n+def stack_cat_dp(fits_catalog, spatial_footprint, observation_id, raw_frame_id=None):\n     \"\"\"Creates a VisStackedFrameCatalog from a FITS catalog\n \n     Parameters\n@@ -1008,9 +1019,10 @@ def stack_cat_dp(fits_catalog, spatial_footprint, observation_id, raw_frame_id=U\n     # from cat:observationCatalog\n     data.ObservationId = observation_id\n     data.Instrument = base_instrument()\n-    data.InputRawFrameReference = sys_stub.dataProductReference()\n-    data.InputRawFrameReference.ProductID = raw_frame_id\n-    data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n+    if raw_frame_id is not None:\n+      data.InputRawFrameReference = sys_stub.dataProductReference()\n+      data.InputRawFrameReference.ProductID = raw_frame_id\n+      data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n \n     # DataStorage\n     data_storage = DmUtils.create_fits_storage(vis_stub.visStackedCatalogFitsFile,\n@@ -1027,7 +1039,8 @@ def stack_cat_dp(fits_catalog, spatial_footprint, observation_id, raw_frame_id=U\n # Calibrated exposure data product\n #---------------------------------\n \n-def calib_exp_dp(exposure: str, prnu_corr_exposure: str, psf: str, background: str, weightmap: str, raw_frame_id: str):\n+def calib_exp_dp( exposure: str, prnu_corr_exposure: str, psf: str,\n+                  background: str, weightmap: str, raw_meta: dict = None):\n     \"\"\"Creates a DpdVisCalibratedFrame from a FITS single exposure + associated files\n \n     Parameters\n@@ -1037,6 +1050,7 @@ def calib_exp_dp(exposure: str, prnu_corr_exposure: str, psf: str, background: s\n         psf: filename of the PSF model FITS file\n         background: filename of the background FITS file\n         weightmap: filename of the weightmap FITS file\n+        metadata: optional config.cfg section containing raw frame metadata\n \n     Returns\n     -------\n@@ -1097,19 +1111,18 @@ def calib_exp_dp(exposure: str, prnu_corr_exposure: str, psf: str, background: s\n     data.WLQ2Exposures = create_WLQ2_exposures(prnu_corr_exposure)\n     data.PsfModelStorage = DmUtils.create_fits_storage(\n                                vis_stub.visPsfModelStorageFitsFile,\n-                               psf, \"vis.psfModel\", \"0.1\")\n+                               os.path.basename( psf), \"vis.psfModel\", \"0.1\")\n     data.BackgroundStorage = DmUtils.create_fits_storage(\n                                vis_stub.visBackgroundStorageFitsFile,\n-                               background, \"vis.backgroundMap\", \"0.1\")\n+                               os.path.basename( background), \"vis.backgroundMap\", \"0.1\")\n     data.WeightStorage = DmUtils.create_fits_storage(\n                                vis_stub.visWeightStorageFitsFile,\n-                               weightmap, \"vis.weightMap\", \"0.1\")\n+                               os.path.basename( weightmap), \"vis.weightMap\", \"0.1\")\n     data.DataStorage = DmUtils.create_fits_storage(\n                                vis_stub.visCalibratedStorageFitsFile,\n-                               os.path.basename(exposure), \"vis.calibratedFrame\", \"0.1\")\n+                               os.path.basename( exposure), \"vis.calibratedFrame\", \"0.1\")\n \n     # DetectorList\n-#    data.DetectorList = img_stub.detectorList()\n     data.DetectorList = vis_stub.visCalFrameDetectorList()\n     ext_header_list = []\n     ext = 1\n@@ -1127,17 +1140,23 @@ def calib_exp_dp(exposure: str, prnu_corr_exposure: str, psf: str, background: s\n     data.ImgSpatialFootprint = DmUtils.create_spatial_footprint_from_header_list(ext_header_list,\n                                                                                  N_FOOTPRINT_SAMPLE_STEPS)\n \n-    # InputRawFrameReference\n-    data.InputRawFrameReference = sys_stub.dataProductReference()\n-    data.InputRawFrameReference.ProductID = raw_frame_id\n-    data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n+    if raw_meta is not None:\n+      if \"DpdVisRawFrame_ProductID\" in raw_meta:\n+        data.InputRawFrameReference = sys_stub.dataProductReference()\n+        data.InputRawFrameReference.ProductID = raw_meta[\"DpdVisRawFrame_ProductID\"]\n+        data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n+      if \"dp.Data.ObservationSequence.CalblockId\" in raw_meta:\n+        data.ObservationSequence.CalblockId = raw_meta[\"dp.Data.ObservationSequence.CalblockId\"]\n+      if \"dp.Data.ObservationSequence.CalblockVariant\"in raw_meta:\n+        data.ObservationSequence.CalblockVariant = raw_meta[\"dp.Data.ObservationSequence.CalblockVariant\"]\n \n     data_product.Data = data\n \n     return data_product\n \n \n-def stack_dp(stack: str, survey_type: str, obsid: int, psf: str, background: str, weightmap: str):\n+def stack_dp( stack: str, survey_type: str, obsid: int, calblock_id: str,\n+              psf: str, background: str, weightmap: str):\n     \"\"\"Creates a DpDVisStackedFrame from a FITS stack file + associated files\n \n     Parameters\n@@ -1183,24 +1202,59 @@ def stack_dp(stack: str, survey_type: str, obsid: int, psf: str, background: str\n     data.ImgSpatialFootprint = DmUtils.create_spatial_footprint_from_header(ext_header,\n                                                                             N_FOOTPRINT_SAMPLE_STEPS)\n     data.ObservationId = obsid\n+    data.CalblockId = str( calblock_id)\n     data.DataStorage = DmUtils.create_fits_storage(\n                                vis_stub.visStackedStorageFitsFile,\n                                os.path.basename(stack), \"vis.stackedImage\", \"0.1\")\n-    if len( fits) > 1:\n+    data.BackgroundStorage = DmUtils.create_fits_storage(\n+                               vis_stub.visBackgroundStorageFitsFile,\n+                               os.path.basename(background), \"vis.backgroundMap\", \"0.1\")\n+    data.WeightStorage = DmUtils.create_fits_storage(\n+                               vis_stub.visWeightStorageFitsFile,\n+                               os.path.basename(weightmap), \"vis.weightMap\", \"0.1\")\n+    if psf is not None:\n       data.PsfModelStorage = DmUtils.create_fits_storage(\n                                  vis_stub.visPsfModelStorageFitsFile,\n-                                 psf, \"vis.psfModel\", \"0.1\")\n-      data.BackgroundStorage = DmUtils.create_fits_storage(\n-                                 vis_stub.visBackgroundStorageFitsFile,\n-                                 background, \"vis.backgroundMap\", \"0.1\")\n-      data.WeightStorage = DmUtils.create_fits_storage(\n-                                 vis_stub.visWeightStorageFitsFile,\n-                                 weightmap, \"vis.weightMap\", \"0.1\")\n+                                 os.path.basename(psf), \"vis.psfModel\", \"0.1\")\n     data_product.Data = data\n \n     return data_product\n \n \n+def cti_model_dp():\n+    \"\"\"\n+    Creates a DpdVisCTICalibrationResultsobject\n+\n+    Parameters\n+    ----------\n+    fits_file: filename containing cti calibration results (e.g. .zip files)\n+\n+    Returns\n+    -------\n+    DpdVisCTICalibrationResults object\n+    \"\"\"\n+\n+    import ST_DataModelBindings.ins_stub as ins_dict\n+\n+    # Create the data product\n+    data_product = viscticalibrationresults_stub.DpdVisCTICalibrationResults()\n+\n+    # Add Header\n+    data_product.Header = vis_generic_header(\"DpdVisCTICalibrationResults\", UNKNOWN_STRING)\n+\n+    # Add Data\n+    data = vis_stub.visCTICalibrationResults.Factory()\n+    # data.visCTICalibrationResults = storage\n+\n+    date_range = ins_dict.calibrationValidPeriod()\n+    date_range.TimestampStart = \"2023-02-21T00:00:00.000Z\"\n+    date_range.TimestampEnd = \"2023-02-22T00:00:00.000Z\"\n+\n+    data.ValidityRange = create_validity_range()\n+    data_product.Data = data\n+    return data_product\n+\n+\n def psfmodel_dp(fits_file: str, fits_file_grid: str):\n     \"\"\"Creates a DpdVisPSFModel\n \n@@ -1393,7 +1447,9 @@ def create_validation_requirementresult(requirement_id, requirement_result_passe\n \n     return requirement_result\n \n-def create_analysis_dp(result_passed: bool, aux_file: str, raw_frame_id=None, analysis_id=None, obs_seq_foranalysis=None, analysis_data=None):\n+\n+def create_analysis_dp( result_passed: bool, aux_file: str, raw_frame_id=None,\n+                        analysis_id=None, obs_seq_foranalysis=None, analysis_data=None):\n     \"\"\"Create a new DpdVisAnalysisResult data product, normally with an associated tar/zip file.\n     Parameters\n     ----------\n@@ -1435,21 +1491,19 @@ def create_analysis_dp(result_passed: bool, aux_file: str, raw_frame_id=None, an\n \n \n     # InputRawFrameReference\n-    if raw_frame_id is None:\n-      # SM 3 apr 2023: dummy value waiting for \"InputRawFrameReference\" being optional in DM > 9.1.5\n-      raw_frame_id = \"PPO_VIS_TEST_PV-001_R0-2-outputSIM-0_MIGRATED_1679500947.7778997\"\n-    data.InputRawFrameReference = sys_stub.dataProductReference()\n-    data.InputRawFrameReference.ProductID = raw_frame_id\n-    data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n+    if raw_frame_id is not None:\n+      data.InputRawFrameReference = sys_stub.dataProductReference()\n+      data.InputRawFrameReference.ProductID = raw_frame_id\n+      data.InputRawFrameReference.ProductType = \"DpdVisRawFrame\"\n \n     if analysis_data is not None:\n        for filename in analysis_data:\n-         analysis_data = vis_stub.visAnalysisData.Factory()\n-         analysis_data.DataContainer = dss_stub.dataContainer.Factory()\n-         analysis_data.DataContainer.FileName = os.path.basename(filename)\n-         analysis_data.DataContainer.filestatus = \"PROPOSED\"\n-         analysis_data.DataContainer.FileType = \"fits\" \n-         data.AnalysisData.append(analysis_data)\n+         if (filename is not None) and (len( filename.strip()) > 0):\n+           vis_analysis_data = vis_stub.visAnalysisData.Factory()\n+           vis_analysis_data.DataContainer = dss_stub.dataContainer.Factory()\n+           vis_analysis_data.DataContainer.FileName = os.path.basename( filename)\n+           vis_analysis_data.DataContainer.filestatus = \"PROPOSED\"\n+           data.AnalysisData.append( vis_analysis_data)\n \n     data_product.Data = data\n     return data_product\n@@ -1767,6 +1821,10 @@ def create_DpdVisBloomingModel( file_name: str) ->  visbloomingmodel_stub.DpdVis\n   fill_visFileContainer_dp( dp, [file_name])\n   return dp\n \n+def create_DpdVisBFEModel( file_name: str) -> visbfemodel_stub.DpdVisBFEModel:\n+   dp = visbfemodel_stub.DpdVisBFEModel()\n+   fill_visFileContainer_dp( dp, [file_name])\n+   return dp\n \n def create_DpdVisGainModel( file_name: str) ->  visgainmodel_stub.DpdVisGainModel:\n   dp = visgainmodel_stub.DpdVisGainModel()\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_Common/python/VIS_Tasks_Common/Modules.py": [
                        [
                            "@@ -172,12 +172,13 @@ def CorrectDark( config, infile, outfile):\n \n     logger.debug( f\"CorrectDark() Output: {outfile}\")\n \n-def CorrectNonLin( config, infile, outfile, quad_name):\n+def CorrectNonLin( config, config_filename, infile, outfile, quad_name):\n \n     workdir = config.get( \"General\", \"workdir\")\n     nlcorr_model = os.path.join( workdir, config.get( \"CalibDataProducts\", \"nlcorr_model\"))\n     non_lin_cor_io.file_layer_entry_point(infile,\n                                           nlcorr_model,\n+                                          config_filename,\n                                           outfile,\n                                           quad_name)\n \n@@ -388,7 +389,7 @@ def Calibrate_Astrometry(config, section, workdir, catalog, star_file, distormod\n            logger.error(f\"Astrometry subprocess error code: {cpe}\")\n            raise\n \n-def Wcsfit(config, catalogue, star_file, exp_list, plot_dir=None):\n+def Wcsfit(config, catalogue, star_file, exp_list, exptime, plot_dir=None):\n \n     # Populate command line for runwcsfit module\n     cmdline  = f\"use-fpa-model \"\n@@ -399,7 +400,7 @@ def Wcsfit(config, catalogue, star_file, exp_list, plot_dir=None):\n     cmdline += f\"--target-precision 3 \"\n     cmdline += f\"--niter 3 \"\n     cmdline += f\"--refmagcol VIS \"\n-    cmdline += f\"--refmagmin 18.5 \"\n+    cmdline += f\"--refmagmin {18.5 + 2.5 * np.log10(exptime / 565.0)} \"\n     cmdline += f\"--refmagmax 22.0 \"\n     cmdline += f\"--writeto {' '.join(map(str, exp_list))} {catalogue} \"\n     cmdline += f\"--loglevel debug \"\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_Common/tests/python/FromToXML_test.py": [
                        [
                            "@@ -279,7 +279,8 @@ class TestFromToXML(object):\n \n         # Create input data\n         filename = \"CalibratedExp.fits\"\n-        fits = fitsio.FITS(filename, 'rw', ignore_empty=True)\n+        if os.path.exists(filename): os.remove(filename)\n+        fits = fitsio.FITS(filename, 'rw')\n         nccds = 4\n         naxis1 = 4\n         naxis2 = 5\n@@ -375,8 +376,13 @@ class TestFromToXML(object):\n         spatial_footprint = DmUtils.create_spatial_footprint_from_header_list(ext_header_list, 5)\n         fits.close()\n \n+        raw_meta = {\n+          \"DpdVisRawFrame_ProductID\": \"rawpid\",\n+          \"dp.Data.ObservationSequence.CalblockId\": \"cbid\",\n+          \"dp.Data.ObservationSequence.CalblockVariant\": \"cbvar\"\n+        }\n         dp = FromToXML.calib_exp_dp(os.path.abspath(filename), \"prnu.fits\", \"psf.fits\", \"background.fits\", \"weight.fits\",\n-                                    raw_frame_id=FromToXML.UNKNOWN_STRING)\n+                                    raw_meta=raw_meta)\n         DmUtils.save_product_metadata( dp, dp.Header.ProductType + \"_test.xml\")\n \n         assert DmUtils.validate(dp)\n@@ -399,7 +405,7 @@ class TestFromToXML(object):\n         assert data.Instrument.InstrumentName == \"VIS\"\n         assert data.Instrument.TelescopeName == \"Euclid\"\n         assert data.Filter.Name == \"VIS\"\n-#        assert data.InputRawFrameReference.ProductID == FromToXML.UNKNOWN_STRING\n+        assert data.InputRawFrameReference.ProductID.value() == \"rawpid\"\n         assert data.InputRawFrameReference.ProductType == \"DpdVisRawFrame\"\n \n         ### assert data.ImgSpatialFootprint.Polygon == spatial_footprint.Polygon\n@@ -413,6 +419,8 @@ class TestFromToXML(object):\n         assert data.ObservationSequence.PointingId == pri_header[\"PTGID\"]\n         assert data.ObservationSequence.Exposure == pri_header[\"EXPNUM\"]\n         assert data.ObservationSequence.TotalExposure == pri_header[\"TOTEXP\"]\n+        assert data.ObservationSequence.CalblockId == \"cbid\"\n+        assert data.ObservationSequence.CalblockVariant == \"cbvar\"\n         assert data.InstrumentMode == \"Science\"\n         assert data.CommandedFPAPointing.RA == pri_header[\"RA\"]\n         assert data.CommandedFPAPointing.Dec == pri_header[\"DEC\"]\n@@ -500,7 +508,8 @@ class TestFromToXML(object):\n         date_obs_start = \"1997-05-24T11:40:24.802000Z\"\n         date_obs_end = \"1998-05-24T11:40:24.802000Z\"\n         obsid = 52929\n-        dp = FromToXML.stack_dp(os.path.abspath(filename), \"WIDE\", obsid,\n+        calblock_id = \"PV-001\"\n+        dp = FromToXML.stack_dp(os.path.abspath(filename), \"WIDE\", obsid, calblock_id,\n                                 \"psf.fits\", \"background.fits\", \"weight.fits\")\n         DmUtils.save_product_metadata( dp, dp.Header.ProductType + \"_test.xml\")\n \n@@ -551,6 +560,7 @@ class TestFromToXML(object):\n         assert data.BackgroundStorage.DataContainer.FileName == \"background.fits\"\n         assert data.WeightStorage.DataContainer.FileName == \"weight.fits\"\n         assert data.DataStorage.DataContainer.FileName == filename\n+        assert data.CalblockId == calblock_id\n \n         # delete test files\n         os.remove( dp.Header.ProductType + \"_test.xml\")\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/CMakeLists.txt": [
                        [
                            "@@ -67,10 +67,12 @@ elements_add_python_program(VIS_process_bias_quad VIS_Tasks_M.VIS_process_bias_q\n elements_add_python_program(VIS_quad_to_fpa VIS_Tasks_M.VIS_quad_to_fpa)\n elements_add_python_program(VIS_process_quad VIS_Tasks_M.VIS_process_quad)\n elements_add_python_program(VIS_gather_ccd VIS_Tasks_M.VIS_gather_ccd)\n+elements_add_python_program(VIS_gather_quadrants VIS_Tasks_M.VIS_gather_quadrants)\n elements_add_python_program(VIS_process_flat_quad VIS_Tasks_M.VIS_process_flat_quad)\n elements_add_python_program(VIS_process_dark_quad VIS_Tasks_M.VIS_process_dark_quad)\n elements_add_python_program(VIS_calib_xml_out VIS_Tasks_M.VIS_calib_xml_out)\n elements_add_python_program(VIS_cti_xml_out VIS_Tasks_M.VIS_cti_xml_out)\n+elements_add_python_program(VIS_cti_master_ci_xml_out VIS_Tasks_M.VIS_cti_master_ci_xml_out)\n elements_add_python_program(VIS_science_xml_out VIS_Tasks_M.VIS_science_xml_out)\n elements_add_python_program(VIS_xml_in VIS_Tasks_M.VIS_xml_in)\n elements_add_python_program(VIS_process_extract_exp VIS_Tasks_M.VIS_process_extract_exp)\n@@ -81,7 +83,6 @@ elements_add_python_program(VIS_Astrometry_calib VIS_Tasks_M.VIS_Astrometry_cali\n elements_add_python_program(VIS_PSF_calib VIS_Tasks_M.VIS_PSF_calib)\n elements_add_python_program(VIS_LargeFlat_calib VIS_Tasks_M.VIS_LargeFlat_calib)\n elements_add_python_program(VIS_CTI_Calibration VIS_Tasks_M.VIS_CTI_Calibration)\n-elements_add_python_program(VIS_CTI_Data_Visualize VIS_Tasks_M.VIS_CTI_Data_Visualize)\n elements_add_python_program(VIS_CTI_Master_CI VIS_Tasks_M.VIS_CTI_Master_CI)\n elements_add_python_program(VIS_process_astro_field_largeflat VIS_Tasks_M.VIS_process_astro_field_largeflat)\n elements_add_python_program(VIS_gain_calib VIS_Tasks_M.VIS_gain_calib)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ],
                        [
                            "@@ -67,6 +67,7 @@ elements_add_python_program(VIS_process_bias_quad VIS_Tasks_M.VIS_process_bias_q\n elements_add_python_program(VIS_quad_to_fpa VIS_Tasks_M.VIS_quad_to_fpa)\n elements_add_python_program(VIS_process_quad VIS_Tasks_M.VIS_process_quad)\n elements_add_python_program(VIS_gather_ccd VIS_Tasks_M.VIS_gather_ccd)\n+elements_add_python_program(VIS_gather_quadrants VIS_Tasks_M.VIS_gather_quadrants)\n elements_add_python_program(VIS_process_flat_quad VIS_Tasks_M.VIS_process_flat_quad)\n elements_add_python_program(VIS_process_dark_quad VIS_Tasks_M.VIS_process_dark_quad)\n elements_add_python_program(VIS_calib_xml_out VIS_Tasks_M.VIS_calib_xml_out)\n",
                            "VIS_gather_quadrants.py: first draft version allowing to run VIS_AstrometryCalibration per quadrant",
                            "Sylvain Mottet",
                            "2023-08-23T10:37:59.000+02:00",
                            "a9a0acc55312d966ab1f24f12df89b724c31b1d0"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_CTI_Calibration.py": [
                        [
                            "@@ -89,6 +89,12 @@ def mainMethod(args):\n     pipe_tools.log_task_environment(logger)\n     logger.info(\"#\")\n \n+    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n+    os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n+    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n+    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n+    os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n+\n     workdir = args.workdir + \"/\"\n     logdir = args.logdir\n     config_filename = workdir + args.config_filename\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_CTI_Data_Visualize.py": [
                        [
                            "@@ -1,175 +0,0 @@\n-# Copyright (C) 2012-2020 Euclid Science Ground Segment\r\n-#\r\n-# This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General\r\n-# Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option)\r\n-# any later version.\r\n-#\r\n-# This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied\r\n-# warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\r\n-# details.\r\n-#\r\n-# You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to\r\n-# the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\r\n-#\r\n-import argparse\r\n-import os\r\n-import sys\r\n-\r\n-if sys.version_info[0] < 3:\r\n-    from ConfigParser import RawConfigParser\r\n-else:\r\n-    from configparser import RawConfigParser\r\n-import shutil\r\n-\r\n-# Euclid specific imports\r\n-import ElementsKernel.Logging as log\r\n-from astropy.io import fits\r\n-import numpy as np\r\n-import re\r\n-\r\n-# Local imports\r\n-from VIS_PyLibrary_Common import pe_run_information\r\n-\r\n-from VIS_Tasks_Common.FromToXML import read_json, write_json\r\n-from VIS_PyLibrary_Common import pipe_tools\r\n-\r\n-# initialise global logger\r\n-import ElementsKernel.Logging\r\n-\r\n-logger = ElementsKernel.Logging.getLogger(__name__)\r\n-\r\n-\r\n-def defineSpecificProgramOptions():\r\n-    \"\"\"\r\n-    @brief Allows to define the (command line and configuration file) options\r\n-    specific to this program\r\n-\r\n-    @details\r\n-    See the Elements documentation for more details.\r\n-    @return\r\n-    An  ArgumentParser.\r\n-    \"\"\"\r\n-\r\n-    # Inputs\r\n-    parser = argparse.ArgumentParser()\r\n-\r\n-    parser.add_argument(\"--workdir\", type=str, help=\"absolute path to the workdir\")\r\n-    parser.add_argument(\r\n-        \"--logdir\", type=str, help=\"path to the logdir, relative to the workdir\"\r\n-    )\r\n-    parser.add_argument(\"--config\", type=str, help=\"configuration file path\")\r\n-    parser.add_argument(\"--input_quads\", type=str)\r\n-    # Outputs\r\n-    parser.add_argument(\"--visualize_output\", type=str, help=\"Visualize json file\")\r\n-\r\n-    return parser\r\n-\r\n-\r\n-def mainMethod(args):\r\n-\r\n-    logger.info(\"#\")\r\n-    logger.info(\"# Entering %s mainMethod()\" % __name__)\r\n-    logger.info(\"# command line: \" + pipe_tools.get_erun_commandline())\r\n-    pipe_tools.log_task_environment(logger)\r\n-    logger.info(\"#\")\r\n-\r\n-    logdir = os.path.join(args.workdir, args.logdir)\r\n-    config_filename = os.path.join(args.workdir, args.config)\r\n-    input_quads = os.path.join(args.workdir, args.input_quads)\r\n-    visualize_output_name = os.path.join(args.workdir, args.visualize_output)\r\n-\r\n-    logger.info(\"ccdlistname: \" + visualize_output_name)\r\n-\r\n-    # Check that configuration file exists\r\n-    if os.path.isfile(config_filename) == False:\r\n-        logger.error(\"%s does not exist or is not readable\", config_filename)\r\n-        exit(1)\r\n-    # Instantiate the configuration file parser\r\n-    config = RawConfigParser()\r\n-    # Make it case-sensitive for parameter names\r\n-    config.optionxform = str\r\n-    # Open the configuration file\r\n-    config.read(config_filename)\r\n-\r\n-    # Set up logger\r\n-    logger.setLevel(\"DEBUG\")\r\n-\r\n-    logger.info(input_quads)\r\n-\r\n-    # Read parameters from config file\r\n-    # initialise PERunInformation dictionary for Data Quality Control\r\n-    # To be done later.\r\n-    peri = dict()\r\n-\r\n-    # Load quad name files.\r\n-    # NOTE: This is NOT 8*144 yet but 8 files/tuplelist.  These 8 files contain the 144 quads\r\n-    # I thin because of how we defined it in IAL Pip/Pkg these 8 files are in a file as a list of list\r\n-    # Basically a list of 8x1 where 1 is the file name.  Probably could be fixed in IAL definition.\r\n-    # Just in case it is 8x? higher than 1, will load up everything below.\r\n-    quad_name_files = [read_json(input_quads)]\r\n-    logger.info(\"Names of Quad Files:\")\r\n-    [logger.info(quad_name) for quad_name in quad_name_files]\r\n-\r\n-    visualize_output = []\r\n-\r\n-    # prefix file names with workdir\r\n-    all_quads = []\r\n-    print(f\"NUM FILES (FPARef_A): {quad_name_files}\")\r\n-    for i in range(len(quad_name_files)):\r\n-\r\n-        logger.info(\r\n-            f\"Quad Name file {i}: \"\r\n-            + str(quad_name_files[i])\r\n-            + \" len: \"\r\n-            + str(len(quad_name_files[i]))\r\n-        )\r\n-\r\n-        logger.info(\"quad name files[][]: \" + str(quad_name_files[i][0]))\r\n-\r\n-        for qname_tuple_file in quad_name_files[i]:\r\n-\r\n-            logger.info(\"Print QuadName TupleList: \" + qname_tuple_file)\r\n-            quad_144_list = read_json(qname_tuple_file)\r\n-            logger.info(\"Len quad_144_list: \" + str(quad_144_list))\r\n-            all_quads.append(quad_144_list)\r\n-\r\n-    # In a perfect world this [8][144] should be the same size and we don't bother opening fits files.\r\n-    # we just make file references from these lists. We are going for now to assume were in a perfect world\r\n-    # also a big assumption that every 4 quadrants are part of the same CCD.\r\n-    num_quads = len(all_quads[0])\r\n-\r\n-    # print(\"NUM FILES (FPARef_B): \" + str(all_quads))\r\n-    logger.info(\r\n-        \"NUM Quads in index 0: \"\r\n-        + str(num_quads)\r\n-        + \" Num of indexes: \"\r\n-        + str(len(all_quads[0]))\r\n-    )\r\n-\r\n-    print(all_quads)\r\n-    dddd\r\n-\r\n-    ccd_num = int(num_quads / 4)\r\n-    logger.info(\"Number of ccd to do: \" + str(ccd_num))\r\n-    for i in range(ccd_num):\r\n-        # For now lets store these names and everything in the gater_ccd directory instead of workdir.\r\n-        # Note it has .json names to this file we should do a strip or something here.\r\n-\r\n-        # file_name = visualize_output_name + \"_cti.\" + str(i) + \".json\"\r\n-        file_name = os.path.basename(visualize_output_name) + \"fpa_quad_\" + str(i) + \".png\"\r\n-\r\n-        ccd_quads = []\r\n-        visualize_output.append(file_name)\r\n-        for j in range(len(all_quads)):\r\n-            start_range = i * 4\r\n-            end_range = i * 4 + 4\r\n-            for k in range(start_range, end_range):\r\n-                ccd_quads.append(all_quads[j][k])\r\n-\r\n-\r\n-    logger.info(\"Writing to: \" + os.path.basename(visualize_output_name))\r\n-    write_json(visualize_output, visualize_output_name)\r\n-\r\n-    logger.info(\"#\")\r\n-    logger.info(\"# Exiting %s mainMethod()\" % __name__)\r\n-    logger.info(\"#\")\r\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_PTC_NL_calib.py": [
                        [
                            "@@ -156,7 +156,7 @@ def mainMethod(args):\n     ## if bf_corr_typ==1: correction with covariance;\n     ## if bf_corr_typ==2: correction with rebinning\n     bf_corr_typ = 2\n-    tile_s      = 4  ## when bf_corr_typ ==2 , tile_s must be a small number.\n+    tile_s      = 3  ## when bf_corr_typ ==2 , tile_s must be a small number.\n     kernelsize  = 12 ## if bf_corr_typ==2:  we rebin in superpixels of size kernelsize x kernelsize original pixels\n \n     # Initialize JSON model file names\n@@ -238,9 +238,16 @@ def mainMethod(args):\n         n_steps_tot = 2000\n \n         ## initialisation\n+        ndim = 5\n         p1_r = np.array([0.5,1.5])\n         p2_r = np.array([-0.5,0.5])\n         p3_r = np.array([-0.5,0.5])\n+        tab_r = np.array([p1_r])\n+        # print(tab_r)\n+        for i in range(1,ndim):\n+            tab_r = np.append(tab_r,[np.array([-0.5,0.5])],axis=0)\n+        p = np.zeros(ndim)\n+        p[0] = 1/3.\n         tab_r = np.array([p1_r,p2_r,p3_r]) #range on which vary the parameters\n \n         p = np.array([0.3,0,0]) # initial guess for the parameter\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_cti_master_ci_xml_out.py": [
                        [
                            "@@ -0,0 +1,147 @@\n+\"\"\"\r\n+@file: python/Tasks/VIS_master_ci_xml_out.py\r\n+@author: user\r\n+@date: 21/02/2023\r\n+\"\"\"\r\n+\r\n+#Standard library imports\r\n+import argparse\r\n+import os\r\n+import fitsio\r\n+import json\r\n+from pathlib import Path\r\n+import shutil\r\n+from typing import Union\r\n+\r\n+\r\n+# Euclid specific imports\r\n+from ST_DM_FilenameProvider.FilenameProvider import FileNameProvider\r\n+from ST_DM_DmUtils import DmUtils\r\n+from ST_DataModelBindings.bas import img_stub\r\n+from ST_DataModelBindings.pro import vis_stub\r\n+from ST_DataModelBindings.dpd.vis import vismasterciframe_stub\r\n+from VIS_Tasks_Common.FromToXML import (\r\n+  vis_filter,\r\n+  create_validity_range,\r\n+  vis_generic_header,\r\n+  create_img_type,\r\n+  UNKNOWN_STRING,\r\n+  add_quality_parameter_file\r\n+)\r\n+\r\n+from VIS_PyLibrary_Common.zip_tools import tar_zip_files\r\n+from VIS_PyLibrary_Common import pipe_tools\r\n+\r\n+import ElementsKernel.Logging\r\n+logger = ElementsKernel.Logging.getLogger( __name__)\r\n+\r\n+\r\n+################################################################################\r\n+\r\n+def defineSpecificProgramOptions():\r\n+\r\n+    parser = argparse.ArgumentParser()\r\n+    # inputs\r\n+    parser.add_argument( '--workdir',                    required=True,  help='absolute path to the workdir')\r\n+    parser.add_argument( '--logdir',                     required=True,  help='path to the logdir, relative to the workdir')\r\n+    parser.add_argument('--config',                                      help='configuration file path')\r\n+    parser.add_argument( '--cti_master_ci_list', required=True,  help='Path to the master CI results json file')\r\n+    # outputs\r\n+    parser.add_argument( '--master_ci_xml_out',                required=True,  help='output master CI results xml filename')\r\n+    return parser\r\n+\r\n+################################################################################\r\n+\r\n+def write_tar_zip(in_files: list[Union[Path, str]], out_file: Path) -> None:\r\n+  tar_zip_files(\" \".join(map(str, in_files)), str(out_file), False)\r\n+\r\n+def mainMethod( args):\r\n+\r\n+  logger.info( '#')\r\n+  logger.info( '# Entering %s mainMethod()' % __name__)\r\n+  logger.info( '# command line: ' + pipe_tools.get_erun_commandline())\r\n+  pipe_tools.log_task_environment( logger)\r\n+  logger.info( '#')\r\n+  \r\n+  # Create workdir folders for XML files if not present:\r\n+  \r\n+  if not os.path.isabs( args.master_ci_xml_out):\r\n+    args.master_ci_xml_out = os.path.join( args.workdir, args.master_ci_xml_out)\r\n+\r\n+  out_dir = os.path.split(args.master_ci_xml_out )[0]\r\n+  os.makedirs(out_dir, exist_ok=True)\r\n+\r\n+  with open(args.cti_master_ci_list, \"r\") as f:\r\n+    master_ci_filename_fits = json.load(f)\r\n+\r\n+  workdir_master_path = os.path.join(args.workdir, master_ci_filename_fits)\r\n+  data_path = os.path.join(args.workdir, \"data\")\r\n+  shutil.copy(workdir_master_path, data_path)\r\n+\r\n+  master_ci_data_path_fits = os.path.join(data_path, master_ci_filename_fits)\r\n+\r\n+  # Create the data product\r\n+  data_product = vismasterciframe_stub.DpdVisMasterCIFrame()\r\n+\r\n+  # Add Header\r\n+  data_product.Header = vis_generic_header(\"DpdVisMasterCIFrame\")\r\n+\r\n+  # Add Data\r\n+  data = vis_stub.visMasterCIFrame.Factory()\r\n+\r\n+  # Open the FITS file\r\n+  fits = fitsio.FITS(master_ci_data_path_fits, 'r')\r\n+  nhdu = fits[-1].get_extnum()\r\n+  header = fits[1].read_header()\r\n+\r\n+  data.ImgType = create_img_type(\"CALIB\", \"CHARGE_INJECTION\", \"OTHER\", \"CALIBRATION\")\r\n+  data.ImgArea = img_stub.imgArea(Name=\"QUADRANT\")\r\n+  data.ImgNumber = nhdu\r\n+  data.AxisNumber = header[\"NAXIS\"]\r\n+\r\n+  data.AxisLengths = [header[\"NAXIS1\"], header[\"NAXIS2\"]]\r\n+  data.DataSize = header[\"BITPIX\"]\r\n+  data.DataLength = header[\"NAXIS1\"] * header[\"NAXIS2\"]\r\n+  data.ValidityRange = create_validity_range()\r\n+  data.Filter = vis_filter()\r\n+  # DataStorage\r\n+  data.DataStorage = DmUtils.create_fits_storage(\r\n+    vis_stub.visMasterCIStorageFitsFile,\r\n+    os.path.basename(master_ci_filename_fits), \"vis.masterCI\", \"0.1\"\r\n+  )\r\n+\r\n+  fits.close()\r\n+\r\n+\r\n+  # Add data to data product\r\n+  data_product.Data = data\r\n+\r\n+\r\n+  # All data quality images (.png files) are in the folder workdir/data/parallel_images_***\r\n+  # We zip this folder into a .tar file and add it to the data product\r\n+\r\n+  dq_file = FileNameProvider().get_allowed_filename(\r\n+    processing_function='VIS',\r\n+    type_name=\"QC-PLOTS-MASTER-CI\",\r\n+    instance_id='',\r\n+    extension='.tar.gz'\r\n+  )\r\n+\r\n+  dq_folder = os.path.join(args.workdir, \"data\", \"images\")\r\n+  dq_tar_path = os.path.join(args.workdir, \"data\", dq_file)\r\n+\r\n+  write_tar_zip([dq_folder], dq_tar_path)\r\n+\r\n+  add_quality_parameter_file(data_product, dq_file)\r\n+\r\n+  dqc_params_dm = vis_stub.visDqcParams.Factory()\r\n+\r\n+  data_product.QualityParams = dqc_params_dm\r\n+\r\n+  # Output to XML^M                                                                                   \r\n+\r\n+  DmUtils.save_product_metadata(data_product, args.master_ci_xml_out)\r\n+\r\n+  logger.info( \"#\")\r\n+  logger.info( \"# Exiting %s mainMethod()\" % __name__)\r\n+  logger.info( \"#\")\r\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_cti_xml_out.py": [
                        [
                            "@@ -1,5 +1,5 @@\n \"\"\"\r\n-@file: python/Tasks/VIS_cti_xml_out.py\r\n+@file: python/Tasks/VIS_parallel_cti_xml_out.py\r\n @author: user\r\n @date: 21/02/2023\r\n \"\"\"\r\n@@ -7,24 +7,19 @@\n #Standard library imports\r\n import argparse\r\n import os\r\n-import shutil\r\n import json\r\n+from pathlib import Path\r\n import tarfile\r\n-import time\r\n-\r\n-from types import SimpleNamespace as Namespace\r\n+from typing import Union\r\n \r\n \r\n # Euclid specific imports\r\n from ST_DM_FilenameProvider.FilenameProvider import FileNameProvider\r\n-from ST_DM_DmUtils import DmUtils\r\n-from ST_DataModelBindings.dpd.vis import visctimodel_stub\r\n from ST_DataModelBindings.pro import vis_stub\r\n-import ST_DataModelBindings.ins_stub as ins_dict\r\n from ST_DM_DmUtils.DmUtils import create_data_container, save_product_metadata\r\n-from VIS_Tasks_Common.FromToXML import vis_generic_header, UNKNOWN_STRING\r\n+from VIS_Tasks_Common.FromToXML import create_validity_range, vis_generic_header, UNKNOWN_STRING, add_quality_parameter_file\r\n \r\n-from VIS_Tasks_Common import FromToXML\r\n+from VIS_PyLibrary_Common.zip_tools import tar_zip_files\r\n from VIS_PyLibrary_Common import pipe_tools\r\n \r\n import ElementsKernel.Logging\r\n@@ -39,13 +34,22 @@ def defineSpecificProgramOptions():\n     # inputs\r\n     parser.add_argument( '--workdir',                    required=True,  help='absolute path to the workdir')\r\n     parser.add_argument( '--logdir',                     required=True,  help='path to the logdir, relative to the workdir')\r\n-    parser.add_argument( '--calibrate_cti_results_list', required=True,  help='Path to the CTI calibration results json file')\r\n+    parser.add_argument('--config',                                      help='configuration file path')\r\n+    parser.add_argument( '--parallel_calibrate_cti_results_list', required=True,  help='Path to the parallel CTI calibration results json file')\r\n+    parser.add_argument( '--serial_calibrate_cti_results_list', required=True,  help='Path to the serial CTI calibration results json file')\r\n     # outputs\r\n-    parser.add_argument( '--cti_xml_out',                required=True,  help='output CTI results xml filename')\r\n+    parser.add_argument( '--parallel_cti_xml_out',                required=True,  help='output parallel CTI results xml filename')\r\n+    parser.add_argument( '--serial_cti_xml_out',                required=True,  help='output serial CTI results xml filename')\r\n     return parser\r\n \r\n ################################################################################\r\n \r\n+def create_tar_gz(output_filename, folders_to_compress):\r\n+  with tarfile.open(output_filename, \"w:gz\") as tar:\r\n+    for folder in folders_to_compress:\r\n+      tar.add(folder, arcname=os.path.basename(folder))\r\n+\r\n+\r\n def mainMethod( args):\r\n \r\n   logger.info( '#')\r\n@@ -56,93 +60,235 @@ def mainMethod( args):\n   \r\n   # Create workdir folders for XML files if not present:\r\n   \r\n-  if not os.path.isabs( args.cti_xml_out):\r\n-    args.cti_xml_out = os.path.join( args.workdir, args.cti_xml_out)\r\n+  if not os.path.isabs( args.parallel_cti_xml_out):\r\n+    args.parallel_cti_xml_out = os.path.join( args.workdir, args.parallel_cti_xml_out)\r\n+\r\n+  if not os.path.isabs( args.serial_cti_xml_out):\r\n+    args.serial_cti_xml_out = os.path.join( args.workdir, args.serial_cti_xml_out)\r\n \r\n-  out_dir = os.path.split(args.cti_xml_out )[0]\r\n+  out_dir = os.path.split(args.parallel_cti_xml_out )[0]\r\n   os.makedirs(out_dir, exist_ok=True)\r\n \r\n-  # get an Euclid compliant filename to store XML\r\n+  \"\"\"\r\n+  calibrate_cti_results_list: is a .json file containing strings with the names of 72.json files in the workdir, \r\n+  each are parallel and serial fits per ccd.\r\n   \r\n-  # TODO : Need to check if type_name is a good name or not\r\n-  \r\n-  datafile_out = FileNameProvider().get_allowed_filename( processing_function='VIS',\r\n-                                                          type_name          =\"MDL-CTICORR-000-000000-0000000\",\r\n-                                                          instance_id        ='',\r\n-                                                          extension          ='.json')\r\n-  datafile_out = os.path.join( args.workdir, \"data\", datafile_out)\r\n-  \r\n-  # concatenate the CTI models in datafile_out\r\n+  cti_per_ccd is a list of 72 .json files, where each .json files is a dictionary of the results of the CTI calibration p\r\n+  ipeline. For each, the first .json file is called CCD1-1_parallel.json, which will look something like the following:\r\n \r\n-  # calibrate_cti_results_list: is a .json file containing strings with the names of 72.json files in the workdir, each are parallel and serial fits per ccd.\r\n-  # cti_per_ccd is a list of 72 .json files, where each .json files is a dictionary of the results of the CTI calibration pipeline.\r\n-  # For each, the first .json file is called CCD1-1_parallel.json, which will look something like the following:\r\n-\r\n-  # {\"MultiNestZip\": \"/sps/euclid/Users/jnightin/workspaces/ctical/ppo_root/workdir//output/2022-10-13T11:37:14.399Z_CCD6-4/parallel[x1]/2f929b8acbe296cfb1102229221a7438.zip\",\r\n+  # {\"ModelResult\": \"/sps/euclid/Users/jnightin/workspaces/ctical/ppo_root/workdir//output/2022-10-13T11:37:14.399Z_CCD6-4/parallel[x1]/2f929b8acbe296cfb1102229221a7438.zip\",\r\n   # {\"ModelInfo\": \"/sps/euclid/Users/jnightin/workspaces/ctical/ppo_root/workdir//output/2022-10-13T11:37:14.399Z_CCD6-4/parallel[x1]/2f929b8acbe296cfb1102229221a7438/model.info\"}\r\n+  \"\"\"\r\n \r\n-  cti_per_ccd = FromToXML.load_list_from_file( args.calibrate_cti_results_list)\r\n+  with open(args.parallel_calibrate_cti_results_list, \"r\") as f:\r\n+    parallel_cti_per_ccd_list = json.load(f)\r\n \r\n-  logger.info( f\"reading {len( cti_per_ccd)} CCD CTI calibration files\")\r\n+  with open(args.serial_calibrate_cti_results_list, \"r\") as f:\r\n+    serial_cti_per_ccd_list = json.load(f)\r\n \r\n-  cti_out = dict()\r\n-  cti_out[\"creator\"]     = __file__\r\n-  cti_out[\"date\"]        = time.ctime()\r\n-  cti_out[\"description\"] = \"The CTI Calibration results contains for each CCD the results of a Dynesty model-fit to charge injection data and other datasets. \"\r\n+  cti_per_ccd_list = parallel_cti_per_ccd_list + serial_cti_per_ccd_list\r\n \r\n-  # This loop updates the cti_out dictionary with each key of the .json file containing the 72 model fits.\r\n+  cti_result_list = []\r\n \r\n-  # For example, the first iteration loads the result:\r\n+  for cti_for_ccd_list in cti_per_ccd_list:\r\n \r\n-  # {\"Something\":/sps/euclid/Users/jnightin/workspaces/ctical/ppo_root/workdir/cti_calibration_results/CCD1-1_parallel.json\"}\r\n+     with open(cti_for_ccd_list, \"r\") as f:\r\n \r\n-  for cti_file in cti_per_ccd:\r\n-    with open( cti_file, \"r\") as f:\r\n-      cti_out.update( json.load( f))\r\n-  logger.info( \"writing CTI calibration data product to \" + datafile_out)\r\n+        cti_result = json.load(f)\r\n+        cti_result_list.append(cti_result)\r\n \r\n-  # This loop dumps each result in this dictionary to a new .json file, for some reason.\r\n \r\n-  with open( datafile_out, \"w\") as f:\r\n-    json.dump( cti_out, f, indent=2)\r\n+  logger.info( f\"reading {len( cti_per_ccd_list)} CCD CTI calibration files\")\r\n \r\n-  # create and write the xml data product encapsulating the datafile_out json file\r\n-  logger.info( \"writing data product XML to \" + args.cti_xml_out)\r\n-  data_product = FromToXML.cti_model_dp()\r\n+  from ST_DM_DmUtils import DmUtils\r\n+  from ST_DataModelBindings.dpd.vis import viscticalibrationresults_stub\r\n+  import ST_DataModelBindings.ins_stub as ins_dict\r\n \r\n+  \"\"\"Create the date range\"\"\"\r\n   date_range = ins_dict.calibrationValidPeriod()\r\n   date_range.TimestampStart = \"2023-02-21T00:00:00.000Z\"\r\n   date_range.TimestampEnd = \"2023-02-22T00:00:00.000Z\"\r\n \r\n-  phase = vis_stub.visCTIModelPhaseData()\r\n-  phase.MultiNestZIP = create_data_container(\"multinestzip.zip\")\r\n-  phase.ModelInfo = create_data_container(\"modelinfo.json\")\r\n-  phase.ModelResults = create_data_container(\"modelresults.json\")\r\n-  phase.DetectorId = \"1-1\"\r\n-  phase.DateRange = date_range\r\n-  phase.ValidityRange = date_range\r\n-  phase.PhaseNumber = 1\r\n-  phase.PhaseMetaData = create_data_container(\"blah.json\")\r\n-  phase.OptimizerPickle = create_data_container(\"blah.json\")\r\n-  phase.ModelPickle = create_data_container(\"blah.json\")\r\n-  phase.TrimmingLevel = create_data_container(\"blah.json\")\r\n-\r\n-  qualPramStorage = vis_stub.visQualityParameterStorageFitsFile()\r\n+  \"\"\"Create a CTI model fit entry\"\"\"\r\n+  parallel_fit_list = []\r\n+  serial_fit_list = []\r\n+\r\n+  \"\"\"\r\n+  __Fit Results__\r\n+  \r\n+  The cti_result_list contains the results of all 72 CTI calibrations performed in this pipeline\r\n+  run (36 x parallel CTI, 36 x serial CTI).\r\n+\r\n+  This loop iterates over all results, creates a visCTIModelFitData() object for each\r\n+  and then stores in them data.Fits at the end.\r\n+  \"\"\"\r\n+\r\n+  for cti_result in cti_result_list:\r\n+\r\n+    fit = vis_stub.visCTIModelFitData()\r\n+\r\n+    fit.ValidityRange = date_range\r\n+    fit.DateRange = date_range\r\n+    fit.DetectorId = cti_result[\"DetectorId\"]\r\n+\r\n+    fit.FitMetaData = create_data_container(cti_result[\"FitMetaData\"])\r\n+    fit.SearchPickle = create_data_container(cti_result[\"SearchPickle\"])\r\n+    fit.ModelPickle = create_data_container(cti_result[\"ModelPickle\"])\r\n+    fit.ModelInfo = create_data_container(cti_result[\"ModelInfo\"])\r\n+    fit.Model = create_data_container(cti_result[\"Model\"])\r\n+\r\n+    fit.ModelResults = create_data_container(cti_result[\"ModelResults\"])\r\n+    fit.SearchInfo = create_data_container(cti_result[\"SearchInfo\"])\r\n+\r\n+    fit.MaxLikelihoodInstance = create_data_container(cti_result[\"MaxLikelihoodInstance\"])\r\n+\r\n+    fit.CovarianceMatrix = create_data_container(cti_result[\"CovarianceMatrix\"])\r\n+\r\n+    fit.ExtractedEPERs = create_data_container(str(cti_result[\"ExtractedEPERs\"]))\r\n+\r\n+    fit.TotalTraps = cti_result[\"TotalTraps\"]\r\n+    fit.TrapType = cti_result[\"TrapType\"]\r\n+    fit.CCDType = cti_result[\"CCDType\"]\r\n+    fit.IsFitBinned = cti_result[\"IsFitBinned\"]\r\n+    fit.IsParallelSerialSimultaneous = cti_result[\"IsParallelSerialSimultaneous\"]\r\n+    fit.IsraelDeltaEllipticity = cti_result[\"IsraelDeltaEllipticity\"]\r\n+    fit.TimeFromZero = cti_result[\"TimeFromZero\"]\r\n+\r\n+    if cti_result[\"IsParallel\"]:\r\n+      parallel_fit_list.append(fit)\r\n+      cti_result_parallel = cti_result\r\n+    else:\r\n+      serial_fit_list.append(fit)\r\n+      cti_result_serial = cti_result  \r\n+\r\n+  \"\"\"\r\n+  __Data Quality Files__\r\n+  \r\n+  Create a list of the Data Quality image folders which contain all .png images of the fits \r\n+  for parallel and serial CTI.\r\n+  \r\n+  There are 72 folders in total, 36 for parallel and 36 for serial, which will be collectively\r\n+  zipped into two .tar files.\r\n+  \"\"\"\r\n+\r\n+  parallel_dq_file_list = []\r\n+  serial_dq_file_list = []\r\n+\r\n+  for cti_result in cti_result_list:\r\n+\r\n+    file = os.path.join(args.workdir, \"data\", cti_result[\"DQImage\"])\r\n+\r\n+    if cti_result[\"IsParallel\"]:\r\n+      parallel_dq_file_list.append(file)\r\n+    else:\r\n+      serial_dq_file_list.append(file)\r\n+\r\n+  \"\"\"PARALLEL Data Product\"\"\"\r\n+\r\n+  # Create the data product\r\n+  data_product = viscticalibrationresults_stub.DpdVisCTICalibrationResults()\r\n+\r\n+  # Add Header\r\n+  data_product.Header = vis_generic_header(\"DpdVisCTICalibrationResults\", UNKNOWN_STRING)\r\n+\r\n+  # Create the Data\r\n+  data = vis_stub.visCTICalibrationResults.Factory()\r\n+  data.ValidityRange = create_validity_range()\r\n+  data.DateRange = date_range\r\n+  data.PreviousCTIModelsUsedID = \"0\"\r\n+\r\n+  trap = cti_result_parallel[\"TrapType\"]\r\n+  ccd = cti_result_parallel[\"CCDType\"]\r\n+  time_from_zero = cti_result_parallel[\"TimeFromZero\"]\r\n+  \r\n+  data.FitID = f'Par_{time_from_zero}_{trap}_{ccd}'\r\n+\r\n+  # Add fit to data\r\n+  data.Fits = parallel_fit_list\r\n+\r\n+  # Add data to data product\r\n+  data_product.Data = data\r\n+\r\n+  # All data quality images (.png files) are in the folder workdir/data/parallel_images_***\r\n+  # We zip this folder into a .tar file and add it to the data product\r\n+\r\n+  parallel_dq_file = FileNameProvider().get_allowed_filename(\r\n+    processing_function='VIS',\r\n+    type_name=\"QC-PLOTS-CTI-PARALLEL\",\r\n+    instance_id='',\r\n+    extension='.tar.gz'\r\n+  )\r\n+\r\n+  # parallel_dq_tar_path contains a list of folders, which are now zipped into a .tar.gz file:\r\n+  parallel_dq_tar_path = os.path.join(args.workdir, \"data\", parallel_dq_file)\r\n+\r\n+  create_tar_gz(\r\n+    output_filename=parallel_dq_tar_path,\r\n+    folders_to_compress=parallel_dq_file_list\r\n+  )\r\n+  add_quality_parameter_file(data_product, parallel_dq_file)\r\n+\r\n+  dqc_params_dm = vis_stub.visDqcParams.Factory()\r\n+\r\n+  data_product.QualityParams = dqc_params_dm\r\n+\r\n+  # Output to XML^M                                                                                   \r\n+\r\n+  DmUtils.save_product_metadata(data_product, args.parallel_cti_xml_out)\r\n+\r\n+  \"\"\"SERIAL Data Product\"\"\"\r\n+\r\n+  # Create the data product\r\n+  data_product = viscticalibrationresults_stub.DpdVisCTICalibrationResults()\r\n+\r\n+  # Add Header\r\n+  data_product.Header = vis_generic_header(\"DpdVisCTICalibrationResults\", UNKNOWN_STRING)\r\n+\r\n+  # Create the Data\r\n+  data = vis_stub.visCTICalibrationResults.Factory()\r\n+  data.ValidityRange = create_validity_range()\r\n+  data.DateRange = date_range\r\n+  data.PreviousCTIModelsUsedID = \"0\"\r\n+  \r\n+  trap = cti_result_serial[\"TrapType\"]\r\n+  ccd = cti_result_serial[\"CCDType\"]\r\n+  time_from_zero = cti_result_serial[\"TimeFromZero\"]\r\n+  \r\n+  data.FitID = f'Ser_{time_from_zero}_{trap}_{ccd}'\r\n+\r\n+  # Add fit to data\r\n+  data.Fits = serial_fit_list\r\n+\r\n+  # Add data to data product\r\n+  data_product.Data = data\r\n+\r\n+  # All data quality images (.png files) are in the folder workdir/data/serial_images_***\r\n+  # We zip this folder into a .tar file and add it to the data product\r\n+\r\n+  serial_dq_file = FileNameProvider().get_allowed_filename(\r\n+    processing_function='VIS',\r\n+    type_name=\"QC-PLOTS-CTI-SERIAL\",\r\n+    instance_id='',\r\n+    extension='.tar.gz'\r\n+  )\r\n+\r\n+  serial_dq_tar_path = os.path.join(args.workdir, \"data\", serial_dq_file)\r\n+\r\n+  create_tar_gz(\r\n+    output_filename=serial_dq_tar_path,\r\n+    folders_to_compress=serial_dq_file_list\r\n+  )\r\n+  add_quality_parameter_file(data_product, serial_dq_file)\r\n+  \r\n   dqc_params_dm = vis_stub.visDqcParams.Factory()\r\n \r\n-  data_product.ValidityRange = date_range\r\n-  data_product.DateRange = date_range\r\n-  data_product.PreviousCTIModelsUsedID = 0\r\n-  data_product.PhaseTotal = 1\r\n-  data_product.PhaseID = 123\r\n-  data_product.Phases = [phase]\r\n-  data_product.CILFrames = 0\r\n-  data_product.visCTIModelPhaseData = 0\r\n-  data_product.cilFrameRefList = 0\r\n-  data_product.QualityParameterStorage = qualPramStorage\r\n+#  data_product.QualityParameterStorage = qualPramStorage\r\n   data_product.QualityParams = dqc_params_dm\r\n \r\n-  DmUtils.save_product_metadata( data_product, args.cti_xml_out)\r\n+  # Output to XML\r\n+\r\n+  DmUtils.save_product_metadata(data_product, args.serial_cti_xml_out)\r\n+\r\n \r\n   logger.info( \"#\")\r\n   logger.info( \"# Exiting %s mainMethod()\" % __name__)\r\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_process_astro_field.py": [
                        [
                            "@@ -129,11 +129,15 @@ def mainMethod(args):\n         else:\n             # run wcsfit in 'use-fpa-model' mode to copy distmodel to headers and reconstruct pointing\n             for catalogue, exposure in zip( catalog_list, exposure_list):\n+                # Open the catalog to find exposure duration (EXPTIME in #HDU 0)\n+                with fits.open(catalogue, mode='readonly',  memmap=False) as hdul:\n+                    exptime = hdul[0].header[\"EXPTIME\"]\n                 logger.info (\"==> Performing Wcsfit run on %s...\", catalogue)\n                 Modules.Wcsfit( config,\n                                 catalogue,\n                                 reference_starcat,\n                                 headers_to_update( exposure),\n+                                exptime,\n                                 os.path.join( visdef.logdir_name( args.logdir), visdiags.DIAGDIR))\n \n     # write output lists to JSON files\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_process_quad.py": [
                        [
                            "@@ -140,7 +140,7 @@ def mainMethod(args):\n     # get visible pixel indices\n     xmin, xmax, ymin, ymax = FPA_quadrant.get_quadrant_real_pixels( mef_hdul[1])\n \n-    \n+\n     # get an objects_mask flagmap if one available\n     objmask_cfg_section = f\"object_mask_ObsId-{prim_hdr['OBS_ID']}-PointingId-{prim_hdr['PTGID']}\"\n     objmask_name = None\n@@ -148,7 +148,7 @@ def mainMethod(args):\n       objmask_name = config.get( objmask_cfg_section, \"flagmap_FITS\")\n       flagmap_list.append( os.path.join( DATADIR, objmask_name))\n       logger.info( f\"found {objmask_name} to mask objects in {args.raw_exp_in}\")\n-    \n+\n     for hdu_in in mef_hdul[1:]:\n       # read quadrant extension from MEF\n       quad_dat = hdu_in.data.astype( \"float32\")\n@@ -271,7 +271,7 @@ def mainMethod(args):\n       outfile = out_prefix # eg. /workdir/EUC_VIS_EXP_052929_01.fits\n       logger.info( \"==> Processing \" + os.path.basename( outfile + dot_ext))\n \n-       \n+\n       # Flag saturated pixels\n       if config.has_option('CalibDataProducts', 'saturation_model'):\n         infile = outfile\n@@ -293,8 +293,8 @@ def mainMethod(args):\n           os.remove( infile + dot_ext)\n       else:\n         logger.info( \"### Saturated pixels flagging deactivated in \" + args.config)\n-        \n-    \n+\n+\n     # Correct XTalk\n     # all quadrants of a block must be in the same processing state to correct XTalk\n     if config.has_option('CalibDataProducts', 'xtalk_model'):\n@@ -309,7 +309,7 @@ def mainMethod(args):\n         os.remove( infile + dot_ext)\n     else:\n       logger.info( \"### CrossTalk correction deactivated in \" + args.config)\n-    \n+\n     processed_quad_fits = list()\n     processed_quad_flg_fits = list()\n     outfile_init = outfile\n@@ -349,7 +349,7 @@ def mainMethod(args):\n       if config.has_option('CalibDataProducts', 'nlcorr_model'):\n         infile = outfile\n         outfile = infile + '_nl' # eg. /workdir/EUC_VIS_EXP_052929_01.fits_sa_xt_bs_nl\n-        CorrectNonLin( config, infile + dot_ext, outfile + dot_ext, quad_name)\n+        CorrectNonLin( config, args.config, infile + dot_ext, outfile + dot_ext, quad_name)\n         logger.info( f\"==> CorrectNonLin done on {quad_name} ({iquad+1}/{len( qnames)})\")\n         if rm_tmpfiles:\n           os.remove( infile + dot_ext)\n@@ -381,33 +381,33 @@ def mainMethod(args):\n #        if rm_tmpfiles:\n #          os.remove( infile + dot_ext)\n \n-      # Cosmics      \n-      logger.info(f\"flagmap before Cosmics is : {flg_file}\")      \n-      \n+      # Cosmics\n+      logger.info(f\"flagmap before Cosmics is : {flg_file}\")\n+\n       start_time = time.time()\n       if config.getboolean('General', 'FlagCosmics') == True:\n         infile = outfile\n         # This processing step does not modify the input image\n         # hence, we do directly : outfile = infile\n         outfile = infile\n-        Cosmics( config, quad_name, infile + dot_ext, flg_file + dot_ext)        \n+        Cosmics( config, quad_name, infile + dot_ext, flg_file + dot_ext)\n         outfile = infile\n         logger.info(f\"outfile after Cosmics is : {outfile+dot_ext}\")\n         logger.info(f\"flagmap after Cosmics is : {flg_file+dot_ext}\")\n         logger.info( f\"==> Cosmics flagged in {quad_name} ({iquad+1}/{len( qnames)})\")\n       else:\n         logger.info( \"### Cosmics flagging deactivated in \" + args.config)\n-      \n+\n       end_time = time.time()\n       total_time = end_time - start_time\n       logger.info(f\"Flagging of CR takes {total_time} seconds\")\n-      \n+\n       # Correct Brighter-Fatter\n       if config.has_option('CalibDataProducts', 'bfe_model'):\n         infile = outfile\n         outfile = infile + '_bfe' # eg. /workdir/EUC_VIS_EXP_052929_01.fits_sa_xt_bs_nl_cti_bfe\n-        BrighterFatter_correction(infile + dot_ext, flg_file + dot_ext, \n-                                  bfe_kernels_dict[quad_name], quad_name, config.getfloat( \"GainPerQuad\", quad_name), \n+        BrighterFatter_correction(infile + dot_ext, flg_file + dot_ext,\n+                                  bfe_kernels_dict[quad_name], quad_name, config.getfloat( \"GainPerQuad\", quad_name),\n                                   outfile + dot_ext)\n         logger.info( f\"==> CorrectBFE done on {quad_name} ({iquad+1}/{len( qnames)})\")\n         if rm_tmpfiles:\n@@ -428,7 +428,7 @@ def mainMethod(args):\n           os.remove( infile + dot_ext)\n       else:\n         logger.info( \"### Pre/Overscan pixels removal deactivated in \" + args.config)\n-        \n+\n       # Correct flat field\n       if config.has_option('CalibDataProducts', 'master_flat'):\n         infile = outfile\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_science_xml_out.py": [
                        [
                            "@@ -335,10 +335,12 @@ def mainMethod(args):\n         update_background_zodimodel( background, config, int( primhdr['EXPTIME']))\n \n         # Get Euclid filenames\n+        # SWL-DET has 3 extensions per detector, bug in fitsio prevents from generating it compressed\n+        # so generate it uncompressed and compress it after data product creation\n         euc_det_file = filename_provider_obj.get_allowed_filename(processing_function='VIS',\n                                                           type_name=\"SWL-DET-\" + exp_id + \"-0000000\",\n                                                           instance_id='',\n-                                                          extension=fits_ext)\n+                                                          extension=\".fits\")\n         euc_bkg_file = filename_provider_obj.get_allowed_filename(processing_function='VIS',\n                                                              type_name=\"SWL-BKG-\" + exp_id + \"-0000000\",\n                                                              instance_id='',\n@@ -379,7 +381,15 @@ def mainMethod(args):\n                                                    euc_psf_file,\n                                                    euc_bkg_file,\n                                                    euc_wgt_file,\n-                                                   raw_frame_id = raw_frame_ids[-1])\n+                                                   raw_meta = exposure_config_section)\n+        if ZIP_OUTPUTS:\n+          # compress SWL-DET FITS file and put its compressed name in data product\n+          zip_tools.zip_file( os.path.join( datadir, euc_det_file),\n+                              os.path.join( datadir, euc_det_file + \".gz\"),\n+                              options=\"\")\n+          euc_det_file += \".gz\"\n+          vis_cal_frame_dp.Data.DataStorage.DataContainer.FileName = euc_det_file\n+\n         # Spatial footprint observation sequence and observation date will be needed for single exposure catalog\n         spatial_footprint = vis_cal_frame_dp.Data.ImgSpatialFootprint\n         observation_sequence = vis_cal_frame_dp.Data.ObservationSequence\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Tasks_M/python/VIS_Tasks_M/VIS_xml_in.py": [
                        [
                            "@@ -373,6 +373,10 @@ def mainMethod(args):\n     config.set( raw_frame_section, \"Header.CreationDate\",    dp.Header.CreationDate)\n     config.set( raw_frame_section, \"Data.ImgType.Category\",  dp.Data.ImgType.Category)\n     config.set( raw_frame_section, \"Data.ImgType.FirstType\", dp.Data.ImgType.FirstType)\n+    if hasattr( dp.Data.ObservationSequence, \"CalblockId\"):\n+      config.set( raw_frame_section, \"dp.Data.ObservationSequence.CalblockId\", dp.Data.ObservationSequence.CalblockId)\n+    if hasattr( dp.Data.ObservationSequence, \"CalblockVariant\"):\n+      config.set( raw_frame_section, \"dp.Data.ObservationSequence.CalblockVariant\", dp.Data.ObservationSequence.CalblockVariant)\n     if dp.Data.ImgType.FirstType == \"FLAT\":\n       for hkey in (\"CULEDMSK\", \"CULEDDUR\"):\n         config.set( raw_frame_section, hkey, primhdr[hkey])\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_Analysis/fitting/results.py": [
                        [
                            "@@ -13,20 +13,21 @@ class Result:\n     row: Optional[int]\n     col: int\n     phase: int\n-    model_fit: ModelFit\n+    model_fit: Optional[ModelFit]\n     fitted_points: dict[float, float]\n     cr_flags: int\n \n     def asdict(self) -> dict[Union[str, float], Union[str, float]]:\n         result = {\"row\": self.row} if self.row is not None else {}\n-        result.update(\n-            {\n-                \"column\": self.col,\n-                \"phase\": self.phase,\n-                \"model_name\": self.model_fit.name,\n-                **self.model_fit.corrected_params,\n-                **self.fitted_points,\n-                \"cr_flags\": self.cr_flags,\n-            }\n-        )\n+        result.update({\"column\": self.col, \"phase\": self.phase})\n+\n+        if self.model_fit is not None:\n+            result.update(\n+                {\n+                    \"model_name\": self.model_fit.name,\n+                    **self.model_fit.corrected_params,\n+                }\n+            )\n+\n+        result.update({**self.fitted_points, \"cr_flags\": self.cr_flags})\n         return result\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_Analysis/traps/abstract.py": [
                        [
                            "@@ -1,6 +1,7 @@\n from abc import ABC, abstractmethod\n from collections.abc import Iterator, Sized\n from dataclasses import dataclass\n+from functools import partial\n from typing import Union\n \n import numpy as np\n@@ -66,19 +67,26 @@ class Trap(ABC):\n         return 0\n \n     def run_model_fitting(self) -> Result:\n+        part_result = partial(\n+            Result,\n+            fitted_points=self.fitted_points,\n+            cr_flags=self.cr_flags,\n+        )\n+        if not self.models:  # no models to fit\n+            return part_result(\n+                row=self.row, col=self.col, phase=0, model_fit=None\n+            )\n+\n         model_fits = map(ModelFit.fit, self.models)\n         model_comparator = ModelComparator(\n             list(filter(lambda fit: fit.has_params, model_fits))\n         )\n         best_fit = model_comparator.get_best_model()\n+\n+        row, col = self.adjusted_row_col\n         phase = best_fit.get_phase(self.phase_options, self.is_low_high)\n-        return Result(\n-            *self.adjusted_row_col,\n-            phase,\n-            best_fit,\n-            self.fitted_points,\n-            self.cr_flags,\n-        )\n+\n+        return part_result(row=row, col=col, phase=phase, model_fit=best_fit)\n \n     @property\n     @abstractmethod\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_Analysis/traps/parallel.py": [
                        [
                            "@@ -34,8 +34,13 @@ class ParallelTrap(Trap):\n     @property\n     def models(self) -> list[Model]:\n         if np.ma.is_masked(self.dipole_intensities):\n-            # Some values are masked by CR flagging. Mask corresponding t_ph\n-            # values and override Clock object.\n+            # Some values are masked by CR flagging.\n+            # Check if too many masked\n+            n_masked = np.ma.count_masked(self.dipole_intensities)\n+            if n_masked > round(len(self.clock.t_var) / 2):\n+                return []  # not worth trying to fit a model\n+\n+            # Mask corresponding t_ph values and override Clock object.\n             t_var = self.clock.t_var[~self.dipole_intensities.mask]\n             clock = Clock(self.clock.scheme, self.clock.cycles, t_var)\n             # Remove masked dipole intensities\n@@ -106,7 +111,9 @@ class ParallelTrapLocator(TrapLocator):\n         self, dipoles: ndarray, config: AnalysisConfig\n     ) -> ndarray:\n         cycles = self.data.clock.cycles\n-        dipole_intensities = np.diff(-dipoles).squeeze().T / (cycles * 2)\n+        dipole_intensities = np.diff(-dipoles).squeeze(axis=-1).T / (\n+            cycles * 2\n+        )\n \n         if config.mask_crs:\n             # Sum low side + high side of dipoles\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_Analysis/traps/serial.py": [
                        [
                            "@@ -76,7 +76,7 @@ class SerialTrapLocator(TrapLocator):\n         self, dipoles: ndarray, config: AnalysisConfig\n     ) -> ndarray:\n         cycles = self.data.clock.cycles\n-        return np.diff(-dipoles, axis=1).squeeze().T / (cycles * 2)\n+        return np.diff(-dipoles, axis=1).squeeze(axis=-2).T / (cycles * 2)\n \n     def _get_image_row_col(\n         self, region_row: int, region_col: int\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_IO/file_entry_point.py": [
                        [
                            "@@ -19,6 +19,7 @@ writes the output files.\n # 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n #\n \n+from functools import partial\n from pathlib import Path\n \n from fitsio import FITSHDR, read_header\n@@ -34,7 +35,7 @@ from ..VIS_TrapPumping_Analysis.fitting import fit_models\n from ..VIS_TrapPumping_Analysis.traps.parallel import ParallelTrapLocator\n from ..VIS_TrapPumping_Analysis.traps.serial import SerialTrapLocator\n \n-from .file_validation import input_fits_files_valid\n+from .file_validation import check_input_fits_files_valid\n from .pre_processing import prepare_tp_data\n from .qc_stats import get_frame_stats, get_output_stats\n \n@@ -83,35 +84,25 @@ def file_layer_entry_point(\n     logger.info(\"argument 'config': %s\", config)\n     logger.info(\"argument 'tp_params_csv': %s\", tp_params_csv)\n \n-    if input_fits_files_valid(input_files, serial):\n-        data = prepare_tp_data(input_files, serial)\n-        tp_params = run_tp_analysis(data, serial, config)\n-\n-        tp_params.insert(0, \"ccd\", ext_hdr[\"CCDID\"])\n-        tp_params.insert(1, \"quad\", ext_hdr[\"QUADID\"])\n-        tp_params.to_csv(tp_params_csv, index=False)\n-\n-        obs_id = read_header(input_files[-1])[\"OBS_ID\"]\n-        output_stats = get_output_stats(tp_params, config[\"mask_crs\"])\n-        save_peri(\n-            output_stats,\n-            str(tp_params_csv),\n-            \"VIS_TrapPumping_Output\",\n-            context_name=ext_hdr[\"EXTNAME\"],\n-            obs_id=obs_id,\n-        )\n-\n-        if not serial:\n-            frame_stats = get_frame_stats(data, input_files[1:])  # pumped only\n-            save_peri(\n-                frame_stats,\n-                str(tp_params_csv),\n-                \"VIS_TrapPumping_Frames\",\n-                context_name=ext_hdr[\"EXTNAME\"],\n-                obs_id=obs_id,\n-            )\n+    check_input_fits_files_valid(input_files, serial)\n \n-    else:\n-        logger.error(\"%s: input files failed validity tests\", __name__)\n+    data = prepare_tp_data(input_files, serial)\n+    tp_params = run_tp_analysis(data, serial, config)\n+\n+    tp_params.insert(0, \"ccd\", ext_hdr[\"CCDID\"])\n+    tp_params.insert(1, \"quad\", ext_hdr[\"QUADID\"])\n+    tp_params.to_csv(tp_params_csv, index=False)\n+\n+    write_stats = partial(\n+        save_peri,\n+        context_name=ext_hdr[\"EXTNAME\"],\n+        obs_id=read_header(input_files[-1])[\"OBS_ID\"],\n+    )\n+    output_stats = get_output_stats(tp_params, config[\"mask_crs\"])\n+    write_stats(output_stats, str(tp_params_csv), \"VIS_TrapPumping_Output\")\n+\n+    if not serial:\n+        frame_stats = get_frame_stats(data, input_files[1:])  # pumped only\n+        write_stats(frame_stats, str(tp_params_csv), \"VIS_TrapPumping_Frames\")\n \n     logger.info(\"%s complete.\", __name__)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_IO/file_validation.py": [
                        [
                            "@@ -21,10 +21,10 @@ import ElementsKernel.Logging as elog\n \n import VIS_PyLibrary_M.fits_validation as plfv\n \n-__all__ = [\"input_fits_files_valid\"]\n+__all__ = [\"check_input_fits_files_valid\"]\n \n \n-def input_fits_files_valid(input_files, serial):\n+def check_input_fits_files_valid(input_files, serial):\n     \"\"\"\n     Checks the validity of the input FITS files, for the purpose of flat-field calibration.\n \n@@ -36,20 +36,19 @@ def input_fits_files_valid(input_files, serial):\n     serial : bool\n         True if serial trap pumping, else false.\n \n-    Returns\n-    -------\n-    boolean\n-        True if the files pass all the checks.\n+    Raises\n+    ------\n+    ValueError\n+        If any check is failed.\n     \"\"\"\n     log = elog.getLogger(__name__)\n-    valid = True\n \n     if serial and len(input_files) > 2:\n-        valid = False\n         log.error(\n-            \"STP processing requires a maximum of 2 files: the main one being\"\n+            \"STP processing requires a maximum of 2 files: the main one being \"\n             \"processed and the counterpart with an extra dwell time.\"\n         )\n+        raise ValueError(\"STP processing can handle a maximum of 2 files.\")\n \n     # TODO check more than one of the files\n     handle = fio.FITS(input_files[0])\n@@ -61,9 +60,7 @@ def input_fits_files_valid(input_files, serial):\n         image_header = handle[0].read_header()\n     pre_or_overscan = plfv.contains_pre_or_over_scan(image_header)\n     if not pre_or_overscan:\n-        valid = False\n         log.error(\n             \"input_fits_files_valid(): input file does not contain prescan and/or overscan data\"\n         )\n-\n-    return valid\n+        raise ValueError(\"Frames must contain prescan and overscan.\")\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_IO/plotting.py": [
                        [
                            "@@ -21,9 +21,10 @@ def write_plots(\n     if serial:\n         t_var += 4.75  # phase time\n \n-    fpa_data = results.loc[\n-        notnull(results.tau_e) & (results.tau_e < 1e6) & notnull(results.P_c)\n-    ]\n+    # Defend against tau_e, P_c not in results\n+    tau_e = results.get(\"tau_e\", Series(index=results.index))\n+    P_c = results.get(\"P_c\", Series(index=results.index))\n+    fpa_data = results.loc[notnull(tau_e) & (tau_e < 1e6) & notnull(P_c)]\n     plot_filepaths = []\n     for quad_id in (\"E\", \"F\", \"G\", \"H\"):\n         quad_data = fpa_data.loc[fpa_data.quad == quad_id]\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_TrapPumping/python/VIS_TrapPumping/VIS_TrapPumping_IO/qc_stats.py": [
                        [
                            "@@ -4,19 +4,17 @@ from pathlib import Path\n from types import SimpleNamespace as Namespace\n \n from numpy import count_nonzero\n-from pandas import DataFrame, notnull\n-\n-from ST_DM_FilenameProvider.FilenameProvider import FileNameProvider\n+from pandas import DataFrame, Series, notnull\n \n import VIS_PyLibrary_Common.common_definitions as commdef\n import VIS_PyLibrary_Common.pe_run_information as peri\n-import VIS_PyLibrary_Common.zip_tools as zip_tools\n \n from ..VIS_TrapPumping_Analysis.data import Data\n \n \n def get_output_stats(tp_params: DataFrame, mask_crs: bool) -> dict:\n-    valid = tp_params.loc[notnull(tp_params.tau_e)]\n+    tau_e = tp_params.get(\"tau_e\", Series(index=tp_params.index))\n+    valid = tp_params.loc[notnull(tau_e)]\n     stats = {\"num_traps\": valid.shape[0]}\n     if mask_crs:\n         stats[\"num_cr_affected\"] = count_nonzero(valid.cr_flags)\n",
                            "Merge branch 'develop' into wcsfit-use-pms",
                            "Leigh Smith",
                            "2023-08-24T14:18:04.000+01:00",
                            "78f4b09e79cd313740c13e2144f6112e6d7f3b61"
                        ]
                    ],
                    "VIS_Astrometry/python/VIS_Astrometry/wcsfit.py": [
                        [
                            "@@ -2075,8 +2075,15 @@ def refine_pointing_pa(\n                 # todo\n                 #   3 * mad(total_residual)\n                 #   ought to do the trick but should be tested and refined\n-                # but lets keep the matching radius below 3 arcsec\n-                rad_mas = np.clip(3 * np.median(tot_residuals), 0, 3000)\n+                rad_mas = 3 * np.median(tot_residuals)\n+                if rad_mas > 3000:\n+                    # but lets keep the matching radius below 3 arcsec\n+                    if logger is not None:\n+                        logger.warning(\n+                            f\"matching radius of {_ccd.extname} artificially \"\n+                            f\"reduced from {rad_mas/1000:.3f} to 3 arcsec\"\n+                        )\n+                    rad_mas = 3000.0\n                 # now run a sky match\n                 rad_arcsec = rad_mas * 1E-3\n                 _ccd.match_to_ref_sky(\n",
                            "warn when sky match rad is clipped #23622",
                            "Leigh Smith",
                            "2023-08-24T12:10:24.000+01:00",
                            "0b50593e49f14ce7cabff2aec3ede235e5c3f600"
                        ],
                        [
                            "@@ -2075,7 +2075,8 @@ def refine_pointing_pa(\n                 # todo\n                 #   3 * mad(total_residual)\n                 #   ought to do the trick but should be tested and refined\n-                rad_mas = 3 * np.median(tot_residuals)\n+                # but lets keep the matching radius below 3 arcsec\n+                rad_mas = np.clip(3 * np.median(tot_residuals), 0, 3000)\n                 # now run a sky match\n                 rad_arcsec = rad_mas * 1E-3\n                 _ccd.match_to_ref_sky(\n",
                            "keep sky match rad < 3 arcsec during pointing/PA refinement - #23622",
                            "Leigh Smith",
                            "2023-08-24T11:41:50.000+01:00",
                            "a978e89385c4ab865fb2c9da441243f74acf3cd6"
                        ],
                        [
                            "@@ -1344,7 +1344,7 @@ def use_fpa_model(\n     )\n \n     # ecrf to icrs\n-    #ecrf_to_icrs(observation, logger=logger)\n+    ecrf_to_icrs(observation, logger=logger)\n \n \n def ecrf_to_icrs(observation, num_cps_per_axis=100, logger=None):\n",
                            "turn wcs tranformation back on, woops",
                            "Leigh Smith",
                            "2023-08-24T11:02:47.000+01:00",
                            "05f9b2ac81ed0c858d9ce8cb623b38dbda246dd8"
                        ],
                        [
                            "@@ -768,6 +768,10 @@ def plot_residuals(obsdataset, directory):\n         x, y, dx, dy = [], [], [], []\n \n         for ext, ccd in observation.ccds.items():\n+            if np.count_nonzero(ccd.is_ref) == 0:\n+                # no reference stars, nothing to do\n+                continue\n+\n             # equatorial coordinates of pixel origin\n             ra0, de0 = ccd.wcs.all_pix2world(\n                 ccd.x_pos[ccd.is_ref], ccd.y_pos[ccd.is_ref], 1\n@@ -827,7 +831,7 @@ def plot_residuals(obsdataset, directory):\n \n         # include the colorbar\n         m = plt.cm.ScalarMappable(cmap=cmap)\n-        cbar = plt.colorbar(m)\n+        cbar = plt.colorbar(m, ax=plt.gca())\n \n         # axis labels\n         plt.ylabel(r'$\\chi$ (degrees)')\n@@ -1057,6 +1061,10 @@ def plot_local_mean_residual(obsdataset, fpa_model, directory):\n         # extract data from each CCD\n         xi, xn, dxy = {}, {}, {}\n         for ext, ccd in observation.ccds.items():\n+            if np.count_nonzero(ccd.is_ref) == 0:\n+                # no reference stars, nothing to do\n+                continue\n+\n             # get x and y positions of reference stars\n             xx = ccd.x_pos[ccd.is_ref]\n             yy = ccd.y_pos[ccd.is_ref]\n@@ -1336,7 +1344,7 @@ def use_fpa_model(\n     )\n \n     # ecrf to icrs\n-    ecrf_to_icrs(observation, logger=logger)\n+    #ecrf_to_icrs(observation, logger=logger)\n \n \n def ecrf_to_icrs(observation, num_cps_per_axis=100, logger=None):\n@@ -2058,6 +2066,9 @@ def refine_pointing_pa(\n         else:\n             # subsequent iterations use a sky match\n             for _ccd in observation.ccds.values():\n+                if np.count_nonzero(_ccd.is_ref) == 0:\n+                    # no reference sources, nothing to do\n+                    continue\n                 # find a new sky matching radius which should incorporate most\n                 # genuine matches, this might vary between CCDs hence this loop\n                 tot_residuals = _ccd.calc_ref_residuals(mode='total')\n@@ -2092,12 +2103,16 @@ def refine_pointing_pa(\n         # record the final reference source count\n         final_ref_count = np.count_nonzero(_ccd.is_ref)\n \n+        # deal with Nones\n+        _rms0 = \"   None\" if _rms0 is None else f\"{_rms0:7.0f}\"\n+        final_rms = \" None\" if final_rms is None else f\"{final_rms:5.0f}\"\n+\n         # log some info\n         if logger is not None:\n             logger.debug(\n                 f'{_ccd.extname} | {iteration:3d} iters | '\n                 f'{_count0:4d} -> {final_ref_count:4d} refs | '\n-                f'{_rms0:7.0f} -> {final_rms:5.0f} mas rms'\n+                f'{_rms0:7s} -> {final_rms:5s} mas rms'\n             )\n \n     if logger is not None:\n@@ -2175,13 +2190,10 @@ def include_VIS_mean_residual(ccd):\n     ccd : CCDData instance\n         The CCDData instance for which header cards should be updated\n     \"\"\"\n-    if np.count_nonzero(ccd.is_ref) == 0:\n-        ccd.header['AVGRESID'] = None\n-    else:\n-        # measure the residuals for this ccd of this observation\n-        dra, ddec = ccd.calc_ref_residuals(mode='ordinary').T\n-        # update the header with the VIS mean residual\n-        ccd.header['AVGRESID'] = vis_mean_residual(dra, ddec)\n+    # measure the residuals for this ccd of this observation\n+    dra, ddec = ccd.calc_ref_residuals(mode='ordinary').T\n+    # update the header with the VIS mean residual\n+    ccd.header['AVGRESID'] = vis_mean_residual(dra, ddec)\n \n \n def vis_mean_residual(dra, ddec):\n@@ -2200,7 +2212,12 @@ def vis_mean_residual(dra, ddec):\n     -------\n     The mean residual\n     \"\"\"\n-    return np.sqrt(np.mean(dra) ** 2 + np.mean(ddec) ** 2)\n+    if len(dra) != len(ddec):\n+        raise RuntimeError(\"The input arrays should be the same length\")\n+    elif len(dra) == 0:\n+        return None\n+    else:\n+        return np.sqrt(np.mean(dra) ** 2 + np.mean(ddec) ** 2)\n \n \n def get_stats(observation):\n@@ -2218,15 +2235,18 @@ def get_stats(observation):\n     \"\"\"\n     stats = {}\n     for ext, ccd in observation.ccds.items():\n+        # did this ccd end up with any reference stars?\n+        has_refs = np.count_nonzero(ccd.is_ref) > 0\n+        # get arrays of residuals\n         dra, ddec = ccd.calc_ref_residuals(mode='ordinary').T\n         # generate dictionary of stats for this CCD\n         ccd_stats = {\n             'n_init_ref_sources': int(ccd.initial_ref_count),\n             'n_final_ref_sources': int(np.count_nonzero(ccd.is_ref)),\n-            'ra_res_final_mean': np.mean(dra),\n-            'dec_res_final_mean': np.mean(ddec),\n-            'ra_res_final_std': np.std(dra),\n-            'dec_res_final_std': np.std(ddec),\n+            'ra_res_final_mean': np.mean(dra) if has_refs else None,\n+            'dec_res_final_mean': np.mean(ddec) if has_refs else None,\n+            'ra_res_final_std': np.std(dra) if has_refs else None,\n+            'dec_res_final_std': np.std(ddec) if has_refs else None,\n         }\n         # add to the dictionary for this observation\n         stats.update({ext: ccd_stats})\n@@ -2236,6 +2256,7 @@ def get_stats(observation):\n     glob_fin_refs = np.sum([v['n_final_ref_sources'] for v in stats.values()])\n     # residuals\n     glob_dra, glob_ddec = observation.calc_ref_residuals().T\n+    # we're going to assume there were some reference stars in the entire exposure\n     # global stats dictionary\n     glob_stats = {\n         'n_init_ref_sources': int(glob_init_refs),\n@@ -3813,7 +3834,7 @@ def plot_fpa(chi, eta, zdata, zlabel, filename, cmap='jet'):\n     m = plt.cm.ScalarMappable(cmap=cmap)\n     m.set_array(px_size_flat)\n     m.set_clim(vmin, vmax)\n-    cbar = plt.colorbar(m)\n+    cbar = plt.colorbar(m, ax=plt.gca())\n \n     # axis labels\n     plt.ylabel(r'$\\chi$ (degrees)')\n",
                            "deal with mean of empty slice warnings #23622, and cbar deprecation warnings",
                            "Leigh Smith",
                            "2023-08-24T10:56:09.000+01:00",
                            "74b0f8689ef9a58f14a1c5144fc0acd6cb39e83a"
                        ],
                        [
                            "@@ -2132,16 +2132,32 @@ def print_summary(observation, logger):\n     logger.info(f'File: {observation.filename}')\n \n     # Create a format string for the summary information\n-    fmt = 'Extn: {:2d} - {:s} - NUMBRMS: {:7d} - ' \\\n-          'STDCRMS: {:7.3f} - AVGRESID: {:7.3f}'\n+    fmt = 'Extn: {:3d} - {:s} - NUMBRMS: {:7s} - ' \\\n+          'STDCRMS: {:7s} - AVGRESID: {:7s}'\n \n     # For each extension in the file\n     for i, ccd in enumerate(observation.ccds.values()):\n         # Get the data needed for the summary\n         ccdid = ccd.extname\n-        num_fit_stars = ccd.header.get('NUMBRMS', -1)\n-        fit_dist_rms = ccd.header.get('STDCRMS', -1)\n-        avgresid = ccd.header.get('AVGRESID', -1)\n+        num_fit_stars = ccd.header.get('NUMBRMS', None)\n+        fit_dist_rms = ccd.header.get('STDCRMS', None)\n+        avgresid = ccd.header.get('AVGRESID', None)\n+\n+        # deal with null values\n+        if num_fit_stars is None:\n+            num_fit_stars = \"   None\"\n+        else:\n+            num_fit_stars = f\"{num_fit_stars:7d}\"\n+        # --\n+        if fit_dist_rms is None:\n+            fit_dist_rms = \"   None\"\n+        else:\n+            fit_dist_rms = f\"{fit_dist_rms:7.3f}\"\n+        # --\n+        if avgresid is None:\n+            avgresid = \"   None\"\n+        else:\n+            avgresid = f\"{avgresid:7.3f}\"\n \n         # Print the summary info in the desired format\n         logger.info(fmt.format(i, ccdid, num_fit_stars, fit_dist_rms, avgresid))\n@@ -2159,10 +2175,13 @@ def include_VIS_mean_residual(ccd):\n     ccd : CCDData instance\n         The CCDData instance for which header cards should be updated\n     \"\"\"\n-    # measure the residuals for this ccd of this observation\n-    dra, ddec = ccd.calc_ref_residuals(mode='ordinary').T\n-    # update the header with the VIS mean residual\n-    ccd.header['AVGRESID'] = vis_mean_residual(dra, ddec)\n+    if np.count_nonzero(ccd.is_ref) == 0:\n+        ccd.header['AVGRESID'] = None\n+    else:\n+        # measure the residuals for this ccd of this observation\n+        dra, ddec = ccd.calc_ref_residuals(mode='ordinary').T\n+        # update the header with the VIS mean residual\n+        ccd.header['AVGRESID'] = vis_mean_residual(dra, ddec)\n \n \n def vis_mean_residual(dra, ddec):\n",
                            "deal with mean of empty slice error #23622, warnings still to do",
                            "Leigh Smith",
                            "2023-08-24T09:57:43.000+01:00",
                            "8db842d5ac1760c1f45c8f2a7c7c798ca2e26a02"
                        ],
                        [
                            "@@ -42,7 +42,7 @@ import sys\n import numpy as np\n from astropy import wcs\n from astropy.io import fits\n-from astropy.coordinates import SkyCoord, GCRS, CartesianRepresentation\n+from astropy.coordinates import SkyCoord, GCRS, ICRS, CartesianRepresentation\n import astropy.units as u\n from astropy.time import Time\n from configobj import ConfigObj\n@@ -50,6 +50,7 @@ import itertools\n import matplotlib.pyplot as plt\n import matplotlib.colors\n from scipy.spatial.distance import cdist\n+from scipy.optimize import least_squares\n \n from . import wcsfit_core\n from .sip_tpv import sip_to_pv\n@@ -218,6 +219,18 @@ def get_parser():\n         '--refmagmax', type=float, default=None, metavar='MAX_MAG',\n         help='maximum reference star magnitude (default: no limit)'\n     )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-min', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Lower limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-max', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Upper limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-peak', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Approximate reference star FLUX_RADIUS distribution peak'\n+    )\n \n     # minimiser termination optional arguments\n     term_opt_args_group = parser.add_argument_group(\n@@ -325,6 +338,16 @@ def check_cmdargs(_args):\n         plural = '' if len(mag_lims) > 1 else 's'\n         parser.error(' and '.join(mag_lims) + f' require{plural} --refmagcol')\n \n+    # check that the FLUX_RADIUS related arguments are sensible\n+    if _args.flux_rad_min is not None and _args.flux_rad_max is not None:\n+        if _args.flux_rad_min >= _args.flux_rad_max:\n+            parser.error(\"FLUX_RADIUS minimum must be smaller than FLUX_RADIUS maximum\")\n+        if _args.flux_rad_peak is not None:\n+            if _args.flux_rad_peak <= _args.flux_rad_min:\n+                parser.error(\"FLUX_RADIUS peak must be larger than FLUX_RADIUS minimum\")\n+            if _args.flux_rad_peak >= _args.flux_rad_max:\n+                parser.error(\"FLUX_RADIUS peak must be smaller than FLUX_RADIUS maximum\")\n+\n     # check that the number of minimiser iterations is greater than zero\n     if not _args.niter > 0:\n         parser.error('--niter must be >0')\n@@ -444,6 +467,9 @@ def runwcsfit(args, logger=None):\n         refmagkey=args.refmagcol,\n         refmagmin=args.refmagmin,\n         refmagmax=args.refmagmax,\n+        flux_rad_min=args.flux_rad_min,\n+        flux_rad_max=args.flux_rad_max,\n+        flux_rad_peak=args.flux_rad_peak,\n         logger=logger\n     )\n \n@@ -481,12 +507,15 @@ def runwcsfit(args, logger=None):\n \n         # plot the plate scale if requested\n         if args.make_plots is not None:\n-            id_string = os.path.basename(args.output_fpa_model)\n-            plot_plate_scale(\n-                fpa_model, args.make_plots,\n-                alt_fpa_model=args.init_fpa_model,\n-                file_identifier_string=id_string\n-            )\n+            try:\n+                id_string = os.path.basename(args.output_fpa_model)\n+                plot_plate_scale(\n+                    fpa_model, args.make_plots,\n+                    alt_fpa_model=args.init_fpa_model,\n+                    file_identifier_string=id_string\n+                )\n+            except Exception as e:\n+                logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     elif args.mode == 'use-fpa-model':\n \n@@ -564,10 +593,13 @@ def runwcsfit(args, logger=None):\n \n     # Produce plots if requested\n     if args.make_plots is not None:\n-        plot_sausages(obsdataset, args.make_plots)\n-        plot_vpds(obsdataset, args.make_plots)\n-        plot_residuals(obsdataset, args.make_plots)\n-        plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        try:\n+            plot_sausages(obsdataset, args.make_plots)\n+            plot_vpds(obsdataset, args.make_plots)\n+            plot_residuals(obsdataset, args.make_plots)\n+            plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        except Exception as e:\n+            logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     # store the tables of reference source matches if requested\n     if args.matchcat is not None:\n@@ -1203,6 +1235,21 @@ def copy_wcs_to_file(source_observation, target_file, fpamodel=None):\n               sip_to_pv(src_hdr)\n             replace_header_wcs(src_hdr, dest_hdr)\n \n+            # record the original pointing and position angle\n+            dest_hdr['RA_COMM'] = (\n+                source_observation.commanded_pointing[0],\n+                'commanding pointing RA'\n+            )\n+            dest_hdr['DEC_COMM'] = (\n+                source_observation.commanded_pointing[1],\n+                'commanding pointing DEC'\n+            )\n+            dest_hdr['PA_COMM'] = (\n+                source_observation.commanded_pa,\n+                'commanding position angle'\n+            )\n+\n+            # todo consider the reference frame\n             # add the reconstructed pointing\n             dest_hdr['RA'] = (\n                 source_observation.pointing[0],\n@@ -1288,6 +1335,144 @@ def use_fpa_model(\n         logger=logger\n     )\n \n+    # ecrf to icrs\n+    ecrf_to_icrs(observation, logger=logger)\n+\n+\n+def ecrf_to_icrs(observation, num_cps_per_axis=100, logger=None):\n+    \"\"\"\n+    Take the WCS information from the observation, which maps array\n+    coordinates to the ECRF, and map it from array coords to the ICRS\n+\n+    Parameters\n+    ----------\n+    observation : wcsfit_core.Observation\n+        The observation containing the base WCS\n+    num_cps_per_axis : int, optional\n+        The number of control points to use per axis, default 100.\n+    logger : Logger, optional\n+        A Logger for logging messages (None for no logging, default).\n+    \"\"\"\n+    for ext, ccd in observation.ccds.items():\n+        if logger is not None:\n+            logger.debug(f\"Transforming {ext} WCS from ECRF to ICRS\")\n+\n+        # read the header\n+        orig_hdr = ccd.header.copy()\n+\n+        # generate the control point grid\n+        xy = np.meshgrid(np.linspace(1, orig_hdr[\"NAXIS1\"], num_cps_per_axis),\n+                         np.linspace(1, orig_hdr[\"NAXIS2\"], num_cps_per_axis))\n+        x, y = map(lambda arr: arr.flatten(), xy)\n+\n+        # define the Euclid-centric reference frame\n+        _pos_offset = CartesianRepresentation(\n+            x=observation.pos_offset[0],\n+            y=observation.pos_offset[1],\n+            z=observation.pos_offset[2],\n+            unit=u.km\n+        )\n+        _vel_offset = CartesianRepresentation(\n+            x=observation.vel_offset[0],\n+            y=observation.vel_offset[1],\n+            z=observation.vel_offset[2],\n+            unit=u.km / u.s\n+        )\n+        _obs_time = Time(observation.ref_epoch, format='jyear')\n+        ECRF = GCRS(obstime=_obs_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+\n+        # project array coords to ecrf\n+        ecrf_a, ecrf_d = wcs.WCS(orig_hdr).all_pix2world(x, y, 1)\n+        ecrf_sc = SkyCoord(ecrf_a, ecrf_d, frame=ECRF, unit='deg')\n+\n+        # transform to ICRS\n+        icrs_sc = ecrf_sc.transform_to(ICRS)\n+        # must do this otherwise astropy thinks they aren't the same frame\n+        icrs_sc = SkyCoord(icrs_sc.ra.deg, icrs_sc.dec.deg, frame='icrs', unit='deg')\n+\n+        # transform reference star coords back to ICRS\n+        ref_sc_ecrf = SkyCoord(\n+            ccd.ra[ccd.has_sky_coord], ccd.dec[ccd.has_sky_coord], frame=ECRF, unit='deg'\n+        )\n+        ref_sc_icrs = ref_sc_ecrf.transform_to(ICRS)\n+        ccd.ra[ccd.has_sky_coord] = ref_sc_icrs.ra.deg\n+        ccd.dec[ccd.has_sky_coord] = ref_sc_icrs.dec.deg\n+        ccd.update_skycoord_obj()\n+\n+        # compute the new WCS\n+        orig_crvals = np.array([orig_hdr['CRVAL1'], orig_hdr['CRVAL2']])\n+        orig_cdmtx = wcsfit_core.get_cd(orig_hdr).flatten()\n+        orig_sip = wcsfit_core.get_sip_coeffs(orig_hdr)\n+        # starting parameters\n+        p0 = np.concatenate((orig_crvals, orig_cdmtx, orig_sip))\n+\n+        # temporary header\n+        tmp_hdr = orig_hdr.copy()\n+\n+        def min_func(parameters):\n+            # extract the linear terms\n+            _crvals = parameters[:2]\n+            _cd_mtx = parameters[2:6].reshape(2, 2)\n+            _sip_coeffs = parameters[6:]\n+\n+            # send the linear terms to the header\n+            for i in range(1, 3):\n+                tmp_hdr[f'CRVAL{i}'] = _crvals[i - 1]\n+                for j in range(1, 3):\n+                    tmp_hdr[f'CD{i}_{j}'] = _cd_mtx[i - 1, j - 1]\n+\n+            # send the distortion model to the header\n+            k = 0\n+            for dim in 'AB':\n+                order = tmp_hdr[f'{dim}_ORDER']\n+                for p in range(order + 1):\n+                    for q in range(order + 1):\n+                        if 2 <= p + q <= order:\n+                            tmp_hdr[f'{dim}_{p}_{q}'] = _sip_coeffs[k]\n+                            k += 1\n+\n+            # generate the WCS object\n+            tmp_wcs = wcs.WCS(tmp_hdr)\n+\n+            # evaluate the positions of the control points\n+            _ra, _dec = tmp_wcs.all_pix2world(x, y, 1)\n+            _sc = SkyCoord(_ra, _dec, frame='icrs', unit='deg')\n+\n+            # calculate separations between true and predicted positions\n+            dra, ddec = icrs_sc.spherical_offsets_to(_sc)\n+            # convert to milliarcsec\n+            dra, ddec = dra.arcsec * 1000, ddec.arcsec * 1000\n+\n+            # return the flattened residuals\n+            return np.concatenate((dra, ddec)).flatten()\n+\n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(orig_crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(orig_crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(orig_crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(orig_crvals[1] + 5, -90.0, 90.0)\n+\n+        # run the minimizer\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n+\n+        # set the resultant values and get the header\n+        abs_residuals = np.abs(min_func(solution.x))\n+        if logger is not None:\n+            logger.debug(f\"mean, max residual = \"\n+                         f\"{abs_residuals.mean():.2e}, \"\n+                         f\"{abs_residuals.max():.2e} (mas)\")\n+\n+        # send the header to the CCD object\n+        ccd.header = tmp_hdr.copy()\n+        ccd.wcs = wcs.WCS(ccd.header)\n+\n+\n+\n+\n \n def fit_fpa_model(\n         obsdataset,\n@@ -1596,6 +1781,7 @@ def fpa_model_from_observation(observation, sip_order, logger=None):\n def read_observations_references(\n         source_catalogues, reference_catalogues,\n         refmagkey=None, refmagmin=None, refmagmax=None,\n+        flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n         logger=None\n ):\n     \"\"\"\n@@ -1616,6 +1802,15 @@ def read_observations_references(\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A Logger for logging messages (None for no logging, default).\n \n@@ -1638,9 +1833,23 @@ def read_observations_references(\n             source_catalogues, reference_catalogues,\n             fillvalue=reference_catalogues[0]\n     ):\n+\n+        # grab data from the observation primary header\n+        pri_hdr = fits.getheader(src_cat_path, 0)\n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # read the reference catalogue\n         refdata = get_references(\n             ref_cat_path,\n+            obs_epoch,\n+            pos,\n+            vel,\n             refmagkey=refmagkey,\n             refmagmin=refmagmin,\n             refmagmax=refmagmax,\n@@ -1650,7 +1859,8 @@ def read_observations_references(\n         observations.append(get_observation(\n             src_cat_path, refdata,\n             refmagmin=refmagmin, refmagmax=refmagmax,\n-            logger=logger\n+            flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+            flux_rad_peak=flux_rad_peak, logger=logger\n         ))\n \n     # return an ObservationSet instance\n@@ -2116,6 +2326,7 @@ def is_an_astrometric_kwd(kwd, include_naxis=False):\n \n \n def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n+                    flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                     logger=None):\n     \"\"\"\n     Grab data and headers from the catalogue of the relevant observation.\n@@ -2134,6 +2345,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n+    logger : Logger, optional\n+        A logger which will be used for printing the logging messages (None for\n+        no printing).\n \n     Returns\n     -------\n@@ -2157,6 +2380,14 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n         # grab the primary header\n         pri_hdr = srccat_hdul[0].header\n \n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos_vec = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel_vec = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # record the primary header exptime if present\n         if 'EXPTIME' in pri_hdr.keys():\n             exptimes.append(float(pri_hdr['EXPTIME']))\n@@ -2218,8 +2449,8 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n             _mask = filter_catalogue(\n                 mags, fluxrad,\n                 mag_min=refmagmin, mag_max=refmagmax, mag_margin=0.5,\n-                flux_rad_min=2.0, flux_rad_max=4.0,\n-                logger=logger\n+                flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+                flux_rad_peak=flux_rad_peak, logger=logger\n             )\n \n             # require no large sextractor error flags\n@@ -2278,13 +2509,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     # build an Observation object instance from the list of ccds\n     observation = wcsfit_core.Observation(\n         ccd_list, refdata, pointings[0], pas[0], exptimes[0],\n+        ref_epoch=obs_epoch, pos_offset=pos_vec, vel_offset=vel_vec,\n         filename=src_cat_path, header=pri_hdr\n     )\n \n+    # record the commanded pointing and position angle\n+    observation.commanded_pointing = pointings[0]\n+    observation.commanded_pa = pas[0]\n+\n     return observation\n \n \n-def get_references(refcat,\n+def get_references(refcat, obs_epoch, pos, vel,\n                    refmagkey=None, refmagmin=None, refmagmax=None,\n                    logger=None):\n     \"\"\"\n@@ -2294,6 +2530,12 @@ def get_references(refcat,\n     ----------\n     refcat : str\n         A FITS file with catalogue of position reference stars.\n+    obs_epoch : float\n+        Julian year of observation.\n+    pos : tuple\n+        Geocentric position vector of the instrument.\n+    vel : tuple\n+        Geocentric velocity vector of the instrument.\n     refmagkey : str, optional\n         Name of the column which contains the catalogue magnitudes.\n     refmagmin : float, optional\n@@ -2320,6 +2562,12 @@ def get_references(refcat,\n         ref_ra_error = ref_hdulist[1].data.field('GAIA_RA_ERROR')\n         ref_dec_error = ref_hdulist[1].data.field('GAIA_DEC_ERROR')\n \n+        # Read astrometry from the reference catalogue\n+        ref_epoch = ref_hdulist[1].data.field('REF_EPOCH')\n+        ref_pmra = ref_hdulist[1].data.field('PMRA')\n+        ref_pmdec = ref_hdulist[1].data.field('PMDEC')\n+        ref_parallax = ref_hdulist[1].data.field('PARALLAX')\n+\n         # read the reference star mags\n         if refmagkey is not None:\n             ref_mag = ref_hdulist[1].data.field(refmagkey)\n@@ -2370,6 +2618,15 @@ def get_references(refcat,\n                 f'magnitude selection'\n             )\n \n+    # propagate positions to the correct reference frame and epoch\n+    if logger is not None:\n+        logger.info(f\"rejecting {np.count_nonzero(np.isnan(ref_parallax[mask]))} ref stars with null parallaxes\")\n+    mask[np.isnan(ref_parallax)] = False\n+    ra, dec = icrs_to_ecrf(\n+        ref_ra[mask], ref_dec[mask], ref_parallax[mask], ref_pmra[mask], ref_pmdec[mask],\n+        ref_epoch[mask], obs_epoch, pos, vel\n+    )\n+\n     # filter ref mags if not None\n     if ref_mag is not None:\n         ref_mag = ref_mag[mask]\n@@ -2385,7 +2642,7 @@ def get_references(refcat,\n \n     # create the RefData object\n     refdata = wcsfit_core.RefData(\n-        ref_ra[mask], ref_dec[mask],\n+        ra, dec,\n         ref_ra_error[mask], ref_dec_error[mask],\n         mags=ref_mag, filename=refcat\n     )\n@@ -2428,11 +2685,14 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n     new_dec : array-like\n         Declination in the Euclid-centric reference frame at the target_epoch\n     \"\"\"\n+    # deal with negative parallaxes\n+    _parallax = np.clip(parallax, 1E-10, np.inf)\n+\n     # icrs sky coordinates at reference epoch\n     icrs0 = SkyCoord(\n         ra * u.deg, dec * u.deg,\n         pm_ra_cosdec=pmra * u.mas / u.yr, pm_dec=pmdec * u.mas / u.yr,\n-        distance=1000.0 / parallax * u.pc,\n+        distance=1000.0 / _parallax * u.pc,\n         obstime=Time(ref_epoch, format='jyear'),\n         frame='icrs'\n     )\n@@ -2449,11 +2709,11 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n         x=vel_offset[0], y=vel_offset[1], z=vel_offset[2],\n         unit=u.km / u.s\n     )\n-    ECRF = GCRS(obstime=Time(target_time), obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+    ECRF = GCRS(obstime=target_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n     # transform coordinates to ECRF system\n     ecrf1 = icrs1.transform_to(ECRF)\n \n-    return ecrf1.ra.dec, ecrf1.dec.deg\n+    return ecrf1.ra.deg, ecrf1.dec.deg\n \n \n def run_checks(srccats=None, refcats=None,\n@@ -2803,7 +3063,7 @@ def _check_srccat(filename, logger=None):\n             try:\n                 extname = hdr['EXTNAME']\n \n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with an '\n@@ -2913,7 +3173,7 @@ def _check_fpa_model(filename, logger=None):\n     # Check that the sections have the expected format\n     if result:\n         for section_key in config.keys():\n-            if not re.match('CCDID [1-6]-[1-6]', section_key):\n+            if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', section_key):\n                 if not section_key == 'GLOBAL':\n                     if logger is not None:\n                         logger.error(\n@@ -3100,7 +3360,7 @@ def _check_imgfile(filename, logger=None):\n         for i in range(1, len(hdulist)):\n             try:\n                 extname = hdulist[i].name\n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with a '\n@@ -3311,13 +3571,12 @@ def gauss_func(x, a, x0, sigma):\n     )\n \n \n-def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n+def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max, flux_rad_peak):\n     \"\"\"\n     Perform a Gaussian fit (courtesy of Sylvain Mottet)\n     \"\"\"\n     # initial parameters\n-    flux_rad_med = 0.5 * (flux_rad_min + flux_rad_max)\n-    p0 = np.array([0.5, flux_rad_med, 0.15])\n+    p0 = np.array([0.5, flux_rad_peak, 0.15])\n \n     def func(x, p00, p01, p02):\n         # composite of two Gaussian functions\n@@ -3332,8 +3591,9 @@ def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n     )\n     return popt  # a, x0, sigma\n \n+\n def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n-                     flux_rad_min=0.5, flux_rad_max=2.5,\n+                     flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                      logger=None):\n     \"\"\"\n     Filter a set of sources by magnitude and flux radius\n@@ -3351,9 +3611,14 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     mag_margin : float, optional\n         Allowable margin for error on the magnitudes\n     flux_rad_min : float, optional\n-        Lower limit of acceptable flux_radius range, default 0.5.\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n     flux_rad_max : float, optional\n-        Upper limit of acceptable flux_radius range, default 2.5.\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A logger which will be used for printing the logging messages (None for\n         no printing).\n@@ -3376,8 +3641,10 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     if mag_max is not None:\n         mask[mag > (mag_max + mag_margin)] = False\n \n-    \"\"\"\n-    # Commented out: see https://euclid.roe.ac.uk/issues/23190#note-57\n+    if flux_rad_min is None:\n+        flux_rad_min = 0.0\n+    if flux_rad_max is None:\n+        flux_rad_max = np.inf\n \n     # basic fluxrad filtering\n     mask[(fluxrad < flux_rad_min) | (fluxrad > flux_rad_max)] = False\n@@ -3385,41 +3652,32 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     # check on current source count, can we do something fancy?\n     num_in_range = np.count_nonzero(mask)\n \n-    # some testing suggests 30 sources is enough to do something with\n-    if num_in_range >= 30:\n-        bin_count = 50\n-        # get flux radius histogram\n-        radius_count, bin_edges = np.histogram(\n-            fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n-        )\n-        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n+    if flux_rad_peak is not None:\n+        # some testing suggests 30 sources is enough to do something with\n+        if num_in_range >= 30:\n+            bin_count = 50\n+            # get flux radius histogram\n+            radius_count, bin_edges = np.histogram(\n+                fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n+            )\n+            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n \n-        # try to fit a gaussian to the flux_radius distribution\n-        try:\n-            a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max)\n-            mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n-\n-            # plt.hist(\n-            #     fluxrad[mask], bins=bin_edges, density=True, histtype='step'\n-            # )\n-            # xx = np.linspace(0.5, 2.5, 1000)\n-            # yy = gauss_func(xx, a, x0, sig)\n-            # plt.plot(xx, yy)\n-            # plt.gca().axvline(x0+5*sig)\n-            # plt.gca().axvline(x0-5*sig)\n-            # print(a, x0, sig)  #, a_2, x0_2, sig_2)\n-            # plt.show()\n-        except RuntimeError:\n+            # try to fit a gaussian to the flux_radius distribution\n+            try:\n+                a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max, flux_rad_peak)\n+                mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n+            except RuntimeError:\n+                if logger is not None:\n+                    logger.info(\n+                        f\"flux_radius distribution fitting was unsuccessful\"\n+                    )\n+        else:\n             if logger is not None:\n                 logger.info(\n-                    f\"flux_radius distribution fitting was unsuccessful\"\n+                    f\"too few sources to attempt flux_radius distribution fitting\"\n                 )\n-    else:\n-        if logger is not None:\n-            logger.info(\n-                f\"too few sources to attempt flux_radius distribution fitting\"\n-            )\n-    \"\"\"\n+    elif logger is not None:\n+        logger.debug(\"flux_radius fitting not performed\")\n \n     return mask\n \n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:48:12.000+02:00",
                            "6f4bb5791ddb1a33888248a6fb0cbd43c4b1bd55"
                        ],
                        [
                            "@@ -42,7 +42,7 @@ import sys\n import numpy as np\n from astropy import wcs\n from astropy.io import fits\n-from astropy.coordinates import SkyCoord, GCRS, CartesianRepresentation\n+from astropy.coordinates import SkyCoord, GCRS, ICRS, CartesianRepresentation\n import astropy.units as u\n from astropy.time import Time\n from configobj import ConfigObj\n@@ -50,6 +50,7 @@ import itertools\n import matplotlib.pyplot as plt\n import matplotlib.colors\n from scipy.spatial.distance import cdist\n+from scipy.optimize import least_squares\n \n from . import wcsfit_core\n from .sip_tpv import sip_to_pv\n@@ -218,6 +219,18 @@ def get_parser():\n         '--refmagmax', type=float, default=None, metavar='MAX_MAG',\n         help='maximum reference star magnitude (default: no limit)'\n     )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-min', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Lower limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-max', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Upper limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-peak', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Approximate reference star FLUX_RADIUS distribution peak'\n+    )\n \n     # minimiser termination optional arguments\n     term_opt_args_group = parser.add_argument_group(\n@@ -325,6 +338,16 @@ def check_cmdargs(_args):\n         plural = '' if len(mag_lims) > 1 else 's'\n         parser.error(' and '.join(mag_lims) + f' require{plural} --refmagcol')\n \n+    # check that the FLUX_RADIUS related arguments are sensible\n+    if _args.flux_rad_min is not None and _args.flux_rad_max is not None:\n+        if _args.flux_rad_min >= _args.flux_rad_max:\n+            parser.error(\"FLUX_RADIUS minimum must be smaller than FLUX_RADIUS maximum\")\n+        if _args.flux_rad_peak is not None:\n+            if _args.flux_rad_peak <= _args.flux_rad_min:\n+                parser.error(\"FLUX_RADIUS peak must be larger than FLUX_RADIUS minimum\")\n+            if _args.flux_rad_peak >= _args.flux_rad_max:\n+                parser.error(\"FLUX_RADIUS peak must be smaller than FLUX_RADIUS maximum\")\n+\n     # check that the number of minimiser iterations is greater than zero\n     if not _args.niter > 0:\n         parser.error('--niter must be >0')\n@@ -444,6 +467,9 @@ def runwcsfit(args, logger=None):\n         refmagkey=args.refmagcol,\n         refmagmin=args.refmagmin,\n         refmagmax=args.refmagmax,\n+        flux_rad_min=args.flux_rad_min,\n+        flux_rad_max=args.flux_rad_max,\n+        flux_rad_peak=args.flux_rad_peak,\n         logger=logger\n     )\n \n@@ -481,12 +507,15 @@ def runwcsfit(args, logger=None):\n \n         # plot the plate scale if requested\n         if args.make_plots is not None:\n-            id_string = os.path.basename(args.output_fpa_model)\n-            plot_plate_scale(\n-                fpa_model, args.make_plots,\n-                alt_fpa_model=args.init_fpa_model,\n-                file_identifier_string=id_string\n-            )\n+            try:\n+                id_string = os.path.basename(args.output_fpa_model)\n+                plot_plate_scale(\n+                    fpa_model, args.make_plots,\n+                    alt_fpa_model=args.init_fpa_model,\n+                    file_identifier_string=id_string\n+                )\n+            except Exception as e:\n+                logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     elif args.mode == 'use-fpa-model':\n \n@@ -564,10 +593,13 @@ def runwcsfit(args, logger=None):\n \n     # Produce plots if requested\n     if args.make_plots is not None:\n-        plot_sausages(obsdataset, args.make_plots)\n-        plot_vpds(obsdataset, args.make_plots)\n-        plot_residuals(obsdataset, args.make_plots)\n-        plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        try:\n+            plot_sausages(obsdataset, args.make_plots)\n+            plot_vpds(obsdataset, args.make_plots)\n+            plot_residuals(obsdataset, args.make_plots)\n+            plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        except Exception as e:\n+            logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     # store the tables of reference source matches if requested\n     if args.matchcat is not None:\n@@ -1203,6 +1235,21 @@ def copy_wcs_to_file(source_observation, target_file, fpamodel=None):\n               sip_to_pv(src_hdr)\n             replace_header_wcs(src_hdr, dest_hdr)\n \n+            # record the original pointing and position angle\n+            dest_hdr['RA_COMM'] = (\n+                source_observation.commanded_pointing[0],\n+                'commanding pointing RA'\n+            )\n+            dest_hdr['DEC_COMM'] = (\n+                source_observation.commanded_pointing[1],\n+                'commanding pointing DEC'\n+            )\n+            dest_hdr['PA_COMM'] = (\n+                source_observation.commanded_pa,\n+                'commanding position angle'\n+            )\n+\n+            # todo consider the reference frame\n             # add the reconstructed pointing\n             dest_hdr['RA'] = (\n                 source_observation.pointing[0],\n@@ -1288,6 +1335,144 @@ def use_fpa_model(\n         logger=logger\n     )\n \n+    # ecrf to icrs\n+    ecrf_to_icrs(observation, logger=logger)\n+\n+\n+def ecrf_to_icrs(observation, num_cps_per_axis=100, logger=None):\n+    \"\"\"\n+    Take the WCS information from the observation, which maps array\n+    coordinates to the ECRF, and map it from array coords to the ICRS\n+\n+    Parameters\n+    ----------\n+    observation : wcsfit_core.Observation\n+        The observation containing the base WCS\n+    num_cps_per_axis : int, optional\n+        The number of control points to use per axis, default 100.\n+    logger : Logger, optional\n+        A Logger for logging messages (None for no logging, default).\n+    \"\"\"\n+    for ext, ccd in observation.ccds.items():\n+        if logger is not None:\n+            logger.debug(f\"Transforming {ext} WCS from ECRF to ICRS\")\n+\n+        # read the header\n+        orig_hdr = ccd.header.copy()\n+\n+        # generate the control point grid\n+        xy = np.meshgrid(np.linspace(1, orig_hdr[\"NAXIS1\"], num_cps_per_axis),\n+                         np.linspace(1, orig_hdr[\"NAXIS2\"], num_cps_per_axis))\n+        x, y = map(lambda arr: arr.flatten(), xy)\n+\n+        # define the Euclid-centric reference frame\n+        _pos_offset = CartesianRepresentation(\n+            x=observation.pos_offset[0],\n+            y=observation.pos_offset[1],\n+            z=observation.pos_offset[2],\n+            unit=u.km\n+        )\n+        _vel_offset = CartesianRepresentation(\n+            x=observation.vel_offset[0],\n+            y=observation.vel_offset[1],\n+            z=observation.vel_offset[2],\n+            unit=u.km / u.s\n+        )\n+        _obs_time = Time(observation.ref_epoch, format='jyear')\n+        ECRF = GCRS(obstime=_obs_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+\n+        # project array coords to ecrf\n+        ecrf_a, ecrf_d = wcs.WCS(orig_hdr).all_pix2world(x, y, 1)\n+        ecrf_sc = SkyCoord(ecrf_a, ecrf_d, frame=ECRF, unit='deg')\n+\n+        # transform to ICRS\n+        icrs_sc = ecrf_sc.transform_to(ICRS)\n+        # must do this otherwise astropy thinks they aren't the same frame\n+        icrs_sc = SkyCoord(icrs_sc.ra.deg, icrs_sc.dec.deg, frame='icrs', unit='deg')\n+\n+        # transform reference star coords back to ICRS\n+        ref_sc_ecrf = SkyCoord(\n+            ccd.ra[ccd.has_sky_coord], ccd.dec[ccd.has_sky_coord], frame=ECRF, unit='deg'\n+        )\n+        ref_sc_icrs = ref_sc_ecrf.transform_to(ICRS)\n+        ccd.ra[ccd.has_sky_coord] = ref_sc_icrs.ra.deg\n+        ccd.dec[ccd.has_sky_coord] = ref_sc_icrs.dec.deg\n+        ccd.update_skycoord_obj()\n+\n+        # compute the new WCS\n+        orig_crvals = np.array([orig_hdr['CRVAL1'], orig_hdr['CRVAL2']])\n+        orig_cdmtx = wcsfit_core.get_cd(orig_hdr).flatten()\n+        orig_sip = wcsfit_core.get_sip_coeffs(orig_hdr)\n+        # starting parameters\n+        p0 = np.concatenate((orig_crvals, orig_cdmtx, orig_sip))\n+\n+        # temporary header\n+        tmp_hdr = orig_hdr.copy()\n+\n+        def min_func(parameters):\n+            # extract the linear terms\n+            _crvals = parameters[:2]\n+            _cd_mtx = parameters[2:6].reshape(2, 2)\n+            _sip_coeffs = parameters[6:]\n+\n+            # send the linear terms to the header\n+            for i in range(1, 3):\n+                tmp_hdr[f'CRVAL{i}'] = _crvals[i - 1]\n+                for j in range(1, 3):\n+                    tmp_hdr[f'CD{i}_{j}'] = _cd_mtx[i - 1, j - 1]\n+\n+            # send the distortion model to the header\n+            k = 0\n+            for dim in 'AB':\n+                order = tmp_hdr[f'{dim}_ORDER']\n+                for p in range(order + 1):\n+                    for q in range(order + 1):\n+                        if 2 <= p + q <= order:\n+                            tmp_hdr[f'{dim}_{p}_{q}'] = _sip_coeffs[k]\n+                            k += 1\n+\n+            # generate the WCS object\n+            tmp_wcs = wcs.WCS(tmp_hdr)\n+\n+            # evaluate the positions of the control points\n+            _ra, _dec = tmp_wcs.all_pix2world(x, y, 1)\n+            _sc = SkyCoord(_ra, _dec, frame='icrs', unit='deg')\n+\n+            # calculate separations between true and predicted positions\n+            dra, ddec = icrs_sc.spherical_offsets_to(_sc)\n+            # convert to milliarcsec\n+            dra, ddec = dra.arcsec * 1000, ddec.arcsec * 1000\n+\n+            # return the flattened residuals\n+            return np.concatenate((dra, ddec)).flatten()\n+\n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(orig_crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(orig_crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(orig_crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(orig_crvals[1] + 5, -90.0, 90.0)\n+\n+        # run the minimizer\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n+\n+        # set the resultant values and get the header\n+        abs_residuals = np.abs(min_func(solution.x))\n+        if logger is not None:\n+            logger.debug(f\"mean, max residual = \"\n+                         f\"{abs_residuals.mean():.2e}, \"\n+                         f\"{abs_residuals.max():.2e} (mas)\")\n+\n+        # send the header to the CCD object\n+        ccd.header = tmp_hdr.copy()\n+        ccd.wcs = wcs.WCS(ccd.header)\n+\n+\n+\n+\n \n def fit_fpa_model(\n         obsdataset,\n@@ -1596,6 +1781,7 @@ def fpa_model_from_observation(observation, sip_order, logger=None):\n def read_observations_references(\n         source_catalogues, reference_catalogues,\n         refmagkey=None, refmagmin=None, refmagmax=None,\n+        flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n         logger=None\n ):\n     \"\"\"\n@@ -1616,6 +1802,15 @@ def read_observations_references(\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A Logger for logging messages (None for no logging, default).\n \n@@ -1638,9 +1833,23 @@ def read_observations_references(\n             source_catalogues, reference_catalogues,\n             fillvalue=reference_catalogues[0]\n     ):\n+\n+        # grab data from the observation primary header\n+        pri_hdr = fits.getheader(src_cat_path, 0)\n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # read the reference catalogue\n         refdata = get_references(\n             ref_cat_path,\n+            obs_epoch,\n+            pos,\n+            vel,\n             refmagkey=refmagkey,\n             refmagmin=refmagmin,\n             refmagmax=refmagmax,\n@@ -1650,7 +1859,8 @@ def read_observations_references(\n         observations.append(get_observation(\n             src_cat_path, refdata,\n             refmagmin=refmagmin, refmagmax=refmagmax,\n-            logger=logger\n+            flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+            flux_rad_peak=flux_rad_peak, logger=logger\n         ))\n \n     # return an ObservationSet instance\n@@ -2116,6 +2326,7 @@ def is_an_astrometric_kwd(kwd, include_naxis=False):\n \n \n def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n+                    flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                     logger=None):\n     \"\"\"\n     Grab data and headers from the catalogue of the relevant observation.\n@@ -2134,6 +2345,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n+    logger : Logger, optional\n+        A logger which will be used for printing the logging messages (None for\n+        no printing).\n \n     Returns\n     -------\n@@ -2157,6 +2380,14 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n         # grab the primary header\n         pri_hdr = srccat_hdul[0].header\n \n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos_vec = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel_vec = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # record the primary header exptime if present\n         if 'EXPTIME' in pri_hdr.keys():\n             exptimes.append(float(pri_hdr['EXPTIME']))\n@@ -2218,8 +2449,8 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n             _mask = filter_catalogue(\n                 mags, fluxrad,\n                 mag_min=refmagmin, mag_max=refmagmax, mag_margin=0.5,\n-                flux_rad_min=2.0, flux_rad_max=4.0,\n-                logger=logger\n+                flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+                flux_rad_peak=flux_rad_peak, logger=logger\n             )\n \n             # require no large sextractor error flags\n@@ -2278,13 +2509,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     # build an Observation object instance from the list of ccds\n     observation = wcsfit_core.Observation(\n         ccd_list, refdata, pointings[0], pas[0], exptimes[0],\n+        ref_epoch=obs_epoch, pos_offset=pos_vec, vel_offset=vel_vec,\n         filename=src_cat_path, header=pri_hdr\n     )\n \n+    # record the commanded pointing and position angle\n+    observation.commanded_pointing = pointings[0]\n+    observation.commanded_pa = pas[0]\n+\n     return observation\n \n \n-def get_references(refcat,\n+def get_references(refcat, obs_epoch, pos, vel,\n                    refmagkey=None, refmagmin=None, refmagmax=None,\n                    logger=None):\n     \"\"\"\n@@ -2294,6 +2530,12 @@ def get_references(refcat,\n     ----------\n     refcat : str\n         A FITS file with catalogue of position reference stars.\n+    obs_epoch : float\n+        Julian year of observation.\n+    pos : tuple\n+        Geocentric position vector of the instrument.\n+    vel : tuple\n+        Geocentric velocity vector of the instrument.\n     refmagkey : str, optional\n         Name of the column which contains the catalogue magnitudes.\n     refmagmin : float, optional\n@@ -2320,6 +2562,12 @@ def get_references(refcat,\n         ref_ra_error = ref_hdulist[1].data.field('GAIA_RA_ERROR')\n         ref_dec_error = ref_hdulist[1].data.field('GAIA_DEC_ERROR')\n \n+        # Read astrometry from the reference catalogue\n+        ref_epoch = ref_hdulist[1].data.field('REF_EPOCH')\n+        ref_pmra = ref_hdulist[1].data.field('PMRA')\n+        ref_pmdec = ref_hdulist[1].data.field('PMDEC')\n+        ref_parallax = ref_hdulist[1].data.field('PARALLAX')\n+\n         # read the reference star mags\n         if refmagkey is not None:\n             ref_mag = ref_hdulist[1].data.field(refmagkey)\n@@ -2370,6 +2618,15 @@ def get_references(refcat,\n                 f'magnitude selection'\n             )\n \n+    # propagate positions to the correct reference frame and epoch\n+    if logger is not None:\n+        logger.info(f\"rejecting {np.count_nonzero(np.isnan(ref_parallax[mask]))} ref stars with null parallaxes\")\n+    mask[np.isnan(ref_parallax)] = False\n+    ra, dec = icrs_to_ecrf(\n+        ref_ra[mask], ref_dec[mask], ref_parallax[mask], ref_pmra[mask], ref_pmdec[mask],\n+        ref_epoch[mask], obs_epoch, pos, vel\n+    )\n+\n     # filter ref mags if not None\n     if ref_mag is not None:\n         ref_mag = ref_mag[mask]\n@@ -2385,7 +2642,7 @@ def get_references(refcat,\n \n     # create the RefData object\n     refdata = wcsfit_core.RefData(\n-        ref_ra[mask], ref_dec[mask],\n+        ra, dec,\n         ref_ra_error[mask], ref_dec_error[mask],\n         mags=ref_mag, filename=refcat\n     )\n@@ -2428,11 +2685,14 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n     new_dec : array-like\n         Declination in the Euclid-centric reference frame at the target_epoch\n     \"\"\"\n+    # deal with negative parallaxes\n+    _parallax = np.clip(parallax, 1E-10, np.inf)\n+\n     # icrs sky coordinates at reference epoch\n     icrs0 = SkyCoord(\n         ra * u.deg, dec * u.deg,\n         pm_ra_cosdec=pmra * u.mas / u.yr, pm_dec=pmdec * u.mas / u.yr,\n-        distance=1000.0 / parallax * u.pc,\n+        distance=1000.0 / _parallax * u.pc,\n         obstime=Time(ref_epoch, format='jyear'),\n         frame='icrs'\n     )\n@@ -2449,11 +2709,11 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n         x=vel_offset[0], y=vel_offset[1], z=vel_offset[2],\n         unit=u.km / u.s\n     )\n-    ECRF = GCRS(obstime=Time(target_time), obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+    ECRF = GCRS(obstime=target_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n     # transform coordinates to ECRF system\n     ecrf1 = icrs1.transform_to(ECRF)\n \n-    return ecrf1.ra.dec, ecrf1.dec.deg\n+    return ecrf1.ra.deg, ecrf1.dec.deg\n \n \n def run_checks(srccats=None, refcats=None,\n@@ -2803,7 +3063,7 @@ def _check_srccat(filename, logger=None):\n             try:\n                 extname = hdr['EXTNAME']\n \n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with an '\n@@ -2913,7 +3173,7 @@ def _check_fpa_model(filename, logger=None):\n     # Check that the sections have the expected format\n     if result:\n         for section_key in config.keys():\n-            if not re.match('CCDID [1-6]-[1-6]', section_key):\n+            if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', section_key):\n                 if not section_key == 'GLOBAL':\n                     if logger is not None:\n                         logger.error(\n@@ -3100,7 +3360,7 @@ def _check_imgfile(filename, logger=None):\n         for i in range(1, len(hdulist)):\n             try:\n                 extname = hdulist[i].name\n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with a '\n@@ -3311,13 +3571,12 @@ def gauss_func(x, a, x0, sigma):\n     )\n \n \n-def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n+def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max, flux_rad_peak):\n     \"\"\"\n     Perform a Gaussian fit (courtesy of Sylvain Mottet)\n     \"\"\"\n     # initial parameters\n-    flux_rad_med = 0.5 * (flux_rad_min + flux_rad_max)\n-    p0 = np.array([0.5, flux_rad_med, 0.15])\n+    p0 = np.array([0.5, flux_rad_peak, 0.15])\n \n     def func(x, p00, p01, p02):\n         # composite of two Gaussian functions\n@@ -3332,8 +3591,9 @@ def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n     )\n     return popt  # a, x0, sigma\n \n+\n def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n-                     flux_rad_min=0.5, flux_rad_max=2.5,\n+                     flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                      logger=None):\n     \"\"\"\n     Filter a set of sources by magnitude and flux radius\n@@ -3351,9 +3611,14 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     mag_margin : float, optional\n         Allowable margin for error on the magnitudes\n     flux_rad_min : float, optional\n-        Lower limit of acceptable flux_radius range, default 0.5.\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n     flux_rad_max : float, optional\n-        Upper limit of acceptable flux_radius range, default 2.5.\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A logger which will be used for printing the logging messages (None for\n         no printing).\n@@ -3376,8 +3641,10 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     if mag_max is not None:\n         mask[mag > (mag_max + mag_margin)] = False\n \n-    \"\"\"\n-    # Commented out: see https://euclid.roe.ac.uk/issues/23190#note-57\n+    if flux_rad_min is None:\n+        flux_rad_min = 0.0\n+    if flux_rad_max is None:\n+        flux_rad_max = np.inf\n \n     # basic fluxrad filtering\n     mask[(fluxrad < flux_rad_min) | (fluxrad > flux_rad_max)] = False\n@@ -3385,41 +3652,32 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     # check on current source count, can we do something fancy?\n     num_in_range = np.count_nonzero(mask)\n \n-    # some testing suggests 30 sources is enough to do something with\n-    if num_in_range >= 30:\n-        bin_count = 50\n-        # get flux radius histogram\n-        radius_count, bin_edges = np.histogram(\n-            fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n-        )\n-        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n+    if flux_rad_peak is not None:\n+        # some testing suggests 30 sources is enough to do something with\n+        if num_in_range >= 30:\n+            bin_count = 50\n+            # get flux radius histogram\n+            radius_count, bin_edges = np.histogram(\n+                fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n+            )\n+            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n \n-        # try to fit a gaussian to the flux_radius distribution\n-        try:\n-            a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max)\n-            mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n-\n-            # plt.hist(\n-            #     fluxrad[mask], bins=bin_edges, density=True, histtype='step'\n-            # )\n-            # xx = np.linspace(0.5, 2.5, 1000)\n-            # yy = gauss_func(xx, a, x0, sig)\n-            # plt.plot(xx, yy)\n-            # plt.gca().axvline(x0+5*sig)\n-            # plt.gca().axvline(x0-5*sig)\n-            # print(a, x0, sig)  #, a_2, x0_2, sig_2)\n-            # plt.show()\n-        except RuntimeError:\n+            # try to fit a gaussian to the flux_radius distribution\n+            try:\n+                a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max, flux_rad_peak)\n+                mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n+            except RuntimeError:\n+                if logger is not None:\n+                    logger.info(\n+                        f\"flux_radius distribution fitting was unsuccessful\"\n+                    )\n+        else:\n             if logger is not None:\n                 logger.info(\n-                    f\"flux_radius distribution fitting was unsuccessful\"\n+                    f\"too few sources to attempt flux_radius distribution fitting\"\n                 )\n-    else:\n-        if logger is not None:\n-            logger.info(\n-                f\"too few sources to attempt flux_radius distribution fitting\"\n-            )\n-    \"\"\"\n+    elif logger is not None:\n+        logger.debug(\"flux_radius fitting not performed\")\n \n     return mask\n \n",
                            "Merge branch 'release-13.0' of https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks into release-13.0",
                            "James Nightingale",
                            "2023-08-22T19:47:57.000+02:00",
                            "3887ab35499dbc803602db855483bfdea07cf240"
                        ],
                        [
                            "@@ -42,7 +42,7 @@ import sys\n import numpy as np\n from astropy import wcs\n from astropy.io import fits\n-from astropy.coordinates import SkyCoord, GCRS, CartesianRepresentation\n+from astropy.coordinates import SkyCoord, GCRS, ICRS, CartesianRepresentation\n import astropy.units as u\n from astropy.time import Time\n from configobj import ConfigObj\n@@ -50,6 +50,7 @@ import itertools\n import matplotlib.pyplot as plt\n import matplotlib.colors\n from scipy.spatial.distance import cdist\n+from scipy.optimize import least_squares\n \n from . import wcsfit_core\n from .sip_tpv import sip_to_pv\n@@ -218,6 +219,18 @@ def get_parser():\n         '--refmagmax', type=float, default=None, metavar='MAX_MAG',\n         help='maximum reference star magnitude (default: no limit)'\n     )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-min', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Lower limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-max', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Upper limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-peak', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Approximate reference star FLUX_RADIUS distribution peak'\n+    )\n \n     # minimiser termination optional arguments\n     term_opt_args_group = parser.add_argument_group(\n@@ -325,6 +338,16 @@ def check_cmdargs(_args):\n         plural = '' if len(mag_lims) > 1 else 's'\n         parser.error(' and '.join(mag_lims) + f' require{plural} --refmagcol')\n \n+    # check that the FLUX_RADIUS related arguments are sensible\n+    if _args.flux_rad_min is not None and _args.flux_rad_max is not None:\n+        if _args.flux_rad_min >= _args.flux_rad_max:\n+            parser.error(\"FLUX_RADIUS minimum must be smaller than FLUX_RADIUS maximum\")\n+        if _args.flux_rad_peak is not None:\n+            if _args.flux_rad_peak <= _args.flux_rad_min:\n+                parser.error(\"FLUX_RADIUS peak must be larger than FLUX_RADIUS minimum\")\n+            if _args.flux_rad_peak >= _args.flux_rad_max:\n+                parser.error(\"FLUX_RADIUS peak must be smaller than FLUX_RADIUS maximum\")\n+\n     # check that the number of minimiser iterations is greater than zero\n     if not _args.niter > 0:\n         parser.error('--niter must be >0')\n@@ -444,6 +467,9 @@ def runwcsfit(args, logger=None):\n         refmagkey=args.refmagcol,\n         refmagmin=args.refmagmin,\n         refmagmax=args.refmagmax,\n+        flux_rad_min=args.flux_rad_min,\n+        flux_rad_max=args.flux_rad_max,\n+        flux_rad_peak=args.flux_rad_peak,\n         logger=logger\n     )\n \n@@ -481,12 +507,15 @@ def runwcsfit(args, logger=None):\n \n         # plot the plate scale if requested\n         if args.make_plots is not None:\n-            id_string = os.path.basename(args.output_fpa_model)\n-            plot_plate_scale(\n-                fpa_model, args.make_plots,\n-                alt_fpa_model=args.init_fpa_model,\n-                file_identifier_string=id_string\n-            )\n+            try:\n+                id_string = os.path.basename(args.output_fpa_model)\n+                plot_plate_scale(\n+                    fpa_model, args.make_plots,\n+                    alt_fpa_model=args.init_fpa_model,\n+                    file_identifier_string=id_string\n+                )\n+            except Exception as e:\n+                logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     elif args.mode == 'use-fpa-model':\n \n@@ -564,10 +593,13 @@ def runwcsfit(args, logger=None):\n \n     # Produce plots if requested\n     if args.make_plots is not None:\n-        plot_sausages(obsdataset, args.make_plots)\n-        plot_vpds(obsdataset, args.make_plots)\n-        plot_residuals(obsdataset, args.make_plots)\n-        plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        try:\n+            plot_sausages(obsdataset, args.make_plots)\n+            plot_vpds(obsdataset, args.make_plots)\n+            plot_residuals(obsdataset, args.make_plots)\n+            plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        except Exception as e:\n+            logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     # store the tables of reference source matches if requested\n     if args.matchcat is not None:\n@@ -1203,6 +1235,21 @@ def copy_wcs_to_file(source_observation, target_file, fpamodel=None):\n               sip_to_pv(src_hdr)\n             replace_header_wcs(src_hdr, dest_hdr)\n \n+            # record the original pointing and position angle\n+            dest_hdr['RA_COMM'] = (\n+                source_observation.commanded_pointing[0],\n+                'commanding pointing RA'\n+            )\n+            dest_hdr['DEC_COMM'] = (\n+                source_observation.commanded_pointing[1],\n+                'commanding pointing DEC'\n+            )\n+            dest_hdr['PA_COMM'] = (\n+                source_observation.commanded_pa,\n+                'commanding position angle'\n+            )\n+\n+            # todo consider the reference frame\n             # add the reconstructed pointing\n             dest_hdr['RA'] = (\n                 source_observation.pointing[0],\n@@ -1288,6 +1335,144 @@ def use_fpa_model(\n         logger=logger\n     )\n \n+    # ecrf to icrs\n+    ecrf_to_icrs(observation, logger=logger)\n+\n+\n+def ecrf_to_icrs(observation, num_cps_per_axis=100, logger=None):\n+    \"\"\"\n+    Take the WCS information from the observation, which maps array\n+    coordinates to the ECRF, and map it from array coords to the ICRS\n+\n+    Parameters\n+    ----------\n+    observation : wcsfit_core.Observation\n+        The observation containing the base WCS\n+    num_cps_per_axis : int, optional\n+        The number of control points to use per axis, default 100.\n+    logger : Logger, optional\n+        A Logger for logging messages (None for no logging, default).\n+    \"\"\"\n+    for ext, ccd in observation.ccds.items():\n+        if logger is not None:\n+            logger.debug(f\"Transforming {ext} WCS from ECRF to ICRS\")\n+\n+        # read the header\n+        orig_hdr = ccd.header.copy()\n+\n+        # generate the control point grid\n+        xy = np.meshgrid(np.linspace(1, orig_hdr[\"NAXIS1\"], num_cps_per_axis),\n+                         np.linspace(1, orig_hdr[\"NAXIS2\"], num_cps_per_axis))\n+        x, y = map(lambda arr: arr.flatten(), xy)\n+\n+        # define the Euclid-centric reference frame\n+        _pos_offset = CartesianRepresentation(\n+            x=observation.pos_offset[0],\n+            y=observation.pos_offset[1],\n+            z=observation.pos_offset[2],\n+            unit=u.km\n+        )\n+        _vel_offset = CartesianRepresentation(\n+            x=observation.vel_offset[0],\n+            y=observation.vel_offset[1],\n+            z=observation.vel_offset[2],\n+            unit=u.km / u.s\n+        )\n+        _obs_time = Time(observation.ref_epoch, format='jyear')\n+        ECRF = GCRS(obstime=_obs_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+\n+        # project array coords to ecrf\n+        ecrf_a, ecrf_d = wcs.WCS(orig_hdr).all_pix2world(x, y, 1)\n+        ecrf_sc = SkyCoord(ecrf_a, ecrf_d, frame=ECRF, unit='deg')\n+\n+        # transform to ICRS\n+        icrs_sc = ecrf_sc.transform_to(ICRS)\n+        # must do this otherwise astropy thinks they aren't the same frame\n+        icrs_sc = SkyCoord(icrs_sc.ra.deg, icrs_sc.dec.deg, frame='icrs', unit='deg')\n+\n+        # transform reference star coords back to ICRS\n+        ref_sc_ecrf = SkyCoord(\n+            ccd.ra[ccd.has_sky_coord], ccd.dec[ccd.has_sky_coord], frame=ECRF, unit='deg'\n+        )\n+        ref_sc_icrs = ref_sc_ecrf.transform_to(ICRS)\n+        ccd.ra[ccd.has_sky_coord] = ref_sc_icrs.ra.deg\n+        ccd.dec[ccd.has_sky_coord] = ref_sc_icrs.dec.deg\n+        ccd.update_skycoord_obj()\n+\n+        # compute the new WCS\n+        orig_crvals = np.array([orig_hdr['CRVAL1'], orig_hdr['CRVAL2']])\n+        orig_cdmtx = wcsfit_core.get_cd(orig_hdr).flatten()\n+        orig_sip = wcsfit_core.get_sip_coeffs(orig_hdr)\n+        # starting parameters\n+        p0 = np.concatenate((orig_crvals, orig_cdmtx, orig_sip))\n+\n+        # temporary header\n+        tmp_hdr = orig_hdr.copy()\n+\n+        def min_func(parameters):\n+            # extract the linear terms\n+            _crvals = parameters[:2]\n+            _cd_mtx = parameters[2:6].reshape(2, 2)\n+            _sip_coeffs = parameters[6:]\n+\n+            # send the linear terms to the header\n+            for i in range(1, 3):\n+                tmp_hdr[f'CRVAL{i}'] = _crvals[i - 1]\n+                for j in range(1, 3):\n+                    tmp_hdr[f'CD{i}_{j}'] = _cd_mtx[i - 1, j - 1]\n+\n+            # send the distortion model to the header\n+            k = 0\n+            for dim in 'AB':\n+                order = tmp_hdr[f'{dim}_ORDER']\n+                for p in range(order + 1):\n+                    for q in range(order + 1):\n+                        if 2 <= p + q <= order:\n+                            tmp_hdr[f'{dim}_{p}_{q}'] = _sip_coeffs[k]\n+                            k += 1\n+\n+            # generate the WCS object\n+            tmp_wcs = wcs.WCS(tmp_hdr)\n+\n+            # evaluate the positions of the control points\n+            _ra, _dec = tmp_wcs.all_pix2world(x, y, 1)\n+            _sc = SkyCoord(_ra, _dec, frame='icrs', unit='deg')\n+\n+            # calculate separations between true and predicted positions\n+            dra, ddec = icrs_sc.spherical_offsets_to(_sc)\n+            # convert to milliarcsec\n+            dra, ddec = dra.arcsec * 1000, ddec.arcsec * 1000\n+\n+            # return the flattened residuals\n+            return np.concatenate((dra, ddec)).flatten()\n+\n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(orig_crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(orig_crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(orig_crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(orig_crvals[1] + 5, -90.0, 90.0)\n+\n+        # run the minimizer\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n+\n+        # set the resultant values and get the header\n+        abs_residuals = np.abs(min_func(solution.x))\n+        if logger is not None:\n+            logger.debug(f\"mean, max residual = \"\n+                         f\"{abs_residuals.mean():.2e}, \"\n+                         f\"{abs_residuals.max():.2e} (mas)\")\n+\n+        # send the header to the CCD object\n+        ccd.header = tmp_hdr.copy()\n+        ccd.wcs = wcs.WCS(ccd.header)\n+\n+\n+\n+\n \n def fit_fpa_model(\n         obsdataset,\n@@ -1596,6 +1781,7 @@ def fpa_model_from_observation(observation, sip_order, logger=None):\n def read_observations_references(\n         source_catalogues, reference_catalogues,\n         refmagkey=None, refmagmin=None, refmagmax=None,\n+        flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n         logger=None\n ):\n     \"\"\"\n@@ -1616,6 +1802,15 @@ def read_observations_references(\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A Logger for logging messages (None for no logging, default).\n \n@@ -1638,9 +1833,23 @@ def read_observations_references(\n             source_catalogues, reference_catalogues,\n             fillvalue=reference_catalogues[0]\n     ):\n+\n+        # grab data from the observation primary header\n+        pri_hdr = fits.getheader(src_cat_path, 0)\n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # read the reference catalogue\n         refdata = get_references(\n             ref_cat_path,\n+            obs_epoch,\n+            pos,\n+            vel,\n             refmagkey=refmagkey,\n             refmagmin=refmagmin,\n             refmagmax=refmagmax,\n@@ -1650,7 +1859,8 @@ def read_observations_references(\n         observations.append(get_observation(\n             src_cat_path, refdata,\n             refmagmin=refmagmin, refmagmax=refmagmax,\n-            logger=logger\n+            flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+            flux_rad_peak=flux_rad_peak, logger=logger\n         ))\n \n     # return an ObservationSet instance\n@@ -2116,6 +2326,7 @@ def is_an_astrometric_kwd(kwd, include_naxis=False):\n \n \n def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n+                    flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                     logger=None):\n     \"\"\"\n     Grab data and headers from the catalogue of the relevant observation.\n@@ -2134,6 +2345,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n+    logger : Logger, optional\n+        A logger which will be used for printing the logging messages (None for\n+        no printing).\n \n     Returns\n     -------\n@@ -2157,6 +2380,14 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n         # grab the primary header\n         pri_hdr = srccat_hdul[0].header\n \n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos_vec = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel_vec = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # record the primary header exptime if present\n         if 'EXPTIME' in pri_hdr.keys():\n             exptimes.append(float(pri_hdr['EXPTIME']))\n@@ -2218,8 +2449,8 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n             _mask = filter_catalogue(\n                 mags, fluxrad,\n                 mag_min=refmagmin, mag_max=refmagmax, mag_margin=0.5,\n-                flux_rad_min=2.0, flux_rad_max=4.0,\n-                logger=logger\n+                flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+                flux_rad_peak=flux_rad_peak, logger=logger\n             )\n \n             # require no large sextractor error flags\n@@ -2278,13 +2509,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     # build an Observation object instance from the list of ccds\n     observation = wcsfit_core.Observation(\n         ccd_list, refdata, pointings[0], pas[0], exptimes[0],\n+        ref_epoch=obs_epoch, pos_offset=pos_vec, vel_offset=vel_vec,\n         filename=src_cat_path, header=pri_hdr\n     )\n \n+    # record the commanded pointing and position angle\n+    observation.commanded_pointing = pointings[0]\n+    observation.commanded_pa = pas[0]\n+\n     return observation\n \n \n-def get_references(refcat,\n+def get_references(refcat, obs_epoch, pos, vel,\n                    refmagkey=None, refmagmin=None, refmagmax=None,\n                    logger=None):\n     \"\"\"\n@@ -2294,6 +2530,12 @@ def get_references(refcat,\n     ----------\n     refcat : str\n         A FITS file with catalogue of position reference stars.\n+    obs_epoch : float\n+        Julian year of observation.\n+    pos : tuple\n+        Geocentric position vector of the instrument.\n+    vel : tuple\n+        Geocentric velocity vector of the instrument.\n     refmagkey : str, optional\n         Name of the column which contains the catalogue magnitudes.\n     refmagmin : float, optional\n@@ -2320,6 +2562,12 @@ def get_references(refcat,\n         ref_ra_error = ref_hdulist[1].data.field('GAIA_RA_ERROR')\n         ref_dec_error = ref_hdulist[1].data.field('GAIA_DEC_ERROR')\n \n+        # Read astrometry from the reference catalogue\n+        ref_epoch = ref_hdulist[1].data.field('REF_EPOCH')\n+        ref_pmra = ref_hdulist[1].data.field('PMRA')\n+        ref_pmdec = ref_hdulist[1].data.field('PMDEC')\n+        ref_parallax = ref_hdulist[1].data.field('PARALLAX')\n+\n         # read the reference star mags\n         if refmagkey is not None:\n             ref_mag = ref_hdulist[1].data.field(refmagkey)\n@@ -2370,6 +2618,15 @@ def get_references(refcat,\n                 f'magnitude selection'\n             )\n \n+    # propagate positions to the correct reference frame and epoch\n+    if logger is not None:\n+        logger.info(f\"rejecting {np.count_nonzero(np.isnan(ref_parallax[mask]))} ref stars with null parallaxes\")\n+    mask[np.isnan(ref_parallax)] = False\n+    ra, dec = icrs_to_ecrf(\n+        ref_ra[mask], ref_dec[mask], ref_parallax[mask], ref_pmra[mask], ref_pmdec[mask],\n+        ref_epoch[mask], obs_epoch, pos, vel\n+    )\n+\n     # filter ref mags if not None\n     if ref_mag is not None:\n         ref_mag = ref_mag[mask]\n@@ -2385,7 +2642,7 @@ def get_references(refcat,\n \n     # create the RefData object\n     refdata = wcsfit_core.RefData(\n-        ref_ra[mask], ref_dec[mask],\n+        ra, dec,\n         ref_ra_error[mask], ref_dec_error[mask],\n         mags=ref_mag, filename=refcat\n     )\n@@ -2428,11 +2685,14 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n     new_dec : array-like\n         Declination in the Euclid-centric reference frame at the target_epoch\n     \"\"\"\n+    # deal with negative parallaxes\n+    _parallax = np.clip(parallax, 1E-10, np.inf)\n+\n     # icrs sky coordinates at reference epoch\n     icrs0 = SkyCoord(\n         ra * u.deg, dec * u.deg,\n         pm_ra_cosdec=pmra * u.mas / u.yr, pm_dec=pmdec * u.mas / u.yr,\n-        distance=1000.0 / parallax * u.pc,\n+        distance=1000.0 / _parallax * u.pc,\n         obstime=Time(ref_epoch, format='jyear'),\n         frame='icrs'\n     )\n@@ -2449,11 +2709,11 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n         x=vel_offset[0], y=vel_offset[1], z=vel_offset[2],\n         unit=u.km / u.s\n     )\n-    ECRF = GCRS(obstime=Time(target_time), obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+    ECRF = GCRS(obstime=target_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n     # transform coordinates to ECRF system\n     ecrf1 = icrs1.transform_to(ECRF)\n \n-    return ecrf1.ra.dec, ecrf1.dec.deg\n+    return ecrf1.ra.deg, ecrf1.dec.deg\n \n \n def run_checks(srccats=None, refcats=None,\n@@ -2803,7 +3063,7 @@ def _check_srccat(filename, logger=None):\n             try:\n                 extname = hdr['EXTNAME']\n \n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with an '\n@@ -2913,7 +3173,7 @@ def _check_fpa_model(filename, logger=None):\n     # Check that the sections have the expected format\n     if result:\n         for section_key in config.keys():\n-            if not re.match('CCDID [1-6]-[1-6]', section_key):\n+            if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', section_key):\n                 if not section_key == 'GLOBAL':\n                     if logger is not None:\n                         logger.error(\n@@ -3100,7 +3360,7 @@ def _check_imgfile(filename, logger=None):\n         for i in range(1, len(hdulist)):\n             try:\n                 extname = hdulist[i].name\n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with a '\n@@ -3311,13 +3571,12 @@ def gauss_func(x, a, x0, sigma):\n     )\n \n \n-def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n+def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max, flux_rad_peak):\n     \"\"\"\n     Perform a Gaussian fit (courtesy of Sylvain Mottet)\n     \"\"\"\n     # initial parameters\n-    flux_rad_med = 0.5 * (flux_rad_min + flux_rad_max)\n-    p0 = np.array([0.5, flux_rad_med, 0.15])\n+    p0 = np.array([0.5, flux_rad_peak, 0.15])\n \n     def func(x, p00, p01, p02):\n         # composite of two Gaussian functions\n@@ -3332,8 +3591,9 @@ def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n     )\n     return popt  # a, x0, sigma\n \n+\n def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n-                     flux_rad_min=0.5, flux_rad_max=2.5,\n+                     flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                      logger=None):\n     \"\"\"\n     Filter a set of sources by magnitude and flux radius\n@@ -3351,9 +3611,14 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     mag_margin : float, optional\n         Allowable margin for error on the magnitudes\n     flux_rad_min : float, optional\n-        Lower limit of acceptable flux_radius range, default 0.5.\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n     flux_rad_max : float, optional\n-        Upper limit of acceptable flux_radius range, default 2.5.\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A logger which will be used for printing the logging messages (None for\n         no printing).\n@@ -3376,8 +3641,10 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     if mag_max is not None:\n         mask[mag > (mag_max + mag_margin)] = False\n \n-    \"\"\"\n-    # Commented out: see https://euclid.roe.ac.uk/issues/23190#note-57\n+    if flux_rad_min is None:\n+        flux_rad_min = 0.0\n+    if flux_rad_max is None:\n+        flux_rad_max = np.inf\n \n     # basic fluxrad filtering\n     mask[(fluxrad < flux_rad_min) | (fluxrad > flux_rad_max)] = False\n@@ -3385,41 +3652,32 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     # check on current source count, can we do something fancy?\n     num_in_range = np.count_nonzero(mask)\n \n-    # some testing suggests 30 sources is enough to do something with\n-    if num_in_range >= 30:\n-        bin_count = 50\n-        # get flux radius histogram\n-        radius_count, bin_edges = np.histogram(\n-            fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n-        )\n-        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n+    if flux_rad_peak is not None:\n+        # some testing suggests 30 sources is enough to do something with\n+        if num_in_range >= 30:\n+            bin_count = 50\n+            # get flux radius histogram\n+            radius_count, bin_edges = np.histogram(\n+                fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n+            )\n+            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n \n-        # try to fit a gaussian to the flux_radius distribution\n-        try:\n-            a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max)\n-            mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n-\n-            # plt.hist(\n-            #     fluxrad[mask], bins=bin_edges, density=True, histtype='step'\n-            # )\n-            # xx = np.linspace(0.5, 2.5, 1000)\n-            # yy = gauss_func(xx, a, x0, sig)\n-            # plt.plot(xx, yy)\n-            # plt.gca().axvline(x0+5*sig)\n-            # plt.gca().axvline(x0-5*sig)\n-            # print(a, x0, sig)  #, a_2, x0_2, sig_2)\n-            # plt.show()\n-        except RuntimeError:\n+            # try to fit a gaussian to the flux_radius distribution\n+            try:\n+                a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max, flux_rad_peak)\n+                mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n+            except RuntimeError:\n+                if logger is not None:\n+                    logger.info(\n+                        f\"flux_radius distribution fitting was unsuccessful\"\n+                    )\n+        else:\n             if logger is not None:\n                 logger.info(\n-                    f\"flux_radius distribution fitting was unsuccessful\"\n+                    f\"too few sources to attempt flux_radius distribution fitting\"\n                 )\n-    else:\n-        if logger is not None:\n-            logger.info(\n-                f\"too few sources to attempt flux_radius distribution fitting\"\n-            )\n-    \"\"\"\n+    elif logger is not None:\n+        logger.debug(\"flux_radius fitting not performed\")\n \n     return mask\n \n",
                            "Merge branch 'release-13.0' into 'feature_#23227_BloomingCalib'",
                            "Thomas Flanet",
                            "2023-08-22T15:48:48.000+00:00",
                            "2d156ac3b1fd63e45e002c217d42c01a4c694040"
                        ],
                        [
                            "@@ -42,7 +42,7 @@ import sys\n import numpy as np\n from astropy import wcs\n from astropy.io import fits\n-from astropy.coordinates import SkyCoord, GCRS, CartesianRepresentation\n+from astropy.coordinates import SkyCoord, GCRS, ICRS, CartesianRepresentation\n import astropy.units as u\n from astropy.time import Time\n from configobj import ConfigObj\n@@ -50,6 +50,7 @@ import itertools\n import matplotlib.pyplot as plt\n import matplotlib.colors\n from scipy.spatial.distance import cdist\n+from scipy.optimize import least_squares\n \n from . import wcsfit_core\n from .sip_tpv import sip_to_pv\n@@ -218,6 +219,18 @@ def get_parser():\n         '--refmagmax', type=float, default=None, metavar='MAX_MAG',\n         help='maximum reference star magnitude (default: no limit)'\n     )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-min', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Lower limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-max', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Upper limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-peak', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Approximate reference star FLUX_RADIUS distribution peak'\n+    )\n \n     # minimiser termination optional arguments\n     term_opt_args_group = parser.add_argument_group(\n@@ -325,6 +338,16 @@ def check_cmdargs(_args):\n         plural = '' if len(mag_lims) > 1 else 's'\n         parser.error(' and '.join(mag_lims) + f' require{plural} --refmagcol')\n \n+    # check that the FLUX_RADIUS related arguments are sensible\n+    if _args.flux_rad_min is not None and _args.flux_rad_max is not None:\n+        if _args.flux_rad_min >= _args.flux_rad_max:\n+            parser.error(\"FLUX_RADIUS minimum must be smaller than FLUX_RADIUS maximum\")\n+        if _args.flux_rad_peak is not None:\n+            if _args.flux_rad_peak <= _args.flux_rad_min:\n+                parser.error(\"FLUX_RADIUS peak must be larger than FLUX_RADIUS minimum\")\n+            if _args.flux_rad_peak >= _args.flux_rad_max:\n+                parser.error(\"FLUX_RADIUS peak must be smaller than FLUX_RADIUS maximum\")\n+\n     # check that the number of minimiser iterations is greater than zero\n     if not _args.niter > 0:\n         parser.error('--niter must be >0')\n@@ -444,6 +467,9 @@ def runwcsfit(args, logger=None):\n         refmagkey=args.refmagcol,\n         refmagmin=args.refmagmin,\n         refmagmax=args.refmagmax,\n+        flux_rad_min=args.flux_rad_min,\n+        flux_rad_max=args.flux_rad_max,\n+        flux_rad_peak=args.flux_rad_peak,\n         logger=logger\n     )\n \n@@ -481,12 +507,15 @@ def runwcsfit(args, logger=None):\n \n         # plot the plate scale if requested\n         if args.make_plots is not None:\n-            id_string = os.path.basename(args.output_fpa_model)\n-            plot_plate_scale(\n-                fpa_model, args.make_plots,\n-                alt_fpa_model=args.init_fpa_model,\n-                file_identifier_string=id_string\n-            )\n+            try:\n+                id_string = os.path.basename(args.output_fpa_model)\n+                plot_plate_scale(\n+                    fpa_model, args.make_plots,\n+                    alt_fpa_model=args.init_fpa_model,\n+                    file_identifier_string=id_string\n+                )\n+            except Exception as e:\n+                logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     elif args.mode == 'use-fpa-model':\n \n@@ -564,10 +593,13 @@ def runwcsfit(args, logger=None):\n \n     # Produce plots if requested\n     if args.make_plots is not None:\n-        plot_sausages(obsdataset, args.make_plots)\n-        plot_vpds(obsdataset, args.make_plots)\n-        plot_residuals(obsdataset, args.make_plots)\n-        plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        try:\n+            plot_sausages(obsdataset, args.make_plots)\n+            plot_vpds(obsdataset, args.make_plots)\n+            plot_residuals(obsdataset, args.make_plots)\n+            plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        except Exception as e:\n+            logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     # store the tables of reference source matches if requested\n     if args.matchcat is not None:\n@@ -1203,6 +1235,21 @@ def copy_wcs_to_file(source_observation, target_file, fpamodel=None):\n               sip_to_pv(src_hdr)\n             replace_header_wcs(src_hdr, dest_hdr)\n \n+            # record the original pointing and position angle\n+            dest_hdr['RA_COMM'] = (\n+                source_observation.commanded_pointing[0],\n+                'commanding pointing RA'\n+            )\n+            dest_hdr['DEC_COMM'] = (\n+                source_observation.commanded_pointing[1],\n+                'commanding pointing DEC'\n+            )\n+            dest_hdr['PA_COMM'] = (\n+                source_observation.commanded_pa,\n+                'commanding position angle'\n+            )\n+\n+            # todo consider the reference frame\n             # add the reconstructed pointing\n             dest_hdr['RA'] = (\n                 source_observation.pointing[0],\n@@ -1288,6 +1335,144 @@ def use_fpa_model(\n         logger=logger\n     )\n \n+    # ecrf to icrs\n+    ecrf_to_icrs(observation, logger=logger)\n+\n+\n+def ecrf_to_icrs(observation, num_cps_per_axis=100, logger=None):\n+    \"\"\"\n+    Take the WCS information from the observation, which maps array\n+    coordinates to the ECRF, and map it from array coords to the ICRS\n+\n+    Parameters\n+    ----------\n+    observation : wcsfit_core.Observation\n+        The observation containing the base WCS\n+    num_cps_per_axis : int, optional\n+        The number of control points to use per axis, default 100.\n+    logger : Logger, optional\n+        A Logger for logging messages (None for no logging, default).\n+    \"\"\"\n+    for ext, ccd in observation.ccds.items():\n+        if logger is not None:\n+            logger.debug(f\"Transforming {ext} WCS from ECRF to ICRS\")\n+\n+        # read the header\n+        orig_hdr = ccd.header.copy()\n+\n+        # generate the control point grid\n+        xy = np.meshgrid(np.linspace(1, orig_hdr[\"NAXIS1\"], num_cps_per_axis),\n+                         np.linspace(1, orig_hdr[\"NAXIS2\"], num_cps_per_axis))\n+        x, y = map(lambda arr: arr.flatten(), xy)\n+\n+        # define the Euclid-centric reference frame\n+        _pos_offset = CartesianRepresentation(\n+            x=observation.pos_offset[0],\n+            y=observation.pos_offset[1],\n+            z=observation.pos_offset[2],\n+            unit=u.km\n+        )\n+        _vel_offset = CartesianRepresentation(\n+            x=observation.vel_offset[0],\n+            y=observation.vel_offset[1],\n+            z=observation.vel_offset[2],\n+            unit=u.km / u.s\n+        )\n+        _obs_time = Time(observation.ref_epoch, format='jyear')\n+        ECRF = GCRS(obstime=_obs_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+\n+        # project array coords to ecrf\n+        ecrf_a, ecrf_d = wcs.WCS(orig_hdr).all_pix2world(x, y, 1)\n+        ecrf_sc = SkyCoord(ecrf_a, ecrf_d, frame=ECRF, unit='deg')\n+\n+        # transform to ICRS\n+        icrs_sc = ecrf_sc.transform_to(ICRS)\n+        # must do this otherwise astropy thinks they aren't the same frame\n+        icrs_sc = SkyCoord(icrs_sc.ra.deg, icrs_sc.dec.deg, frame='icrs', unit='deg')\n+\n+        # transform reference star coords back to ICRS\n+        ref_sc_ecrf = SkyCoord(\n+            ccd.ra[ccd.has_sky_coord], ccd.dec[ccd.has_sky_coord], frame=ECRF, unit='deg'\n+        )\n+        ref_sc_icrs = ref_sc_ecrf.transform_to(ICRS)\n+        ccd.ra[ccd.has_sky_coord] = ref_sc_icrs.ra.deg\n+        ccd.dec[ccd.has_sky_coord] = ref_sc_icrs.dec.deg\n+        ccd.update_skycoord_obj()\n+\n+        # compute the new WCS\n+        orig_crvals = np.array([orig_hdr['CRVAL1'], orig_hdr['CRVAL2']])\n+        orig_cdmtx = wcsfit_core.get_cd(orig_hdr).flatten()\n+        orig_sip = wcsfit_core.get_sip_coeffs(orig_hdr)\n+        # starting parameters\n+        p0 = np.concatenate((orig_crvals, orig_cdmtx, orig_sip))\n+\n+        # temporary header\n+        tmp_hdr = orig_hdr.copy()\n+\n+        def min_func(parameters):\n+            # extract the linear terms\n+            _crvals = parameters[:2]\n+            _cd_mtx = parameters[2:6].reshape(2, 2)\n+            _sip_coeffs = parameters[6:]\n+\n+            # send the linear terms to the header\n+            for i in range(1, 3):\n+                tmp_hdr[f'CRVAL{i}'] = _crvals[i - 1]\n+                for j in range(1, 3):\n+                    tmp_hdr[f'CD{i}_{j}'] = _cd_mtx[i - 1, j - 1]\n+\n+            # send the distortion model to the header\n+            k = 0\n+            for dim in 'AB':\n+                order = tmp_hdr[f'{dim}_ORDER']\n+                for p in range(order + 1):\n+                    for q in range(order + 1):\n+                        if 2 <= p + q <= order:\n+                            tmp_hdr[f'{dim}_{p}_{q}'] = _sip_coeffs[k]\n+                            k += 1\n+\n+            # generate the WCS object\n+            tmp_wcs = wcs.WCS(tmp_hdr)\n+\n+            # evaluate the positions of the control points\n+            _ra, _dec = tmp_wcs.all_pix2world(x, y, 1)\n+            _sc = SkyCoord(_ra, _dec, frame='icrs', unit='deg')\n+\n+            # calculate separations between true and predicted positions\n+            dra, ddec = icrs_sc.spherical_offsets_to(_sc)\n+            # convert to milliarcsec\n+            dra, ddec = dra.arcsec * 1000, ddec.arcsec * 1000\n+\n+            # return the flattened residuals\n+            return np.concatenate((dra, ddec)).flatten()\n+\n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(orig_crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(orig_crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(orig_crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(orig_crvals[1] + 5, -90.0, 90.0)\n+\n+        # run the minimizer\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n+\n+        # set the resultant values and get the header\n+        abs_residuals = np.abs(min_func(solution.x))\n+        if logger is not None:\n+            logger.debug(f\"mean, max residual = \"\n+                         f\"{abs_residuals.mean():.2e}, \"\n+                         f\"{abs_residuals.max():.2e} (mas)\")\n+\n+        # send the header to the CCD object\n+        ccd.header = tmp_hdr.copy()\n+        ccd.wcs = wcs.WCS(ccd.header)\n+\n+\n+\n+\n \n def fit_fpa_model(\n         obsdataset,\n@@ -1596,6 +1781,7 @@ def fpa_model_from_observation(observation, sip_order, logger=None):\n def read_observations_references(\n         source_catalogues, reference_catalogues,\n         refmagkey=None, refmagmin=None, refmagmax=None,\n+        flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n         logger=None\n ):\n     \"\"\"\n@@ -1616,6 +1802,15 @@ def read_observations_references(\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A Logger for logging messages (None for no logging, default).\n \n@@ -1638,9 +1833,23 @@ def read_observations_references(\n             source_catalogues, reference_catalogues,\n             fillvalue=reference_catalogues[0]\n     ):\n+\n+        # grab data from the observation primary header\n+        pri_hdr = fits.getheader(src_cat_path, 0)\n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # read the reference catalogue\n         refdata = get_references(\n             ref_cat_path,\n+            obs_epoch,\n+            pos,\n+            vel,\n             refmagkey=refmagkey,\n             refmagmin=refmagmin,\n             refmagmax=refmagmax,\n@@ -1650,7 +1859,8 @@ def read_observations_references(\n         observations.append(get_observation(\n             src_cat_path, refdata,\n             refmagmin=refmagmin, refmagmax=refmagmax,\n-            logger=logger\n+            flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+            flux_rad_peak=flux_rad_peak, logger=logger\n         ))\n \n     # return an ObservationSet instance\n@@ -2116,6 +2326,7 @@ def is_an_astrometric_kwd(kwd, include_naxis=False):\n \n \n def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n+                    flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                     logger=None):\n     \"\"\"\n     Grab data and headers from the catalogue of the relevant observation.\n@@ -2134,6 +2345,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n+    logger : Logger, optional\n+        A logger which will be used for printing the logging messages (None for\n+        no printing).\n \n     Returns\n     -------\n@@ -2157,6 +2380,14 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n         # grab the primary header\n         pri_hdr = srccat_hdul[0].header\n \n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos_vec = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel_vec = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # record the primary header exptime if present\n         if 'EXPTIME' in pri_hdr.keys():\n             exptimes.append(float(pri_hdr['EXPTIME']))\n@@ -2218,8 +2449,8 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n             _mask = filter_catalogue(\n                 mags, fluxrad,\n                 mag_min=refmagmin, mag_max=refmagmax, mag_margin=0.5,\n-                flux_rad_min=2.0, flux_rad_max=4.0,\n-                logger=logger\n+                flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+                flux_rad_peak=flux_rad_peak, logger=logger\n             )\n \n             # require no large sextractor error flags\n@@ -2278,13 +2509,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     # build an Observation object instance from the list of ccds\n     observation = wcsfit_core.Observation(\n         ccd_list, refdata, pointings[0], pas[0], exptimes[0],\n+        ref_epoch=obs_epoch, pos_offset=pos_vec, vel_offset=vel_vec,\n         filename=src_cat_path, header=pri_hdr\n     )\n \n+    # record the commanded pointing and position angle\n+    observation.commanded_pointing = pointings[0]\n+    observation.commanded_pa = pas[0]\n+\n     return observation\n \n \n-def get_references(refcat,\n+def get_references(refcat, obs_epoch, pos, vel,\n                    refmagkey=None, refmagmin=None, refmagmax=None,\n                    logger=None):\n     \"\"\"\n@@ -2294,6 +2530,12 @@ def get_references(refcat,\n     ----------\n     refcat : str\n         A FITS file with catalogue of position reference stars.\n+    obs_epoch : float\n+        Julian year of observation.\n+    pos : tuple\n+        Geocentric position vector of the instrument.\n+    vel : tuple\n+        Geocentric velocity vector of the instrument.\n     refmagkey : str, optional\n         Name of the column which contains the catalogue magnitudes.\n     refmagmin : float, optional\n@@ -2320,6 +2562,12 @@ def get_references(refcat,\n         ref_ra_error = ref_hdulist[1].data.field('GAIA_RA_ERROR')\n         ref_dec_error = ref_hdulist[1].data.field('GAIA_DEC_ERROR')\n \n+        # Read astrometry from the reference catalogue\n+        ref_epoch = ref_hdulist[1].data.field('REF_EPOCH')\n+        ref_pmra = ref_hdulist[1].data.field('PMRA')\n+        ref_pmdec = ref_hdulist[1].data.field('PMDEC')\n+        ref_parallax = ref_hdulist[1].data.field('PARALLAX')\n+\n         # read the reference star mags\n         if refmagkey is not None:\n             ref_mag = ref_hdulist[1].data.field(refmagkey)\n@@ -2370,6 +2618,15 @@ def get_references(refcat,\n                 f'magnitude selection'\n             )\n \n+    # propagate positions to the correct reference frame and epoch\n+    if logger is not None:\n+        logger.info(f\"rejecting {np.count_nonzero(np.isnan(ref_parallax[mask]))} ref stars with null parallaxes\")\n+    mask[np.isnan(ref_parallax)] = False\n+    ra, dec = icrs_to_ecrf(\n+        ref_ra[mask], ref_dec[mask], ref_parallax[mask], ref_pmra[mask], ref_pmdec[mask],\n+        ref_epoch[mask], obs_epoch, pos, vel\n+    )\n+\n     # filter ref mags if not None\n     if ref_mag is not None:\n         ref_mag = ref_mag[mask]\n@@ -2385,7 +2642,7 @@ def get_references(refcat,\n \n     # create the RefData object\n     refdata = wcsfit_core.RefData(\n-        ref_ra[mask], ref_dec[mask],\n+        ra, dec,\n         ref_ra_error[mask], ref_dec_error[mask],\n         mags=ref_mag, filename=refcat\n     )\n@@ -2428,11 +2685,14 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n     new_dec : array-like\n         Declination in the Euclid-centric reference frame at the target_epoch\n     \"\"\"\n+    # deal with negative parallaxes\n+    _parallax = np.clip(parallax, 1E-10, np.inf)\n+\n     # icrs sky coordinates at reference epoch\n     icrs0 = SkyCoord(\n         ra * u.deg, dec * u.deg,\n         pm_ra_cosdec=pmra * u.mas / u.yr, pm_dec=pmdec * u.mas / u.yr,\n-        distance=1000.0 / parallax * u.pc,\n+        distance=1000.0 / _parallax * u.pc,\n         obstime=Time(ref_epoch, format='jyear'),\n         frame='icrs'\n     )\n@@ -2449,11 +2709,11 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n         x=vel_offset[0], y=vel_offset[1], z=vel_offset[2],\n         unit=u.km / u.s\n     )\n-    ECRF = GCRS(obstime=Time(target_time), obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+    ECRF = GCRS(obstime=target_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n     # transform coordinates to ECRF system\n     ecrf1 = icrs1.transform_to(ECRF)\n \n-    return ecrf1.ra.dec, ecrf1.dec.deg\n+    return ecrf1.ra.deg, ecrf1.dec.deg\n \n \n def run_checks(srccats=None, refcats=None,\n@@ -2803,7 +3063,7 @@ def _check_srccat(filename, logger=None):\n             try:\n                 extname = hdr['EXTNAME']\n \n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with an '\n@@ -2913,7 +3173,7 @@ def _check_fpa_model(filename, logger=None):\n     # Check that the sections have the expected format\n     if result:\n         for section_key in config.keys():\n-            if not re.match('CCDID [1-6]-[1-6]', section_key):\n+            if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', section_key):\n                 if not section_key == 'GLOBAL':\n                     if logger is not None:\n                         logger.error(\n@@ -3100,7 +3360,7 @@ def _check_imgfile(filename, logger=None):\n         for i in range(1, len(hdulist)):\n             try:\n                 extname = hdulist[i].name\n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with a '\n@@ -3311,13 +3571,12 @@ def gauss_func(x, a, x0, sigma):\n     )\n \n \n-def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n+def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max, flux_rad_peak):\n     \"\"\"\n     Perform a Gaussian fit (courtesy of Sylvain Mottet)\n     \"\"\"\n     # initial parameters\n-    flux_rad_med = 0.5 * (flux_rad_min + flux_rad_max)\n-    p0 = np.array([0.5, flux_rad_med, 0.15])\n+    p0 = np.array([0.5, flux_rad_peak, 0.15])\n \n     def func(x, p00, p01, p02):\n         # composite of two Gaussian functions\n@@ -3332,8 +3591,9 @@ def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n     )\n     return popt  # a, x0, sigma\n \n+\n def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n-                     flux_rad_min=0.5, flux_rad_max=2.5,\n+                     flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                      logger=None):\n     \"\"\"\n     Filter a set of sources by magnitude and flux radius\n@@ -3351,9 +3611,14 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     mag_margin : float, optional\n         Allowable margin for error on the magnitudes\n     flux_rad_min : float, optional\n-        Lower limit of acceptable flux_radius range, default 0.5.\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n     flux_rad_max : float, optional\n-        Upper limit of acceptable flux_radius range, default 2.5.\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A logger which will be used for printing the logging messages (None for\n         no printing).\n@@ -3376,8 +3641,10 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     if mag_max is not None:\n         mask[mag > (mag_max + mag_margin)] = False\n \n-    \"\"\"\n-    # Commented out: see https://euclid.roe.ac.uk/issues/23190#note-57\n+    if flux_rad_min is None:\n+        flux_rad_min = 0.0\n+    if flux_rad_max is None:\n+        flux_rad_max = np.inf\n \n     # basic fluxrad filtering\n     mask[(fluxrad < flux_rad_min) | (fluxrad > flux_rad_max)] = False\n@@ -3385,41 +3652,32 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     # check on current source count, can we do something fancy?\n     num_in_range = np.count_nonzero(mask)\n \n-    # some testing suggests 30 sources is enough to do something with\n-    if num_in_range >= 30:\n-        bin_count = 50\n-        # get flux radius histogram\n-        radius_count, bin_edges = np.histogram(\n-            fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n-        )\n-        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n+    if flux_rad_peak is not None:\n+        # some testing suggests 30 sources is enough to do something with\n+        if num_in_range >= 30:\n+            bin_count = 50\n+            # get flux radius histogram\n+            radius_count, bin_edges = np.histogram(\n+                fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n+            )\n+            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n \n-        # try to fit a gaussian to the flux_radius distribution\n-        try:\n-            a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max)\n-            mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n-\n-            # plt.hist(\n-            #     fluxrad[mask], bins=bin_edges, density=True, histtype='step'\n-            # )\n-            # xx = np.linspace(0.5, 2.5, 1000)\n-            # yy = gauss_func(xx, a, x0, sig)\n-            # plt.plot(xx, yy)\n-            # plt.gca().axvline(x0+5*sig)\n-            # plt.gca().axvline(x0-5*sig)\n-            # print(a, x0, sig)  #, a_2, x0_2, sig_2)\n-            # plt.show()\n-        except RuntimeError:\n+            # try to fit a gaussian to the flux_radius distribution\n+            try:\n+                a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max, flux_rad_peak)\n+                mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n+            except RuntimeError:\n+                if logger is not None:\n+                    logger.info(\n+                        f\"flux_radius distribution fitting was unsuccessful\"\n+                    )\n+        else:\n             if logger is not None:\n                 logger.info(\n-                    f\"flux_radius distribution fitting was unsuccessful\"\n+                    f\"too few sources to attempt flux_radius distribution fitting\"\n                 )\n-    else:\n-        if logger is not None:\n-            logger.info(\n-                f\"too few sources to attempt flux_radius distribution fitting\"\n-            )\n-    \"\"\"\n+    elif logger is not None:\n+        logger.debug(\"flux_radius fitting not performed\")\n \n     return mask\n \n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23453 Velocity aberration",
                            "Catherine Grenet",
                            "2023-08-22T15:29:05.000+02:00",
                            "b222e19a905c21e2330dc9018a73bfae8e9b014f"
                        ],
                        [
                            "@@ -42,7 +42,7 @@ import sys\n import numpy as np\n from astropy import wcs\n from astropy.io import fits\n-from astropy.coordinates import SkyCoord, GCRS, CartesianRepresentation\n+from astropy.coordinates import SkyCoord, GCRS, ICRS, CartesianRepresentation\n import astropy.units as u\n from astropy.time import Time\n from configobj import ConfigObj\n@@ -50,6 +50,7 @@ import itertools\n import matplotlib.pyplot as plt\n import matplotlib.colors\n from scipy.spatial.distance import cdist\n+from scipy.optimize import least_squares\n \n from . import wcsfit_core\n from .sip_tpv import sip_to_pv\n@@ -218,6 +219,18 @@ def get_parser():\n         '--refmagmax', type=float, default=None, metavar='MAX_MAG',\n         help='maximum reference star magnitude (default: no limit)'\n     )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-min', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Lower limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-max', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Upper limit on reference star FLUX_RADIUS'\n+    )\n+    ref_opt_args_group.add_argument(\n+        '--flux-rad-peak', type=float, default=None, metavar='FLUX_RADIUS',\n+        help='Approximate reference star FLUX_RADIUS distribution peak'\n+    )\n \n     # minimiser termination optional arguments\n     term_opt_args_group = parser.add_argument_group(\n@@ -325,6 +338,16 @@ def check_cmdargs(_args):\n         plural = '' if len(mag_lims) > 1 else 's'\n         parser.error(' and '.join(mag_lims) + f' require{plural} --refmagcol')\n \n+    # check that the FLUX_RADIUS related arguments are sensible\n+    if _args.flux_rad_min is not None and _args.flux_rad_max is not None:\n+        if _args.flux_rad_min >= _args.flux_rad_max:\n+            parser.error(\"FLUX_RADIUS minimum must be smaller than FLUX_RADIUS maximum\")\n+        if _args.flux_rad_peak is not None:\n+            if _args.flux_rad_peak <= _args.flux_rad_min:\n+                parser.error(\"FLUX_RADIUS peak must be larger than FLUX_RADIUS minimum\")\n+            if _args.flux_rad_peak >= _args.flux_rad_max:\n+                parser.error(\"FLUX_RADIUS peak must be smaller than FLUX_RADIUS maximum\")\n+\n     # check that the number of minimiser iterations is greater than zero\n     if not _args.niter > 0:\n         parser.error('--niter must be >0')\n@@ -444,6 +467,9 @@ def runwcsfit(args, logger=None):\n         refmagkey=args.refmagcol,\n         refmagmin=args.refmagmin,\n         refmagmax=args.refmagmax,\n+        flux_rad_min=args.flux_rad_min,\n+        flux_rad_max=args.flux_rad_max,\n+        flux_rad_peak=args.flux_rad_peak,\n         logger=logger\n     )\n \n@@ -481,12 +507,15 @@ def runwcsfit(args, logger=None):\n \n         # plot the plate scale if requested\n         if args.make_plots is not None:\n-            id_string = os.path.basename(args.output_fpa_model)\n-            plot_plate_scale(\n-                fpa_model, args.make_plots,\n-                alt_fpa_model=args.init_fpa_model,\n-                file_identifier_string=id_string\n-            )\n+            try:\n+                id_string = os.path.basename(args.output_fpa_model)\n+                plot_plate_scale(\n+                    fpa_model, args.make_plots,\n+                    alt_fpa_model=args.init_fpa_model,\n+                    file_identifier_string=id_string\n+                )\n+            except Exception as e:\n+                logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     elif args.mode == 'use-fpa-model':\n \n@@ -564,10 +593,13 @@ def runwcsfit(args, logger=None):\n \n     # Produce plots if requested\n     if args.make_plots is not None:\n-        plot_sausages(obsdataset, args.make_plots)\n-        plot_vpds(obsdataset, args.make_plots)\n-        plot_residuals(obsdataset, args.make_plots)\n-        plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        try:\n+            plot_sausages(obsdataset, args.make_plots)\n+            plot_vpds(obsdataset, args.make_plots)\n+            plot_residuals(obsdataset, args.make_plots)\n+            plot_local_mean_residual(obsdataset, fpa_model, args.make_plots)\n+        except Exception as e:\n+            logger.error(f\"!!! caught exception '{e}' in wcsfit make_plots, ignoring....\")\n \n     # store the tables of reference source matches if requested\n     if args.matchcat is not None:\n@@ -1203,6 +1235,21 @@ def copy_wcs_to_file(source_observation, target_file, fpamodel=None):\n               sip_to_pv(src_hdr)\n             replace_header_wcs(src_hdr, dest_hdr)\n \n+            # record the original pointing and position angle\n+            dest_hdr['RA_COMM'] = (\n+                source_observation.commanded_pointing[0],\n+                'commanding pointing RA'\n+            )\n+            dest_hdr['DEC_COMM'] = (\n+                source_observation.commanded_pointing[1],\n+                'commanding pointing DEC'\n+            )\n+            dest_hdr['PA_COMM'] = (\n+                source_observation.commanded_pa,\n+                'commanding position angle'\n+            )\n+\n+            # todo consider the reference frame\n             # add the reconstructed pointing\n             dest_hdr['RA'] = (\n                 source_observation.pointing[0],\n@@ -1288,6 +1335,144 @@ def use_fpa_model(\n         logger=logger\n     )\n \n+    # ecrf to icrs\n+    ecrf_to_icrs(observation, logger=logger)\n+\n+\n+def ecrf_to_icrs(observation, num_cps_per_axis=100, logger=None):\n+    \"\"\"\n+    Take the WCS information from the observation, which maps array\n+    coordinates to the ECRF, and map it from array coords to the ICRS\n+\n+    Parameters\n+    ----------\n+    observation : wcsfit_core.Observation\n+        The observation containing the base WCS\n+    num_cps_per_axis : int, optional\n+        The number of control points to use per axis, default 100.\n+    logger : Logger, optional\n+        A Logger for logging messages (None for no logging, default).\n+    \"\"\"\n+    for ext, ccd in observation.ccds.items():\n+        if logger is not None:\n+            logger.debug(f\"Transforming {ext} WCS from ECRF to ICRS\")\n+\n+        # read the header\n+        orig_hdr = ccd.header.copy()\n+\n+        # generate the control point grid\n+        xy = np.meshgrid(np.linspace(1, orig_hdr[\"NAXIS1\"], num_cps_per_axis),\n+                         np.linspace(1, orig_hdr[\"NAXIS2\"], num_cps_per_axis))\n+        x, y = map(lambda arr: arr.flatten(), xy)\n+\n+        # define the Euclid-centric reference frame\n+        _pos_offset = CartesianRepresentation(\n+            x=observation.pos_offset[0],\n+            y=observation.pos_offset[1],\n+            z=observation.pos_offset[2],\n+            unit=u.km\n+        )\n+        _vel_offset = CartesianRepresentation(\n+            x=observation.vel_offset[0],\n+            y=observation.vel_offset[1],\n+            z=observation.vel_offset[2],\n+            unit=u.km / u.s\n+        )\n+        _obs_time = Time(observation.ref_epoch, format='jyear')\n+        ECRF = GCRS(obstime=_obs_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+\n+        # project array coords to ecrf\n+        ecrf_a, ecrf_d = wcs.WCS(orig_hdr).all_pix2world(x, y, 1)\n+        ecrf_sc = SkyCoord(ecrf_a, ecrf_d, frame=ECRF, unit='deg')\n+\n+        # transform to ICRS\n+        icrs_sc = ecrf_sc.transform_to(ICRS)\n+        # must do this otherwise astropy thinks they aren't the same frame\n+        icrs_sc = SkyCoord(icrs_sc.ra.deg, icrs_sc.dec.deg, frame='icrs', unit='deg')\n+\n+        # transform reference star coords back to ICRS\n+        ref_sc_ecrf = SkyCoord(\n+            ccd.ra[ccd.has_sky_coord], ccd.dec[ccd.has_sky_coord], frame=ECRF, unit='deg'\n+        )\n+        ref_sc_icrs = ref_sc_ecrf.transform_to(ICRS)\n+        ccd.ra[ccd.has_sky_coord] = ref_sc_icrs.ra.deg\n+        ccd.dec[ccd.has_sky_coord] = ref_sc_icrs.dec.deg\n+        ccd.update_skycoord_obj()\n+\n+        # compute the new WCS\n+        orig_crvals = np.array([orig_hdr['CRVAL1'], orig_hdr['CRVAL2']])\n+        orig_cdmtx = wcsfit_core.get_cd(orig_hdr).flatten()\n+        orig_sip = wcsfit_core.get_sip_coeffs(orig_hdr)\n+        # starting parameters\n+        p0 = np.concatenate((orig_crvals, orig_cdmtx, orig_sip))\n+\n+        # temporary header\n+        tmp_hdr = orig_hdr.copy()\n+\n+        def min_func(parameters):\n+            # extract the linear terms\n+            _crvals = parameters[:2]\n+            _cd_mtx = parameters[2:6].reshape(2, 2)\n+            _sip_coeffs = parameters[6:]\n+\n+            # send the linear terms to the header\n+            for i in range(1, 3):\n+                tmp_hdr[f'CRVAL{i}'] = _crvals[i - 1]\n+                for j in range(1, 3):\n+                    tmp_hdr[f'CD{i}_{j}'] = _cd_mtx[i - 1, j - 1]\n+\n+            # send the distortion model to the header\n+            k = 0\n+            for dim in 'AB':\n+                order = tmp_hdr[f'{dim}_ORDER']\n+                for p in range(order + 1):\n+                    for q in range(order + 1):\n+                        if 2 <= p + q <= order:\n+                            tmp_hdr[f'{dim}_{p}_{q}'] = _sip_coeffs[k]\n+                            k += 1\n+\n+            # generate the WCS object\n+            tmp_wcs = wcs.WCS(tmp_hdr)\n+\n+            # evaluate the positions of the control points\n+            _ra, _dec = tmp_wcs.all_pix2world(x, y, 1)\n+            _sc = SkyCoord(_ra, _dec, frame='icrs', unit='deg')\n+\n+            # calculate separations between true and predicted positions\n+            dra, ddec = icrs_sc.spherical_offsets_to(_sc)\n+            # convert to milliarcsec\n+            dra, ddec = dra.arcsec * 1000, ddec.arcsec * 1000\n+\n+            # return the flattened residuals\n+            return np.concatenate((dra, ddec)).flatten()\n+\n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(orig_crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(orig_crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(orig_crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(orig_crvals[1] + 5, -90.0, 90.0)\n+\n+        # run the minimizer\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n+\n+        # set the resultant values and get the header\n+        abs_residuals = np.abs(min_func(solution.x))\n+        if logger is not None:\n+            logger.debug(f\"mean, max residual = \"\n+                         f\"{abs_residuals.mean():.2e}, \"\n+                         f\"{abs_residuals.max():.2e} (mas)\")\n+\n+        # send the header to the CCD object\n+        ccd.header = tmp_hdr.copy()\n+        ccd.wcs = wcs.WCS(ccd.header)\n+\n+\n+\n+\n \n def fit_fpa_model(\n         obsdataset,\n@@ -1596,6 +1781,7 @@ def fpa_model_from_observation(observation, sip_order, logger=None):\n def read_observations_references(\n         source_catalogues, reference_catalogues,\n         refmagkey=None, refmagmin=None, refmagmax=None,\n+        flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n         logger=None\n ):\n     \"\"\"\n@@ -1616,6 +1802,15 @@ def read_observations_references(\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A Logger for logging messages (None for no logging, default).\n \n@@ -1638,9 +1833,23 @@ def read_observations_references(\n             source_catalogues, reference_catalogues,\n             fillvalue=reference_catalogues[0]\n     ):\n+\n+        # grab data from the observation primary header\n+        pri_hdr = fits.getheader(src_cat_path, 0)\n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # read the reference catalogue\n         refdata = get_references(\n             ref_cat_path,\n+            obs_epoch,\n+            pos,\n+            vel,\n             refmagkey=refmagkey,\n             refmagmin=refmagmin,\n             refmagmax=refmagmax,\n@@ -1650,7 +1859,8 @@ def read_observations_references(\n         observations.append(get_observation(\n             src_cat_path, refdata,\n             refmagmin=refmagmin, refmagmax=refmagmax,\n-            logger=logger\n+            flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+            flux_rad_peak=flux_rad_peak, logger=logger\n         ))\n \n     # return an ObservationSet instance\n@@ -2116,6 +2326,7 @@ def is_an_astrometric_kwd(kwd, include_naxis=False):\n \n \n def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n+                    flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                     logger=None):\n     \"\"\"\n     Grab data and headers from the catalogue of the relevant observation.\n@@ -2134,6 +2345,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     refmagmax : float, optional\n         Maximum magnitude of the reference stars used from refcat (None for\n         no maximum limit).\n+    flux_rad_min : float, optional\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n+    flux_rad_max : float, optional\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n+    logger : Logger, optional\n+        A logger which will be used for printing the logging messages (None for\n+        no printing).\n \n     Returns\n     -------\n@@ -2157,6 +2380,14 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n         # grab the primary header\n         pri_hdr = srccat_hdul[0].header\n \n+        # date of the vis observation\n+        mjdobs = pri_hdr['MJD-OBS']\n+        obs_epoch = Time(mjdobs, format='mjd').jyear\n+        # position of the instrument\n+        pos_vec = tuple([pri_hdr[f\"POS_{elem}\"] for elem in \"XYZ\"])\n+        # velocity of the instrument\n+        vel_vec = tuple([pri_hdr[f\"VEL_{elem}\"] for elem in \"XYZ\"])\n+\n         # record the primary header exptime if present\n         if 'EXPTIME' in pri_hdr.keys():\n             exptimes.append(float(pri_hdr['EXPTIME']))\n@@ -2218,8 +2449,8 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n             _mask = filter_catalogue(\n                 mags, fluxrad,\n                 mag_min=refmagmin, mag_max=refmagmax, mag_margin=0.5,\n-                flux_rad_min=2.0, flux_rad_max=4.0,\n-                logger=logger\n+                flux_rad_min=flux_rad_min, flux_rad_max=flux_rad_max,\n+                flux_rad_peak=flux_rad_peak, logger=logger\n             )\n \n             # require no large sextractor error flags\n@@ -2278,13 +2509,18 @@ def get_observation(src_cat_path, refdata, refmagmin=None, refmagmax=None,\n     # build an Observation object instance from the list of ccds\n     observation = wcsfit_core.Observation(\n         ccd_list, refdata, pointings[0], pas[0], exptimes[0],\n+        ref_epoch=obs_epoch, pos_offset=pos_vec, vel_offset=vel_vec,\n         filename=src_cat_path, header=pri_hdr\n     )\n \n+    # record the commanded pointing and position angle\n+    observation.commanded_pointing = pointings[0]\n+    observation.commanded_pa = pas[0]\n+\n     return observation\n \n \n-def get_references(refcat,\n+def get_references(refcat, obs_epoch, pos, vel,\n                    refmagkey=None, refmagmin=None, refmagmax=None,\n                    logger=None):\n     \"\"\"\n@@ -2294,6 +2530,12 @@ def get_references(refcat,\n     ----------\n     refcat : str\n         A FITS file with catalogue of position reference stars.\n+    obs_epoch : float\n+        Julian year of observation.\n+    pos : tuple\n+        Geocentric position vector of the instrument.\n+    vel : tuple\n+        Geocentric velocity vector of the instrument.\n     refmagkey : str, optional\n         Name of the column which contains the catalogue magnitudes.\n     refmagmin : float, optional\n@@ -2320,6 +2562,12 @@ def get_references(refcat,\n         ref_ra_error = ref_hdulist[1].data.field('GAIA_RA_ERROR')\n         ref_dec_error = ref_hdulist[1].data.field('GAIA_DEC_ERROR')\n \n+        # Read astrometry from the reference catalogue\n+        ref_epoch = ref_hdulist[1].data.field('REF_EPOCH')\n+        ref_pmra = ref_hdulist[1].data.field('PMRA')\n+        ref_pmdec = ref_hdulist[1].data.field('PMDEC')\n+        ref_parallax = ref_hdulist[1].data.field('PARALLAX')\n+\n         # read the reference star mags\n         if refmagkey is not None:\n             ref_mag = ref_hdulist[1].data.field(refmagkey)\n@@ -2370,6 +2618,15 @@ def get_references(refcat,\n                 f'magnitude selection'\n             )\n \n+    # propagate positions to the correct reference frame and epoch\n+    if logger is not None:\n+        logger.info(f\"rejecting {np.count_nonzero(np.isnan(ref_parallax[mask]))} ref stars with null parallaxes\")\n+    mask[np.isnan(ref_parallax)] = False\n+    ra, dec = icrs_to_ecrf(\n+        ref_ra[mask], ref_dec[mask], ref_parallax[mask], ref_pmra[mask], ref_pmdec[mask],\n+        ref_epoch[mask], obs_epoch, pos, vel\n+    )\n+\n     # filter ref mags if not None\n     if ref_mag is not None:\n         ref_mag = ref_mag[mask]\n@@ -2385,7 +2642,7 @@ def get_references(refcat,\n \n     # create the RefData object\n     refdata = wcsfit_core.RefData(\n-        ref_ra[mask], ref_dec[mask],\n+        ra, dec,\n         ref_ra_error[mask], ref_dec_error[mask],\n         mags=ref_mag, filename=refcat\n     )\n@@ -2428,11 +2685,14 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n     new_dec : array-like\n         Declination in the Euclid-centric reference frame at the target_epoch\n     \"\"\"\n+    # deal with negative parallaxes\n+    _parallax = np.clip(parallax, 1E-10, np.inf)\n+\n     # icrs sky coordinates at reference epoch\n     icrs0 = SkyCoord(\n         ra * u.deg, dec * u.deg,\n         pm_ra_cosdec=pmra * u.mas / u.yr, pm_dec=pmdec * u.mas / u.yr,\n-        distance=1000.0 / parallax * u.pc,\n+        distance=1000.0 / _parallax * u.pc,\n         obstime=Time(ref_epoch, format='jyear'),\n         frame='icrs'\n     )\n@@ -2449,11 +2709,11 @@ def icrs_to_ecrf(ra, dec, parallax, pmra, pmdec, ref_epoch,\n         x=vel_offset[0], y=vel_offset[1], z=vel_offset[2],\n         unit=u.km / u.s\n     )\n-    ECRF = GCRS(obstime=Time(target_time), obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n+    ECRF = GCRS(obstime=target_time, obsgeoloc=_pos_offset, obsgeovel=_vel_offset)\n     # transform coordinates to ECRF system\n     ecrf1 = icrs1.transform_to(ECRF)\n \n-    return ecrf1.ra.dec, ecrf1.dec.deg\n+    return ecrf1.ra.deg, ecrf1.dec.deg\n \n \n def run_checks(srccats=None, refcats=None,\n@@ -2803,7 +3063,7 @@ def _check_srccat(filename, logger=None):\n             try:\n                 extname = hdr['EXTNAME']\n \n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with an '\n@@ -2913,7 +3173,7 @@ def _check_fpa_model(filename, logger=None):\n     # Check that the sections have the expected format\n     if result:\n         for section_key in config.keys():\n-            if not re.match('CCDID [1-6]-[1-6]', section_key):\n+            if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', section_key):\n                 if not section_key == 'GLOBAL':\n                     if logger is not None:\n                         logger.error(\n@@ -3100,7 +3360,7 @@ def _check_imgfile(filename, logger=None):\n         for i in range(1, len(hdulist)):\n             try:\n                 extname = hdulist[i].name\n-                if not re.match('CCDID [1-6]-[1-6]', extname):\n+                if not re.match('(CCDID )?[1-6]-[1-6](\\\\.[EFGH])?', extname):\n                     if logger is not None:\n                         logger.error(\n                             f'{filename}[{i}] contains a header with a '\n@@ -3311,13 +3571,12 @@ def gauss_func(x, a, x0, sigma):\n     )\n \n \n-def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n+def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max, flux_rad_peak):\n     \"\"\"\n     Perform a Gaussian fit (courtesy of Sylvain Mottet)\n     \"\"\"\n     # initial parameters\n-    flux_rad_med = 0.5 * (flux_rad_min + flux_rad_max)\n-    p0 = np.array([0.5, flux_rad_med, 0.15])\n+    p0 = np.array([0.5, flux_rad_peak, 0.15])\n \n     def func(x, p00, p01, p02):\n         # composite of two Gaussian functions\n@@ -3332,8 +3591,9 @@ def gauss_fit(xdat, ydat, flux_rad_min, flux_rad_max):\n     )\n     return popt  # a, x0, sigma\n \n+\n def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n-                     flux_rad_min=0.5, flux_rad_max=2.5,\n+                     flux_rad_min=None, flux_rad_max=None, flux_rad_peak=None,\n                      logger=None):\n     \"\"\"\n     Filter a set of sources by magnitude and flux radius\n@@ -3351,9 +3611,14 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     mag_margin : float, optional\n         Allowable margin for error on the magnitudes\n     flux_rad_min : float, optional\n-        Lower limit of acceptable flux_radius range, default 0.5.\n+        Lower limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no minimum FLUX_RADIUS.\n     flux_rad_max : float, optional\n-        Upper limit of acceptable flux_radius range, default 2.5.\n+        Upper limit of acceptable FLUX_RADIUS range for real stars.\n+        Default None, i.e. no maximum FLUX_RADIUS.\n+    flux_rad_peak : float, optional\n+        The peak of the FLUX_RADIUS distribution for real stars.\n+        Default None, i.e. no FLUX_RADIUS distribution fitting.\n     logger : Logger, optional\n         A logger which will be used for printing the logging messages (None for\n         no printing).\n@@ -3376,8 +3641,10 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     if mag_max is not None:\n         mask[mag > (mag_max + mag_margin)] = False\n \n-    \"\"\"\n-    # Commented out: see https://euclid.roe.ac.uk/issues/23190#note-57\n+    if flux_rad_min is None:\n+        flux_rad_min = 0.0\n+    if flux_rad_max is None:\n+        flux_rad_max = np.inf\n \n     # basic fluxrad filtering\n     mask[(fluxrad < flux_rad_min) | (fluxrad > flux_rad_max)] = False\n@@ -3385,41 +3652,32 @@ def filter_catalogue(mag, fluxrad, mag_min=None, mag_max=None, mag_margin=0.0,\n     # check on current source count, can we do something fancy?\n     num_in_range = np.count_nonzero(mask)\n \n-    # some testing suggests 30 sources is enough to do something with\n-    if num_in_range >= 30:\n-        bin_count = 50\n-        # get flux radius histogram\n-        radius_count, bin_edges = np.histogram(\n-            fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n-        )\n-        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n+    if flux_rad_peak is not None:\n+        # some testing suggests 30 sources is enough to do something with\n+        if num_in_range >= 30:\n+            bin_count = 50\n+            # get flux radius histogram\n+            radius_count, bin_edges = np.histogram(\n+                fluxrad[mask], bins=bin_count, range=[flux_rad_min, flux_rad_max], density=True\n+            )\n+            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n \n-        # try to fit a gaussian to the flux_radius distribution\n-        try:\n-            a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max)\n-            mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n-\n-            # plt.hist(\n-            #     fluxrad[mask], bins=bin_edges, density=True, histtype='step'\n-            # )\n-            # xx = np.linspace(0.5, 2.5, 1000)\n-            # yy = gauss_func(xx, a, x0, sig)\n-            # plt.plot(xx, yy)\n-            # plt.gca().axvline(x0+5*sig)\n-            # plt.gca().axvline(x0-5*sig)\n-            # print(a, x0, sig)  #, a_2, x0_2, sig_2)\n-            # plt.show()\n-        except RuntimeError:\n+            # try to fit a gaussian to the flux_radius distribution\n+            try:\n+                a, x0, sig = gauss_fit(bin_centers, radius_count, flux_rad_min, flux_rad_max, flux_rad_peak)\n+                mask[np.abs(fluxrad - x0) > (5 * sig)] = False\n+            except RuntimeError:\n+                if logger is not None:\n+                    logger.info(\n+                        f\"flux_radius distribution fitting was unsuccessful\"\n+                    )\n+        else:\n             if logger is not None:\n                 logger.info(\n-                    f\"flux_radius distribution fitting was unsuccessful\"\n+                    f\"too few sources to attempt flux_radius distribution fitting\"\n                 )\n-    else:\n-        if logger is not None:\n-            logger.info(\n-                f\"too few sources to attempt flux_radius distribution fitting\"\n-            )\n-    \"\"\"\n+    elif logger is not None:\n+        logger.debug(\"flux_radius fitting not performed\")\n \n     return mask\n \n",
                            "Merge branch 'hotfix/wcsfit-latest' into 'release-13.0': [FIX]#23453 Velocity aberration",
                            "Catherine Grenet",
                            "2023-08-22T13:27:01.000+00:00",
                            "01ffa5761a241a058ab54fad1f6d060636dd3e30"
                        ]
                    ],
                    "VIS_Astrometry/python/VIS_Astrometry/wcsfit_core.py": [
                        [
                            "@@ -1546,11 +1546,9 @@ class CCDData(object):\n         \"\"\"\n         # calc the values\n         residuals = self.calc_ref_residuals(mode='total')\n-        numbrms = np.count_nonzero(self.is_ref)\n-        stdcrms = None if numbrms == 0 else rms(residuals)\n         # record the values\n-        self.header['STDCRMS'] = stdcrms\n-        self.header['NUMBRMS'] = numbrms\n+        self.header['STDCRMS'] = rms(residuals)\n+        self.header['NUMBRMS'] = np.count_nonzero(self.is_ref)\n \n     def read_hkw_2d(self, prefix):\n         \"\"\"\n@@ -2417,7 +2415,10 @@ def rms(array):\n     -------\n     Floating point RMS.\n     \"\"\"\n-    return np.sqrt(np.mean(np.asarray(array) ** 2))\n+    if len(array) == 0:\n+        return None\n+    else:\n+        return np.sqrt(np.mean(np.asarray(array) ** 2))\n \n \n def get_cd_matrix_analytic(X, Y, x, y):\n",
                            "deal with mean of empty slice warnings #23622, and cbar deprecation warnings",
                            "Leigh Smith",
                            "2023-08-24T10:56:09.000+01:00",
                            "74b0f8689ef9a58f14a1c5144fc0acd6cb39e83a"
                        ],
                        [
                            "@@ -1544,9 +1544,13 @@ class CCDData(object):\n         \"\"\"\n         Update header stats (NUMBRMS and STDCRMS) for this CCD\n         \"\"\"\n+        # calc the values\n         residuals = self.calc_ref_residuals(mode='total')\n-        self.header['STDCRMS'] = rms(residuals)\n-        self.header['NUMBRMS'] = np.count_nonzero(self.is_ref)\n+        numbrms = np.count_nonzero(self.is_ref)\n+        stdcrms = None if numbrms == 0 else rms(residuals)\n+        # record the values\n+        self.header['STDCRMS'] = stdcrms\n+        self.header['NUMBRMS'] = numbrms\n \n     def read_hkw_2d(self, prefix):\n         \"\"\"\n",
                            "deal with mean of empty slice error #23622, warnings still to do",
                            "Leigh Smith",
                            "2023-08-24T09:57:43.000+01:00",
                            "8db842d5ac1760c1f45c8f2a7c7c798ca2e26a02"
                        ],
                        [
                            "@@ -707,11 +707,15 @@ class FPAModel(object):\n             jac_sparsity=jsparse,\n             max_nfev=50  # if not enough then something is probably wrong\n         )\n-        # todo\n-        #   check on active_mask? (i.e. is solution at bounds)\n+\n+        # check on active_mask (i.e. is solution at bounds)\n+        if np.count_nonzero(solution.active_mask) != 0 and logger is not None:\n+            logger.warning(\n+                'At least one of the fpa model parameters is at the bounds'\n+            )\n \n         # warn if max_nfev was the reason for termination\n-        if solution.status == 0:\n+        if solution.status == 0 and logger is not None:\n             logger.warning(\n                 'The fitter terminated because the maximum number of '\n                 'function evaluations was reached'\n@@ -1123,6 +1127,7 @@ class Observation(object):\n \n     def __init__(\n             self, ccd_data, ref_data, pointing, position_angle, exptime,\n+            ref_epoch=None, pos_offset=None, vel_offset=None,\n             filename=None, header=None\n     ):\n         \"\"\"\n@@ -1141,6 +1146,14 @@ class Observation(object):\n             Floating point position angle of observation in decimal degrees.\n         exptime : float\n             Exposure time in seconds.\n+        ref_epoch : float, optional\n+            Reference epoch (i.e. observation epoch) in Julian years\n+        pos_offset : tuple, optional\n+            The 3-component positional offset of the observatory from the\n+            origin of the geocentric reference frame at the target_epoch in km\n+        vel_offset : tuple, optional\n+            The 3-component velocity offset of the observatory from the\n+            origin of the geocentric reference frame at the target epoch in km/s\n         filename : string, optional\n             The filename of the observation.\n         header : header, optional\n@@ -1161,6 +1174,11 @@ class Observation(object):\n         self.position_angle = position_angle\n         self.exptime = exptime\n \n+        # store additional info\n+        self.ref_epoch = ref_epoch\n+        self.pos_offset = pos_offset\n+        self.vel_offset = vel_offset\n+\n         # create an ordered dictionary of CCDData objects\n         self.ccds = OrderedDict()\n         for n, ccd in enumerate(ccd_data):\n@@ -2249,8 +2267,18 @@ class CCDData(object):\n             # return the flattened residuals\n             return self.calc_ref_residuals(mode='standardised').flatten()\n \n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(crvals[1] + 5, -90.0, 90.0)\n+\n         # run the minimizer\n-        solution = least_squares(min_func, p0, x_scale='jac', jac='3-point')\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n \n         # set the resultant values\n         _ = min_func(solution.x)\n",
                            "Merge branch 'release-13.0' into memory_requirements",
                            "James Nightingale",
                            "2023-08-22T19:48:12.000+02:00",
                            "6f4bb5791ddb1a33888248a6fb0cbd43c4b1bd55"
                        ],
                        [
                            "@@ -707,11 +707,15 @@ class FPAModel(object):\n             jac_sparsity=jsparse,\n             max_nfev=50  # if not enough then something is probably wrong\n         )\n-        # todo\n-        #   check on active_mask? (i.e. is solution at bounds)\n+\n+        # check on active_mask (i.e. is solution at bounds)\n+        if np.count_nonzero(solution.active_mask) != 0 and logger is not None:\n+            logger.warning(\n+                'At least one of the fpa model parameters is at the bounds'\n+            )\n \n         # warn if max_nfev was the reason for termination\n-        if solution.status == 0:\n+        if solution.status == 0 and logger is not None:\n             logger.warning(\n                 'The fitter terminated because the maximum number of '\n                 'function evaluations was reached'\n@@ -1123,6 +1127,7 @@ class Observation(object):\n \n     def __init__(\n             self, ccd_data, ref_data, pointing, position_angle, exptime,\n+            ref_epoch=None, pos_offset=None, vel_offset=None,\n             filename=None, header=None\n     ):\n         \"\"\"\n@@ -1141,6 +1146,14 @@ class Observation(object):\n             Floating point position angle of observation in decimal degrees.\n         exptime : float\n             Exposure time in seconds.\n+        ref_epoch : float, optional\n+            Reference epoch (i.e. observation epoch) in Julian years\n+        pos_offset : tuple, optional\n+            The 3-component positional offset of the observatory from the\n+            origin of the geocentric reference frame at the target_epoch in km\n+        vel_offset : tuple, optional\n+            The 3-component velocity offset of the observatory from the\n+            origin of the geocentric reference frame at the target epoch in km/s\n         filename : string, optional\n             The filename of the observation.\n         header : header, optional\n@@ -1161,6 +1174,11 @@ class Observation(object):\n         self.position_angle = position_angle\n         self.exptime = exptime\n \n+        # store additional info\n+        self.ref_epoch = ref_epoch\n+        self.pos_offset = pos_offset\n+        self.vel_offset = vel_offset\n+\n         # create an ordered dictionary of CCDData objects\n         self.ccds = OrderedDict()\n         for n, ccd in enumerate(ccd_data):\n@@ -2249,8 +2267,18 @@ class CCDData(object):\n             # return the flattened residuals\n             return self.calc_ref_residuals(mode='standardised').flatten()\n \n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(crvals[1] + 5, -90.0, 90.0)\n+\n         # run the minimizer\n-        solution = least_squares(min_func, p0, x_scale='jac', jac='3-point')\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n \n         # set the resultant values\n         _ = min_func(solution.x)\n",
                            "Merge branch 'release-13.0' of https://gitlab.euclid-sgs.uk/PF-VIS/VIS_Tasks into release-13.0",
                            "James Nightingale",
                            "2023-08-22T19:47:57.000+02:00",
                            "3887ab35499dbc803602db855483bfdea07cf240"
                        ],
                        [
                            "@@ -707,11 +707,15 @@ class FPAModel(object):\n             jac_sparsity=jsparse,\n             max_nfev=50  # if not enough then something is probably wrong\n         )\n-        # todo\n-        #   check on active_mask? (i.e. is solution at bounds)\n+\n+        # check on active_mask (i.e. is solution at bounds)\n+        if np.count_nonzero(solution.active_mask) != 0 and logger is not None:\n+            logger.warning(\n+                'At least one of the fpa model parameters is at the bounds'\n+            )\n \n         # warn if max_nfev was the reason for termination\n-        if solution.status == 0:\n+        if solution.status == 0 and logger is not None:\n             logger.warning(\n                 'The fitter terminated because the maximum number of '\n                 'function evaluations was reached'\n@@ -1123,6 +1127,7 @@ class Observation(object):\n \n     def __init__(\n             self, ccd_data, ref_data, pointing, position_angle, exptime,\n+            ref_epoch=None, pos_offset=None, vel_offset=None,\n             filename=None, header=None\n     ):\n         \"\"\"\n@@ -1141,6 +1146,14 @@ class Observation(object):\n             Floating point position angle of observation in decimal degrees.\n         exptime : float\n             Exposure time in seconds.\n+        ref_epoch : float, optional\n+            Reference epoch (i.e. observation epoch) in Julian years\n+        pos_offset : tuple, optional\n+            The 3-component positional offset of the observatory from the\n+            origin of the geocentric reference frame at the target_epoch in km\n+        vel_offset : tuple, optional\n+            The 3-component velocity offset of the observatory from the\n+            origin of the geocentric reference frame at the target epoch in km/s\n         filename : string, optional\n             The filename of the observation.\n         header : header, optional\n@@ -1161,6 +1174,11 @@ class Observation(object):\n         self.position_angle = position_angle\n         self.exptime = exptime\n \n+        # store additional info\n+        self.ref_epoch = ref_epoch\n+        self.pos_offset = pos_offset\n+        self.vel_offset = vel_offset\n+\n         # create an ordered dictionary of CCDData objects\n         self.ccds = OrderedDict()\n         for n, ccd in enumerate(ccd_data):\n@@ -2249,8 +2267,18 @@ class CCDData(object):\n             # return the flattened residuals\n             return self.calc_ref_residuals(mode='standardised').flatten()\n \n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(crvals[1] + 5, -90.0, 90.0)\n+\n         # run the minimizer\n-        solution = least_squares(min_func, p0, x_scale='jac', jac='3-point')\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n \n         # set the resultant values\n         _ = min_func(solution.x)\n",
                            "Merge branch 'release-13.0' into 'feature_#23227_BloomingCalib'",
                            "Thomas Flanet",
                            "2023-08-22T15:48:48.000+00:00",
                            "2d156ac3b1fd63e45e002c217d42c01a4c694040"
                        ],
                        [
                            "@@ -707,11 +707,15 @@ class FPAModel(object):\n             jac_sparsity=jsparse,\n             max_nfev=50  # if not enough then something is probably wrong\n         )\n-        # todo\n-        #   check on active_mask? (i.e. is solution at bounds)\n+\n+        # check on active_mask (i.e. is solution at bounds)\n+        if np.count_nonzero(solution.active_mask) != 0 and logger is not None:\n+            logger.warning(\n+                'At least one of the fpa model parameters is at the bounds'\n+            )\n \n         # warn if max_nfev was the reason for termination\n-        if solution.status == 0:\n+        if solution.status == 0 and logger is not None:\n             logger.warning(\n                 'The fitter terminated because the maximum number of '\n                 'function evaluations was reached'\n@@ -1123,6 +1127,7 @@ class Observation(object):\n \n     def __init__(\n             self, ccd_data, ref_data, pointing, position_angle, exptime,\n+            ref_epoch=None, pos_offset=None, vel_offset=None,\n             filename=None, header=None\n     ):\n         \"\"\"\n@@ -1141,6 +1146,14 @@ class Observation(object):\n             Floating point position angle of observation in decimal degrees.\n         exptime : float\n             Exposure time in seconds.\n+        ref_epoch : float, optional\n+            Reference epoch (i.e. observation epoch) in Julian years\n+        pos_offset : tuple, optional\n+            The 3-component positional offset of the observatory from the\n+            origin of the geocentric reference frame at the target_epoch in km\n+        vel_offset : tuple, optional\n+            The 3-component velocity offset of the observatory from the\n+            origin of the geocentric reference frame at the target epoch in km/s\n         filename : string, optional\n             The filename of the observation.\n         header : header, optional\n@@ -1161,6 +1174,11 @@ class Observation(object):\n         self.position_angle = position_angle\n         self.exptime = exptime\n \n+        # store additional info\n+        self.ref_epoch = ref_epoch\n+        self.pos_offset = pos_offset\n+        self.vel_offset = vel_offset\n+\n         # create an ordered dictionary of CCDData objects\n         self.ccds = OrderedDict()\n         for n, ccd in enumerate(ccd_data):\n@@ -2249,8 +2267,18 @@ class CCDData(object):\n             # return the flattened residuals\n             return self.calc_ref_residuals(mode='standardised').flatten()\n \n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(crvals[1] + 5, -90.0, 90.0)\n+\n         # run the minimizer\n-        solution = least_squares(min_func, p0, x_scale='jac', jac='3-point')\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n \n         # set the resultant values\n         _ = min_func(solution.x)\n",
                            "Merge branch 'release-13.0' into develop: [FIX]#23453 Velocity aberration",
                            "Catherine Grenet",
                            "2023-08-22T15:29:05.000+02:00",
                            "b222e19a905c21e2330dc9018a73bfae8e9b014f"
                        ],
                        [
                            "@@ -707,11 +707,15 @@ class FPAModel(object):\n             jac_sparsity=jsparse,\n             max_nfev=50  # if not enough then something is probably wrong\n         )\n-        # todo\n-        #   check on active_mask? (i.e. is solution at bounds)\n+\n+        # check on active_mask (i.e. is solution at bounds)\n+        if np.count_nonzero(solution.active_mask) != 0 and logger is not None:\n+            logger.warning(\n+                'At least one of the fpa model parameters is at the bounds'\n+            )\n \n         # warn if max_nfev was the reason for termination\n-        if solution.status == 0:\n+        if solution.status == 0 and logger is not None:\n             logger.warning(\n                 'The fitter terminated because the maximum number of '\n                 'function evaluations was reached'\n@@ -1123,6 +1127,7 @@ class Observation(object):\n \n     def __init__(\n             self, ccd_data, ref_data, pointing, position_angle, exptime,\n+            ref_epoch=None, pos_offset=None, vel_offset=None,\n             filename=None, header=None\n     ):\n         \"\"\"\n@@ -1141,6 +1146,14 @@ class Observation(object):\n             Floating point position angle of observation in decimal degrees.\n         exptime : float\n             Exposure time in seconds.\n+        ref_epoch : float, optional\n+            Reference epoch (i.e. observation epoch) in Julian years\n+        pos_offset : tuple, optional\n+            The 3-component positional offset of the observatory from the\n+            origin of the geocentric reference frame at the target_epoch in km\n+        vel_offset : tuple, optional\n+            The 3-component velocity offset of the observatory from the\n+            origin of the geocentric reference frame at the target epoch in km/s\n         filename : string, optional\n             The filename of the observation.\n         header : header, optional\n@@ -1161,6 +1174,11 @@ class Observation(object):\n         self.position_angle = position_angle\n         self.exptime = exptime\n \n+        # store additional info\n+        self.ref_epoch = ref_epoch\n+        self.pos_offset = pos_offset\n+        self.vel_offset = vel_offset\n+\n         # create an ordered dictionary of CCDData objects\n         self.ccds = OrderedDict()\n         for n, ccd in enumerate(ccd_data):\n@@ -2249,8 +2267,18 @@ class CCDData(object):\n             # return the flattened residuals\n             return self.calc_ref_residuals(mode='standardised').flatten()\n \n+        # set bounds\n+        ubounds = np.full_like(p0, np.inf)\n+        lbounds = np.full_like(p0, -np.inf)\n+        lbounds[0] = np.clip(crvals[0] - 5, 0.0, 360.)  # don't allow ra to go crazy\n+        ubounds[0] = np.clip(crvals[0] + 5, 0.0, 360.)\n+        lbounds[1] = np.clip(crvals[1] - 5, -90.0, 90.0)  # don't allow dec to go crazy\n+        ubounds[1] = np.clip(crvals[1] + 5, -90.0, 90.0)\n+\n         # run the minimizer\n-        solution = least_squares(min_func, p0, x_scale='jac', jac='3-point')\n+        solution = least_squares(\n+            min_func, p0, bounds=(lbounds, ubounds), x_scale='jac', jac='3-point'\n+        )\n \n         # set the resultant values\n         _ = min_func(solution.x)\n",
                            "Merge branch 'hotfix/wcsfit-latest' into 'release-13.0': [FIX]#23453 Velocity aberration",
                            "Catherine Grenet",
                            "2023-08-22T13:27:01.000+00:00",
                            "01ffa5761a241a058ab54fad1f6d060636dd3e30"
                        ]
                    ]
                },
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": [
                    {
                        "name": "13.0.14",
                        "created_at": "2023-08-22T13:27:01.000+00:00",
                        "author_name": "Catherine Grenet"
                    },
                    {
                        "name": "13.0.15",
                        "created_at": "2023-08-24T07:19:17.000+00:00",
                        "author_name": "Catherine Grenet"
                    },
                    {
                        "name": "13.0.16",
                        "created_at": "2023-08-29T14:03:14.000+00:00",
                        "author_name": "Catherine Grenet"
                    }
                ]
            },
            "PF-VIS/VIS_SatPixels": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Photometry": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_NonLinCalibration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_RemovePRNU": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_PyLibrary": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_MeasurePRNU": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_RemoveLamp": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_MeasureLamp": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_MasterBias": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_ImageTools": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Ghosts": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_CTICalibrate": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_CTICorrect": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_CTIAdd": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_CTI_from_Git": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Background": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-VIS/VIS_Astrometry": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            }
        },
        "PF-NIR": {
            "PF-NIR/NIR_Baseline": {
                "start date": "2023-08-22T10:31:15",
                "end date": "-",
                "start tag": "2.4.1",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": [
                    {
                        "name": "2.4.1",
                        "created_at": "2023-08-22T10:31:15.000+02:00",
                        "author_name": "Marco Frailis"
                    },
                    {
                        "name": "2.4.2",
                        "created_at": "2023-08-28T08:59:35.000+00:00",
                        "author_name": "Marco Frailis"
                    },
                    {
                        "name": "2.5.0",
                        "created_at": "2023-09-04T14:34:23.000+02:00",
                        "author_name": "Marco Frailis"
                    }
                ]
            },
            "PF-NIR/NIR_Documentation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_DQC_Pipeline": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_TransientDetection": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Dependencies": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_SelfCalib": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_IAL_Pipelines": {
                "start date": "2023-08-22T16:51:07",
                "end date": "2023-08-30T14:50:00",
                "start tag": "2.4.1",
                "end tag": "2.4.3",
                "count_files_modified": "11",
                "modifications_by_file": {
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/PackageDef_NIR.py": [
                        [
                            "@@ -2,7 +2,7 @@ from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingReso\n \n \n proj_version = \"\"\n-ec_wrapper = \"EuclidWrapper --forward-ial-api --ecdm_config_xpath='//ConfigurationFile[SubModuleIdentifier=\\'{}\\']/DataContainer/FileName'\"\n+ec_wrapper = 'EuclidWrapper --forward-ial-api --ecdm_config_xpath=\"//ConfigurationFile[SubModuleIdentifier=\\'{}\\']/DataContainer/FileName\"'\n \n extractLists2 = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_ExtractLists',\n                            inputs=[Input('inlist', content_type='listfile')],\n",
                            "Fixing EuclidWrapper xpath string for special escape characters",
                            "Marco Frailis",
                            "2023-08-30T11:49:19.000+00:00",
                            "b7358f10b4ba469c4a40473e3549402ec68eafed"
                        ],
                        [
                            "@@ -175,8 +175,8 @@ createScientificDpds = Executable(command=f'E-Run NIR_Utilities {proj_version} N\n # - New executables - #\n doAstromAbsCal = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_version} NIR_DoastromProgram --create-catalogs --create-scamp-plots False --create-check-plots False',\n                             inputs=[Input('listofdithers', content_type='listfile'),\n+                                    Input('listofpsflists', content_type='listfile'),                                    \n                                     Input('ref_catalog', content_type='listfile'),\n-                                    Input('listofpsflists', content_type='listfile'),\n                                     Input('presolution', content_type='listfile'),\n                                     Input('config')],\n                             outputs=[Output(\"listofcaldithers\",  content_type=\"listfile\", mime_type=\"json\"),\n@@ -194,6 +194,16 @@ doAstromSelfCal = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_ve\n                                       Output('fullcatalog', mime_type='fits')],\n                              resources=ComputingResources(cores=1, ram=3.0, walltime=3.0))\n \n+doAstromDistortion = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_version} NIR_DoastromProgram --create-distortion-model',\n+                      inputs=[Input('listofdithers', content_type='listfile'),\n+                              Input('listofpsflists', content_type='listfile'),\n+                              Input('ref_catalog', content_type='listfile'),\n+                              Input('presolution', content_type='listfile'),\n+                              Input('config')],\n+                      outputs=[Output('distortion-model'),\n+                               Output(\"listofcaldithers\",  content_type=\"listfile\", mime_type=\"json\")],\n+                      resources=ComputingResources(cores=1, ram=3.0, walltime=3.0))\n+\n makeAbsoluteCalib = Executable(command=f\"E-Run NIR_AbsolutePhotometry {proj_version} NIR_ComputeAbsolutePhotometry\",\n                              inputs=[Input(\"inlist\", content_type=\"listfile\"),\n                                      Input(\"mdbfile\"),\n@@ -203,15 +213,6 @@ makeAbsoluteCalib = Executable(command=f\"E-Run NIR_AbsolutePhotometry {proj_vers\n                             outputs=[Output(\"product\")],\n                             resources=ComputingResources(cores=1, ram=3.0, walltime=1.0))\n \n-doAstromDistortion = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_version} NIR_DoastromProgram --create-distortion-model',\n-                      inputs=[Input('listofdithers', content_type='listfile'),\n-                              Input('ref_catalog', content_type='listfile'),\n-                              Input('presolution', content_type='listfile'),\n-                              Input('config')],\n-                      outputs=[Output('distortion-model'),\n-                               Output(\"listofcaldithers\",  content_type=\"listfile\", mime_type=\"json\")],\n-                      resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n-\n makeMasterDark = Executable(command=f\"E-Run NIR_DarkBiasSubtraction {proj_version} makeNirMasterDark\",\n                           inputs=[Input(\"darklist\", content_type=\"listfile\"), Input(\"config\")],\n                           outputs=[Output(\"outfile\")],\n",
                            "Adding psfForDither to geometric distortion pipeline",
                            "Marco Frailis",
                            "2023-08-28T14:18:04.000+00:00",
                            "649aac7055e3d2ddffeba76f2efe048da565b914"
                        ],
                        [
                            "@@ -2,6 +2,7 @@ from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingReso\n \n \n proj_version = \"\"\n+ec_wrapper = \"EuclidWrapper --forward-ial-api --ecdm_config_xpath='//ConfigurationFile[SubModuleIdentifier=\\'{}\\']/DataContainer/FileName'\"\n \n extractLists2 = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_ExtractLists',\n                            inputs=[Input('inlist', content_type='listfile')],\n@@ -126,8 +127,9 @@ gsclMasking = Executable(command=f'E-Run NIR_GhostBusters {proj_version} NIR_mas\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n \n-catalogPhotDither = Executable(command=f'E-Run NIR_CatalogExtraction {proj_version} DitherCatalogExtraction',\n-                               inputs=[Input('infile'), Input('listofpsffiles')],\n+catalogPhotDither = Executable(command=f'E-Run NIR_CatalogExtraction {proj_version} {ec_wrapper} DitherCatalogExtraction',\n+                               inputs=[Input('ecdm_config_xml'), Input('infile'),\n+                                       Input('listofpsffiles')],\n                                outputs=[Output('outputcat', mime_type='fits')],\n                                resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n",
                            "Merge branch 'mfrailis/use_ecwrapper' into 'develop'",
                            "Marco Frailis",
                            "2023-08-28T08:06:21.000+00:00",
                            "52fc241e6ab8aeb1c20434aa897d39a1b924ffc5"
                        ],
                        [
                            "@@ -2,6 +2,7 @@ from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingReso\n \n \n proj_version = \"\"\n+ec_wrapper = \"EuclidWrapper --forward-ial-api --ecdm_config_xpath='//ConfigurationFile[SubModuleIdentifier=\\'{}\\']/DataContainer/FileName'\"\n \n extractLists2 = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_ExtractLists',\n                            inputs=[Input('inlist', content_type='listfile')],\n@@ -126,8 +127,9 @@ gsclMasking = Executable(command=f'E-Run NIR_GhostBusters {proj_version} NIR_mas\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n \n-catalogPhotDither = Executable(command=f'E-Run NIR_CatalogExtraction {proj_version} DitherCatalogExtraction',\n-                               inputs=[Input('infile'), Input('listofpsffiles')],\n+catalogPhotDither = Executable(command=f'E-Run NIR_CatalogExtraction {proj_version} {ec_wrapper} DitherCatalogExtraction',\n+                               inputs=[Input('ecdm_config_xml'), Input('infile'),\n+                                       Input('listofpsffiles')],\n                                outputs=[Output('outputcat', mime_type='fits')],\n                                resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n",
                            "Adding configuration file to DitherCatalogExtraction, using the EuclidWrapper",
                            "Marco Frailis",
                            "2023-08-25T20:46:12.000+02:00",
                            "51a09e8ecf44f1f1819984a93603bea6fc134c9c"
                        ],
                        [
                            "@@ -1,84 +1,87 @@\n from euclidwf.framework.taskdefs import Executable, Input, Output, ComputingResources\n \n-extractLists2 = Executable(command='E-Run NIR_Utilities  NIR_ExtractLists',\n+\n+proj_version = \"\"\n+\n+extractLists2 = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_ExtractLists',\n                            inputs=[Input('inlist', content_type='listfile')],\n                            outputs=[Output('outlist1', content_type='listfile', mime_type='json'),\n                                     Output('outlist2', content_type='listfile', mime_type='json')],\n                            resources=ComputingResources(cores=1, ram=1.0, walltime=1.0))\n \n-combineLists2 = Executable(command='E-Run NIR_Utilities  NIR_CombineLists',\n+combineLists2 = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_CombineLists',\n                            inputs=[Input('inlist1', content_type='listfile'),\n                                    Input('inlist2', content_type='listfile')],\n                            outputs=[Output('outlist', content_type='listfile', mime_type='json')],\n                            resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n \n-runInitialize = Executable(command='E-Run NIR_Init  NIR_runInitialize',\n+runInitialize = Executable(command=f'E-Run NIR_Init {proj_version} NIR_runInitialize',\n                            inputs=[Input('infile'), Input('mdbfile')],\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n # To be used only for Baseline Map pre-processing\n-runInitializeBaseline = Executable(command='E-Run NIR_Init  NIR_runInitialize --no_trim',\n+runInitializeBaseline = Executable(command=f'E-Run NIR_Init {proj_version} NIR_runInitialize --no_trim',\n                            inputs=[Input('infile'), Input('mdbfile')],\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-badPixMasking = Executable(command='E-Run NIR_BadPixelMasking  maskBadpixels',\n+badPixMasking = Executable(command=f'E-Run NIR_BadPixelMasking {proj_version} maskBadpixels',\n                            inputs=[Input('infile'), Input('xmlfile')],\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-saturation = Executable(command='E-Run NIR_NonLinearSaturation  maskSaturation',\n+saturation = Executable(command=f'E-Run NIR_NonLinearSaturation {proj_version} maskSaturation',\n                         inputs=[Input('infile'), Input('xmlfile')],\n                         outputs=[Output('outfile', mime_type='fits')],\n                         resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-nonLinearity = Executable(command='E-Run NIR_NonLinearSaturation  correctNonlinearity',\n+nonLinearity = Executable(command=f'E-Run NIR_NonLinearSaturation {proj_version} correctNonlinearity',\n                           inputs=[Input('infile'), Input('xmlfile'), Input('config')],\n                           outputs=[Output('outfile', mime_type='fits')],\n-                          resources=ComputingResources(cores=1, ram=20.0, walltime=1.0))\n+                          resources=ComputingResources(cores=1, ram=4.0, vms=20, walltime=1.0))\n \n-darkSubtract = Executable(command='E-Run NIR_DarkBiasSubtraction  darkSubtraction',\n+darkSubtract = Executable(command=f'E-Run NIR_DarkBiasSubtraction {proj_version} darkSubtraction',\n                           inputs=[Input('infile'), Input('masterdark')],\n                           outputs=[Output('outfile', mime_type='fits')],\n                           resources=ComputingResources(cores=1, ram=3.0, walltime=1.0))\n \n-crRejectionSF = Executable(command='E-Run NIR_CrRejectionSingleFrame  NIR_cr_rejection',\n+crRejectionSF = Executable(command=f'E-Run NIR_CrRejectionSingleFrame {proj_version} NIR_cr_rejection',\n                            inputs=[Input('infile'), Input('config')],\n                            outputs=[Output('output', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-smallFlatCorrect = Executable(command='E-Run NIR_FlatFieldCorrection  NIR_apply_flat_field_correction',\n+smallFlatCorrect = Executable(command=f'E-Run NIR_FlatFieldCorrection {proj_version} NIR_apply_flat_field_correction',\n                               inputs=[Input('infile'), Input('masterflat')],\n                               outputs=[Output('output', mime_type='fits')],\n                               resources=ComputingResources(cores=1, ram=3.0, walltime=1.0))\n \n-largeFlatCorrect = Executable(command='E-Run NIR_SelfCalib  NIR_Apply',\n+largeFlatCorrect = Executable(command=f'E-Run NIR_SelfCalib {proj_version} NIR_Apply',\n                               inputs=[Input('infile'), Input('flat')],\n                               outputs=[Output('outfile', mime_type='fits')],\n                               resources=ComputingResources(cores=1, ram=3.0, walltime=1.0))\n \n-backEstimate = Executable(command='E-Run NIR_BackgroundEstimation  NIR_estimate_background',\n+backEstimate = Executable(command=f'E-Run NIR_BackgroundEstimation {proj_version} NIR_estimate_background',\n                           inputs=[Input('input')],\n                           outputs=[Output('outputbkg', mime_type='fits')],\n                           resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-psfForDither = Executable(command='E-Run NIR_PointSpreadFunction NIR_PSFModelling --imagetype=dither --psfex_cpp False',\n+psfForDither = Executable(command=f'E-Run NIR_PointSpreadFunction {proj_version} NIR_PSFModelling --imagetype=dither --psfex_cpp False',\n                           inputs=[Input('listofimages', content_type='listfile'),\n                                   Input('mdbfile'), Input('cal_data'), Input('config')],\n                           outputs=[Output('listofpsflists', content_type='listfile', mime_type='json')],\n                           resources=ComputingResources(cores=1, ram=3.0, walltime=3.0))\n \n-psfForDitherAbsCal = Executable(command='E-Run NIR_PointSpreadFunction NIR_PSFModelling --imagetype=dither --psflistcontent all_dithers --psfex_cpp False',\n+psfForDitherAbsCal = Executable(command=f'E-Run NIR_PointSpreadFunction {proj_version} NIR_PSFModelling --imagetype=dither --psflistcontent all_dithers --psfex_cpp False',\n                           inputs=[\n                                 Input('listofimages', content_type='listfile'),\n                                 Input('mdbfile'),\n                                 Input('config')],\n                           outputs=[Output('listofpsflists', content_type='listfile', mime_type='json')],\n-                          resources=ComputingResources(cores=1, ram=3.0, walltime=3.0))\n+                          resources=ComputingResources(cores=1, ram=3.0, walltime=4.0))\n \n \n-doAstrom = Executable(command='E-Run NIR_AstrometricCalibration  NIR_DoastromProgram',\n+doAstrom = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_version} NIR_DoastromProgram',\n                       inputs=[Input('listofdithers', content_type='listfile'),\n                               Input('listofpsflists', content_type='listfile'),\n                               Input('ref_catalog', content_type='listfile'),\n@@ -88,23 +91,23 @@ doAstrom = Executable(command='E-Run NIR_AstrometricCalibration  NIR_DoastromPro\n                                Output('fullcatalog', mime_type='fits')],\n                       resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n \n-crRejectionMF = Executable(command='E-Run NIR_CrRejectionMultiFrame  CrRejectMulti',\n+crRejectionMF = Executable(command=f'E-Run NIR_CrRejectionMultiFrame {proj_version} CrRejectMulti',\n                            inputs=[Input('dith_file', content_type='listfile')],\n                            outputs=[Output('dith_file_out', content_type='listfile', mime_type='json')],\n                            resources=ComputingResources(cores=1, ram=4.0, walltime=3.0))\n \n-crDeflag = Executable(command='E-Run NIR_CrRejectionMultiFrame  Deflagging',\n+crDeflag = Executable(command=f'E-Run NIR_CrRejectionMultiFrame {proj_version} Deflagging',\n                            inputs=[Input('input_list', content_type='listfile')],\n                            outputs=[Output('output_list', content_type='listfile', mime_type='json')],\n                            resources=ComputingResources(cores=1, ram=4.0, walltime=2.0))\n \n-relPhotoExp = Executable(command='E-Run NIR_RelativePhotometry  NIR_calculate_relative_exposures',\n+relPhotoExp = Executable(command=f'E-Run NIR_RelativePhotometry {proj_version} NIR_calculate_relative_exposures',\n                          inputs=[Input('in_full_catalog'),\n                                  Input('in_calibration_detectors')],\n                          outputs=[Output('rel_exposure_xml')],\n                          resources=ComputingResources(cores=1, ram=6.0, walltime=4.0))\n \n-relPhotoCalib = Executable(command='E-Run NIR_RelativePhotometry  NIR_apply_relative_calibrations',\n+relPhotoCalib = Executable(command=f'E-Run NIR_RelativePhotometry {proj_version} NIR_apply_relative_calibrations',\n                            inputs=[Input('infile'),\n                                    Input('in_calibration_detectors'),\n                                    Input('in_calibration_exposure'),\n@@ -112,48 +115,48 @@ relPhotoCalib = Executable(command='E-Run NIR_RelativePhotometry  NIR_apply_rela\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-absPhotoCalib = Executable(command='E-Run NIR_AbsolutePhotometry  NIR_ApplyAbsolutePhotometry',\n+absPhotoCalib = Executable(command=f'E-Run NIR_AbsolutePhotometry {proj_version} NIR_ApplyAbsolutePhotometry',\n                            inputs=[Input('infile'), Input('cal_data')],\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-gsclMasking = Executable(command='E-Run NIR_GhostBusters  NIR_mask_gscl',\n+gsclMasking = Executable(command=f'E-Run NIR_GhostBusters {proj_version} NIR_mask_gscl',\n                            inputs=[Input('infile'), Input('mdbxml'),\n                                    Input('ref_catalog', content_type='listfile')],\n                            outputs=[Output('outfile', mime_type='fits')],\n                            resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n \n-catalogPhotDither = Executable(command='E-Run NIR_CatalogExtraction  DitherCatalogExtraction',\n+catalogPhotDither = Executable(command=f'E-Run NIR_CatalogExtraction {proj_version} DitherCatalogExtraction',\n                                inputs=[Input('infile'), Input('listofpsffiles')],\n                                outputs=[Output('outputcat', mime_type='fits')],\n                                resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-doResampling = Executable(command='E-Run NIR_Resampling  NIR_DoresamplProgram',\n+doResampling = Executable(command=f'E-Run NIR_Resampling {proj_version} NIR_DoresamplProgram',\n                           inputs=[Input('listofcaldithers', content_type='listfile'),\n                                   Input('listofbackgrounds', content_type='listfile'),\n                                   Input('config')],\n                           outputs=[Output('listofresampled', content_type='listfile', mime_type='json')],\n                           resources=ComputingResources(cores=1, ram=4.0, walltime=3.0))\n \n-doStacking = Executable(command='E-Run NIR_Stacking  NIR_DostackProgram',\n+doStacking = Executable(command=f'E-Run NIR_Stacking {proj_version} NIR_DostackProgram',\n                         inputs=[Input('listofresampled', content_type='listfile'),\n                                 Input('config')],\n                         outputs=[Output('stackedimage', content_type='listfile', mime_type='json')],\n                         resources=ComputingResources(cores=1, ram=10.0, walltime=2.0))\n \n-psfForStack = Executable(command='E-Run NIR_PointSpreadFunction  NIR_PSFModelling --imagetype=stack --psfex_cpp False',\n+psfForStack = Executable(command=f'E-Run NIR_PointSpreadFunction {proj_version} NIR_PSFModelling --imagetype=stack --psfex_cpp False',\n                          inputs=[Input('listofimages', content_type='listfile'),\n                                  Input('mdbfile'), Input('cal_data'), Input('config')],\n                          outputs=[Output('listofpsflists', content_type='listfile', mime_type='json')],\n                          resources=ComputingResources(cores=1, ram=10.0, walltime=1.0))\n \n-catalogPhotStack = Executable(command='E-Run NIR_CatalogExtraction  StackedCatalogExtraction',\n+catalogPhotStack = Executable(command=f'E-Run NIR_CatalogExtraction {proj_version} StackedCatalogExtraction',\n                               inputs=[Input('infile'),\n                                       Input('listofpsflists', content_type='listfile')],\n                               outputs=[Output('outputcat', mime_type='fits')],\n                               resources=ComputingResources(cores=1, ram=6.0, walltime=1.0))\n \n-createScientificDpds = Executable(command='E-Run NIR_Utilities  NIR_CreateScientificDpds',\n+createScientificDpds = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_CreateScientificDpds',\n                                   inputs=[Input('cal_images', content_type='listfile'),\n                                           Input('cal_background', content_type='listfile'),\n                                           Input('cal_psf', content_type='listfile'),\n@@ -168,7 +171,7 @@ createScientificDpds = Executable(command='E-Run NIR_Utilities  NIR_CreateScient\n                                   resources=ComputingResources(cores=1, ram=6.0, walltime=1.0))\n \n # - New executables - #\n-doAstromAbsCal = Executable(command='E-Run NIR_AstrometricCalibration  NIR_DoastromProgram --create-catalogs --create-scamp-plots False --create-check-plots False',\n+doAstromAbsCal = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_version} NIR_DoastromProgram --create-catalogs --create-scamp-plots False --create-check-plots False',\n                             inputs=[Input('listofdithers', content_type='listfile'),\n                                     Input('ref_catalog', content_type='listfile'),\n                                     Input('listofpsflists', content_type='listfile'),\n@@ -179,7 +182,7 @@ doAstromAbsCal = Executable(command='E-Run NIR_AstrometricCalibration  NIR_Doast\n                                      Output('fullcatalog', mime_type='fits')],\n                             resources=ComputingResources(cores=1, ram=6.0, walltime=24.0))\n \n-doAstromSelfCal = Executable(command='E-Run NIR_AstrometricCalibration  NIR_DoastromProgram --create-catalogs',\n+doAstromSelfCal = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_version} NIR_DoastromProgram --create-catalogs',\n                              inputs=[Input('listofdithers', content_type='listfile'),\n                                      Input('ref_catalog', content_type='listfile'),\n                                      Input('presolution', content_type='listfile'),\n@@ -187,9 +190,9 @@ doAstromSelfCal = Executable(command='E-Run NIR_AstrometricCalibration  NIR_Doas\n                              outputs=[Output(\"listofcaldithers\",  content_type=\"listfile\", mime_type=\"json\"),\n                                       Output(\"listofcalcatalogs\", content_type=\"listfile\", mime_type=\"json\"),\n                                       Output('fullcatalog', mime_type='fits')],\n-                             resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n+                             resources=ComputingResources(cores=1, ram=3.0, walltime=3.0))\n \n-makeAbsoluteCalib = Executable(command=\"E-Run NIR_AbsolutePhotometry  NIR_ComputeAbsolutePhotometry\",\n+makeAbsoluteCalib = Executable(command=f\"E-Run NIR_AbsolutePhotometry {proj_version} NIR_ComputeAbsolutePhotometry\",\n                              inputs=[Input(\"inlist\", content_type=\"listfile\"),\n                                      Input(\"mdbfile\"),\n                                      Input(\"indetcoeff\"),\n@@ -198,7 +201,7 @@ makeAbsoluteCalib = Executable(command=\"E-Run NIR_AbsolutePhotometry  NIR_Comput\n                             outputs=[Output(\"product\")],\n                             resources=ComputingResources(cores=1, ram=3.0, walltime=1.0))\n \n-doAstromDistortion = Executable(command='E-Run NIR_AstrometricCalibration  NIR_DoastromProgram --create-distortion-model',\n+doAstromDistortion = Executable(command=f'E-Run NIR_AstrometricCalibration {proj_version} NIR_DoastromProgram --create-distortion-model',\n                       inputs=[Input('listofdithers', content_type='listfile'),\n                               Input('ref_catalog', content_type='listfile'),\n                               Input('presolution', content_type='listfile'),\n@@ -207,12 +210,12 @@ doAstromDistortion = Executable(command='E-Run NIR_AstrometricCalibration  NIR_D\n                                Output(\"listofcaldithers\",  content_type=\"listfile\", mime_type=\"json\")],\n                       resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n \n-makeMasterDark = Executable(command=\"E-Run NIR_DarkBiasSubtraction  makeNirMasterDark\",\n+makeMasterDark = Executable(command=f\"E-Run NIR_DarkBiasSubtraction {proj_version} makeNirMasterDark\",\n                           inputs=[Input(\"darklist\", content_type=\"listfile\"), Input(\"config\")],\n                           outputs=[Output(\"outfile\")],\n                           resources=ComputingResources(cores=1, ram=10.0, walltime=8.0))\n \n-makeSmallScaleFlat = Executable(command=\"E-Run NIR_FlatFieldCorrection  NIR_construct_master_flat\",\n+makeSmallScaleFlat = Executable(command=f\"E-Run NIR_FlatFieldCorrection {proj_version} NIR_construct_master_flat\",\n                                 inputs=[Input(\"rawflatproducts\", content_type=\"listfile\"),\n                                         Input(\"calibratedrawflats\", content_type=\"listfile\"),\n                                         Input(\"config\"),\n@@ -222,69 +225,69 @@ makeSmallScaleFlat = Executable(command=\"E-Run NIR_FlatFieldCorrection  NIR_cons\n                                 outputs=[Output(\"outputfile\")],\n                                 resources=ComputingResources(cores=1, ram=13.0, walltime=2.0))\n \n-makeNirNLCoefs = Executable(command=\"E-Run NIR_NonLinearSaturation makeNLCoefs\",\n+makeNirNLCoefs = Executable(command=f\"E-Run NIR_NonLinearSaturation {proj_version} makeNLCoefs\",\n                             inputs=[Input(\"mdbfile\"), Input(\"xmllistfile\")],\n                             outputs=[Output(\"nl_coef_xml\"),\n                                      Output(\"nl_cov_xml\")],\n                             resources=ComputingResources(cores=1, ram=20.0, walltime=45.0))\n \n-selectExpos = Executable(command='E-Run NIR_Persistence selectTargetExpos',\n+selectExpos = Executable(command=f'E-Run NIR_Persistence {proj_version} selectTargetExpos',\n                          inputs=[Input('expolist', content_type='listfile'), \n                                  Input('masklist', content_type='listfile')],\n                          outputs=[Output(\"targetlist\",  content_type=\"listfile\", mime_type=\"json\")],\n                          resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n \n-persistenceMask = Executable(command='E-Run NIR_Persistence  createPersistenceImage',\n+persistenceMask = Executable(command=f'E-Run NIR_Persistence {proj_version} createPersistenceImage',\n                              inputs=[Input('infile'), Input('prevlist'), Input('mdbxml')],\n                              outputs=[Output('maskfits', mime_type='fits')],\n                              resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n \n-createPersistenceXml = Executable(command='E-Run NIR_Utilities  NIR_CreateDpds',\n+createPersistenceXml = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_CreateDpds',\n                                   inputs=[Input('persistence_mask')],\n                                   outputs=[Output('persistence_mask_xml')],\n                                   resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n \n \n-persistenceApply = Executable(command='E-Run NIR_Persistence  applyPersistenceMask',\n+persistenceApply = Executable(command=f'E-Run NIR_Persistence {proj_version} applyPersistenceMask',\n                           inputs=[Input('infile'), Input('masklist')],\n                           outputs=[Output('outfile', mime_type='fits')],\n                           resources=ComputingResources(cores=1, ram=5.0, walltime=1.0))\n \n-persistenceDiff = Executable(command='E-Run NIR_Persistence computeDiffImage',\n+persistenceDiff = Executable(command=f'E-Run NIR_Persistence {proj_version} computeDiffImage',\n                              inputs=[Input('darkfile'),\n                                      Input('persfile')],\n                              outputs=[Output('outfile', mime_type='fits')],\n                              resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n \n-makeSelfCalib = Executable(command='E-Run NIR_SelfCalib  NIR_Compute',\n+makeSelfCalib = Executable(command=f'E-Run NIR_SelfCalib {proj_version} NIR_Compute',\n                            inputs=[\n                                 Input('inlist', content_type='listfile'),\n                                 Input('inopt'),\n                                 Input('mdbfile')],\n                            outputs=[Output('outflat'), Output('outdetcoeff')],\n-                           resources=ComputingResources(cores=1, ram=4.0, walltime=1.0))\n+                           resources=ComputingResources(cores=1, ram=6.0, walltime=2.0))\n \n # - Duplicated executables - #\n-relPhotoExpSciSelfCal = Executable(command='E-Run NIR_RelativePhotometry  NIR_calculate_relative_exposures',\n+relPhotoExpSciSelfCal = Executable(command=f'E-Run NIR_RelativePhotometry {proj_version} NIR_calculate_relative_exposures',\n                                    inputs=[Input('in_full_catalog'),\n                                            Input('in_calibration_detectors')],\n                                    outputs=[Output('rel_exposure_xml')],\n                                    resources=ComputingResources(cores=1, ram=20.0, walltime=9.0))\n \n-crRejectionMFSciSelfCal = Executable(command='E-Run NIR_CrRejectionMultiFrame  CrRejectMulti',\n+crRejectionMFSciSelfCal = Executable(command=f'E-Run NIR_CrRejectionMultiFrame {proj_version} CrRejectMulti',\n                                      inputs=[Input('dith_file', content_type='listfile')],\n                                      outputs=[Output('dith_file_out', content_type='listfile', mime_type='json')],\n                                      resources=ComputingResources(cores=1, ram=5.0, walltime=6.0))\n \n \n \n-catalogPhotStackSciSelfCal = Executable(command='E-Run NIR_CatalogExtraction  StackedCatalogExtraction',\n+catalogPhotStackSciSelfCal = Executable(command=f'E-Run NIR_CatalogExtraction {proj_version} StackedCatalogExtraction',\n                                         inputs=[Input('infile'),\n                                                 Input('listofpsflists', content_type='listfile')],\n                                         outputs=[Output('outputcat', mime_type='fits')],\n                                         resources=ComputingResources(cores=1, ram=10.0, walltime=1.0))\n \n-createScientificDpdsSciSelfCal = Executable(command='E-Run NIR_Utilities  NIR_CreateScientificDpds',\n+createScientificDpdsSciSelfCal = Executable(command=f'E-Run NIR_Utilities {proj_version} NIR_CreateScientificDpds',\n                                             inputs=[Input('cal_images', content_type='listfile'),\n                                                     Input('cal_background', content_type='listfile'),\n                                                     Input('cal_psf', content_type='listfile'),\n@@ -298,12 +301,12 @@ createScientificDpdsSciSelfCal = Executable(command='E-Run NIR_Utilities  NIR_Cr\n                                                      Output('stk_catalog_xml')],\n                                             resources=ComputingResources(cores=1, ram=10.0, walltime=1.0))\n \n-combineLists1=Executable(command=\"E-Run NIR_Utilities NIR_CombineLists\",\n+combineLists1=Executable(command=f\"E-Run NIR_Utilities {proj_version} NIR_CombineLists\",\n                          inputs=[Input(\"inlist1\", content_type=\"listfile\")],\n                          outputs=[Output(\"outlist\", content_type=\"listfile\", mime_type=\"json\")],\n                          resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-combineLists4=Executable(command=\"E-Run NIR_Utilities NIR_CombineLists\",\n+combineLists4=Executable(command=f\"E-Run NIR_Utilities {proj_version} NIR_CombineLists\",\n                          inputs=[Input(\"inlist1\", content_type=\"listfile\"),\n                                  Input(\"inlist2\", content_type=\"listfile\"),\n                                  Input(\"inlist3\", content_type=\"listfile\"),\n@@ -311,18 +314,18 @@ combineLists4=Executable(command=\"E-Run NIR_Utilities NIR_CombineLists\",\n                          outputs=[Output(\"outlist\", content_type=\"listfile\", mime_type=\"json\")],\n                          resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-makeList1 = Executable(command=\"E-Run NIR_Utilities NIR_MakeList\",\n+makeList1 = Executable(command=f\"E-Run NIR_Utilities {proj_version} NIR_MakeList\",\n                        inputs=[Input(\"infile1\")],\n                        outputs=[Output(\"outlist\", content_type=\"listfile\", mime_type=\"json\")],\n                        resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-makeList2 = Executable(command=\"E-Run NIR_Utilities NIR_MakeList\",\n+makeList2 = Executable(command=f\"E-Run NIR_Utilities {proj_version} NIR_MakeList\",\n                        inputs=[Input(\"infile1\"), \n                                Input(\"infile2\")],\n                        outputs=[Output(\"outlist\", content_type=\"listfile\", mime_type=\"json\")],\n                        resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-makeList4 = Executable(command=\"E-Run NIR_Utilities NIR_MakeList\",\n+makeList4 = Executable(command=f\"E-Run NIR_Utilities {proj_version} NIR_MakeList\",\n                        inputs=[Input(\"infile1\"),\n                                Input(\"infile2\"),\n                                Input(\"infile3\"),\n@@ -330,14 +333,14 @@ makeList4 = Executable(command=\"E-Run NIR_Utilities NIR_MakeList\",\n                        outputs=[Output(\"outlist\", content_type=\"listfile\", mime_type=\"json\")],\n                        resources=ComputingResources(cores=1, ram=2.0, walltime=1.0))\n \n-makeNirBadPixelsMask = Executable(command=\"E-Run NIR_BadPixelMasking makeNirBadPixels\",\n+makeNirBadPixelsMask = Executable(command=f\"E-Run NIR_BadPixelMasking {proj_version} makeNirBadPixels\",\n                             inputs=[Input(\"mdbfile\"),\n                                     Input(\"inputdatalist\")],\n                             outputs=[Output(\"outfile\")],\n                             resources=ComputingResources(cores=1, ram=4.0, walltime=2.0)\n                            )\n \n-findDeadPixels = Executable(command=\"E-Run NIR_BadPixelMasking findDeadPixels\",\n+findDeadPixels = Executable(command=f\"E-Run NIR_BadPixelMasking {proj_version} findDeadPixels\",\n                             inputs=[Input(\"mdbfile\"),\n                                     Input(\"config\"),\n                                     Input(\"exposurelist\")],\n@@ -345,7 +348,7 @@ findDeadPixels = Executable(command=\"E-Run NIR_BadPixelMasking findDeadPixels\",\n                             resources=ComputingResources(cores=1, ram=4.0, walltime=2.0)\n                            )\n  \n-findHotPixels = Executable(command=\"E-Run NIR_BadPixelMasking findHotPixels\",\n+findHotPixels = Executable(command=f\"E-Run NIR_BadPixelMasking {proj_version} findHotPixels\",\n                             inputs=[Input(\"mdbfile\"),\n                                     Input(\"config\"),\n                                     Input(\"exposurelist\")],\n@@ -353,35 +356,35 @@ findHotPixels = Executable(command=\"E-Run NIR_BadPixelMasking findHotPixels\",\n                             resources=ComputingResources(cores=1, ram=4.0, walltime=2.0)\n                            )\n                                     \n-findZeroQEPixels = Executable(command=\"E-Run NIR_BadPixelMasking findZeroQEPixels\",\n+findZeroQEPixels = Executable(command=f\"E-Run NIR_BadPixelMasking {proj_version} findZeroQEPixels\",\n                             inputs=[Input(\"mdbfile\"),\n                                     Input(\"config\")],\n                             outputs=[Output(\"outfile\")],\n                             resources=ComputingResources(cores=1, ram=4.0, walltime=2.0)\n                            )\n                                     \n-findLowQEPixels = Executable(command=\"E-Run NIR_BadPixelMasking findLowQEPixels\",\n+findLowQEPixels = Executable(command=f\"E-Run NIR_BadPixelMasking {proj_version} findLowQEPixels\",\n                             inputs=[Input(\"mdbfile\"),\n                                     Input(\"config\")],\n                             outputs=[Output(\"outfile\")],\n                             resources=ComputingResources(cores=1, ram=4.0, walltime=2.0)\n                            )\n                                     \n-findSuperQEPixels = Executable(command=\"E-Run NIR_BadPixelMasking findSuperQEPixels\",\n+findSuperQEPixels = Executable(command=f\"E-Run NIR_BadPixelMasking {proj_version} findSuperQEPixels\",\n                             inputs=[Input(\"mdbfile\"),\n                                     Input(\"config\")],\n                             outputs=[Output(\"outfile\")],\n                             resources=ComputingResources(cores=1, ram=4.0, walltime=2.0)\n                            )\n                                     \n-findHighLowBaselinePixels = Executable(command=\"E-Run NIR_BadPixelMasking findHighLowBaselinePixels\",\n+findHighLowBaselinePixels = Executable(command=f\"E-Run NIR_BadPixelMasking {proj_version} findHighLowBaselinePixels\",\n                             inputs=[Input(\"mdbfile\"),\n                                     Input(\"config\")],\n                             outputs=[Output(\"outfile\")],\n                             resources=ComputingResources(cores=1, ram=4.0, walltime=2.0)\n                            )\n \n-makeBaseline = Executable(command=\"E-Run NIR_Baseline  makeNirBaseline\",\n+makeBaseline = Executable(command=f\"E-Run NIR_Baseline {proj_version} makeNirBaseline\",\n                           inputs=[Input(\"darklist\", content_type=\"listfile\"), Input(\"config\")],\n                           outputs=[Output(\"outfile\")],\n                           resources=ComputingResources(cores=1, ram=6.0, walltime=8.0))\n",
                            "manually fixing conflicts",
                            "Thomas Gasparetto",
                            "2023-08-25T16:26:13.000+02:00",
                            "7f4f54121de845a8eec94c6a6284c3931821454e"
                        ],
                        [
                            "@@ -39,7 +39,7 @@ saturation = Executable(command=f'E-Run NIR_NonLinearSaturation {proj_version} m\n nonLinearity = Executable(command=f'E-Run NIR_NonLinearSaturation {proj_version} correctNonlinearity',\n                           inputs=[Input('infile'), Input('xmlfile'), Input('config')],\n                           outputs=[Output('outfile', mime_type='fits')],\n-                          resources=ComputingResources(cores=1, ram=20.0, walltime=1.0))\n+                          resources=ComputingResources(cores=1, ram=4.0, vms=20, walltime=1.0))\n \n darkSubtract = Executable(command=f'E-Run NIR_DarkBiasSubtraction {proj_version} darkSubtraction',\n                           inputs=[Input('infile'), Input('masterdark')],\n",
                            "Merge branch 'patch_resources_NL' into 'develop'",
                            "Marco Frailis",
                            "2023-08-25T12:38:28.000+00:00",
                            "ff644ad6a045cd1e57a52e4a86578387666d8fe0"
                        ],
                        [
                            "@@ -39,7 +39,7 @@ saturation = Executable(command=f'E-Run NIR_NonLinearSaturation {proj_version} m\n nonLinearity = Executable(command=f'E-Run NIR_NonLinearSaturation {proj_version} correctNonlinearity',\n                           inputs=[Input('infile'), Input('xmlfile'), Input('config')],\n                           outputs=[Output('outfile', mime_type='fits')],\n-                          resources=ComputingResources(cores=1, ram=20.0, walltime=1.0))\n+                          resources=ComputingResources(cores=1, ram=4.0, vms=20, walltime=1.0))\n \n darkSubtract = Executable(command=f'E-Run NIR_DarkBiasSubtraction {proj_version} darkSubtraction',\n                           inputs=[Input('infile'), Input('masterdark')],\n",
                            "updated resources for Non lInearity task after issue 23583",
                            "Thomas Gasparetto",
                            "2023-08-25T09:36:00.000+00:00",
                            "1c384fda490a3090ccfacbf52bc55c37b95cf8d2"
                        ],
                        [
                            "@@ -211,7 +211,10 @@ doAstromDistortion = Executable(command=f'E-Run NIR_AstrometricCalibration {proj\n                       resources=ComputingResources(cores=1, ram=3.0, walltime=2.0))\n \n makeMasterDark = Executable(command=f\"E-Run NIR_DarkBiasSubtraction {proj_version} makeNirMasterDark\",\n-                          inputs=[Input(\"darklist\", content_type=\"listfile\"), Input(\"config\")],\n+                          inputs=[\n+                                Input(\"darklist\", content_type=\"listfile\"), \n+                                Input(\"config\"),\n+                                Input(\"mdbfile\")],\n                           outputs=[Output(\"outfile\")],\n                           resources=ComputingResources(cores=1, ram=10.0, walltime=8.0))\n \n",
                            "MasterDark using MDB file",
                            "Thomas Gasparetto",
                            "2023-08-24T11:38:55.000+00:00",
                            "faa487b12261b4742e3c86614496ba3e91b4da36"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_DistortionModel_Pipeline/PipDef_NIR_DistortionModel.xml": [
                        [
                            "@@ -29,7 +29,7 @@\n             <DataProductType>DpdMdbDataBase</DataProductType>\n             <InputQuerySpecPlan>mdb.Header.ProductId.LimitedString == \"UNKNOWN\"</InputQuerySpecPlan>\n             <FileFiltering>\n-                <FilesToInclude>\"EUC_NISP_Satu*\",\"EUC_NISP_SINGLE*\",\"EUC_NIR_DISTOR*\",\"NISP_Det_PSF*\",\"NISPDetectorTable*\",\"NISPDetectorSlots*\",\"EUC_NISP_NLPHOTO-*\",\"EUC_NISP_GAIN-*\"</FilesToInclude>\n+                <FilesToInclude>\"EUC_NISP_Satu*\",\"EUC_NISP_SINGLE*\",\"EUC_NIR_DISTOR*\",\"NISP_Det_PSF*\",\"NISPDetectorTable*\",\"NISPDetectorSlots*\",\"EUC_NISP_NLPHOTO-*\",\"EUC_NISP_GAIN-*\",\"EUC_FM_NIP_[JHY]_SCS*.fits\"</FilesToInclude>\n             </FileFiltering>\n             <Cardinality>\n                 <Optionality>MANDATORY</Optionality>\n",
                            "Updating MDB filtering for distortion model",
                            "Marco Frailis",
                            "2023-08-29T13:59:16.000+00:00",
                            "55561a561332df0e7a0c2e558305342b24a796da"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_DistortionModel_Pipeline/PipScript_NIR_DistortionModel.py": [
                        [
                            "@@ -2,7 +2,7 @@ from euclidwf.framework.workflow_dsl import pipeline, parallel\n \n from PackageDef_NIR import (\n         runInitialize, badPixMasking, saturation, nonLinearity, darkSubtract, backEstimate, \n-        smallFlatCorrect, extractLists2, doAstromDistortion)    \n+        smallFlatCorrect, extractLists2, psfForDitherAbsCal, doAstromDistortion)\n     \n \n @parallel(iterable='ditherExposure')\n",
                            "Fixing forgotten task in geometric distortion",
                            "Marco Frailis",
                            "2023-08-28T14:56:55.000+00:00",
                            "d2ebf751801d3f5d423ce99dfb9cde820305d3a6"
                        ],
                        [
                            "@@ -28,10 +28,15 @@ def nir_pipeline(nirConfig, dithers, mdb, masterdark, masterflat, badpixel, sour\n \n     pre_processed, background = extractLists2(inlist=tmp)\n \n+    listofpsflists_dither = psfForDitherAbsCal(listofimages=pre_processed,\n+                                               mdbfile=mdb,\n+                                               config=nirConfig)\n+\n     distortionmodel, listofcaldithers = doAstromDistortion(listofdithers=pre_processed,\n-                               ref_catalog=sourceCatalog,\n-                               presolution=presolution,\n-                               config=nirConfig)\n+                                                           listofpsflists=listofpsflists_dither,\n+                                                           ref_catalog=sourceCatalog,\n+                                                           presolution=presolution,\n+                                                           config=nirConfig)\n \n     return distortionmodel\n \n",
                            "Adding psfForDither to geometric distortion pipeline",
                            "Marco Frailis",
                            "2023-08-28T14:18:04.000+00:00",
                            "649aac7055e3d2ddffeba76f2efe048da565b914"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_AbsolutePhotometry_Pipeline/PipScript_NIR_AbsolutePhotometry.py": [
                        [
                            "@@ -31,8 +31,8 @@ def nir_pipeline(nirConfig, dithers, mdb, masterdark, masterflat, largeflat, bad\n                                          config=nirConfig)\n \n     calibDithers_preLargeScale, astrometricCatalogs, fullcatalog = doAstromAbsCal(listofdithers=largeflats,\n-                                  ref_catalog=sourceCatalog,\n                                   listofpsflists=listofpsflists_dither,\n+                                  ref_catalog=sourceCatalog,\n                                   presolution=presolution,\n                                   config=nirConfig)\n \n",
                            "Adding psfForDither to geometric distortion pipeline",
                            "Marco Frailis",
                            "2023-08-28T14:18:04.000+00:00",
                            "649aac7055e3d2ddffeba76f2efe048da565b914"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_Persistence_Pipeline/PipDef_NIR_Persistence_Model_Check.xml": [
                        [
                            "@@ -14,7 +14,7 @@\n             AND (flatinput.Data.ImgType.Technique == \"IMAGE\") \n             AND (flatinput.Data.ObservationSequence.PointingId == \"UNKNOWN\")\n             AND (flatinput.Data.ObservationSequence.Exposure == 1)\n-            AND (rawBaseline.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")</InputQuerySpecPlan>\n+            AND (flatinput.Header.ManualValidationStatus.ManualValidationStatus != \"INVALID\")</InputQuerySpecPlan>\n             <Cardinality>\n                 <Optionality>MANDATORY</Optionality>\n                 <Min>1</Min>\n",
                            "adding tests. fixing the pipdef",
                            "Thomas Gasparetto",
                            "2023-08-28T11:33:35.000+02:00",
                            "299fa7fc9923c8c90f62f2a5db59c75663575d01"
                        ]
                    ],
                    "NIR_IAL_Pipelines/tests/python/NIR_Persistence_Pipeline_Check_test.py": [
                        [
                            "@@ -0,0 +1,52 @@\n+\n+#\n+# Copyright (C) 2012-2020 Euclid Science Ground Segment\n+#\n+# This library is free software; you can redistribute it and/or modify it under\n+# the terms of the GNU Lesser General Public License as published by the Free\n+# Software Foundation; either version 3.0 of the License, or (at your option)\n+# any later version.\n+#\n+# This library is distributed in the hope that it will be useful, but WITHOUT\n+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n+# FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more\n+# details.\n+#\n+# You should have received a copy of the GNU Lesser General Public License\n+# along with this library; if not, write to the Free Software Foundation, Inc.,\n+# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n+#\n+\n+\"\"\"\n+File: tests/python/NIR_Persistence_Pipeline_Check_test.py\n+\n+Created on: 28/08/2023\n+Author: tgaspare\n+\"\"\"\n+\n+from ElementsKernel.Auxiliary import getAuxiliaryPath\n+\n+from ST_PipelineUnitTesting.PackageDefTester import PackageDefTester\n+from ST_PipelineUnitTesting.PipelineScriptTester import PipelineScriptTester\n+from ST_PipelineUnitTesting.PipelineDefTester import PipelineDefTester\n+\n+\n+persistence_package_def = getAuxiliaryPath('NIR_Pipelines/PackageDef_NIR.py')\n+persistence_pipeline_script = getAuxiliaryPath('NIR_Pipelines/NIR_Persistence_Pipeline/PipScript_NIR_Persistence_Model_Check.py')\n+persistence_pipeline_def = getAuxiliaryPath('NIR_Pipelines/NIR_Persistence_Pipeline/PipDef_NIR_Persistence_Model_Check.xml')\n+\n+\n+class TestPersistencePackageDef(PackageDefTester):\n+   package_def = persistence_package_def\n+\n+class TestPersistencePipelineScript(PipelineScriptTester):\n+   package_def = persistence_package_def\n+   pipeline_script = persistence_pipeline_script\n+\n+class TestPersistencePipelineDef(PipelineDefTester):\n+   pipeline_script = persistence_pipeline_script\n+   pipeline_def = persistence_pipeline_def\n+\n+\n+\n+\n",
                            "adding tests. fixing the pipdef",
                            "Thomas Gasparetto",
                            "2023-08-28T11:33:35.000+02:00",
                            "299fa7fc9923c8c90f62f2a5db59c75663575d01"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_ProcessField_Background_Pipeline/PipScript_NIR_ProcessField_Background.py": [
                        [
                            "@@ -23,14 +23,14 @@ def nir_calib_dither(nirConfig, ditherExposure, mdb, masterdark, masterflat, lar\n \n \n @parallel(iterable=['infile', 'listofpsffiles'])\n-def p_photoCalib(infile, rel_cal_data, rel_exposure_xml, tilesPhotomOffsets, abs_cal_data, listofpsffiles, mdbfile, ref_catalog):\n+def p_photoCalib(nirConfig, infile, rel_cal_data, rel_exposure_xml, tilesPhotomOffsets, abs_cal_data, listofpsffiles, mdbfile, ref_catalog):\n     relcalibrated = relPhotoCalib(infile=infile,\n                                   in_calibration_detectors=rel_cal_data,\n                                   in_calibration_exposure=rel_exposure_xml,\n                                   in_calibration_field=tilesPhotomOffsets)\n     abscalibrated = absPhotoCalib(infile=relcalibrated, cal_data=abs_cal_data)\n     gsclmasked = gsclMasking(infile=abscalibrated, mdbxml=mdbfile, ref_catalog=ref_catalog)\n-    catalog = catalogPhotDither(infile=gsclmasked, listofpsffiles=listofpsffiles)\n+    catalog = catalogPhotDither(ecdm_config_xml=nirConfig, infile=gsclmasked, listofpsffiles=listofpsffiles)\n     return gsclmasked, catalog\n \n \n@@ -59,7 +59,8 @@ def nir_pipeline(nirConfig, dithers, mdb, masterdark, masterflat, largeflat, bad\n     # rel_exposure_xml = relPhotoExp(in_full_catalog=fullcatalog,\n                                    # in_calibration_detectors=detectorsPhotomOffsets)\n \n-    tmp = p_photoCalib(infile=astromCalibDithers,\n+    tmp = p_photoCalib(nirConfig=nirConfig,\n+                       infile=astromCalibDithers,\n                        rel_cal_data=detectorsPhotomOffsets,\n                        rel_exposure_xml=relExposure,\n                        tilesPhotomOffsets=tilesPhotomOffsets,\n",
                            "Merge branch 'mfrailis/use_ecwrapper' into 'develop'",
                            "Marco Frailis",
                            "2023-08-28T08:06:21.000+00:00",
                            "52fc241e6ab8aeb1c20434aa897d39a1b924ffc5"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_ProcessField_Pipeline/PipScript_NIR_ProcessField.py": [
                        [
                            "@@ -22,14 +22,14 @@ def nir_calib_dither(nirConfig, ditherExposure, mdb, masterdark, masterflat, lar\n \n \n @parallel(iterable=['infile', 'listofpsffiles'])\n-def p_photoCalib(infile, rel_cal_data, rel_exposure_xml, tilesPhotomOffsets, abs_cal_data, listofpsffiles, mdbfile, ref_catalog):\n+def p_photoCalib(nirConfig, infile, rel_cal_data, rel_exposure_xml, tilesPhotomOffsets, abs_cal_data, listofpsffiles, mdbfile, ref_catalog):\n     relcalibrated = relPhotoCalib(infile=infile,\n                                   in_calibration_detectors=rel_cal_data,\n                                   in_calibration_exposure=rel_exposure_xml,\n                                   in_calibration_field=tilesPhotomOffsets)\n     abscalibrated = absPhotoCalib(infile=relcalibrated, cal_data=abs_cal_data)\n     gsclmasked = gsclMasking(infile=abscalibrated, mdbxml=mdbfile, ref_catalog=ref_catalog)\n-    catalog = catalogPhotDither(infile=gsclmasked, listofpsffiles=listofpsffiles)\n+    catalog = catalogPhotDither(ecdm_config_xml=nirConfig, infile=gsclmasked, listofpsffiles=listofpsffiles)\n     return gsclmasked, catalog\n \n \n@@ -62,7 +62,8 @@ def nir_pipeline(nirConfig, dithers, mdb, masterdark, masterflat, largeflat, bad\n     rel_exposure_xml = relPhotoExp(in_full_catalog=fullcatalog,\n                                    in_calibration_detectors=detectorsPhotomOffsets)\n \n-    tmp = p_photoCalib(infile=astromCalibDithersDeflagged,\n+    tmp = p_photoCalib(nirConfig=nirConfig,\n+                       infile=astromCalibDithersDeflagged,\n                        rel_cal_data=detectorsPhotomOffsets,\n                        rel_exposure_xml=rel_exposure_xml,\n                        tilesPhotomOffsets=tilesPhotomOffsets,\n",
                            "Merge branch 'mfrailis/use_ecwrapper' into 'develop'",
                            "Marco Frailis",
                            "2023-08-28T08:06:21.000+00:00",
                            "52fc241e6ab8aeb1c20434aa897d39a1b924ffc5"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_ProcessField_Pipeline/PipScript_NIR_ProcessFieldSelfCal.py": [
                        [
                            "@@ -22,14 +22,14 @@ def nir_calib_dither(nirConfig, ditherExposure, mdb, masterdark, masterflat, lar\n \n \n @parallel(iterable=['infile', 'listofpsffiles'])\n-def p_photoCalib(infile, rel_cal_data, rel_exposure_xml, tilesPhotomOffsets, abs_cal_data, listofpsffiles, mdbfile, ref_catalog):\n+def p_photoCalib(nirConfig, infile, rel_cal_data, rel_exposure_xml, tilesPhotomOffsets, abs_cal_data, listofpsffiles, mdbfile, ref_catalog):\n     relcalibrated = relPhotoCalib(infile=infile,\n                                   in_calibration_detectors=rel_cal_data,\n                                   in_calibration_exposure=rel_exposure_xml,\n                                   in_calibration_field=tilesPhotomOffsets)\n     abscalibrated = absPhotoCalib(infile=relcalibrated, cal_data=abs_cal_data)\n     gsclmasked = gsclMasking(infile=abscalibrated, mdbxml=mdbfile, ref_catalog=ref_catalog)\n-    catalog = catalogPhotDither(infile=gsclmasked, listofpsffiles=listofpsffiles)\n+    catalog = catalogPhotDither(ecdm_config_xml=nirConfig, infile=gsclmasked, listofpsffiles=listofpsffiles)\n     return gsclmasked, catalog\n \n \n@@ -62,7 +62,8 @@ def nir_pipeline(nirConfig, dithers, mdb, masterdark, masterflat, largeflat, bad\n     rel_exposure_xml = relPhotoExpSciSelfCal(in_full_catalog=fullcatalog,\n                                              in_calibration_detectors=detectorsPhotomOffsets)\n \n-    tmp = p_photoCalib(infile=astromCalibDithersDeflagged,\n+    tmp = p_photoCalib(nirConfig=nirConfig,\n+                       infile=astromCalibDithersDeflagged,\n                        rel_cal_data=detectorsPhotomOffsets,\n                        rel_exposure_xml=rel_exposure_xml,\n                        tilesPhotomOffsets=tilesPhotomOffsets,\n",
                            "Merge branch 'mfrailis/use_ecwrapper' into 'develop'",
                            "Marco Frailis",
                            "2023-08-28T08:06:21.000+00:00",
                            "52fc241e6ab8aeb1c20434aa897d39a1b924ffc5"
                        ]
                    ],
                    "CMakeLists.txt": [
                        [
                            "@@ -12,7 +12,7 @@ find_package(ElementsProject)\n #                         elements_project(MyProject 1.0 USE Element 3.9)\n #===============================================================================\n \n-elements_project(NIR_IAL_Pipelines 2.4\n+elements_project(NIR_IAL_Pipelines 2.5\n                  USE\n                  Elements 6.2.1\n                  ST_DataModel 9.2.0\n",
                            "Merge branch 'develop' into 'persistence_check_CB016'",
                            "Thomas Gasparetto",
                            "2023-08-25T14:28:15.000+00:00",
                            "eb033aab6c7095733d1eaae193114f5b13d3e733"
                        ]
                    ],
                    "NIR_IAL_Pipelines/auxdir/NIR_Pipelines/NIR_MasterDark_Pipeline/PipScript_NIR_MasterDark.py": [
                        [
                            "@@ -17,7 +17,7 @@ def calibration(rawDarks, mdb, badpixel, nirconfig):\n @pipeline(outputs=('outputs',))\n def main(rawDarks, mdb, badpixel, nirConfig):\n     calibrated = calibration(rawDarks=rawDarks, mdb=mdb, badpixel=badpixel, nirconfig=nirConfig)\n-    outputs = makeMasterDark(darklist=calibrated, config=nirConfig)\n+    outputs = makeMasterDark(darklist=calibrated, config=nirConfig, mdbfile=mdb)\n     return outputs\n \n \n",
                            "MasterDark using MDB file",
                            "Thomas Gasparetto",
                            "2023-08-24T11:38:55.000+00:00",
                            "faa487b12261b4742e3c86614496ba3e91b4da36"
                        ]
                    ]
                },
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": [
                    {
                        "name": "2.4.1",
                        "created_at": "2023-08-22T16:51:07.000+02:00",
                        "author_name": "Marco Frailis"
                    },
                    {
                        "name": "2.4.2",
                        "created_at": "2023-08-29T16:45:07.000+00:00",
                        "author_name": "Marco Frailis"
                    },
                    {
                        "name": "2.4.3",
                        "created_at": "2023-08-30T14:50:00.000+00:00",
                        "author_name": "Marco Frailis"
                    }
                ]
            },
            "PF-NIR/NIR_MovingObjectsMasking": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_LargeScaleFlat": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_GhostBusters": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Persistence": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Pipelines_Deprecated": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_NonLinearSaturation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_DarkBiasSubtraction": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_BadPixelMasking": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Testing": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Stacking": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Resampling": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_PointSpreadFunction": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_AbsolutePhotometry": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_SuperflatCalibration": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Utilities": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_RelativePhotometry": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_FlatFieldCorrection": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_BackgroundEstimation": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_CrRejectionSingleFrame": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_Init": {
                "start date": "2023-08-22T10:11:30",
                "end date": "-",
                "start tag": "2.4.1",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": [
                    {
                        "name": "2.4.1",
                        "created_at": "2023-08-22T10:11:30.000+02:00",
                        "author_name": "Marco Frailis"
                    },
                    {
                        "name": "2.5.0",
                        "created_at": "2023-09-04T14:34:23.000+02:00",
                        "author_name": "Marco Frailis"
                    }
                ]
            },
            "PF-NIR/NIR_AstrometricCalibration": {
                "start date": "2023-08-28T10:43:13",
                "end date": "-",
                "start tag": "2.4.1",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": [
                    {
                        "name": "2.4.1",
                        "created_at": "2023-08-28T10:43:13.000+02:00",
                        "author_name": "Marco Frailis"
                    },
                    {
                        "name": "2.5.0",
                        "created_at": "2023-09-04T14:34:23.000+02:00",
                        "author_name": "Marco Frailis"
                    }
                ]
            },
            "PF-NIR/NIR_Workflow": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_CatalogExtraction": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            },
            "PF-NIR/NIR_CrRejectionMultiFrame": {
                "start date": "-",
                "end date": "-",
                "start tag": "",
                "end tag": "",
                "count_files_modified": "0",
                "modifications_by_file": {},
                "selected_modifications": {},
                "count_selected_modifications": "0",
                "tags_in_period": []
            }
        }
    }
}